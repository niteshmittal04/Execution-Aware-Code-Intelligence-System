{"dimension": 384, "ids": ["kb:7d43e928-ded2-47a5-9f7c-8c3d0735eeb9", "kb:3673803f-652e-453d-9306-4cc0e5d843f3", "kb:9c43ce7d-5d4b-49e5-9871-9a4534720acf", "kb:1c570767-2f5a-4811-8e17-2b02a4e47fe1", "kb:3e001a2d-92fd-46cd-9eff-99f83d184f13", "kb:76550b3c-af44-4633-8d08-caaaf3112550", "kb:8331d943-8dba-47bf-84d7-494573ce6263", "kb:2f6c144b-4b68-497a-81d8-3693d23adec9", "kb:815d4d45-d6f0-4007-be22-be4473e63abb", "kb:727d2380-38e8-40b6-a1f2-68eabfc40714", "kb:5e99f4fc-3b6c-442d-988d-713cef14ec86", "kb:70efaa98-679f-4c61-85dd-b28180d9b2b1", "kb:71d7fcf6-a1b8-4d47-8f2d-cf08afb11ffd", "kb:bce33477-efd6-4bad-8e32-e19873369ae7", "kb:d6a203e6-350c-4f5b-9d9d-e77809b60d4f", "kb:bb78d29a-67d1-4c70-8284-a41cdac5a5d7", "kb:b37ff177-3fcf-46b4-b4a1-8167020de213", "kb:28660e2a-97c8-49af-8330-126727b30782", "kb:f54fd18c-4a3d-477c-a02e-e8d212bf5743", "kb:7adee526-0bee-42e6-ba48-41c9013fe864", "kb:938c0230-0b42-405a-9c08-7ed7f39070c2", "kb:7d46c0e4-01d5-4074-9b90-a9ce3f436367", "kb:d7bbe627-ef23-41cf-b23b-6c83ae0aede0", "kb:27026465-be3c-4c99-8c42-bad7fc0e8480", "kb:073cc390-ab10-4164-a434-b2c4ea60856e", "kb:8882d998-f299-4cd3-b9d9-51575dbfa4bd", "kb:4ca6a5fb-b202-4cb0-96a0-654cf14a352c", "kb:e8e66a94-ca00-49d3-88b9-b22698e1fbe1", "kb:20fd9494-66a5-4d0d-8c1b-444763d0dcf9", "kb:2e2c2f28-b2f9-4ab9-979c-318e11ce3f68", "kb:6b3f609a-791b-478c-8a5d-18d43764379a", "kb:78a8b902-923f-4862-abb2-8a82cc210861", "kb:f2a60b88-b89d-4565-b952-8bba93d1775c", "kb:796e34e4-0c37-4c9b-81d7-07f4ed854136", "kb:532002f5-f670-499f-9d07-bd87872efc96", "kb:dcbd8a8a-af77-43a5-95d8-e58b4edcf8a1", "kb:590db587-625e-4687-8dad-3d3cd0d74ec6", "kb:d8488e6a-6fb6-423b-8ab5-4e8f1ee958b9", "kb:06d5f02d-053f-41ed-8eba-1b5369eef175", "kb:01b4bcae-84ce-4ae5-8c8d-1a2f09d27ea9", "kb:a0b25496-990e-41fc-bc7a-bb0d8f9f73c4", "kb:e84459ce-ae65-4f4c-8f51-67ff4a055b71", "kb:5be5fd63-6768-4cf6-937b-21ddf2d2f293", "kb:c3d049cd-d7cf-4cd3-9c9a-9943ef41c12f", "kb:c1a92ff2-0cde-4dc1-93fd-b439e43111ee", "kb:a039419f-ae49-4c3a-8e64-118b9c9b1a52", "kb:90a9e36a-50b8-4f26-a908-6ea347a7625a", "kb:62128db7-5d89-4644-ad92-8a78068e1234", "kb:d4855603-15e0-43bb-a2da-5d9ad6879db4", "kb:9533f064-e714-485b-bc0d-3f869243fc41", "kb:e931b945-3f7e-471e-a107-52cf67d12ad1", "kb:c65271fc-4f64-45b4-823f-bd2477df0ea6", "kb:ad3fb0e8-ae80-4ef2-a33d-ada1cb2dbdd9", "kb:e39aef2c-afe2-4617-9f56-27b972268fc8", "kb:2d651606-6f39-4586-91ae-f5b2482b6d50", "kb:9e932dd2-25db-4f20-982b-5fe53a6e0122", "kb:e9294156-651d-499c-8fd7-2755235b3547", "kb:ff31d71a-33ea-4414-a3e7-cc243cc6bc28", "kb:e58ded68-d9ab-4139-b3df-f8bdc2641fe3", "kb:c66b9658-792d-46dd-8609-8dd6b79bb0b3", "kb:ba498d78-b3cf-4a85-9bbb-6931e684a788", "kb:7851db22-a1fc-4ac6-be68-ff7c0ac40cf0", "kb:bd7ca042-1abd-4433-9f22-d3c02159c2d9", "kb:da966da5-21ba-414b-b0b2-9fe51dda51fb", "kb:077ed16e-3b01-4f2a-8526-872bfcb00d4b", "kb:b89328e3-36a5-43c6-a16e-636ea3d4db62", "kb:2f45f53f-a0e4-44a5-94b9-5594dbe2c5e9", "kb:fc07bf70-3462-483d-a37a-2771e3db3f18", "kb:12a2917e-5816-41b3-8df1-66bd0877da5a", "kb:81620b22-0123-4f64-8732-c62f90827e6a", "kb:7403b976-871c-4016-9cc1-3a2225329bbe", "kb:704075e7-573c-42a3-a066-64b8a190f256", "kb:d1d42f99-aa5c-46f3-b266-a76c63d7a08f", "kb:bb236202-9931-4c82-a481-dea14bbedead", "kb:4326d01d-2f75-40de-849d-ac222b20bd6a", "kb:47fc6444-b233-45ba-a392-d581a74fd352", "kb:e1bb0026-2f96-4c86-b9c8-0a2b65fab017", "kb:d10fe05b-ceaf-4040-bda9-4089576c7cf1", "kb:885d4710-a6f7-4d22-ab63-12c33e767977", "kb:4ea0d6fc-cf89-4a94-adb3-713e98b104a6", "kb:35766141-70ed-4cb8-bfff-736f8ed7f5b0", "kb:f48e34fa-a29b-4c76-914e-2a031ab96e12", "kb:169c9468-e055-4213-90b1-f68385df366a", "kb:dd9a7f69-5a7d-4c30-8231-edb0d1afac3b", "kb:0afbce9d-6fc4-4a71-9a5a-a854dd9127fc", "kb:7eaec7ff-ab39-4507-9f69-f4a05c8a424b", "kb:852b858f-2b1c-4c0d-a175-cbff2887eb05", "kb:d557af26-28a3-45ce-99aa-888711a2fbab", "kb:4d534a57-0a5f-4299-87f3-afcc4d968e0d", "kb:736c8f5f-cc91-465c-a6fd-19ad85bb92f6", "kb:ceaf93aa-f3cd-47d7-9f4b-8c342986c9ac", "kb:0628a03d-c4fd-45b2-9404-1bd4637ff1c0", "kb:9796a1a1-3399-4c54-a137-ada1edc7126a", "kb:f5edd406-63ab-46c6-a4b3-890fe25e5b47", "kb:dc5cc6b1-788e-4235-8c5d-618d86bfde6b", "kb:fd80ac7b-665f-43e8-87cd-a1869dcacf39", "kb:99162d2a-cb22-40ec-a3df-297e998e5089", "kb:3ad14c68-c949-489c-85fd-012b94d986f7", "kb:23a3985d-0aa7-403c-8719-94d7425f62f0", "kb:59854682-75a5-4dd2-955d-787dc7e25a69", "kb:97c394f6-9858-481b-a6f6-a441455615ad", "kb:06b833dd-3caa-4888-b204-a5c98da8e197", "kb:7b6b1c36-5784-4a0e-82b3-e8a55bb5bca2", "kb:756717ac-2fc0-4378-b5bf-053a4567dc93", "kb:1a3aac30-7d9b-4f29-a7fc-f936bafd3c77", "kb:373e98bd-124b-41ba-9e51-de34299523f2", "kb:2c03853d-ea98-4404-b40e-b8605d05842d", "kb:ac757187-869b-4ad2-934c-e1b930f427d5", "kb:86f74c08-2727-487e-ba4b-bbdba43c06f9", "kb:36228d3c-aafb-4b32-8e21-7809e224524d", "kb:e0017c5a-9f51-4480-b8a8-da78c65eaf44", "kb:a6f576e7-1645-4401-bfa4-8ffefabf8f24", "kb:ea287a7f-4fdf-4e4b-a7d4-5f7dd91cbd9e", "kb:9e4f793f-0466-453e-9ae6-acad6841146b", "kb:dfbca5b7-a7d7-4de9-ad31-a414aa846590", "kb:4268d402-d595-4a2b-88c3-8a79837db961", "kb:e466f864-ebc9-44a4-bd2a-7fb8b9d53cff", "kb:190031d5-d9b8-4f97-a998-a17770886846", "kb:cf6b3fc2-f8f0-43b8-9bb9-a7e27a6b5148", "kb:e648d739-b396-41c4-8636-eaba5192bc28", "kb:2ffb96a9-c072-46e3-8a7f-7713cadad658", "kb:1f2c1c67-f5d9-4742-bbc4-c13e21d26917", "kb:6366c6a4-931a-441f-af06-0043acb5b0c6", "kb:9b4471ef-315d-44f1-98f6-91d6d1bb019f", "kb:c3d59b69-3ac4-4d62-b37f-3d4d789ce8c0", "kb:95d9a3f6-bfc1-442a-93d2-4f3ff912c385", "kb:4f762aba-b1a9-455f-8cf0-c7872ab3860d", "kb:aa0e0913-874d-47cb-9209-46f352cfe2ee", "kb:cff7d83b-5bf3-4f59-bba9-fc78e5696f9a", "kb:ec3caf73-5c41-497f-a34e-3723e9453d4a", "kb:e7047478-24d0-4d4c-a8c8-19f6cd3e7b00", "kb:b5da7911-7536-4146-9b30-5e0b9616c2dd", "kb:277e8c3f-988b-4f46-82b3-cd6ef8299ec6", "kb:115785be-d045-4427-a02f-9d064ee878ed", "kb:5dbbb342-6dab-43c9-b221-99695d41d321", "kb:4453bf1e-b374-4576-a807-279809854651", "kb:6d16a39f-dafb-4eab-b47d-f192e0345aae", "kb:ad34200c-726f-4f54-a61f-43076df4c24d", "kb:79ef6666-6342-4b08-91d0-5defb62354df", "kb:f26a2848-1c39-4b7d-9aca-8b7d02b20f5b", "kb:6d808b52-996c-45ca-8365-fbb5860ff999", "kb:69611d5e-9db3-4c4c-9458-e95467c651b1", "kb:064a6425-60d2-4601-a6d5-4593b85c98b0", "kb:6f875f67-f282-4f9f-8d90-7f43df5dbb4c", "kb:01225355-e23c-4540-89e6-519b33741d75", "kb:fa0c957f-9abd-4ffd-b77b-e4e702b4928d", "kb:704996b6-0321-4a46-8285-0c5951468e00", "kb:8b1a95ce-ac98-48b9-a2a9-55c7581b3138", "kb:9d3a026a-373e-4a1f-a91a-fe6c137d618f", "kb:295fa90c-4f66-4d4e-9c50-e808dae01306", "kb:c65c6afc-83ff-4623-b00c-9fac8e31a0f1", "kb:84fb110e-9949-4176-b151-64620b1e095f", "kb:534f606f-2269-49e8-afb9-7243984f59a1", "kb:d2e3eda8-aa7c-423e-8f8a-63c21646b432", "kb:0755c5a7-5037-435c-8bcc-11202b01108b", "kb:27fb96c6-5970-4a02-b2c0-749ec350a5aa", "kb:36139519-9a25-463e-a350-8dec574bb57e", "kb:f55ab10b-eba4-410c-8cba-0cc9f7ef7eeb", "kb:10b36a39-8ca1-4486-88ab-4279d3bf1565", "kb:bc232e1d-968d-4a96-8463-e872cfcd38df", "kb:70b879bc-7a9e-430c-903a-bb00f882914e", "kb:1fa2e98e-142d-4a7a-b502-02e7732c58e9", "kb:20f56d6c-8507-4378-9406-e776e30bb00b", "kb:0d3823d0-f083-412a-accd-2c0533995d30", "kb:57c3a341-0848-42bb-bcb3-54e0b38cb6ed", "kb:1616156a-e195-4ee5-b937-62ee88d9e4e6", "kb:01770363-bca2-41c6-a35f-2ee605776015", "kb:5a540e47-67a5-495a-b401-4600a0653e0e", "kb:633bddff-7e7f-4fd3-934e-039c4fe7a5c9", "kb:109edd02-f4ce-42bd-bccd-7f43657878d6", "kb:260743f4-8779-4fde-aecd-a6c912c296fc", "kb:0e6f7fb5-8690-4db5-8d66-2ee8ad6e149a", "kb:aa10f4e3-139b-4b68-8d33-5eee1ae0a379", "kb:abaa3a98-d860-460d-8b87-d9456df87783", "kb:841e5e8c-5993-46dc-81c4-7a234d5b3c64", "kb:5b873dcb-fcb2-4bb9-9f01-baf211dc81f8", "kb:1f331420-41c0-4fd2-9f4a-172ca5637429", "kb:ad1697ef-ff08-4b93-9b51-c94cf0bc60af", "kb:32d687d3-3358-4a56-942a-35760e1cebe2", "kb:e4d184cc-fc0e-4d6d-a7c1-b448c9dfe451", "kb:2fc08a05-fb9b-4bcd-8849-75e8f5ee52ef", "kb:85e1ef11-f0c8-408c-b38c-e2de84d60f22", "kb:86add8fd-0283-43a4-98e8-c9c6b65a73b2", "kb:adf195be-9453-4e7f-843e-fa8473ba647c", "kb:d5d1bcc7-8f43-42ad-ac18-f6724b4f22e0", "kb:422705f6-854d-4870-b129-c5828faf6ba2", "kb:5a3dc02d-b349-4144-b07d-448f4c29b454", "kb:d0d8cdc3-c5ea-419a-b084-3e2568109e15", "kb:5e7bf536-5ada-45f1-b889-6495abcc22b9", "kb:c5796659-1d45-4058-8543-be5c5524f349", "kb:d622515a-72bc-4c20-8fb2-5e78ca492104", "kb:624a866d-7f0c-4a6c-baa8-16e557e4d6d6", "kb:37a56d46-1011-4796-b288-4a0005b2850a", "kb:cba34db0-69b0-4746-a74d-da4f661a1a9c", "kb:e7d8bb8b-90db-4295-9e4f-dd4fdde8d023", "kb:01b31f53-78e6-4439-9dfa-ac45e99aa8c6", "kb:987a397b-86c9-4044-ba92-058e3dbc4985", "kb:e5f96a2d-d30e-444f-bcb5-9e5426d9a6ff", "kb:759fbcf5-c0a0-4952-963a-dce9773c899d", "kb:9a94e037-cbd9-4e33-ac0c-94f0ec72bb76", "kb:ad0ef856-8018-4e11-8110-074050bf98bc", "kb:767628da-e024-45cc-ac69-2c24085a9593", "kb:57579c67-ac1a-4d94-8bad-6e138581d0ea", "kb:7d869120-37e9-4db5-8c2e-58b866b2df17", "kb:0336c308-7214-4661-a594-f94d47416a22", "kb:214f91b6-77a2-4245-a75c-1fa3125f2e44", "kb:98c75e08-c89d-4ee9-ab43-7f65280fe3b5", "kb:62a04145-367a-4408-b28e-cafe26d0f43e", "kb:c4ca29b3-090b-473f-95f0-cf8afbc26ac7", "kb:825cdb0e-9281-4303-b060-4089665950d2", "kb:6fa7497c-f8df-4e8a-850e-f8588a42fdb6", "kb:03ee52ef-ac0a-4da3-bbc1-97667ac549af", "kb:fabe47be-aa5d-4e27-bd0e-9aa9e75ee381", "kb:156a4c2f-7191-4546-a8fa-3be0e4a3a545", "kb:683ea517-a7ab-4115-8d46-cde07a1fe04d", "kb:9d05eea0-9b49-4559-bdfc-dce07a9b6eba", "kb:cb953775-b89f-4ba4-bc1d-df5b934f14fc", "kb:1742731b-3f46-478d-ab4f-ae3c305b9acb", "kb:2ea78e60-1220-4cd7-b487-afefdf7ca380", "kb:5389eeae-d33c-491a-abf9-fcce4ba63127", "kb:94771d31-9e79-4164-8395-8bb680b614f1", "kb:e95dfdbb-6fbe-4930-a370-903674241474", "kb:374bc038-fef6-44c6-bc45-88eee2a13e45", "kb:f374a546-3254-4dc2-823a-a2ec682b66c2", "kb:e9852464-bc6a-4ca0-b858-25060d90f5a1", "kb:5fee7364-2e4f-41dc-a57a-24bc67392792", "kb:2ddeb9fb-f5d9-45d5-9107-83e1b0b63183", "kb:ea9dfe1a-1a32-45f7-9819-59737cb95777", "kb:511d02f8-d5fe-48f2-8476-1c7692ff5c04", "kb:e28ac6c8-42d3-49a7-93ab-84e0d1cc7542", "kb:705cb82f-fc84-4563-b52c-ffd25a2bf85a", "kb:c9600e52-05d8-420f-8167-bff0e41d21bf", "kb:9864d55e-8695-4ca2-a268-c4bab601b7e1", "kb:e55dab4c-c680-4d5f-85fa-18de7bf48abe", "kb:70d80e68-b2c8-4d30-bb2a-d1dcb9a2c0b9", "kb:411e198a-8dd7-4c68-94b2-bd3225c6cf62", "kb:9d095a36-20f2-482a-ab01-f694dbfad5a9", "kb:e8f9c554-f2ce-47d6-a024-b3242f05119f", "kb:fc4019f2-0b39-4271-b476-34da16810337", "kb:1bac8e53-e9ad-4be4-bafb-b1a9b2056cd0", "kb:bd59a931-8e1c-421e-a943-67c7f4fd5336", "kb:22e08b61-6acd-4a77-bcbd-824f992f64ba", "kb:bc5a6dea-cac6-4529-84bc-870f70f224e0", "kb:88c4a0fe-a7a2-4ff3-938a-8d53e7cb4a59", "kb:4156e63e-ab72-43b4-915d-eb247feb915a", "kb:9d7184dc-b8a2-44ef-98ca-8a1f7885a0fe", "kb:5f13b38c-0ba7-424f-965b-b2655a06da20", "kb:e79be205-d183-489e-84a7-91d82d603373", "kb:c77dc5cb-51a0-43a6-967c-868aa70b1294", "kb:7a5ef0f3-c74d-474e-b6cc-112e9df1f65e", "kb:06f81827-d5ad-4f29-bd5a-5034e9f6b382", "kb:3aacf486-04d0-4c0b-92ca-979e7714ac78", "kb:5e2dc0aa-91ed-4417-8b9d-8137cc628179", "kb:b6d3296b-1303-4260-876b-674d53853829", "kb:43ea60f0-f9a0-42cf-b788-23b42815a790", "kb:6cfbba59-7e33-4e1e-8b77-442824f206bf", "kb:acec6be5-0d0a-4d39-8ea5-f279f6b68d44", "kb:7c0891fd-adf6-4983-8c48-55f101c00355", "kb:279a9d23-2f32-4627-ab29-86d473c5e0b0", "kb:cfdb6bb1-0871-4c87-bd99-7ecbd55bab63", "kb:b073ca24-a0a4-46d8-a7d8-5cf7a7505c2c", "kb:cbb557a6-70c0-43dc-bef1-3dc71f3dbb1a", "kb:4aaea669-bfa0-4c75-a209-101d6fa8aee0", "kb:fed15f29-2177-4945-87a9-11291564d8a3", "kb:4eb2050e-2c96-4785-9430-134f37f20096", "kb:458cd673-428f-4dc6-b0b8-dd3b39105575", "kb:c8cc9c2e-3628-4ff3-b930-0e47758f4203", "kb:c4651ff8-a452-480f-a04c-6a442bfcbe3b", "kb:e0192a36-17e0-434e-baee-2cfd583be8d8", "kb:1f218e1e-8265-45d7-8686-65ef21834920", "kb:ee539032-6b5f-4bc8-b855-9027d8755f9e", "kb:718073f4-6070-460d-a5f2-e5e3a516c107", "kb:e2ae6609-491e-4f80-a546-e5fb2ceab2d4", "kb:eced8047-9c83-4e9a-bfab-9582e999abdb", "kb:e4bc9e33-6ba9-4f1f-a695-a2b9f155756d", "kb:d5f5ddd3-3b15-402a-9770-efb76535ba6d", "kb:42d59678-6ff2-4404-ad30-37f2be598cf8", "kb:42424893-77bb-4439-9673-0d67624e098f", "kb:dc23103f-6f43-4bd7-bc5e-ce093950df21", "kb:cdc6a135-0597-4ada-b086-20f6671f6f33", "kb:85e50969-d5b9-476c-9787-0c7f212c07c1", "kb:7bca433f-f049-4882-ba3a-50824c7186a5", "kb:6179cd88-c9c4-465e-9c78-dfc9022c1733", "kb:fe4f322d-34b5-4a9f-b5f5-320ff224da5b", "kb:d08ad5e7-f37f-4ae4-aeca-a57045f967e9", "kb:ac86da7d-032c-4659-a993-936393d7a1fb", "kb:00a88d9b-a279-4266-bbc8-2b7fee44c520", "kb:cb195781-c89e-4758-b0d2-c09aa58de832", "kb:444ce7b8-8744-4618-a7a8-bb8e4364b093", "kb:e7b2ab3c-9465-43bd-92d7-876a7105435e", "kb:fa434176-1d15-427c-8ef3-0123ddb010eb", "kb:c88ebec2-aca1-4f0c-8042-16ab319f5919", "kb:9a01aa2a-df53-4c95-8a32-e2ff817c0a7b", "kb:2fe4282b-79f6-4b37-b369-99b5dffdf2b9", "kb:92b380dd-981d-4ff6-b418-3647c920e09f", "kb:82087894-6123-475b-ab74-3dbe23b79e85", "kb:24af0c29-4b87-4e7a-8d94-be34d8b0d0ef", "kb:6c306988-1a9a-4676-b268-8318dd16553a", "kb:54571af3-9310-427d-a215-c27d2099d459", "kb:53b35be1-0e26-4c89-9dea-6c4d70a6db7f", "kb:f1968080-5de3-46ef-b639-1eea3e1f5452", "kb:f30039ac-3ca2-4839-8205-f7bc8c633283", "kb:7c16f7e6-4ffd-4328-b54c-16d8f97de9c6", "kb:79a6428f-8934-4c5e-b012-323b80e3c5a8", "kb:568d4954-8ec7-4bd3-b016-6abbac622124", "kb:0ad78bc6-5f2e-4bf8-a486-1e85d3d79333", "kb:2f8414d9-fd05-4fb6-9be3-092f5ce8471c", "kb:5fb585c8-8767-4435-883b-109d86c1de36", "kb:b1fd4cd3-2fe1-4bd6-aba9-9f75660e56df", "kb:a18b5fc5-efbd-4147-956c-134902546ef0", "kb:270f8a98-0192-455b-871f-c22a4c886dce", "kb:dae90fbd-954e-4f2f-94bb-7ba09b6eda19", "kb:8b924b76-7f31-4452-9df7-e4fbbdb750f3", "kb:7ed4ecde-1e44-4e8c-928c-33670a8e3e69", "kb:7ba69af9-baf0-413b-9161-ca339f401265", "kb:6c78cabb-77a0-42e4-a67f-83bb37abe274", "kb:b4eca5bd-3474-4d7e-b22b-461308606d62", "kb:c5767e47-76e8-4fca-8daa-645c20028b49", "kb:baf6a521-3d2b-49fd-8c32-2f37d35d69fd", "kb:05d38304-7d0c-400b-9b13-8d4cab200bbd", "kb:9e652b1a-240d-40a2-b7e8-957bdf78eb03", "kb:300eed94-19bb-46bd-9538-6fe24bfc695e", "kb:9482c227-0776-46bf-bb9f-187c6004b22f", "kb:2d330121-29be-4af7-aa74-c4b2ef8817a3", "kb:2220ff68-392f-4923-97c5-6464d2c6f8cd", "kb:e59cfb01-341a-49c3-9892-dbfd7c62af7d", "kb:1f4c34ed-fe19-40fc-a148-f7f64d7096ef", "kb:1780de86-7d17-4722-8dc1-4ebe79c4c52f", "kb:94585900-20d6-4af3-ba35-de071695e35d", "kb:1a6aba4c-7581-4467-985b-8fcf88d0d395", "kb:aff533d3-368f-4e33-a857-9eaee89fdf17", "kb:37b125f0-860a-4c82-9d39-5cd9900580ba", "kb:51f93e99-00ce-4fbb-a134-a8162759f8c0", "kb:5d9070ab-89d6-4a09-9651-03e2867e2f5e", "kb:f64a3471-e379-4276-b09f-2d38bdc7fd49", "kb:15994367-e764-48e1-9ab1-e705064fb4db", "kb:acf54ac1-a885-44c2-9654-9be9b9cc3f6a", "kb:0c04f5b2-70d7-449b-9765-369567507f8b", "kb:634a1bd6-ade2-421a-bb64-afd7bc6c8295", "kb:16c0e347-e666-4a3d-8b3d-2830eb4dc337", "kb:1ec00635-1f97-49ec-a87c-a3fca3e48823", "kb:3ec29d23-034c-4373-b23a-8c77ae8fb0bf", "kb:40d9bc83-2a6c-48a6-ac07-f4eca89676f8", "kb:88a93267-cde5-4c42-92f2-56389b5c0f9e", "kb:4497af6c-a02c-4dbd-9e4d-fe9d5d0e06ce", "kb:f576d553-989b-4097-a3cd-7c2278fb2669", "kb:96c37d08-22ff-41ed-8825-fd0eefaf6360", "kb:02f50f29-89f9-4957-a7df-8b6f4dccc50b", "kb:b2400fc3-76f6-4704-adcf-5fba52c063c0", "kb:bad8970c-0c2b-4d2c-9829-dbe0ce0438ad", "kb:a5a7f873-62f1-474f-b938-463fbf945ef4", "kb:38799911-32c3-4bd7-9baf-d138bd538322", "kb:9b7820a6-3923-4081-ad29-f537ea12ac74", "kb:cc1e4b66-c778-4bab-9c65-1700db66e4b5", "kb:5050fa50-175a-427a-ac81-db89b158ad0b", "kb:b6e7b06c-8709-4878-b7ba-bbcfbd9dc994", "kb:e6220e3e-9c10-4655-addd-d90e1fc87570", "kb:8e092a24-0501-4720-8714-fe2799756f1f", "kb:20c43b14-ce57-4bd3-a92a-f54eeae50530", "kb:63400391-8b82-433c-9a8c-3a7c699ab454", "kb:2c6a55c4-451a-4638-acbd-83c4326ded0f", "kb:a92277aa-9ee9-4fdd-937e-9da8f7b73208", "kb:b37d3a2a-e6b8-40f9-9c72-2409eecf91d7", "kb:3b0b9cba-3b53-44d1-ae1c-e6d79efad2b5", "kb:5cad7d1e-18f7-4c02-b505-90726720c9e3", "kb:cca621c4-c5d7-4df2-a068-a41513162c4c", "kb:7b7327c5-f2dd-4db2-9167-a553a562351d", "kb:9b4c1039-94bb-4919-bf34-d07b2a54439b", "kb:fa5d7323-c258-4c91-a861-96104f03d838", "kb:66e556fa-a0a1-464f-a6f4-20b4e129c9ec", "kb:cb1f3f90-f0b3-48c8-8c4f-d5492e1055dc", "kb:d7faab7f-3280-4954-8e0b-21e2f24e1d62", "kb:8130c8d2-8b73-4e7c-bc25-1128faee5996", "kb:c622b562-7ddf-4c84-8539-dc014e9da1e4", "kb:6b4b17e8-6386-4055-9bb6-a1e72a6934b3", "kb:ce3726a9-66bb-4ac7-8923-73f8f7ccb96e", "kb:7c79f4f3-13a0-402e-986c-fbd6d7619809", "kb:a0def889-3002-42c3-85b5-191496039f7b", "kb:9d62678e-54a7-4427-8bda-6ba66d1a94bd", "kb:098d9d47-d2a7-4b5c-b1c3-2f055d13b7b2", "kb:4b519eb6-677c-4193-bc40-c1be1b86a910", "kb:dc68a09a-aaa3-4ed1-ac86-12e06b73f18f", "kb:1c76a9e1-6357-4989-9050-0ac76e20ebdd", "kb:18fb5e13-2c9f-4cb2-852c-bd04fe14e877", "kb:7563d01c-c6f6-4f81-9aae-69fc929169d6", "kb:461d03c6-f343-4cf4-9959-4662492ed0a4", "kb:ad009653-07d6-464d-826b-01eb2aad1e14", "kb:61699cf2-99c3-4112-a9a6-9bdeb818ccbb", "kb:08cd2ee4-f668-403f-b9e7-3c8133ff801a", "kb:611979f2-0e88-4f37-a674-cffa45ec9fa9", "kb:9919ba82-cd91-4f29-a759-451e37ad806e", "kb:7a3f54c9-8888-4592-9b59-34ffbec73e28", "kb:c48262e5-f9f4-42cf-8e05-69d1e9c32bff", "kb:7132e6d4-daa1-49a8-a578-b6c3ec35f4f5", "kb:0c0f3342-d1ce-4103-aaed-fff670c5a44b", "kb:93294c46-0f28-43c1-bc24-1345643af66d", "kb:64ed976a-8230-4fe3-8d85-62940fe6a52a", "kb:f2db3151-c16b-4474-9148-0397479201a3", "kb:95d57332-3e25-4350-adf7-3c73505abffe", "kb:475f871a-72d4-4a99-b3ff-9663e1e20435", "kb:c92835d9-c9bc-4a4f-823c-2517e8e951f0", "kb:fdaffda1-58f8-440d-9abd-51b417b245c6", "kb:e495844e-5b0c-484b-9a93-674b1a02692a", "kb:f82d3dc9-3acc-4264-bc08-f09e7d80d76c", "kb:10a70b2d-e915-4b8d-9b28-45f77f671cd6", "kb:2d7e50cb-ce4e-4400-bd85-e8cb3668ffb4", "kb:eb306a79-4405-4450-946c-70d60054e426", "kb:c3696374-a7e8-4813-8abd-4e3e92a06328", "kb:9a9867e3-7841-4f74-921c-69f152c660d8", "kb:66fe7fc2-a6c6-4a8d-8015-a38c8356d8b3", "kb:b3d02ee4-32c4-45ba-8147-4cd66c2e6779", "kb:0f5b7309-553b-49f5-9649-5ac0a00e7389", "kb:8729e38d-dbd3-4ed6-afee-5ec16dbdd981", "kb:46ab343a-daff-4056-bf11-e34b0e3f061f", "kb:4e5c2fbe-0ce6-41d5-955c-d0e76ee0c007", "kb:34bc285c-db76-49ba-940f-dac72b6c6f55", "kb:63fa51b9-c2a3-467a-9f80-f5a0d9654bba", "kb:6bd3d061-afe5-4e77-a969-85efec648232", "kb:21090ed8-e00c-4abe-93a0-aa64bb237c96", "kb:7d5d281a-6a3e-428c-afdc-27659a924795", "kb:f413e23b-fdda-4934-a22d-53b2d605837a", "kb:7d6f396d-a683-4bb7-bb29-c75b42258c45", "kb:0af717e5-b0df-4461-8fea-97161c9af3f1", "kb:98328fad-c1a2-4ecd-be0b-e60f3ba9a619", "kb:3b6f1cd9-3469-4a29-af56-e991ff2d0e97", "kb:bdefb5c3-d430-4b02-bd23-d34656620fff", "kb:314706bf-4156-4262-bed2-8936fb9f4c58", "kb:09fffb2a-8355-40db-90db-cb827a48d187", "kb:ace783e4-5631-4521-a55c-0a55ab94b82a", "kb:25a1f393-ad92-44aa-b7c4-e01fce040aad", "kb:c9d69902-0962-4a09-b3ab-f5f9a964472b", "kb:bc3aabc0-2e74-4fbe-b408-9abc89aad8a0", "kb:60b38591-4152-4b38-a0ca-bffc53400bbf", "kb:43cdb84c-cb08-4378-bc79-7c5f6a0545a2", "kb:d87f0023-0bcd-4239-b6be-457a2d1dc7f1", "kb:65546b72-44ef-4b6c-9569-63db4c865849", "kb:bbe9225e-81be-402b-a3b1-7bd0e8033872", "kb:04aaf556-4fb2-4ddb-9da3-7751f5246b8e", "kb:7357df0c-433c-43af-85c3-23b3272f2f47", "kb:45bfdfa7-8b89-49ae-81a5-1d83bf0d2098", "kb:78a40e0a-6f78-4e9d-89d9-79c49ffac98a", "kb:f1573f39-4762-4e6d-a638-900cdd9874be", "kb:7db09454-1d13-4ead-8acf-050a6a0204e4", "kb:ffed51be-2c1c-46a3-b26e-ddec6c52f3f6", "kb:8511bb58-973f-4396-ba27-b7c8321d364c", "kb:3f79a80c-ceaf-4dbb-bde7-ecf3560863c9", "kb:6791cb23-f811-487d-81f7-cf44f6ef746a", "kb:446e1ef3-c6ea-43cb-a460-2220d90b0d76", "kb:28ba3412-f27c-41fd-b788-56e21d223142", "kb:1c390b34-3ce0-4697-96ee-3f6f7d1984db", "kb:dda118ad-7023-4673-a522-48b314224093", "kb:e7a46b3c-ac82-421d-80de-dff515ae64de", "kb:5dd859da-9b08-4a8e-8cbb-ae082e81ab30", "kb:fd4693a1-6bf0-4739-b549-fcf006cd8829", "kb:cf8b5954-a8bd-410c-bceb-ede7926cb4ff", "kb:29246401-885b-40c1-8fa0-aeb8e6f23df7", "kb:383da64b-ca49-40f1-947e-5b2e33862467", "kb:a76253f7-f18c-413d-9687-c553b8590970", "kb:2dd047d6-4bcc-455f-9262-5d2406b02de9", "kb:fc419228-9073-4927-ad57-4f6932b3b5d4", "kb:d0724283-d79a-4b24-b369-9bc2eaca1f52", "kb:18ba3f12-9f8e-40d1-b196-7092581f88b3", "kb:e79e0353-ef07-46ad-b07e-d2a37fff27fc", "kb:d84bdecb-ef67-4a96-9ec2-c761820433e1", "kb:52cf6626-153f-4034-9933-a1a3039b963f", "kb:54b2033b-7f8b-4a29-89dd-62fb617ea541", "kb:a5b9937e-3118-4866-a934-3fb134df1538", "kb:23f3ec6b-60a2-4991-a77c-14f2e24cd069", "kb:8326d809-38a7-4f8f-b26e-f0f4da889871", "kb:f94fe265-bb6d-4237-968c-19bb891b37af", "kb:2064dc8f-c507-4e59-93ab-c932ac1bd517", "kb:bcb00ac7-d64b-44cf-a0f6-4b4e42f70b1d", "kb:9187e694-1a89-49de-80d6-62e37db4f0a3", "kb:21eead2b-1b23-4a88-bc72-9c09d2863337", "kb:9d32051f-b4c2-4363-91d1-2b923e3b9d34", "kb:8a9401c8-aae5-4895-9f21-7ca4f1a33a8c", "kb:31015da2-9239-47d5-8fc2-2e1d5265e162", "kb:cf380039-7796-453d-b939-b017f5c83414", "kb:66a2d577-7280-4f81-8ca8-941f2a7f8c56", "kb:ad6e24e5-9519-47fe-bb1a-56d21f8b8e20", "kb:5b9973dd-9a5b-40a1-b133-16ca1107923b", "kb:429551ed-94fc-42c0-a1fb-a94d63e4a6d2", "kb:ed2880c3-63c6-402b-80f4-47a7db2d4a31", "kb:0faa2a30-4a58-491e-805e-6ed2e50ae711", "kb:ec13a5b3-8751-4a9b-aba7-f04087b33257", "kb:1b083587-ec99-495c-9aa1-3763bc51454a", "kb:957c3d9c-d2cc-49b3-8ecd-2ce887376b1f", "kb:1de3b330-f83b-4086-8d81-813e35ae7601", "kb:24d9de5d-9406-49b0-9851-35d70a207e21", "kb:ef607481-fe65-4985-bbfa-d184a61e4deb", "kb:233e4b55-3909-4867-9e15-4d9633dffde7", "kb:3eca92dd-a76b-4584-8b39-4f5554234ff5", "kb:9c2389a0-8eb1-4b66-8f9f-e5d9706727f3", "kb:8a792c16-1b89-4fd4-bca7-3b04b43404a6", "kb:24dd9eec-42b1-43da-b024-5fe8c4046c38", "kb:b89a0a0e-d608-4e9c-85f6-e3ad4a7bb77c", "kb:f804f050-03dc-495a-a7e4-67ca4844ff70", "kb:7afeb46e-ebd5-4a11-8a8c-b2d053a4af33", "kb:2667407a-b204-4d5e-926b-29dd99ebedea", "kb:5219ea63-67ea-4388-a5fc-181c7e535e14", "kb:79fab260-2663-46f0-949c-cc49953ef9ec", "kb:cad72ad6-522a-4b97-bed2-9f26e92fb0d1", "kb:1e4fba8a-c68b-4617-a365-00641f9b89a7", "kb:e26da5c0-c346-4ddf-aeef-c304e3b9e882", "kb:c922a2cb-e78e-422c-bf85-681c622f0a49", "kb:f4a32c46-d27e-483c-a73b-5fd9f231f78a", "kb:e6dc0db7-7679-40bb-b0f5-d2414a1eae72", "kb:be30b671-e872-4533-9d69-e5be79f903ec", "kb:47650045-f5ad-4a75-84d5-c2473380729d", "kb:d68cdc0e-e1c2-415c-ae35-79751a57fd16", "kb:2345961c-833c-4c09-937a-aa898e7454cd", "kb:f862b51c-0489-4d22-a21b-1ba830dd5dec", "kb:10297186-ec55-48d4-a92d-669c24022ea2", "kb:2f5663d3-fb1a-4101-886d-cb2147df6602", "kb:cebe5a9d-0466-41a3-9c05-90298525e0c1", "kb:0f7cad67-3f2d-4874-af16-f20846a0a189", "kb:2885d037-cd7a-49b0-a15f-8ada6a7c6fbc", "kb:2cf88f9b-3be8-4a46-8aba-537c2ee243ce", "kb:4f9abcad-603a-4213-aae5-68979f9e38d0", "kb:91ccffe1-4c3c-45fd-b57b-bc03585a4d15", "kb:55bf85ed-6515-498d-b094-abdef69858c8", "kb:f49ea47c-1507-4873-a846-f85d29bb8e41", "kb:e054d6f7-7d51-40a9-b4d0-504bd152894c", "kb:3a7deab8-f378-40fa-8781-585a9b73fc3b", "kb:e85b0df2-889e-443a-a6dc-0b564e0dbb0b", "kb:45819d80-554a-456e-857a-8b86b157f743", "kb:18818645-b503-4f99-9960-108d10ace2e1", "kb:a5a298d3-8f06-492c-a5a7-fe079b73e42a", "kb:1b8d9360-2a34-4c73-a849-42a144b47105", "kb:edc6116c-1a7e-473b-a0df-f201696d874e", "kb:49ab22a3-4388-4d2a-957d-4872e823acbc", "kb:cd921ae4-d41a-4548-a81b-9996ed33673e", "kb:93f9b3fb-ff7b-4bd1-9de9-23d2b4f34f47", "kb:ceea941e-3b2b-4bea-ae5a-a94dd70d8faf", "kb:fe3f03ee-ef63-4f61-8299-98b579721f2c", "kb:892d79a0-ae6c-43e6-823d-49e009cc52e8", "kb:2085409b-ca37-4801-8edb-5ea7462bcc98", "kb:1ed5e31b-091b-4a44-81f3-1db7f95fe42c", "kb:fa3de3e4-fd89-4ef0-8428-1b9b138f323c", "kb:4c857a6d-05fe-4f86-a164-3ea60f8c79bd", "kb:6ac4cd0b-7a27-454f-94da-d76d99c05e64", "kb:139417e1-9870-4cc9-9f2c-7f811c20ec44", "kb:9ef1296b-c35e-4751-b535-44b5352637c4", "kb:d505e4ce-af5f-4b63-b30a-048688f85a57", "kb:103149b2-06b3-4038-92a0-79cf3092832a", "kb:3a9a8268-9e23-46ff-ab6d-6aef83f30e37", "kb:ced6d73b-1f4a-428f-bc55-bc8cdc5884df", "kb:6e096eb0-8eaf-4738-948d-749bb5860a7b", "kb:17366ac4-e609-4112-a483-ea449e841090", "kb:fe53c406-d099-417d-afd9-8c3059efdee7", "kb:bfd8e501-ac8f-46a0-8de3-f9df49e85ff5", "kb:bb2d3bde-2e75-4cf2-9910-7b8eab230165", "kb:131a74e0-8237-4008-8fac-1c3a28835198", "kb:e746f31d-e0f8-40f2-b79f-c350a228019e", "kb:04ec9286-9911-4652-92f5-fc366ae6ec49", "kb:e41748e2-b5d2-4b12-8a19-3a3e3cb33ddb", "kb:47cedec7-a19d-4c3c-aeef-91ddb65ac55b", "kb:bd270098-64b4-4aa4-a263-5b127400e32b", "kb:bd16f02c-3d57-4825-8a13-76f46ba0c897", "kb:ea2ec08d-8dca-4f62-b309-defc7ec29aa8", "kb:b7d98b9b-8647-4f40-b856-e9c83e41f813", "kb:3007bcc5-793f-45fd-bc81-8a920a230068", "kb:8785e171-7c5c-4fc0-8315-265943a60620", "kb:bcf8ff32-3156-4c32-86d4-5cbee7f7dcff", "kb:61fed399-750c-416c-8fea-809b3303d23f", "kb:42c290f2-e157-444d-81bb-96677621b07a", "kb:060d6e36-4677-4ffa-a7ed-c1fb7288ca2f", "kb:b1df1ac1-fdcc-4f9b-821a-9e196187dde4", "kb:97672db3-472e-4e79-a265-065f0fd88d9a", "kb:ab841654-7f00-4774-861e-56ed24c8027d", "kb:b7c67a36-01a9-421c-bf30-f234a34b5766", "kb:a92a1bb3-0b37-4914-83b4-0d16f5d2a97b", "kb:6f0793fc-468c-4673-9a81-1fef696157b1", "kb:3aa085ce-8dd6-4f09-a50f-f8178b6c9ea6", "kb:54961e66-94b1-4149-87ac-4b1aa624dc41", "kb:0d7c5c67-c9cb-48d9-bc4a-21cef8cfdeaf", "kb:09750392-acb8-4d2c-bd75-b33d21ecda06", "kb:61b5cba1-ebdf-4291-abc9-346f550b7e03", "kb:8b8d1eb7-e653-4468-8342-1d980df2fb7c", "kb:1cf0f1cb-a229-4397-b12a-5673ea4a367f", "kb:8abe2990-666e-402a-9893-2b5e64d73e79", "kb:69b653f5-88b2-4c72-b9af-6f6ec96d138b", "kb:d8703af0-be43-48fa-9741-90973cf8028a", "kb:b50697ae-7fea-4a77-88bd-3dc7b77ec773", "kb:71a24618-ad24-47da-8ec0-b1b84c55e380", "kb:29300bb5-0e46-427b-abe9-4407a8cd8d57", "kb:4d0986b1-a2bb-4045-9249-87b5f26a05f7", "kb:f2c6abfe-735a-4896-98a4-7c7457e52474", "kb:64e76e20-0cbf-4c07-ba35-e1ac6a1fdedb", "kb:c2696633-5bf4-4a7e-9f3a-f3760b8470e0", "kb:7f1b0972-6c6a-4a4c-8bdc-519b59e7ff76", "kb:0e336493-b495-4788-8619-59ed1c5ca1e1", "kb:76aadb21-cbc8-4136-a78f-0bded6f10679", "kb:6d7bf2e6-e1d1-4f01-ad71-d68fa031e45a", "kb:339ee492-20c7-4f63-96fd-5271b24cc977", "kb:981a525d-48ac-4f01-9413-65ee2bdb2f1a", "kb:e536dbf5-901a-41eb-8418-85776ab76ef0", "kb:718f4a82-43bd-46d0-9b76-5f8bdc8e61a0", "kb:52058554-411c-4ed6-8d67-4cc0e860e362", "kb:0bc30cf8-6684-461f-b560-530b63919629", "kb:da146b0c-402f-4667-ac31-e4215b423f2d", "kb:9becb431-7255-4777-a5e7-b65e56021a4c", "kb:73b119bf-0e48-4640-9411-748c031650f3", "kb:94ea3126-46ab-4fd4-abbe-9fe975ec2947", "kb:4168ffbc-49bd-4c3e-b86d-5509b590f546", "kb:23a5bcd3-c281-4479-8772-dc4cc6b7d302", "kb:f4cfe2d2-b9ab-4219-9607-5342ab8afc69", "kb:c34c0454-109b-4e15-bd7d-0b1a7df40f9b", "kb:c299e4e5-9ca5-4d8b-bce1-f51c421c7410", "kb:e6c0fc9f-8351-49f9-a150-c875c756416f", "kb:5dec7d56-e83b-400f-9714-4fb804f33969", "kb:6d195762-3a16-48c4-9af8-9c438c94e2d1", "kb:e3e42765-9aaa-4e13-a86a-0dcde8aa047b", "kb:6b85932f-8883-47dd-8c71-81d5ed2d2785", "kb:0833662c-5fe4-43aa-8604-5f41f303eaa8", "kb:a9f58aa6-7314-475f-8734-17c0add000d8", "kb:bd04811b-428e-490b-9a82-8e43d4f36db3", "kb:55107fda-4298-4f79-a65e-3d0428e424f0", "kb:d54d494c-faea-402f-8626-4b996e8929f9", "kb:b1b8d36f-2d80-440c-9a33-d428b2cff0d9", "kb:99723715-1dc8-4465-9b94-d5895aa35d73", "kb:ce692790-e421-4d5e-b922-ba5fcf1cefdb", "kb:57c8596b-3096-468e-b4ae-7f99137405ac", "kb:60f2caee-0ef3-4aeb-a0a5-78a46ec9c9f3", "kb:182ebafa-bfa5-4706-963e-6a5a596e13d6", "kb:44d1ce57-7910-45f0-a696-53102f278453", "kb:c4c4d1a0-4af2-4dfd-b43a-76c75e99a490", "kb:4d2530c8-9dcf-4d3c-9317-d5c58affa08e", "kb:cfffc6f1-3e83-4c26-b36b-db55e9637e20", "kb:1742b820-913a-463c-8e07-5f735bd5d73a", "kb:56b662d8-d215-4858-9974-569b0e5f2e0a", "kb:909c8e12-b998-4997-94e3-c9e9c82b9ae0", "kb:65d5f2bc-e2b8-4d00-83ac-de478bcaf74a", "kb:0a5d64b3-e197-473c-a2c7-87e7af343e28", "kb:5fa886c8-f7de-40d6-94ac-a9e0487824c4", "kb:0b60cad1-1f2b-4bd9-8a26-0f2839fe0f18", "kb:4a6d5b60-42f0-4193-98b9-9229ef036fe9", "kb:e7ed50fe-2e33-422a-81b5-e42e3208c8d5", "kb:6169199e-51b6-4da1-8df3-c791534c2902", "kb:81686058-048a-4813-8fd0-4c8bb2f457df", "kb:88edb8c2-7a56-4492-9e42-d7e5ad0de109", "kb:c1da9964-64a4-4808-a4b2-3d53beffca20", "kb:d2bae488-6c82-4909-b811-dd53cc74fab1", "kb:d7a3f6d3-bef7-4338-882d-56ec7bc6d2d4", "kb:6ef818d5-00da-43ae-b634-3b19ad168393", "kb:2090c025-3156-442c-a83f-55f360a32153", "kb:03a57d61-ced9-4f79-937b-eef90a591ef1", "kb:d135c0f2-fbef-432a-b144-cc34e81b7fa9", "kb:281ce140-ae5c-484b-af37-fe68cad2823d", "kb:c2b8e915-a781-4795-b06c-271f07806622", "kb:2bab6571-2f50-423d-ab5a-9ed6ffaf5a22", "kb:6429143d-2d1e-4d8d-aee8-421803e572be", "kb:d33be8e0-408f-4009-b7de-8daa94a5887e", "kb:7aee96b9-0310-41f4-a062-4edd06569754", "kb:73132763-ea45-4ee1-a211-8ea6c6a7a26f", "kb:28e3cb60-cd68-44bd-8ece-fe3460ae34e5", "kb:d426c73e-7967-485a-9779-70e97b483e50", "kb:0c73a029-6045-4303-8b57-ca95f347b819", "kb:5a8bee98-24a2-45ac-861a-8e7a0a0ed380", "kb:ac351aef-4522-4280-ae7d-c219a6542338", "kb:9ddb9e1c-91c5-4fb0-80e8-37a6a580e8ea", "kb:7721e542-7b90-4ac4-aa44-a9b82006d7ed", "kb:9ea18b3f-fc50-4496-a9b7-c95154ca15d6", "kb:a1c367d6-1b2c-490f-8551-1d2445e5d80d", "kb:b4887999-3f83-4943-a9a0-ae02f95f7497", "kb:33036785-f3bc-4359-a99f-3e5be89db8c6", "kb:86377054-1cc8-4ec7-b31c-880db432b24e", "kb:f84f17ec-b275-469c-a2c5-ec62c0922c8c", "kb:c3b46f9f-714b-471b-ac4b-298aebe7e7bc", "kb:a95df8e8-5e0e-42c3-bb28-991435317f56", "kb:90b9562f-969c-46e8-b31d-6bf745948352", "kb:8dabb0b8-a176-48d7-a9c7-ee70dddd1fdc", "kb:47400399-4edb-4315-aa43-8f1533167f83", "kb:76ca63c4-e148-46f7-94a7-6df806c1d763", "kb:fd6df30d-59a2-47a3-9863-52e7fd6c71fb", "kb:76510d15-f206-40cb-9e50-bdd86ebdaa88", "kb:0fda8035-e68b-4b7e-9b51-9e47f3f4dec7", "kb:47142c73-b11f-46fa-80a7-454dc56c736c", "kb:6a93a63f-38a2-4b43-a798-7f572dfbfee5", "kb:4fcc5a97-f6b0-4342-a1a5-2cc29c5051dc", "kb:4f845d01-e877-46cc-b280-d50905a4cd0e", "kb:7662b01e-8d72-4b0b-814e-f996bb4a0548", "kb:f8cdfa80-9513-4687-b03f-7bc40f6c2069", "kb:f3c4fb3b-643b-4840-90c1-2bf069fa9ec7", "kb:fe03e4e0-30dd-4cae-ae85-2f059e1a27d5", "kb:47931292-669c-4cba-add6-4b0295b6eeaa", "kb:d74a0ded-899f-4597-a829-dc9dfce855f0", "kb:bd786fa3-1e7c-46b1-bb02-3795e3db27dd", "kb:e03d17a7-4255-48ef-b3ad-6b7ada74bca2", "kb:4703d317-9645-476b-b0b1-0d1748f96cdb", "kb:ad5c79a5-b21b-4fda-81f9-3ba54418d71a", "kb:5e6afa46-5cd9-42f9-b6af-06e019e98c27", "kb:a008678c-d821-4def-ac7f-28063cea7f0c", "kb:cef9c19f-8426-4063-8d65-0ae1476cade1", "kb:3dbdf12c-d059-4061-bc4e-3f3b03e218a4", "kb:e5b2b8ee-8538-4e68-b90e-9b2894de1381", "kb:d4054dca-091c-4afc-a0b6-ca0bd8632214", "kb:e3903c4a-5076-4848-a6fa-a6346d756e7b", "kb:ff61c90e-6b59-4720-9d1a-36745a70ac08", "kb:1646aef2-05ff-4b96-b546-3bacda798a7d", "kb:dc1182e5-8c9c-4e76-9eb2-376910c15ad8", "kb:601723e5-890c-4ebc-9c2c-5bdaf0cf6bfa", "kb:0143ef56-afdf-4cfa-a35e-01f30dbd599e", "kb:1370aad6-7513-4461-bc96-d3a9ac599a8c", "kb:0a1a0577-0a4f-4c05-8e68-3c10a285a109", "kb:8d43b047-f340-4063-850c-10fb3ce0428d", "kb:2b586a68-0b44-49f7-91cb-8541de4b682d", "kb:57aac642-3e63-4030-be1b-4c959eb4a824", "kb:10799c1c-8c7e-4ac2-b699-485845fd50de", "kb:97389611-1cd0-4aa2-b56c-15f8cf44d636", "kb:f0b28326-01f0-4c93-a611-910cb0d23cd2", "kb:70188618-f097-4cc9-91ad-a7733d0531fb", "kb:f216e043-af85-4e07-a09e-6df0fdcae388", "kb:cda0ae59-c9b0-4d7c-9c49-1fcbdf9b2be8", "kb:0c7cda54-dd7e-480c-81b7-91a6364eb7fb", "kb:414b2724-b97f-46d4-b90f-0501c4c76160", "kb:92e59781-bde4-4cf7-bb46-2af160e8bb11", "kb:99b7ed07-d69e-4363-b4fb-93d6fb8e9e4c", "kb:59900996-3094-41b7-a4bd-4f2490ef43f0", "kb:2bb8faad-a3ed-411e-bd6a-2f7fad441514", "kb:e85de13e-1ec5-4551-9d96-40bdd5179d98", "kb:b7224d05-7bb0-463e-a1ca-357d9b4ba7e9", "kb:c4899a6e-3e3b-44a9-8e48-1f7dd469e1dd", "kb:a675f774-2210-44e0-a1d2-b1bb12af4974", "kb:cec444fb-7f1f-4a5f-9119-917294c4aa42", "kb:da390c79-c17e-4909-b836-d87dca03a80f", "kb:1498bbde-1f06-4925-9b79-b6068526ae40", "kb:c575b8e3-cb3a-4690-871f-94c40e969f7d", "kb:ddeb70d4-7d80-4daa-b549-ddc7ea153f2d", "kb:59bd33a3-1e17-477e-82d9-31df0bb1026c", "kb:b060690a-8ce6-4cbe-bb6c-893599b735e6", "kb:731d576e-b223-46e6-b609-0db74d5e5c4e", "kb:c5e6a1ef-b0bf-44cc-8cb1-50deaf9a9c20", "kb:455df171-2eb0-44ea-98d7-42f158a3ebc8", "kb:4a214a3f-9bd8-45a0-a57f-bca545fed3de", "kb:25662505-da49-4c8e-abff-d834aa268a30", "kb:29576394-5fc7-4a08-9abe-3ead1c0b8ce9", "kb:c9ac3f35-39a7-426b-9a0e-12c32cee9606", "kb:6dff8917-f605-4c95-a2a8-b79044b127c7", "kb:d9769f33-aaa6-4f73-a14c-70cd5da9de12", "kb:c8d9e799-0af8-4dc8-94d7-ccb9881793a4", "kb:a79b3495-b0df-4010-a8c2-a68b326d11e9", "kb:26445d92-1bde-4d0e-97fe-3fc858c959c0", "kb:2a079ef7-d245-47bc-a929-bc9678d7d3df", "kb:298c71ac-3e40-41e3-bf39-337fa9306350", "kb:29bbcd8a-a977-44e0-991e-e078119d7356", "kb:7a23d587-6972-4a2d-aecd-07c681def55a", "kb:93ca9cc2-d8fe-49b6-8352-c572de8472ac", "kb:fb024e4d-4a9f-48dd-b5ef-9d3fb286b2ce", "kb:087941b2-a1f4-4c2e-b0d5-a7b6b5344591", "kb:9c911cbb-8c8b-4fb1-b2b7-3653f7fd735c", "kb:42d26a4b-5b8c-41d1-9109-e96eed682a27", "kb:41246a85-f999-41f4-8e0b-9cc95fdd1f71", "kb:afbd07bc-543a-4807-8512-12a0c9f79aa3", "kb:84263a6b-4c5f-41c4-963b-0638be04667b", "kb:7b2dd68d-f182-41c0-afbd-3dfc3bcf7925", "kb:d9509852-5f82-495f-a512-f3fff68cd440", "kb:1a432f07-4363-4bb4-8641-eed995ef193c", "kb:ad8b7449-079a-41f5-86d4-c0c63abeb4d8", "kb:d8bf5d99-d897-415f-b932-a8ba4d527d1c", "kb:d5c5e941-aeb1-404d-a13d-3a798300cf28", "kb:0c200797-2510-46b4-9a3b-be2ab8ad0dd5", "kb:9e8df60c-12e1-4813-8f09-dfc27a312c67", "kb:5a10fbc3-dc8e-4cd1-b7a4-952418a65ad4", "kb:47a2d21b-cbc4-4ca9-ad2b-4df40eb1c260", "kb:86c5df99-2952-40a1-9180-f25ed7b6f862", "kb:58ab33cc-9272-4cc1-8f86-de101eabb950", "kb:0bb3297f-260e-41e3-b3b6-2b02080f179c", "kb:22afa720-de0b-4747-b3ea-2c66e4338894", "kb:f07585c3-fe6c-4eec-b3ec-862da8d60482", "kb:34e5d3e4-58d2-4db9-8feb-149909e460c4", "kb:95f3bf70-841e-49af-854b-48bcfff64692", "kb:3f96e9cb-db54-4bb1-a847-322a38b22230", "kb:3f2fbb75-7595-4a1f-8126-db757d005876", "kb:aa0c9f8f-6338-4c57-9876-ea45f5efac1d", "kb:a2f228c0-ee6e-4e89-8475-46fba4433bb9", "kb:0d62d5b3-41ec-4e07-bb98-68e3e00a99e7", "kb:95c5c3f7-e14f-4e38-930a-6d4119db0866", "kb:1e0539c2-6d4b-4ad9-9873-5a591b3a4c1b", "kb:2ea631a8-ded5-4c8a-abb8-44c9946b4eb3", "kb:2ca5e7fc-715e-44cb-a230-f11b46389d20", "kb:3632dc6a-86ae-4d9f-97d8-8a05d2e12c0d", "kb:93f300fe-5395-4eb2-b34c-29723fb08900", "kb:1f05c4b3-59f5-40c5-b7de-edcd82e8ce5a", "kb:67df1921-0f03-4d78-bb59-21f31fb001ea", "kb:d05fe93a-9dcd-478b-902b-f7db46b2d7f8", "kb:0f27b876-55de-4447-a40f-bdf0dec253c9", "kb:47f0ff8c-f61c-4c3a-bd32-620d2c8566fe", "kb:030f185b-c8d4-4f42-a30f-ad9597649467", "kb:282382b6-8282-48aa-ba92-cf6ba821ac15", "kb:238490e3-fbff-43e5-b7e5-5e2bb544f877", "kb:9a380bfe-e6ea-497d-8113-5bddf1cbe09e", "kb:9300cd84-36c1-49fa-ba7a-264474932d10", "kb:721a2b66-ae12-4d7a-b8f1-ba12bd3775b0", "kb:13861343-8461-470a-a740-2b6d225d80c2", "kb:71e1c987-5b27-4ff2-886a-088965e8a6d8", "kb:caca9b78-a1e2-466a-b770-8b8f16b7d024", "kb:fac33366-b59c-4eea-bf05-24ce1a308bcb", "kb:e847c87e-e352-4e5f-b383-62bc7fe45416", "kb:36a3439b-7f31-49b0-8e9c-6346be00e4ec", "kb:f4a396da-886f-4f96-969d-5920cbeed74c", "kb:cc552f4b-8382-4dfb-83c2-37f28c38a042", "kb:ba3d1ef3-b32b-4eb0-be13-6a1c5418f99f", "kb:e294c205-bb43-4802-84fb-ad1ffecdf401", "kb:006a29f0-a78b-4837-ba6c-332c536d8ed9", "kb:2fe70f01-90ee-4bcb-8edd-e91e708e9120", "kb:a9a19674-7148-44d1-8fed-124c13240e47", "kb:357e640d-6453-4df9-9105-1a9fbae9374a", "kb:b87dce59-e3b5-4790-b74a-7262291c9142", "kb:acf1dc2f-5eb9-4ffd-ad18-f7e6ddfe6f3d", "kb:7e4303bf-d441-4cd1-8bc9-20085ea86ae1", "kb:03ccea44-56f6-482a-a510-731165bb24bf", "kb:980aff58-5d00-48e2-94ff-71505d61e331", "kb:bb2562b4-7c63-40fb-82a4-7742438f8fb1", "kb:efa2f3fd-b837-406e-8f60-72e5fccf4fae", "kb:04443eff-569d-4e4f-86ba-5d348ed9f4fb", "kb:04c924de-fba3-496f-81a5-1d8943ffb894", "kb:b3d286b0-9a5e-40ab-9522-876c12fb0bf3", "kb:2dd0e1ed-8f8a-49f5-9044-a272b00a62f3", "kb:22ab2a0c-1727-417c-b081-c406db97e024", "kb:7666e514-00b8-4d04-be5c-1c817d021812", "kb:7d8e5fc9-1ca1-4671-ae9e-81009fec0bb8", "kb:05fc4fdf-b2aa-4725-b387-02de7d80ae95", "kb:d0fb3a65-bbed-4b3a-b8fc-e53e5b1193fd", "kb:9d8ef251-283c-453b-b1ec-272d2fd64e2f", "kb:0a2290da-452b-4659-99e2-4a4aaf59dcbc", "kb:b7805e5d-461d-48a3-85a7-e6bdc79c55a5", "kb:63107430-d315-48a1-8856-6406fdca5c1b", "kb:2c6ac981-a3e1-4ed7-a6b7-40270f566e71", "kb:eee5503a-7e1b-4511-a80c-2512d1280dfc", "kb:172c30bb-470b-4502-8d0f-4f617d9f4021", "kb:abb238ed-0a28-4d96-9611-26fd2342ca15", "kb:0d3e111a-c6d3-46ca-9f9c-3e9b0e4f8afb", "kb:4df53127-d34e-4836-8bbf-f6679084b714", "kb:766483a2-d627-4b91-a616-dd4d3cfdc656", "kb:ec45a005-e5fb-417c-afaa-f74d59552ec6", "kb:30d1253a-bd21-4e94-809a-6b0fc4aa5aac", "kb:d1949a96-c9db-4ac9-9718-19a9afc56981", "kb:1d8542ab-318d-4d06-816b-dbf491935ebe", "kb:b2fc5023-2e48-404e-8c12-123548fc0373", "kb:71741ba9-0799-476f-ac94-f53e8f912073", "kb:9dd9ac98-82e7-4d15-b157-f7405e231153", "kb:46aaa0f5-da8d-4db1-a69c-99cabd92801c", "kb:da7164ee-6532-4124-ad14-c467a7306a90", "kb:3dc3d5ed-0f3f-4ec5-afe3-da7c5d5b0675", "kb:3d142493-1f46-40df-aba4-fad1a20cabf7", "kb:23fa46a8-9dab-4566-9934-f4638b7d42f8", "kb:cf383109-aac6-4e11-834c-589146856d47", "kb:c7f156ad-3100-4c6d-9418-081489637a85", "kb:006dca6d-2032-416c-b983-191e6b66fb13", "kb:7e41dca6-9fd7-4103-8552-961b5278c34a", "kb:f7b80187-a2d4-497e-a8ed-dd2ed0ade291", "kb:b621d7ac-5646-4e95-b752-cc4b61153390", "kb:8204956f-c0aa-44de-9d82-f36f07ffe643", "kb:161933f8-36a8-48c3-ab16-904cc06e314d", "kb:d0eb7b65-638b-4c47-b12e-5a87362ed04e", "kb:2eac6192-1e87-483e-aac7-847e68f1a627", "kb:3a7b0aab-8309-4397-bbba-c21a7df3e520", "kb:d28af518-33c8-4eb6-8bfe-dafd753a5ae6", "kb:74d11c46-c50c-44b4-8cf4-6a27e512a42c", "kb:c246aa92-8043-46b0-a000-4d71d8bbcdb4", "kb:b52ad89a-c580-4cb1-bd4a-d32a017d56c0", "kb:b144f275-8dd1-4438-8cb3-d6bb02832107", "kb:efa695d4-b35c-4bfd-b0a7-338b7aacf2a2", "kb:cb5d41d5-d752-4554-94b2-2f0534b7cc13", "kb:71540fda-9e64-4fbd-a7ca-01e121574e01", "kb:625d013b-8d4a-415c-8b42-f9679009269b", "kb:07fa91b3-6595-4ce2-80d0-71d1590a5a67", "kb:c8be90b4-cacb-4c99-a2f3-1b72290d9da1", "kb:029108a5-53a9-482c-9382-2a05eceda026", "kb:c04a4dad-06a4-45aa-a558-4a2480674340", "kb:5559f38e-0040-465a-b9de-e3b3f79fb548", "kb:f9014d5b-ee99-4599-95c6-5be9b361e458", "kb:0465694a-6bf9-48aa-9601-c9346c53fe6d", "kb:c50f985c-0734-4bd5-9524-eecee35d76c9", "kb:09f7988f-0057-4fd7-8c4c-a01996c4cbf9", "kb:a1f174ec-d7d2-4371-872b-f0ddd4f7e498", "kb:e3fe41c1-d66d-42c8-acf2-8edaebd3fc8e", "kb:25eea788-55ab-48c2-9cac-146da9d69050", "kb:22e4cded-af64-493a-a183-d66d3c1241e4", "kb:dac5c811-9aaf-4bbf-8e19-13c621dc08b4", "kb:ac892288-ba31-4e9b-bf5c-133736aaeda8", "kb:5f4b4a82-489a-443f-b0d6-d5350dd3ff31", "kb:8d72ebfc-b68b-4678-a7fd-e0e752b15ef0", "kb:a965decc-237e-4901-8567-80b9dcaaef4e", "kb:0d80bb59-870f-407a-afda-193ace40ba0e", "kb:2f66e1d9-7be7-4020-bb2d-b8b3e9eaeb63", "kb:2ffd8892-4d55-4193-bbaa-f87d7bedacf5", "kb:50cba774-4ce6-4404-9a0a-2294d99494fc", "kb:0a2bbac6-e752-4dfe-98ee-87861362be89", "kb:9a7df6e1-8f23-474a-812e-be3c3b31007a", "kb:be4a87f3-aced-478f-8535-cd88f932316e", "kb:317ad2eb-0d17-4dda-8ed0-2bfc913cfbc0", "kb:1cbd138e-25ee-4b03-b7b6-6f1071cba1c3", "kb:6ac658f3-89ad-4d69-941d-be828d65d6e1", "kb:f644c190-9f7e-4e44-9392-4cc5512b4770", "kb:34026b9b-531a-43c6-8ea0-f9cad8334dfa", "kb:ba2b0233-152a-4dae-919a-b3909900801a", "kb:8758ff68-729a-48e9-9f3c-e5f89a2819f6", "kb:7d257eec-c7c8-47e9-a1be-a389a403246f", "kb:51594211-b89b-43c8-9e4b-b128285cd467", "kb:4a622da2-9373-4007-85d5-9ad97d3491e4", "kb:eb8fdfa3-4b1a-4393-b8e9-e3525c3cd093", "kb:f6ce017a-2eeb-4f13-8440-030141f8c09b", "kb:ceab5fb0-8ce0-425d-8ff7-7d8c5b6b1938", "kb:904dd744-672e-4284-a122-ac444d2b8136", "kb:29bcb491-deef-4c22-bb92-5faedcd03573", "kb:44ab7b84-a41f-455a-9d8e-64caa1ad5f81", "kb:82526b4f-24c6-4a07-a214-9eccb9b5fa91", "kb:727ab28a-70c0-4fe8-8c8a-6fa5f36a2e3e", "kb:9ac3164c-4d05-4220-b485-7c23ce0417a4", "kb:ae3b1086-da53-44f6-b959-b78f705dda11", "kb:05a6479f-2e8e-4fdd-9178-a6f2b57b70f1", "kb:21ac6555-c2b3-40e0-8b7b-97706e8c2e3a", "kb:c8a6db61-6368-4231-b04a-28fe31638e97", "kb:c810202a-0d3b-4dba-acd5-135e1c706cf1", "kb:eef5d852-5b41-4b36-8cf4-ede1ff28b224", "kb:a99f45eb-d38e-4715-a920-68f0f927ae94", "kb:d2f079f0-0717-494e-b70b-97156f6d92bd", "kb:a003b14a-d39b-4332-9d9c-1830595bb3e0", "kb:e6b5413e-346a-48c0-80d4-d1e5ab497531", "kb:e82e7f8c-6263-4194-b7a6-c09a6bd86d6c", "kb:6345c90b-4432-445c-9644-4a549db54988", "kb:7b2c720d-ddcb-451b-a6d8-eac513d9e8a4", "kb:36e1f9fb-59a8-4f80-9fdf-4faaa2cfb7ad", "kb:47e31298-9f38-493a-b625-d9d014c76ca6", "kb:92bf3772-53ff-461a-a16d-f4e1b2215067", "kb:ac797da4-5e01-4c13-a295-d2d020176a93", "kb:af7435ad-fadb-4b2f-8083-c9137490d4c6", "kb:d454aad3-124a-4685-b817-c0bf5458a776", "kb:75371beb-9087-4f38-b964-e9bfedc32037", "kb:1bda3b4c-b7d8-4683-bcc6-f6cce6a1e8ab", "kb:fffab104-5045-40d8-9e9d-21e7e8359b5b", "kb:3090a6dc-831f-4edb-b698-6dbf5d309e6b", "kb:981051b7-7e41-433c-95c7-015e294ea1f6", "kb:b2283003-4340-4cee-abaf-efbc870ab37f", "kb:92a5ef98-11c0-4601-a4de-17dcd42293e8", "kb:ad626ba5-8a11-4242-860e-492ce70f3fac", "kb:9a605cde-011a-42b4-8906-8ade3b4b995d", "kb:228d2dfd-e94e-4c0a-9148-2a471b566650", "kb:87fabd08-7f90-47e7-9921-ec25dc7c916d", "kb:66a980de-8797-4025-9884-d80b56aa19d3", "kb:2b1e5c0a-10e6-4d94-b9e6-30559f70ce4d", "kb:c98fcf5d-8bbf-48f6-be25-78d2cde6f4d5", "kb:eede7887-3dca-4d3b-93b6-54dd355d8a2f", "kb:723e4263-412a-4f6d-b275-3c93cd7c13d4", "kb:3245181a-2687-4a1e-80c1-d4668c2ab2aa", "kb:f9cce29f-d372-441c-be84-cfd2bd0e16b1", "kb:e1ff8d17-44a1-41d5-8c95-bbfe2a112f0e", "kb:78c377f1-6e09-4e4b-8e3b-8caedbee9fe9", "kb:2b8204c3-dfa1-4607-bcea-1fcade87e6ce", "kb:4a50a16a-ae7f-4d18-91d0-0b0e3e9d860e", "kb:836e82c0-9496-4dbe-9641-cb775459b1ab", "kb:98d2598f-3db1-4cba-b312-dd1581805172", "kb:d29437d2-fe3c-489d-8623-1f14a8e4ddb9", "kb:13ab219f-96f4-4990-98a9-daaa6b77e73a", "kb:d1105c92-b541-4a20-9ff1-c4149cd83e39", "kb:792d8836-4b23-40ce-a75e-863aa2b63e4d", "kb:2ba4a4ea-64d7-421b-bd2e-644f3924a6f8", "kb:3531fb57-7942-439a-9999-b07e9cb26212", "kb:e841a792-bc27-4e7c-9cf6-0ff136b816f0", "kb:557e7580-e3b0-48fc-b40a-13e4b869a944", "kb:23bed06a-87c7-450b-bd26-992e292e3020", "kb:63f56e34-91d4-4018-8358-2c3c6c5b736f", "kb:4ac0fc89-8c96-4a5c-8ea2-2084d6de21af", "kb:56f2d3b6-2b6c-4091-8235-549360e83ed7", "kb:fca72146-1ef6-48bc-a430-f22f8a7af6ac", "kb:2dc52c2a-be21-4f8a-bca6-1de8196aa1d1", "kb:87131d3a-91d2-499a-a090-e9b73ea524e7", "kb:566664e9-b6e4-4f52-9003-2d7eb258fc6f", "kb:b6e48d6b-370f-43ac-948c-8ebac1d9aa1b", "kb:233606b6-454a-4885-b711-57c9ca449045", "kb:39359f6c-efc0-4e8a-b9a8-80d93fe83cb3", "kb:a634a331-d1f3-4555-9850-e48f80e89d8d", "kb:429f6dab-2f29-4d09-88a3-ca3281fff799", "kb:78b499b2-5eb8-48c2-a199-ef1f9b9905a7", "kb:dd7444e0-acf9-481f-9ce1-f803bc1656a8", "kb:c9a336a1-dc73-4205-b5d7-075720957a7a", "kb:5f099ac7-133e-4cfa-af34-1696e4cce8e8", "kb:f95eaa55-241b-4f85-8d51-ac753555165f", "kb:0cdbd8a9-f303-4a5d-8abc-9db55ae6815b", "kb:f42c2e82-9826-4826-9df5-068153dc0f2d", "kb:af19dc50-e4f6-413f-9d0a-b7e087090edb", "kb:5ca857fb-2faf-4269-897e-70128964d7b0", "kb:6ce431ac-0342-423b-9779-ff75c421c141", "kb:3abfc9e6-2e2b-43ed-ad33-a5fcb2801668", "kb:d766ad1a-7e1d-4c1f-8c07-9c585dbdb0a3", "kb:a6da5a47-c3f9-4a9c-b475-eaf164cd2610", "kb:00840e79-2c5e-4a15-87a9-1de0942d7d61", "kb:2f4bb21d-01b4-4805-9985-cd61ee6f59ab", "kb:b3cb0b93-fc93-4d21-83d5-8e0ec0497fa8", "kb:ff7bbe5a-16c9-4f74-8c60-cc5eb97116ed", "kb:8c4e6d20-8e9d-41f9-94c0-2f7c147be8a8", "kb:bf82f751-5ed3-4c34-a345-761456266f8f", "kb:4cd03b7f-1099-4dc5-9bf6-c981f72ec5ab", "kb:b94838d7-54f3-4da5-a6e4-1598e4831d49", "kb:d07c998f-ab06-40d2-ae17-14a1942b2419", "kb:a0ad8c98-43a2-455a-88a7-137b53fe1169", "kb:8eefd80f-a799-42e9-83d5-5ddb9304390a", "kb:d3b3810b-209f-4f16-86e3-907c521ed59d", "kb:a3a5bfd1-662a-4d40-94ca-cf55cc2f6c46", "kb:4724bb3e-e57a-4035-b191-76b3f9d6c8cc", "kb:5faf0623-cc3a-44a4-a65f-d4e5605b8449", "kb:2a3b0f77-e2fe-4659-83f5-c8e2f15dd536", "kb:73521710-e0a4-45d6-8853-b06c6defe93b", "kb:ac6dfe68-4c43-4cca-9048-2cac9a0e8db0", "kb:38bad728-08fa-472c-a8bd-d1eca28e24f4", "kb:3579a87d-50be-49d6-a2c7-f0d037333198", "kb:c5bb15e6-335d-4670-bd64-e89a4af69516", "kb:514ac93a-badf-4504-a8b2-8b0cd4c91bce", "kb:fe90125c-ef28-4bee-a275-19b11e119df5", "kb:2a15f2e8-7821-456c-93ba-04befefb3afd", "kb:86f10057-5c85-4142-8c44-1c6680427a03", "kb:69febb33-6e65-4b9b-b26d-66f2d503e242", "kb:d6bf59a0-4186-4116-a315-3902577daf4f", "kb:bffe4867-8501-4f51-9d65-8f14bde89181", "kb:de641413-f885-400e-af4c-b95547dc666b", "kb:90a3a447-83ee-4b95-a4f2-2a21b110bb28", "kb:f23e18c7-5647-4e70-8bdd-6b812828bf20", "kb:b520eb5c-8997-4223-930f-5fc1569064ba", "kb:28876d7f-93cd-4f89-b308-4857e35f5e5d", "kb:5f7ab7ad-addc-4f9a-b2a7-ccbcd1b596b0", "kb:7c736392-4cbb-43c1-8afe-9b973e814dd0", "kb:cc765fc8-1b97-41fd-b50f-c48452547cdf", "kb:16be93a7-0eed-4b3a-bfce-a04348d77ce7", "kb:0bdc768d-b7db-487e-89c3-580007317fdc", "kb:f67134c0-fab0-4666-ab8e-673034f1a287", "kb:3f0aa295-37dd-4d75-a3e5-8ce0242bcaf3", "kb:4154ee74-721b-42d3-9401-396f08581dba", "kb:feb9f381-922f-4f1d-8bb3-e5f4e0643cd9", "kb:5c44ca8d-37af-4806-b046-746c9c8e5f19", "kb:612a12cb-a2cc-4433-b327-9456c8bdc88c", "kb:9c37a358-94fd-4f8c-a94d-0179eed2c19a", "kb:f2965fdb-a74d-4a6f-9f9e-9134c1410297", "kb:fa4e5a57-8601-4a38-89a8-303bbd154ae2", "kb:2778393c-25d7-481c-a411-cab50b19a34d", "kb:b3de3667-b59d-4d83-b441-3b089e4333af", "kb:fe85764a-6cba-40db-9b28-7ee6819e359b", "kb:2fa87447-3398-4ceb-988f-053206331fd1", "kb:a82dd512-c60e-4092-a178-996551fe7f4d", "kb:5d959279-21c2-475f-acfe-a4f210e80d3f", "kb:405cae83-1a46-4a3b-81d7-6f973da9767a", "kb:dade8440-f7e1-43fb-9782-7d1602b28a52", "kb:04e651f5-157a-4140-a0ae-e0a2fd273774", "kb:1cc6d168-4f6e-4a5b-a59e-6c5980c944d5", "kb:4d24778b-d930-48e8-9481-a267b3789928", "kb:f9f4fbaa-d28e-44a6-bf42-73418a529192", "kb:95b1457a-7718-494b-b241-8902a833b5ac", "kb:0894eaca-f6ba-4328-8b39-85730f7d6811", "kb:68db51c3-242d-430c-9cb4-2a4e7d30349d", "kb:4cb043cf-274e-4581-86f2-686bc1a600ea", "kb:5d72c359-3fb6-4282-b470-e017c7206ca2", "kb:b13bb9e1-b3a3-4f14-ac0f-7d0e6fbd5e01", "kb:a1a2bd0b-3622-475f-b287-3f9f37b2e4b3", "kb:919076bf-1143-4345-9a1f-a8059f2e30b5", "kb:feeecde4-c6a3-41d1-9964-145e0306883d", "kb:31869085-0150-4acf-99df-f50224d294b1", "kb:cd631eaa-01f8-4062-a4af-10416a74ece7", "kb:c09e40a7-e210-4e00-b709-02a71a81474a", "kb:f4c9d248-7e04-4026-8bc5-a73ecf8839cd", "kb:404aff84-5914-42cf-b231-adc9304dc57e", "kb:933eb5a5-c9e0-4cde-9ac8-bcf0333e90e9", "kb:787e583a-49ef-4e75-a274-356cd6fee41d", "kb:2c3d03ff-5c97-4e3a-8dc7-21560b6b413b", "kb:658bb87a-695d-44bd-ae24-0113b6f57661", "kb:51c0c6d3-c8e0-4315-abeb-f1b91c4ee8a2", "kb:fed54d13-d20f-40f6-9268-50d23ab3a222", "kb:58d4d137-ebe8-4616-b276-00e4bdf30258", "kb:285270ed-4755-4cad-9d26-7591266f5c5a", "kb:158b45c2-54b9-4acf-b6e3-0fbbcd17e8fe", "kb:a606e766-68b6-489a-bc88-903e36ce0628", "kb:57c49432-d2a8-495c-93ab-1e02b80129aa", "kb:146f29e3-2884-4ad7-ab1f-c766dccfb757", "kb:a4426707-14ce-4bb7-9278-ccf7da35a8ab", "kb:8a1295b3-7a92-4d54-be01-897718316dcf", "kb:995c4d32-3600-49e9-8abe-0bdb3f315ea0", "kb:d76bfbaa-119f-40c1-bc0d-2624b3798042", "kb:19c3dd6b-d242-4d8d-86a9-cd4b3460434a", "kb:ee5aeeb0-3043-4a14-8d72-4f86a8a48a27", "kb:faf74b97-0215-4cf6-9c72-b905a0471e74", "kb:a3b8cf7f-ba84-48ae-bbb2-2ae97b661159", "kb:0797f323-f7fd-4eca-bc17-29e57639c828", "kb:4173e4a7-04a1-4b59-bac1-5682e2b41397", "kb:c2dbf47c-5874-4c95-b199-85943cb32840", "kb:429ad8e5-46cd-4fc7-bfda-fc112ac372a1", "kb:b9d67217-ef6c-4b30-8ceb-135f64834014", "kb:dd6814d3-91d3-43b8-aa1c-9926ec9b918b", "kb:be54710f-1a45-42fa-9edb-f17ac5a3cf44", "kb:baf7083c-27e4-4c2f-9584-bd36f2b312e4", "kb:fdd3d1c7-1b13-40d6-abf5-bfdcd5f4edab", "kb:ed12241e-3861-43c3-b8f3-39e7f3edeb91", "kb:e3de5ae4-16fe-400c-a360-5f11f0aee558", "kb:99b78854-d151-4d3f-a1e1-d1e8e229817f", "kb:1ec71ab7-6dda-4d62-acb4-22f596db2ef4", "kb:712adcf7-9e3e-4b7b-86c4-3b19905a29af", "kb:70cde696-bc47-4301-9fd1-bf65739ff0a9", "kb:88c0659d-abe5-47fe-946f-4747eb34576f", "kb:3eb7c4cb-c16b-4642-b2cc-a1989eaf4d3a", "kb:2ab08505-1c66-4ef0-9274-159a3f9c8ebc", "kb:d8a38a6e-d974-4a9e-86f3-722e322a5efd"], "rows_by_id": {"kb:7d43e928-ded2-47a5-9f7c-8c3d0735eeb9": {"content": "Pandas Merging DataFrames — official reference\nOfficial documentation for Pandas Merging DataFrames. The method signature is `pd.merge(left, right, on, how='inner')`. merges two DataFrames using SQL-style joins. Raises `MergeError` when merge keys produce duplicate columns. See also: df.join, df.concat.\nCode signature: pd.merge(left, right, on, how='inner')", "file_path": "https://docs.pandas.org/en/stable/pd/merge.html", "function_name": "pd.merge(left, right, on, how='inner')", "type": "documentation", "metadata": {"record_id": "7d43e928-ded2-47a5-9f7c-8c3d0735eeb9", "source_type": "documentation", "domain": "NumPy/Pandas", "library": "Pandas", "title": "Pandas Merging DataFrames — official reference", "content": "Official documentation for Pandas Merging DataFrames. The method signature is `pd.merge(left, right, on, how='inner')`. merges two DataFrames using SQL-style joins. Raises `MergeError` when merge keys produce duplicate columns. See also: df.join, df.concat.", "code_signature": "pd.merge(left, right, on, how='inner')", "tags": "vectorization, broadcasting, nan, dtype, reshape", "url": "https://docs.pandas.org/en/stable/pd/merge.html", "author": "Official Docs", "date_published": "15-02-2024", "votes_or_stars": 4496, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:3673803f-652e-453d-9306-4cc0e5d843f3": {"content": "How to handle rate limiting with the requests library?\n```python\nfrom tenacity import retry, wait_exponential, stop_after_attempt\n@retry(wait=wait_exponential(min=1, max=60), stop=stop_after_attempt(5))\ndef call_api(url):\n    r = requests.get(url)\n    r.raise_for_status()\n    return r.json()\n```\nCheck `Retry-After` header in 429 responses. For async: use `aiohttp` with tenacity.\nCode signature: Implement exponential backoff with the tenacity or backoff library.", "file_path": "https://stackoverflow.com/questions/1604451", "function_name": "Implement exponential backoff with the tenacity or backoff library.", "type": "stack_overflow", "metadata": {"record_id": "3673803f-652e-453d-9306-4cc0e5d843f3", "source_type": "stack_overflow", "domain": "Requests/HTTP/APIs", "library": "retry", "title": "How to handle rate limiting with the requests library?", "content": "```python\nfrom tenacity import retry, wait_exponential, stop_after_attempt\n@retry(wait=wait_exponential(min=1, max=60), stop=stop_after_attempt(5))\ndef call_api(url):\n    r = requests.get(url)\n    r.raise_for_status()\n    return r.json()\n```\nCheck `Retry-After` header in 429 responses. For async: use `aiohttp` with tenacity.", "code_signature": "Implement exponential backoff with the tenacity or backoff library.", "tags": "http, oauth, rest-api, authentication, httpx, retry", "url": "https://stackoverflow.com/questions/1604451", "author": "user_885136", "date_published": "22-12-2022", "votes_or_stars": 640, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:9c43ce7d-5d4b-49e5-9871-9a4534720acf": {"content": "How to implement a thread pool in Python?\n```python\nfrom concurrent.futures import ThreadPoolExecutor\nwith ThreadPoolExecutor(max_workers=8) as ex:\n    futures = [ex.submit(task, arg) for arg in args]\n    results = [f.result() for f in futures]\n```\nFor CPU-bound: use ProcessPoolExecutor instead. For async integration: `await loop.run_in_executor(executor, func, *args)`.\nCode signature: Use concurrent.futures.ThreadPoolExecutor for I/O-bound tasks.", "file_path": "https://stackoverflow.com/questions/3646552", "function_name": "Use concurrent.futures.ThreadPoolExecutor for I/O-bound tasks.", "type": "stack_overflow", "metadata": {"record_id": "9c43ce7d-5d4b-49e5-9871-9a4534720acf", "source_type": "stack_overflow", "domain": "Asyncio/Threading", "library": "coroutine", "title": "How to implement a thread pool in Python?", "content": "```python\nfrom concurrent.futures import ThreadPoolExecutor\nwith ThreadPoolExecutor(max_workers=8) as ex:\n    futures = [ex.submit(task, arg) for arg in args]\n    results = [f.result() for f in futures]\n```\nFor CPU-bound: use ProcessPoolExecutor instead. For async integration: `await loop.run_in_executor(executor, func, *args)`.", "code_signature": "Use concurrent.futures.ThreadPoolExecutor for I/O-bound tasks.", "tags": "lock, gather, executor", "url": "https://stackoverflow.com/questions/3646552", "author": "user_469469", "date_published": "02-06-2023", "votes_or_stars": 1532, "relevance_label": "low", "difficulty_level": "advanced"}}, "kb:1c570767-2f5a-4811-8e17-2b02a4e47fe1": {"content": "How to read a large file line by line in Python without loading it all into memory?\n```python\nwith open('large.txt', 'r') as f:\n    for line in f:\n        process(line.strip())\n```\nThis reads one line at a time. For binary files: use `f.read(chunk_size)` in a loop. For CSV: use `pd.read_csv(file, chunksize=10000)`. Never use `f.readlines()` on large files.\nCode signature: Iterate over the file object directly — it yields lines lazily.", "file_path": "https://stackoverflow.com/questions/2737893", "function_name": "Iterate over the file object directly — it yields lines lazily.", "type": "stack_overflow", "metadata": {"record_id": "1c570767-2f5a-4811-8e17-2b02a4e47fe1", "source_type": "stack_overflow", "domain": "OS/Subprocess/File I/O", "library": "shutil", "title": "How to read a large file line by line in Python without loading it all into memory?", "content": "```python\nwith open('large.txt', 'r') as f:\n    for line in f:\n        process(line.strip())\n```\nThis reads one line at a time. For binary files: use `f.read(chunk_size)` in a loop. For CSV: use `pd.read_csv(file, chunksize=10000)`. Never use `f.readlines()` on large files.", "code_signature": "Iterate over the file object directly — it yields lines lazily.", "tags": "pathlib, glob, os", "url": "https://stackoverflow.com/questions/2737893", "author": "user_994539", "date_published": "31-12-2020", "votes_or_stars": 2311, "relevance_label": "low", "difficulty_level": "advanced"}}, "kb:3e001a2d-92fd-46cd-9eff-99f83d184f13": {"content": "BUG: os.path.join ignores prefix when component starts with /\nProblem: os.path.join('base', '/absolute') returns '/absolute', ignoring 'base'.\n\nFix/Discussion: This is documented behavior: os.path.join discards previous components when it encounters an absolute path. Fix: use `pathlib.Path(base) / component.lstrip('/')`. Validate user-provided paths with `Path(path).resolve().is_relative_to(base_dir)` to prevent path traversal.\nCode signature: status:closed", "file_path": "https://github.com/cpython/cpython/issues/7712", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "3e001a2d-92fd-46cd-9eff-99f83d184f13", "source_type": "github_issue", "domain": "OS/Subprocess/File I/O", "library": "cpython", "title": "BUG: os.path.join ignores prefix when component starts with /", "content": "Problem: os.path.join('base', '/absolute') returns '/absolute', ignoring 'base'.\n\nFix/Discussion: This is documented behavior: os.path.join discards previous components when it encounters an absolute path. Fix: use `pathlib.Path(base) / component.lstrip('/')`. Validate user-provided paths with `Path(path).resolve().is_relative_to(base_dir)` to prevent path traversal.", "code_signature": "status:closed", "tags": "os, glob, pipe, stdout", "url": "https://github.com/cpython/cpython/issues/7712", "author": "contributor_8802", "date_published": "31-12-2018", "votes_or_stars": 414, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:76550b3c-af44-4633-8d08-caaaf3112550": {"content": "How to upload a file with requests?\n```python\nwith open('file.pdf', 'rb') as f:\n    r = requests.post(url, files={'file': ('file.pdf', f, 'application/pdf')})\n```\nFor multiple files: pass a list of tuples. For multipart form data with fields: combine `files` and `data` parameters. Don't set Content-Type manually — requests sets it with boundary.\nCode signature: Use the files parameter with a file-like object or tuple.", "file_path": "https://stackoverflow.com/questions/8896868", "function_name": "Use the files parameter with a file-like object or tuple.", "type": "stack_overflow", "metadata": {"record_id": "76550b3c-af44-4633-8d08-caaaf3112550", "source_type": "stack_overflow", "domain": "Requests/HTTP/APIs", "library": "requests", "title": "How to upload a file with requests?", "content": "```python\nwith open('file.pdf', 'rb') as f:\n    r = requests.post(url, files={'file': ('file.pdf', f, 'application/pdf')})\n```\nFor multiple files: pass a list of tuples. For multipart form data with fields: combine `files` and `data` parameters. Don't set Content-Type manually — requests sets it with boundary.", "code_signature": "Use the files parameter with a file-like object or tuple.", "tags": "timeout, authentication, websocket, http", "url": "https://stackoverflow.com/questions/8896868", "author": "user_243238", "date_published": "22-11-2022", "votes_or_stars": 1547, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:8331d943-8dba-47bf-84d7-494573ce6263": {"content": "How do I efficiently fill NaN values in a Pandas DataFrame?\nFor large DataFrames, avoid looping. Use `df.fillna({'col': val})` to fill specific columns. `df.ffill()` propagates last valid observation forward. For time-series data, `df.interpolate()` provides smoother results. Always check `df.isna().sum()` before and after to verify.\nCode signature: Use df.fillna(value) or df.ffill()/df.bfill() for forward/backward fill.", "file_path": "https://stackoverflow.com/questions/6438436", "function_name": "Use df.fillna(value) or df.ffill()/df.bfill() for forward/backward fill.", "type": "stack_overflow", "metadata": {"record_id": "8331d943-8dba-47bf-84d7-494573ce6263", "source_type": "stack_overflow", "domain": "NumPy/Pandas", "library": "loc", "title": "How do I efficiently fill NaN values in a Pandas DataFrame?", "content": "For large DataFrames, avoid looping. Use `df.fillna({'col': val})` to fill specific columns. `df.ffill()` propagates last valid observation forward. For time-series data, `df.interpolate()` provides smoother results. Always check `df.isna().sum()` before and after to verify.", "code_signature": "Use df.fillna(value) or df.ffill()/df.bfill() for forward/backward fill.", "tags": "nan, dataframe, pivot, reshape, loc, iloc", "url": "https://stackoverflow.com/questions/6438436", "author": "user_522340", "date_published": "21-11-2022", "votes_or_stars": 13, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:2f6c144b-4b68-497a-81d8-3693d23adec9": {"content": "PERF: Django template rendering bottleneck in production\nProblem: Template rendering takes 500ms+ for complex pages with many database queries.\n\nFix/Discussion: Primary cause is N+1 queries in template. Solutions: (1) Use select_related/prefetch_related. (2) Cache rendered templates with `cache` template tag. (3) Use `django-cachalot` for query caching. (4) Move heavy computation to views, pass pre-computed context. (5) Consider server-side caching with Redis.\nCode signature: status:closed", "file_path": "https://github.com/django/django/issues/3243", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "2f6c144b-4b68-497a-81d8-3693d23adec9", "source_type": "github_issue", "domain": "FastAPI/Flask/Django", "library": "django", "title": "PERF: Django template rendering bottleneck in production", "content": "Problem: Template rendering takes 500ms+ for complex pages with many database queries.\n\nFix/Discussion: Primary cause is N+1 queries in template. Solutions: (1) Use select_related/prefetch_related. (2) Cache rendered templates with `cache` template tag. (3) Use `django-cachalot` for query caching. (4) Move heavy computation to views, pass pre-computed context. (5) Consider server-side caching with Redis.", "code_signature": "status:closed", "tags": "response, flask, routing", "url": "https://github.com/django/django/issues/3243", "author": "contributor_6988", "date_published": "06-12-2024", "votes_or_stars": 69, "relevance_label": "low", "difficulty_level": "advanced"}}, "kb:815d4d45-d6f0-4007-be22-be4473e63abb": {"content": "How do I efficiently fill NaN values in a Pandas DataFrame?\nFor large DataFrames, avoid looping. Use `df.fillna({'col': val})` to fill specific columns. `df.ffill()` propagates last valid observation forward. For time-series data, `df.interpolate()` provides smoother results. Always check `df.isna().sum()` before and after to verify.\nCode signature: Use df.fillna(value) or df.ffill()/df.bfill() for forward/backward fill.", "file_path": "https://stackoverflow.com/questions/6438436", "function_name": "Use df.fillna(value) or df.ffill()/df.bfill() for forward/backward fill.", "type": "stack_overflow", "metadata": {"record_id": "815d4d45-d6f0-4007-be22-be4473e63abb", "source_type": "stack_overflow", "domain": "NumPy/Pandas", "library": "loc", "title": "How do I efficiently fill NaN values in a Pandas DataFrame?", "content": "For large DataFrames, avoid looping. Use `df.fillna({'col': val})` to fill specific columns. `df.ffill()` propagates last valid observation forward. For time-series data, `df.interpolate()` provides smoother results. Always check `df.isna().sum()` before and after to verify.", "code_signature": "Use df.fillna(value) or df.ffill()/df.bfill() for forward/backward fill.", "tags": "pivot, vectorization, array, iloc, reshape", "url": "https://stackoverflow.com/questions/6438436", "author": "user_522340", "date_published": "23-08-2022", "votes_or_stars": 1, "relevance_label": "low", "difficulty_level": "intermediate"}}, "kb:727d2380-38e8-40b6-a1f2-68eabfc40714": {"content": "BUG: FastAPI dependency_overrides not working in pytest\nProblem: Dependency overrides set in conftest are not applied during test execution.\n\nFix/Discussion: Set overrides on the app object before TestClient is created. Use `app.dependency_overrides[dep] = mock_dep` in a pytest fixture with function scope. Clear overrides after test: `app.dependency_overrides = {}`. Ensure TestClient is created after overrides are set.\nCode signature: status:closed", "file_path": "https://github.com/fastapi/fastapi/issues/8479", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "727d2380-38e8-40b6-a1f2-68eabfc40714", "source_type": "github_issue", "domain": "FastAPI/Flask/Django", "library": "fastapi", "title": "BUG: FastAPI dependency_overrides not working in pytest", "content": "Problem: Dependency overrides set in conftest are not applied during test execution.\n\nFix/Discussion: Set overrides on the app object before TestClient is created. Use `app.dependency_overrides[dep] = mock_dep` in a pytest fixture with function scope. Clear overrides after test: `app.dependency_overrides = {}`. Ensure TestClient is created after overrides are set.", "code_signature": "status:closed", "tags": "template, blueprint, django, request, flask, response", "url": "https://github.com/fastapi/fastapi/issues/8479", "author": "contributor_1994", "date_published": "22-07-2019", "votes_or_stars": 317, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:5e99f4fc-3b6c-442d-988d-713cef14ec86": {"content": "Flask: how to return JSON responses properly?\n`return jsonify({'key': 'value'})` sets Content-Type to application/json. Flask 1.0+ automatically converts returned dicts to JSON responses. For custom status codes: `return jsonify(data), 201`. For errors: use `abort(404)` or return `({'error': 'Not found'}, 404)`.\nCode signature: Use jsonify() or return a dict directly (Flask 1.0+).", "file_path": "https://stackoverflow.com/questions/8930103", "function_name": "Use jsonify() or return a dict directly (Flask 1.0+).", "type": "stack_overflow", "metadata": {"record_id": "5e99f4fc-3b6c-442d-988d-713cef14ec86", "source_type": "stack_overflow", "domain": "FastAPI/Flask/Django", "library": "django", "title": "Flask: how to return JSON responses properly?", "content": "`return jsonify({'key': 'value'})` sets Content-Type to application/json. Flask 1.0+ automatically converts returned dicts to JSON responses. For custom status codes: `return jsonify(data), 201`. For errors: use `abort(404)` or return `({'error': 'Not found'}, 404)`.", "code_signature": "Use jsonify() or return a dict directly (Flask 1.0+).", "tags": "blueprint, pydantic, middleware, routing, template", "url": "https://stackoverflow.com/questions/8930103", "author": "user_264801", "date_published": "12-04-2022", "votes_or_stars": 2211, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:70efaa98-679f-4c61-85dd-b28180d9b2b1": {"content": "How to implement database connection pooling with SQLAlchemy?\n`create_engine(url, pool_size=10, max_overflow=20, pool_timeout=30, pool_recycle=1800)`. Use `pool_pre_ping=True` to detect stale connections. For async: use `AsyncEngine` with `create_async_engine()`. Monitor with `engine.pool.status()`. NullPool disables pooling for scripts/serverless environments.\nCode signature: create_engine() has built-in pooling. Configure pool_size and max_overflow.", "file_path": "https://stackoverflow.com/questions/7358113", "function_name": "create_engine() has built-in pooling. Configure pool_size and max_overflow.", "type": "stack_overflow", "metadata": {"record_id": "70efaa98-679f-4c61-85dd-b28180d9b2b1", "source_type": "stack_overflow", "domain": "SQLAlchemy/Databases", "library": "session", "title": "How to implement database connection pooling with SQLAlchemy?", "content": "`create_engine(url, pool_size=10, max_overflow=20, pool_timeout=30, pool_recycle=1800)`. Use `pool_pre_ping=True` to detect stale connections. For async: use `AsyncEngine` with `create_async_engine()`. Monitor with `engine.pool.status()`. NullPool disables pooling for scripts/serverless environments.", "code_signature": "create_engine() has built-in pooling. Configure pool_size and max_overflow.", "tags": "relationship, sqlalchemy, query", "url": "https://stackoverflow.com/questions/7358113", "author": "user_12260", "date_published": "19-07-2021", "votes_or_stars": 220, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:71d7fcf6-a1b8-4d47-8f2d-cf08afb11ffd": {"content": "Flask: how to return JSON responses properly?\n`return jsonify({'key': 'value'})` sets Content-Type to application/json. Flask 1.0+ automatically converts returned dicts to JSON responses. For custom status codes: `return jsonify(data), 201`. For errors: use `abort(404)` or return `({'error': 'Not found'}, 404)`.\nCode signature: Use jsonify() or return a dict directly (Flask 1.0+).", "file_path": "https://stackoverflow.com/questions/8930103", "function_name": "Use jsonify() or return a dict directly (Flask 1.0+).", "type": "stack_overflow", "metadata": {"record_id": "71d7fcf6-a1b8-4d47-8f2d-cf08afb11ffd", "source_type": "stack_overflow", "domain": "FastAPI/Flask/Django", "library": "django", "title": "Flask: how to return JSON responses properly?", "content": "`return jsonify({'key': 'value'})` sets Content-Type to application/json. Flask 1.0+ automatically converts returned dicts to JSON responses. For custom status codes: `return jsonify(data), 201`. For errors: use `abort(404)` or return `({'error': 'Not found'}, 404)`.", "code_signature": "Use jsonify() or return a dict directly (Flask 1.0+).", "tags": "middleware, flask, orm, rest", "url": "https://stackoverflow.com/questions/8930103", "author": "user_264801", "date_published": "16-08-2018", "votes_or_stars": 2219, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:bce33477-efd6-4bad-8e32-e19873369ae7": {"content": "How to use asyncio.create_task in asyncio\n`asyncio.asyncio.create_task` schedules a coroutine as a Task in the event loop. Example usage:\n```python\ntask = asyncio.create_task(send_email(user))\nawait task\n```\nThis is useful when fire-and-forget background tasks, concurrent task management. Performance tip: use TaskGroup (Python 3.11+) for structured concurrency.\nCode signature: asyncio.create_task", "file_path": "https://docs.asyncio.org/en/stable/asyncio/create_task.html", "function_name": "asyncio.create_task", "type": "documentation", "metadata": {"record_id": "bce33477-efd6-4bad-8e32-e19873369ae7", "source_type": "documentation", "domain": "Asyncio/Threading", "library": "asyncio", "title": "How to use asyncio.create_task in asyncio", "content": "`asyncio.asyncio.create_task` schedules a coroutine as a Task in the event loop. Example usage:\n```python\ntask = asyncio.create_task(send_email(user))\nawait task\n```\nThis is useful when fire-and-forget background tasks, concurrent task management. Performance tip: use TaskGroup (Python 3.11+) for structured concurrency.", "code_signature": "asyncio.create_task", "tags": "race-condition, asyncio, lock, await, executor, gather", "url": "https://docs.asyncio.org/en/stable/asyncio/create_task.html", "author": "Official Docs", "date_published": "04-11-2018", "votes_or_stars": 4144, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:d6a203e6-350c-4f5b-9d9d-e77809b60d4f": {"content": "FEAT: Django: add support for async ORM queries\nProblem: Django ORM is synchronous; using it in async views requires sync_to_async wrapper.\n\nFix/Discussion: Django 4.1+ added native async ORM support. Use `await Model.objects.filter().afirst()`, `async for obj in qs:`. For older versions: wrap with `sync_to_async`: `get_user = sync_to_async(User.objects.get)\nuser = await get_user(pk=1)`.\nCode signature: status:closed", "file_path": "https://github.com/django/django/issues/8618", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "d6a203e6-350c-4f5b-9d9d-e77809b60d4f", "source_type": "github_issue", "domain": "FastAPI/Flask/Django", "library": "django", "title": "FEAT: Django: add support for async ORM queries", "content": "Problem: Django ORM is synchronous; using it in async views requires sync_to_async wrapper.\n\nFix/Discussion: Django 4.1+ added native async ORM support. Use `await Model.objects.filter().afirst()`, `async for obj in qs:`. For older versions: wrap with `sync_to_async`: `get_user = sync_to_async(User.objects.get)\nuser = await get_user(pk=1)`.", "code_signature": "status:closed", "tags": "dependency-injection, pydantic, orm, flask, response", "url": "https://github.com/django/django/issues/8618", "author": "contributor_8921", "date_published": "19-06-2024", "votes_or_stars": 21, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:bb78d29a-67d1-4c70-8284-a41cdac5a5d7": {"content": "os Environment Variables — official reference\nOfficial documentation for os Environment Variables. The method signature is `os.environ(key)`. provides a mapping of the current environment variables. Raises `KeyError` when key is accessed with [] notation and is not set. See also: dotenv, os.getenv, os.putenv.\nCode signature: os.environ(key)", "file_path": "https://docs.os.org/en/stable/os/environ.html", "function_name": "os.environ(key)", "type": "documentation", "metadata": {"record_id": "bb78d29a-67d1-4c70-8284-a41cdac5a5d7", "source_type": "documentation", "domain": "OS/Subprocess/File I/O", "library": "os", "title": "os Environment Variables — official reference", "content": "Official documentation for os Environment Variables. The method signature is `os.environ(key)`. provides a mapping of the current environment variables. Raises `KeyError` when key is accessed with [] notation and is not set. See also: dotenv, os.getenv, os.putenv.", "code_signature": "os.environ(key)", "tags": "pathlib, subprocess, stderr", "url": "https://docs.os.org/en/stable/os/environ.html", "author": "Official Docs", "date_published": "02-11-2018", "votes_or_stars": 2265, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:b37ff177-3fcf-46b4-b4a1-8167020de213": {"content": "PERF: concurrent.futures.ProcessPoolExecutor slow startup on Windows\nProblem: ProcessPoolExecutor takes 3-5 seconds to start on Windows due to process spawning overhead.\n\nFix/Discussion: Windows uses 'spawn' start method (vs 'fork' on Linux). Mitigations: (1) Reuse executor across calls — don't create/destroy in loops. (2) Use 'forkserver' or 'fork' on Linux/macOS. (3) For short tasks, ThreadPoolExecutor may be faster. (4) Use `initializer` param to pre-load modules.\nCode signature: status:closed", "file_path": "https://github.com/cpython/cpython/issues/6647", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "b37ff177-3fcf-46b4-b4a1-8167020de213", "source_type": "github_issue", "domain": "Asyncio/Threading", "library": "cpython", "title": "PERF: concurrent.futures.ProcessPoolExecutor slow startup on Windows", "content": "Problem: ProcessPoolExecutor takes 3-5 seconds to start on Windows due to process spawning overhead.\n\nFix/Discussion: Windows uses 'spawn' start method (vs 'fork' on Linux). Mitigations: (1) Reuse executor across calls — don't create/destroy in loops. (2) Use 'forkserver' or 'fork' on Linux/macOS. (3) For short tasks, ThreadPoolExecutor may be faster. (4) Use `initializer` param to pre-load modules.", "code_signature": "status:closed", "tags": "executor, gather, event-loop, lock", "url": "https://github.com/cpython/cpython/issues/6647", "author": "contributor_4097", "date_published": "27-08-2020", "votes_or_stars": 258, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:28660e2a-97c8-49af-8330-126727b30782": {"content": "How to use @app.route in Flask\n`Flask.@app.route` maps a URL rule to a view function. Example usage:\n```python\n@app.route('/users/<int:uid>')\ndef get_user(uid):\n    return jsonify(user)\n```\nThis is useful when defining HTTP endpoints in a Flask application. Performance tip: use Blueprints to organize large applications.\nCode signature: @app.route", "file_path": "https://docs.flask.org/en/stable/@app/route.html", "function_name": "@app.route", "type": "documentation", "metadata": {"record_id": "28660e2a-97c8-49af-8330-126727b30782", "source_type": "documentation", "domain": "FastAPI/Flask/Django", "library": "Flask", "title": "How to use @app.route in Flask", "content": "`Flask.@app.route` maps a URL rule to a view function. Example usage:\n```python\n@app.route('/users/<int:uid>')\ndef get_user(uid):\n    return jsonify(user)\n```\nThis is useful when defining HTTP endpoints in a Flask application. Performance tip: use Blueprints to organize large applications.", "code_signature": "@app.route", "tags": "rest, flask, orm, request", "url": "https://docs.flask.org/en/stable/@app/route.html", "author": "Official Docs", "date_published": "02-04-2019", "votes_or_stars": 2987, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:f54fd18c-4a3d-477c-a02e-e8d212bf5743": {"content": "NumPy broadcasting error: operands could not be broadcast with shapes\nNumPy broadcasts shapes right-to-left. Arrays (3,4) and (4,) are compatible. Arrays (3,4) and (3,) are not — reshape to (3,1). Use `arr.reshape(-1,1)` to add a dimension. Always print `a.shape, b.shape` before operations to debug. `np.expand_dims(arr, axis=1)` is cleaner than reshape for adding dimensions.\nCode signature: Shapes must be compatible: dimensions must match or one of them must be 1.", "file_path": "https://stackoverflow.com/questions/4860684", "function_name": "Shapes must be compatible: dimensions must match or one of them must be 1.", "type": "stack_overflow", "metadata": {"record_id": "f54fd18c-4a3d-477c-a02e-e8d212bf5743", "source_type": "stack_overflow", "domain": "NumPy/Pandas", "library": "numpy", "title": "NumPy broadcasting error: operands could not be broadcast with shapes", "content": "NumPy broadcasts shapes right-to-left. Arrays (3,4) and (4,) are compatible. Arrays (3,4) and (3,) are not — reshape to (3,1). Use `arr.reshape(-1,1)` to add a dimension. Always print `a.shape, b.shape` before operations to debug. `np.expand_dims(arr, axis=1)` is cleaner than reshape for adding dimensions.", "code_signature": "Shapes must be compatible: dimensions must match or one of them must be 1.", "tags": "loc, numpy, dtype, broadcasting, vectorization", "url": "https://stackoverflow.com/questions/4860684", "author": "user_627024", "date_published": "26-03-2023", "votes_or_stars": 1409, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:7adee526-0bee-42e6-ba48-41c9013fe864": {"content": "How to stream large HTTP responses with requests?\n```python\nwith requests.get(url, stream=True) as r:\n    r.raise_for_status()\n    with open('file', 'wb') as f:\n        for chunk in r.iter_content(chunk_size=8192):\n            f.write(chunk)\n```\nAlways use as context manager with stream=True to ensure connection is released.\nCode signature: Use stream=True and iterate over response.iter_content() or iter_lines().", "file_path": "https://stackoverflow.com/questions/3592974", "function_name": "Use stream=True and iterate over response.iter_content() or iter_lines().", "type": "stack_overflow", "metadata": {"record_id": "7adee526-0bee-42e6-ba48-41c9013fe864", "source_type": "stack_overflow", "domain": "Requests/HTTP/APIs", "library": "json", "title": "How to stream large HTTP responses with requests?", "content": "```python\nwith requests.get(url, stream=True) as r:\n    r.raise_for_status()\n    with open('file', 'wb') as f:\n        for chunk in r.iter_content(chunk_size=8192):\n            f.write(chunk)\n```\nAlways use as context manager with stream=True to ensure connection is released.", "code_signature": "Use stream=True and iterate over response.iter_content() or iter_lines().", "tags": "aiohttp, timeout, authentication", "url": "https://stackoverflow.com/questions/3592974", "author": "user_980733", "date_published": "21-12-2019", "votes_or_stars": 1651, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:938c0230-0b42-405a-9c08-7ed7f39070c2": {"content": "PERF: concurrent.futures.ProcessPoolExecutor slow startup on Windows\nProblem: ProcessPoolExecutor takes 3-5 seconds to start on Windows due to process spawning overhead.\n\nFix/Discussion: Windows uses 'spawn' start method (vs 'fork' on Linux). Mitigations: (1) Reuse executor across calls — don't create/destroy in loops. (2) Use 'forkserver' or 'fork' on Linux/macOS. (3) For short tasks, ThreadPoolExecutor may be faster. (4) Use `initializer` param to pre-load modules.\nCode signature: status:closed", "file_path": "https://github.com/cpython/cpython/issues/6647", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "938c0230-0b42-405a-9c08-7ed7f39070c2", "source_type": "github_issue", "domain": "Asyncio/Threading", "library": "cpython", "title": "PERF: concurrent.futures.ProcessPoolExecutor slow startup on Windows", "content": "Problem: ProcessPoolExecutor takes 3-5 seconds to start on Windows due to process spawning overhead.\n\nFix/Discussion: Windows uses 'spawn' start method (vs 'fork' on Linux). Mitigations: (1) Reuse executor across calls — don't create/destroy in loops. (2) Use 'forkserver' or 'fork' on Linux/macOS. (3) For short tasks, ThreadPoolExecutor may be faster. (4) Use `initializer` param to pre-load modules.", "code_signature": "status:closed", "tags": "asyncio, await, race-condition, deadlock", "url": "https://github.com/cpython/cpython/issues/6647", "author": "contributor_4097", "date_published": "20-02-2024", "votes_or_stars": 245, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:7d46c0e4-01d5-4074-9b90-a9ce3f436367": {"content": "SQLAlchemy: how to avoid DetachedInstanceError?\nDetachedInstanceError occurs when accessing a relationship after the session is closed. Solutions: (1) Use `joinedload()` or `subqueryload()` to eagerly load relationships. (2) Keep session open while accessing objects. (3) Use `expire_on_commit=False` on sessionmaker. (4) Convert to dict/DTO before closing session.\nCode signature: Access lazy-loaded relationships within the session scope, or use eager loading.", "file_path": "https://stackoverflow.com/questions/1247595", "function_name": "Access lazy-loaded relationships within the session scope, or use eager loading.", "type": "stack_overflow", "metadata": {"record_id": "7d46c0e4-01d5-4074-9b90-a9ce3f436367", "source_type": "stack_overflow", "domain": "SQLAlchemy/Databases", "library": "sqlalchemy", "title": "SQLAlchemy: how to avoid DetachedInstanceError?", "content": "DetachedInstanceError occurs when accessing a relationship after the session is closed. Solutions: (1) Use `joinedload()` or `subqueryload()` to eagerly load relationships. (2) Keep session open while accessing objects. (3) Use `expire_on_commit=False` on sessionmaker. (4) Convert to dict/DTO before closing session.", "code_signature": "Access lazy-loaded relationships within the session scope, or use eager loading.", "tags": "orm, connection-pool, relationship, sqlalchemy, alembic", "url": "https://stackoverflow.com/questions/1247595", "author": "user_107793", "date_published": "01-04-2021", "votes_or_stars": 431, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:d7bbe627-ef23-41cf-b23b-6c83ae0aede0": {"content": "BUG: Alembic autogenerate misses index changes\nProblem: Alembic --autogenerate does not detect changes to indexes or constraints on existing tables.\n\nFix/Discussion: Alembic autogenerate compares metadata to database state but has limitations. Workarounds: (1) Manually add `op.create_index()` to migration. (2) Use `include_schemas=True` in env.py. (3) Run `alembic check` to see detected changes. (4) Always review autogenerated migrations before applying.\nCode signature: status:open", "file_path": "https://github.com/alembic/alembic/issues/7344", "function_name": "status:open", "type": "github_issue", "metadata": {"record_id": "d7bbe627-ef23-41cf-b23b-6c83ae0aede0", "source_type": "github_issue", "domain": "SQLAlchemy/Databases", "library": "alembic", "title": "BUG: Alembic autogenerate misses index changes", "content": "Problem: Alembic --autogenerate does not detect changes to indexes or constraints on existing tables.\n\nFix/Discussion: Alembic autogenerate compares metadata to database state but has limitations. Workarounds: (1) Manually add `op.create_index()` to migration. (2) Use `include_schemas=True` in env.py. (3) Run `alembic check` to see detected changes. (4) Always review autogenerated migrations before applying.", "code_signature": "status:open", "tags": "sqlite, orm, session, sqlalchemy, connection-pool, migration", "url": "https://github.com/alembic/alembic/issues/7344", "author": "contributor_3601", "date_published": "02-03-2019", "votes_or_stars": 214, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:27026465-be3c-4c99-8c42-bad7fc0e8480": {"content": "Why is my Pandas merge creating duplicate rows?\nDuplicate rows after merge usually mean the join key is not unique. Diagnose with `df.duplicated(subset=['key']).sum()`. Use `how='left'` carefully when right table has multiple matches. Set `validate='one_to_many'` or `'many_to_one'` to enforce cardinality. Consider deduplicating with `df.drop_duplicates(subset=['key'])` before merging.\nCode signature: Your merge key has duplicates in one or both DataFrames. Use validate='one_to_one' to catch this early.", "file_path": "https://stackoverflow.com/questions/2321324", "function_name": "Your merge key has duplicates in one or both DataFrames. Use validate='one_to_one' to catch this early.", "type": "stack_overflow", "metadata": {"record_id": "27026465-be3c-4c99-8c42-bad7fc0e8480", "source_type": "stack_overflow", "domain": "NumPy/Pandas", "library": "array", "title": "Why is my Pandas merge creating duplicate rows?", "content": "Duplicate rows after merge usually mean the join key is not unique. Diagnose with `df.duplicated(subset=['key']).sum()`. Use `how='left'` carefully when right table has multiple matches. Set `validate='one_to_many'` or `'many_to_one'` to enforce cardinality. Consider deduplicating with `df.drop_duplicates(subset=['key'])` before merging.", "code_signature": "Your merge key has duplicates in one or both DataFrames. Use validate='one_to_one' to catch this early.", "tags": "groupby, pivot, loc", "url": "https://stackoverflow.com/questions/2321324", "author": "user_99814", "date_published": "03-05-2022", "votes_or_stars": 231, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:073cc390-ab10-4164-a434-b2c4ea60856e": {"content": "Why is my Pandas merge creating duplicate rows?\nDuplicate rows after merge usually mean the join key is not unique. Diagnose with `df.duplicated(subset=['key']).sum()`. Use `how='left'` carefully when right table has multiple matches. Set `validate='one_to_many'` or `'many_to_one'` to enforce cardinality. Consider deduplicating with `df.drop_duplicates(subset=['key'])` before merging.\nCode signature: Your merge key has duplicates in one or both DataFrames. Use validate='one_to_one' to catch this early.", "file_path": "https://stackoverflow.com/questions/2321324", "function_name": "Your merge key has duplicates in one or both DataFrames. Use validate='one_to_one' to catch this early.", "type": "stack_overflow", "metadata": {"record_id": "073cc390-ab10-4164-a434-b2c4ea60856e", "source_type": "stack_overflow", "domain": "NumPy/Pandas", "library": "array", "title": "Why is my Pandas merge creating duplicate rows?", "content": "Duplicate rows after merge usually mean the join key is not unique. Diagnose with `df.duplicated(subset=['key']).sum()`. Use `how='left'` carefully when right table has multiple matches. Set `validate='one_to_many'` or `'many_to_one'` to enforce cardinality. Consider deduplicating with `df.drop_duplicates(subset=['key'])` before merging.", "code_signature": "Your merge key has duplicates in one or both DataFrames. Use validate='one_to_one' to catch this early.", "tags": "broadcasting, reshape, vectorization, array", "url": "https://stackoverflow.com/questions/2321324", "author": "user_99814", "date_published": "14-06-2023", "votes_or_stars": 247, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:8882d998-f299-4cd3-b9d9-51575dbfa4bd": {"content": "subprocess — Process Execution\nThe `subprocess.run` function in subprocess runs a subprocess and waits for it to complete. It accepts `args, capture_output=False, timeout=None` as a parameter and returns CompletedProcess. Common use cases include executing shell commands, scripts, or external programs from Python. Note that use shell=False (default) to avoid shell injection vulnerabilities.\nCode signature: subprocess.run(args, capture_output=False, timeout=None) -> CompletedProcess", "file_path": "https://docs.subprocess.org/en/stable/subprocess/run.html", "function_name": "subprocess.run(args, capture_output=False, timeout=None) -> CompletedProcess", "type": "documentation", "metadata": {"record_id": "8882d998-f299-4cd3-b9d9-51575dbfa4bd", "source_type": "documentation", "domain": "OS/Subprocess/File I/O", "library": "subprocess", "title": "subprocess — Process Execution", "content": "The `subprocess.run` function in subprocess runs a subprocess and waits for it to complete. It accepts `args, capture_output=False, timeout=None` as a parameter and returns CompletedProcess. Common use cases include executing shell commands, scripts, or external programs from Python. Note that use shell=False (default) to avoid shell injection vulnerabilities.", "code_signature": "subprocess.run(args, capture_output=False, timeout=None) -> CompletedProcess", "tags": "os, shutil, stdout, tempfile, glob", "url": "https://docs.subprocess.org/en/stable/subprocess/run.html", "author": "Official Docs", "date_published": "10-10-2024", "votes_or_stars": 413, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:4ca6a5fb-b202-4cb0-96a0-654cf14a352c": {"content": "BUG: requests hangs indefinitely on slow server responses\nProblem: requests.get() hangs forever when server accepts connection but never sends response.\n\nFix/Discussion: Always set both connect and read timeouts: `requests.get(url, timeout=(3.05, 27))`. Tuple: (connect_timeout, read_timeout). Or single value for both. Mount a Session with default timeouts using a custom HTTPAdapter subclass. Never use `timeout=None` in production code.\nCode signature: status:closed", "file_path": "https://github.com/requests/requests/issues/4019", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "4ca6a5fb-b202-4cb0-96a0-654cf14a352c", "source_type": "github_issue", "domain": "Requests/HTTP/APIs", "library": "requests", "title": "BUG: requests hangs indefinitely on slow server responses", "content": "Problem: requests.get() hangs forever when server accepts connection but never sends response.\n\nFix/Discussion: Always set both connect and read timeouts: `requests.get(url, timeout=(3.05, 27))`. Tuple: (connect_timeout, read_timeout). Or single value for both. Mount a Session with default timeouts using a custom HTTPAdapter subclass. Never use `timeout=None` in production code.", "code_signature": "status:closed", "tags": "oauth, rest-api, retry", "url": "https://github.com/requests/requests/issues/4019", "author": "contributor_4599", "date_published": "07-12-2023", "votes_or_stars": 253, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:e8e66a94-ca00-49d3-88b9-b22698e1fbe1": {"content": "Python threading: how to share data between threads safely?\nFor simple counters: use `threading.Lock()` as context manager. For complex shared state: prefer `queue.Queue` (thread-safe by design). Avoid global variables without locks. `threading.local()` gives each thread its own data copy. For read-heavy workloads, `threading.RLock` allows re-entrant acquisition.\nCode signature: Use threading.Lock for mutual exclusion or queue.Queue for producer-consumer patterns.", "file_path": "https://stackoverflow.com/questions/3195773", "function_name": "Use threading.Lock for mutual exclusion or queue.Queue for producer-consumer patterns.", "type": "stack_overflow", "metadata": {"record_id": "e8e66a94-ca00-49d3-88b9-b22698e1fbe1", "source_type": "stack_overflow", "domain": "Asyncio/Threading", "library": "lock", "title": "Python threading: how to share data between threads safely?", "content": "For simple counters: use `threading.Lock()` as context manager. For complex shared state: prefer `queue.Queue` (thread-safe by design). Avoid global variables without locks. `threading.local()` gives each thread its own data copy. For read-heavy workloads, `threading.RLock` allows re-entrant acquisition.", "code_signature": "Use threading.Lock for mutual exclusion or queue.Queue for producer-consumer patterns.", "tags": "task, deadlock, asyncio, lock", "url": "https://stackoverflow.com/questions/3195773", "author": "user_714318", "date_published": "30-07-2024", "votes_or_stars": 1109, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:20fd9494-66a5-4d0d-8c1b-444763d0dcf9": {"content": "BUG: Flask session not persisting between requests in tests\nProblem: Session data set in one request is not available in the next request during testing.\n\nFix/Discussion: Use Flask test client's session_transaction() context manager: `with client.session_transaction() as sess:\n    sess['user_id'] = 1`. Ensure SECRET_KEY is set. For testing: `app.config['TESTING'] = True; app.config['SECRET_KEY'] = 'test'`.\nCode signature: status:closed", "file_path": "https://github.com/flask/flask/issues/2141", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "20fd9494-66a5-4d0d-8c1b-444763d0dcf9", "source_type": "github_issue", "domain": "FastAPI/Flask/Django", "library": "flask", "title": "BUG: Flask session not persisting between requests in tests", "content": "Problem: Session data set in one request is not available in the next request during testing.\n\nFix/Discussion: Use Flask test client's session_transaction() context manager: `with client.session_transaction() as sess:\n    sess['user_id'] = 1`. Ensure SECRET_KEY is set. For testing: `app.config['TESTING'] = True; app.config['SECRET_KEY'] = 'test'`.", "code_signature": "status:closed", "tags": "flask, orm, fastapi, rest, dependency-injection", "url": "https://github.com/flask/flask/issues/2141", "author": "contributor_5020", "date_published": "01-06-2021", "votes_or_stars": 193, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:2e2c2f28-b2f9-4ab9-979c-318e11ce3f68": {"content": "Django ORM Queries — official reference\nOfficial documentation for Django ORM Queries. The method signature is `QuerySet.filter(**kwargs)`. filters QuerySet rows matching given field lookups. Raises `FieldError` when an unknown field lookup is provided. See also: QuerySet.exclude, QuerySet.annotate, Q objects.\nCode signature: QuerySet.filter(**kwargs)", "file_path": "https://docs.django.org/en/stable/QuerySet/filter.html", "function_name": "QuerySet.filter(**kwargs)", "type": "documentation", "metadata": {"record_id": "2e2c2f28-b2f9-4ab9-979c-318e11ce3f68", "source_type": "documentation", "domain": "FastAPI/Flask/Django", "library": "Django", "title": "Django ORM Queries — official reference", "content": "Official documentation for Django ORM Queries. The method signature is `QuerySet.filter(**kwargs)`. filters QuerySet rows matching given field lookups. Raises `FieldError` when an unknown field lookup is provided. See also: QuerySet.exclude, QuerySet.annotate, Q objects.", "code_signature": "QuerySet.filter(**kwargs)", "tags": "rest, response, fastapi, request, routing", "url": "https://docs.django.org/en/stable/QuerySet/filter.html", "author": "Official Docs", "date_published": "02-12-2022", "votes_or_stars": 2432, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:6b3f609a-791b-478c-8a5d-18d43764379a": {"content": "How to use httpx.AsyncClient in httpx\n`httpx.httpx.AsyncClient` provides an async HTTP client for non-blocking requests. Example usage:\n```python\nasync with httpx.AsyncClient() as client:\n    r = await client.get(url)\n    data = r.json()\n```\nThis is useful when making concurrent HTTP requests in async applications. Performance tip: set timeout=httpx.Timeout(10.0) to prevent hanging requests.\nCode signature: httpx.AsyncClient", "file_path": "https://docs.httpx.org/en/stable/httpx/AsyncClient.html", "function_name": "httpx.AsyncClient", "type": "documentation", "metadata": {"record_id": "6b3f609a-791b-478c-8a5d-18d43764379a", "source_type": "documentation", "domain": "Requests/HTTP/APIs", "library": "httpx", "title": "How to use httpx.AsyncClient in httpx", "content": "`httpx.httpx.AsyncClient` provides an async HTTP client for non-blocking requests. Example usage:\n```python\nasync with httpx.AsyncClient() as client:\n    r = await client.get(url)\n    data = r.json()\n```\nThis is useful when making concurrent HTTP requests in async applications. Performance tip: set timeout=httpx.Timeout(10.0) to prevent hanging requests.", "code_signature": "httpx.AsyncClient", "tags": "timeout, rate-limiting, requests, oauth, aiohttp", "url": "https://docs.httpx.org/en/stable/httpx/AsyncClient.html", "author": "Official Docs", "date_published": "05-09-2023", "votes_or_stars": 4182, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:78a8b902-923f-4862-abb2-8a82cc210861": {"content": "How to implement JWT authentication in FastAPI?\n```python\nfrom fastapi.security import OAuth2PasswordBearer\nfrom jose import jwt\noauth2_scheme = OAuth2PasswordBearer(tokenUrl='token')\n```\nCreate access tokens with `jwt.encode(payload, SECRET_KEY, algorithm='HS256')`. Verify with `jwt.decode(token, SECRET_KEY, algorithms=['HS256'])`. Store SECRET_KEY in environment variables, never in code.\nCode signature: Use python-jose for JWT and passlib for password hashing with OAuth2PasswordBearer.", "file_path": "https://stackoverflow.com/questions/7754864", "function_name": "Use python-jose for JWT and passlib for password hashing with OAuth2PasswordBearer.", "type": "stack_overflow", "metadata": {"record_id": "78a8b902-923f-4862-abb2-8a82cc210861", "source_type": "stack_overflow", "domain": "FastAPI/Flask/Django", "library": "fastapi", "title": "How to implement JWT authentication in FastAPI?", "content": "```python\nfrom fastapi.security import OAuth2PasswordBearer\nfrom jose import jwt\noauth2_scheme = OAuth2PasswordBearer(tokenUrl='token')\n```\nCreate access tokens with `jwt.encode(payload, SECRET_KEY, algorithm='HS256')`. Verify with `jwt.decode(token, SECRET_KEY, algorithms=['HS256'])`. Store SECRET_KEY in environment variables, never in code.", "code_signature": "Use python-jose for JWT and passlib for password hashing with OAuth2PasswordBearer.", "tags": "dependency-injection, routing, flask, middleware", "url": "https://stackoverflow.com/questions/7754864", "author": "user_773587", "date_published": "12-02-2023", "votes_or_stars": 388, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:f2a60b88-b89d-4565-b952-8bba93d1775c": {"content": "subprocess — Process Execution\nThe `subprocess.run` function in subprocess runs a subprocess and waits for it to complete. It accepts `args, capture_output=False, timeout=None` as a parameter and returns CompletedProcess. Common use cases include executing shell commands, scripts, or external programs from Python. Note that use shell=False (default) to avoid shell injection vulnerabilities.\nCode signature: subprocess.run(args, capture_output=False, timeout=None) -> CompletedProcess", "file_path": "https://docs.subprocess.org/en/stable/subprocess/run.html", "function_name": "subprocess.run(args, capture_output=False, timeout=None) -> CompletedProcess", "type": "documentation", "metadata": {"record_id": "f2a60b88-b89d-4565-b952-8bba93d1775c", "source_type": "documentation", "domain": "OS/Subprocess/File I/O", "library": "subprocess", "title": "subprocess — Process Execution", "content": "The `subprocess.run` function in subprocess runs a subprocess and waits for it to complete. It accepts `args, capture_output=False, timeout=None` as a parameter and returns CompletedProcess. Common use cases include executing shell commands, scripts, or external programs from Python. Note that use shell=False (default) to avoid shell injection vulnerabilities.", "code_signature": "subprocess.run(args, capture_output=False, timeout=None) -> CompletedProcess", "tags": "shutil, stdin, pathlib, stdout", "url": "https://docs.subprocess.org/en/stable/subprocess/run.html", "author": "Official Docs", "date_published": "25-03-2024", "votes_or_stars": 429, "relevance_label": "low", "difficulty_level": "intermediate"}}, "kb:796e34e4-0c37-4c9b-81d7-07f4ed854136": {"content": "FEAT: pathlib: add support for reading/writing with encoding shorthand\nProblem: pathlib.Path.read_text() does not default to UTF-8, causing issues across platforms.\n\nFix/Discussion: Always pass encoding explicitly: `Path('file').read_text(encoding='utf-8')`. Python 3.10+ added `encoding='locale'` default change proposal. Best practice: always specify encoding in all file I/O operations for portability.\nCode signature: status:closed", "file_path": "https://github.com/cpython/cpython/issues/2707", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "796e34e4-0c37-4c9b-81d7-07f4ed854136", "source_type": "github_issue", "domain": "OS/Subprocess/File I/O", "library": "cpython", "title": "FEAT: pathlib: add support for reading/writing with encoding shorthand", "content": "Problem: pathlib.Path.read_text() does not default to UTF-8, causing issues across platforms.\n\nFix/Discussion: Always pass encoding explicitly: `Path('file').read_text(encoding='utf-8')`. Python 3.10+ added `encoding='locale'` default change proposal. Best practice: always specify encoding in all file I/O operations for portability.", "code_signature": "status:closed", "tags": "env-variable, file-io, glob, stdin, os", "url": "https://github.com/cpython/cpython/issues/2707", "author": "contributor_7877", "date_published": "05-12-2022", "votes_or_stars": 489, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:532002f5-f670-499f-9d07-bd87872efc96": {"content": "BUG: SQLAlchemy connection pool exhaustion under load\nProblem: Application hangs under concurrent load — all connections are checked out and pool is exhausted.\n\nFix/Discussion: Increase `pool_size` and `max_overflow`. Ensure sessions are always closed: use `contextmanager` or `with Session() as session:`. Set `pool_timeout` to fail fast. Enable `pool_pre_ping=True`. Monitor with `engine.pool.checkedout()`. In async: use `AsyncSession` with `async_scoped_session`.\nCode signature: status:closed", "file_path": "https://github.com/sqlalchemy/sqlalchemy/issues/106", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "532002f5-f670-499f-9d07-bd87872efc96", "source_type": "github_issue", "domain": "SQLAlchemy/Databases", "library": "sqlalchemy", "title": "BUG: SQLAlchemy connection pool exhaustion under load", "content": "Problem: Application hangs under concurrent load — all connections are checked out and pool is exhausted.\n\nFix/Discussion: Increase `pool_size` and `max_overflow`. Ensure sessions are always closed: use `contextmanager` or `with Session() as session:`. Set `pool_timeout` to fail fast. Enable `pool_pre_ping=True`. Monitor with `engine.pool.checkedout()`. In async: use `AsyncSession` with `async_scoped_session`.", "code_signature": "status:closed", "tags": "foreign-key, orm, session", "url": "https://github.com/sqlalchemy/sqlalchemy/issues/106", "author": "contributor_5078", "date_published": "28-01-2018", "votes_or_stars": 278, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:dcbd8a8a-af77-43a5-95d8-e58b4edcf8a1": {"content": "How to add retry logic to requests?\n```python\nfrom requests.adapters import HTTPAdapter\nfrom urllib3.util.retry import Retry\nretry = Retry(total=3, backoff_factor=1, status_forcelist=[500,502,503,504])\nadapter = HTTPAdapter(max_retries=retry)\nsession.mount('https://', adapter)\n```\nCode signature: Mount an HTTPAdapter with urllib3 Retry on the Session.", "file_path": "https://stackoverflow.com/questions/1669324", "function_name": "Mount an HTTPAdapter with urllib3 Retry on the Session.", "type": "stack_overflow", "metadata": {"record_id": "dcbd8a8a-af77-43a5-95d8-e58b4edcf8a1", "source_type": "stack_overflow", "domain": "Requests/HTTP/APIs", "library": "websocket", "title": "How to add retry logic to requests?", "content": "```python\nfrom requests.adapters import HTTPAdapter\nfrom urllib3.util.retry import Retry\nretry = Retry(total=3, backoff_factor=1, status_forcelist=[500,502,503,504])\nadapter = HTTPAdapter(max_retries=retry)\nsession.mount('https://', adapter)\n```", "code_signature": "Mount an HTTPAdapter with urllib3 Retry on the Session.", "tags": "rest-api, timeout, headers", "url": "https://stackoverflow.com/questions/1669324", "author": "user_952590", "date_published": "28-09-2019", "votes_or_stars": 1532, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:590db587-625e-4687-8dad-3d3cd0d74ec6": {"content": "Django ORM Queries — official reference\nOfficial documentation for Django ORM Queries. The method signature is `QuerySet.filter(**kwargs)`. filters QuerySet rows matching given field lookups. Raises `FieldError` when an unknown field lookup is provided. See also: QuerySet.exclude, QuerySet.annotate, Q objects.\nCode signature: QuerySet.filter(**kwargs)", "file_path": "https://docs.django.org/en/stable/QuerySet/filter.html", "function_name": "QuerySet.filter(**kwargs)", "type": "documentation", "metadata": {"record_id": "590db587-625e-4687-8dad-3d3cd0d74ec6", "source_type": "documentation", "domain": "FastAPI/Flask/Django", "library": "Django", "title": "Django ORM Queries — official reference", "content": "Official documentation for Django ORM Queries. The method signature is `QuerySet.filter(**kwargs)`. filters QuerySet rows matching given field lookups. Raises `FieldError` when an unknown field lookup is provided. See also: QuerySet.exclude, QuerySet.annotate, Q objects.", "code_signature": "QuerySet.filter(**kwargs)", "tags": "dependency-injection, response, rest, orm, routing", "url": "https://docs.django.org/en/stable/QuerySet/filter.html", "author": "Official Docs", "date_published": "06-05-2022", "votes_or_stars": 2432, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:d8488e6a-6fb6-423b-8ab5-4e8f1ee958b9": {"content": "subprocess — Process Execution\nThe `subprocess.run` function in subprocess runs a subprocess and waits for it to complete. It accepts `args, capture_output=False, timeout=None` as a parameter and returns CompletedProcess. Common use cases include executing shell commands, scripts, or external programs from Python. Note that use shell=False (default) to avoid shell injection vulnerabilities.\nCode signature: subprocess.run(args, capture_output=False, timeout=None) -> CompletedProcess", "file_path": "https://docs.subprocess.org/en/stable/subprocess/run.html", "function_name": "subprocess.run(args, capture_output=False, timeout=None) -> CompletedProcess", "type": "documentation", "metadata": {"record_id": "d8488e6a-6fb6-423b-8ab5-4e8f1ee958b9", "source_type": "documentation", "domain": "OS/Subprocess/File I/O", "library": "subprocess", "title": "subprocess — Process Execution", "content": "The `subprocess.run` function in subprocess runs a subprocess and waits for it to complete. It accepts `args, capture_output=False, timeout=None` as a parameter and returns CompletedProcess. Common use cases include executing shell commands, scripts, or external programs from Python. Note that use shell=False (default) to avoid shell injection vulnerabilities.", "code_signature": "subprocess.run(args, capture_output=False, timeout=None) -> CompletedProcess", "tags": "stderr, process, shutil, pipe, subprocess, pathlib", "url": "https://docs.subprocess.org/en/stable/subprocess/run.html", "author": "Official Docs", "date_published": "24-01-2022", "votes_or_stars": 440, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:06d5f02d-053f-41ed-8eba-1b5369eef175": {"content": "os Environment Variables — official reference\nOfficial documentation for os Environment Variables. The method signature is `os.environ(key)`. provides a mapping of the current environment variables. Raises `KeyError` when key is accessed with [] notation and is not set. See also: dotenv, os.getenv, os.putenv.\nCode signature: os.environ(key)", "file_path": "https://docs.os.org/en/stable/os/environ.html", "function_name": "os.environ(key)", "type": "documentation", "metadata": {"record_id": "06d5f02d-053f-41ed-8eba-1b5369eef175", "source_type": "documentation", "domain": "OS/Subprocess/File I/O", "library": "os", "title": "os Environment Variables — official reference", "content": "Official documentation for os Environment Variables. The method signature is `os.environ(key)`. provides a mapping of the current environment variables. Raises `KeyError` when key is accessed with [] notation and is not set. See also: dotenv, os.getenv, os.putenv.", "code_signature": "os.environ(key)", "tags": "process, os, stdin, subprocess, stderr", "url": "https://docs.os.org/en/stable/os/environ.html", "author": "Official Docs", "date_published": "17-03-2024", "votes_or_stars": 2235, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:01b4bcae-84ce-4ae5-8c8d-1a2f09d27ea9": {"content": "How to use create_engine in SQLAlchemy\n`SQLAlchemy.create_engine` creates a database engine representing a connection pool. Example usage:\n```python\nengine = create_engine('postgresql://user:pass@host/db', echo=True)\n```\nThis is useful when establishing the central database connection for an application. Performance tip: set pool_size and max_overflow based on expected concurrency.\nCode signature: create_engine", "file_path": "https://docs.sqlalchemy.org/en/stable/create_engine.html", "function_name": "create_engine", "type": "documentation", "metadata": {"record_id": "01b4bcae-84ce-4ae5-8c8d-1a2f09d27ea9", "source_type": "documentation", "domain": "SQLAlchemy/Databases", "library": "SQLAlchemy", "title": "How to use create_engine in SQLAlchemy", "content": "`SQLAlchemy.create_engine` creates a database engine representing a connection pool. Example usage:\n```python\nengine = create_engine('postgresql://user:pass@host/db', echo=True)\n```\nThis is useful when establishing the central database connection for an application. Performance tip: set pool_size and max_overflow based on expected concurrency.", "code_signature": "create_engine", "tags": "connection-pool, transaction, migration, session", "url": "https://docs.sqlalchemy.org/en/stable/create_engine.html", "author": "Official Docs", "date_published": "24-04-2023", "votes_or_stars": 2067, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:a0b25496-990e-41fc-bc7a-bb0d8f9f73c4": {"content": "BUG: SQLAlchemy connection pool exhaustion under load\nProblem: Application hangs under concurrent load — all connections are checked out and pool is exhausted.\n\nFix/Discussion: Increase `pool_size` and `max_overflow`. Ensure sessions are always closed: use `contextmanager` or `with Session() as session:`. Set `pool_timeout` to fail fast. Enable `pool_pre_ping=True`. Monitor with `engine.pool.checkedout()`. In async: use `AsyncSession` with `async_scoped_session`.\nCode signature: status:closed", "file_path": "https://github.com/sqlalchemy/sqlalchemy/issues/106", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "a0b25496-990e-41fc-bc7a-bb0d8f9f73c4", "source_type": "github_issue", "domain": "SQLAlchemy/Databases", "library": "sqlalchemy", "title": "BUG: SQLAlchemy connection pool exhaustion under load", "content": "Problem: Application hangs under concurrent load — all connections are checked out and pool is exhausted.\n\nFix/Discussion: Increase `pool_size` and `max_overflow`. Ensure sessions are always closed: use `contextmanager` or `with Session() as session:`. Set `pool_timeout` to fail fast. Enable `pool_pre_ping=True`. Monitor with `engine.pool.checkedout()`. In async: use `AsyncSession` with `async_scoped_session`.", "code_signature": "status:closed", "tags": "orm, connection-pool, postgresql, session", "url": "https://github.com/sqlalchemy/sqlalchemy/issues/106", "author": "contributor_5078", "date_published": "14-12-2019", "votes_or_stars": 256, "relevance_label": "low", "difficulty_level": "intermediate"}}, "kb:e84459ce-ae65-4f4c-8f51-67ff4a055b71": {"content": "os Environment Variables — official reference\nOfficial documentation for os Environment Variables. The method signature is `os.environ(key)`. provides a mapping of the current environment variables. Raises `KeyError` when key is accessed with [] notation and is not set. See also: dotenv, os.getenv, os.putenv.\nCode signature: os.environ(key)", "file_path": "https://docs.os.org/en/stable/os/environ.html", "function_name": "os.environ(key)", "type": "documentation", "metadata": {"record_id": "e84459ce-ae65-4f4c-8f51-67ff4a055b71", "source_type": "documentation", "domain": "OS/Subprocess/File I/O", "library": "os", "title": "os Environment Variables — official reference", "content": "Official documentation for os Environment Variables. The method signature is `os.environ(key)`. provides a mapping of the current environment variables. Raises `KeyError` when key is accessed with [] notation and is not set. See also: dotenv, os.getenv, os.putenv.", "code_signature": "os.environ(key)", "tags": "tempfile, pathlib, shutil, file-io, env-variable, process", "url": "https://docs.os.org/en/stable/os/environ.html", "author": "Official Docs", "date_published": "13-03-2019", "votes_or_stars": 2230, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:5be5fd63-6768-4cf6-937b-21ddf2d2f293": {"content": "Flask: how to return JSON responses properly?\n`return jsonify({'key': 'value'})` sets Content-Type to application/json. Flask 1.0+ automatically converts returned dicts to JSON responses. For custom status codes: `return jsonify(data), 201`. For errors: use `abort(404)` or return `({'error': 'Not found'}, 404)`.\nCode signature: Use jsonify() or return a dict directly (Flask 1.0+).", "file_path": "https://stackoverflow.com/questions/8930103", "function_name": "Use jsonify() or return a dict directly (Flask 1.0+).", "type": "stack_overflow", "metadata": {"record_id": "5be5fd63-6768-4cf6-937b-21ddf2d2f293", "source_type": "stack_overflow", "domain": "FastAPI/Flask/Django", "library": "django", "title": "Flask: how to return JSON responses properly?", "content": "`return jsonify({'key': 'value'})` sets Content-Type to application/json. Flask 1.0+ automatically converts returned dicts to JSON responses. For custom status codes: `return jsonify(data), 201`. For errors: use `abort(404)` or return `({'error': 'Not found'}, 404)`.", "code_signature": "Use jsonify() or return a dict directly (Flask 1.0+).", "tags": "dependency-injection, rest, template", "url": "https://stackoverflow.com/questions/8930103", "author": "user_264801", "date_published": "18-04-2022", "votes_or_stars": 2246, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:c3d049cd-d7cf-4cd3-9c9a-9943ef41c12f": {"content": "FEAT: pathlib: add support for reading/writing with encoding shorthand\nProblem: pathlib.Path.read_text() does not default to UTF-8, causing issues across platforms.\n\nFix/Discussion: Always pass encoding explicitly: `Path('file').read_text(encoding='utf-8')`. Python 3.10+ added `encoding='locale'` default change proposal. Best practice: always specify encoding in all file I/O operations for portability.\nCode signature: status:closed", "file_path": "https://github.com/cpython/cpython/issues/2707", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "c3d049cd-d7cf-4cd3-9c9a-9943ef41c12f", "source_type": "github_issue", "domain": "OS/Subprocess/File I/O", "library": "cpython", "title": "FEAT: pathlib: add support for reading/writing with encoding shorthand", "content": "Problem: pathlib.Path.read_text() does not default to UTF-8, causing issues across platforms.\n\nFix/Discussion: Always pass encoding explicitly: `Path('file').read_text(encoding='utf-8')`. Python 3.10+ added `encoding='locale'` default change proposal. Best practice: always specify encoding in all file I/O operations for portability.", "code_signature": "status:closed", "tags": "stdout, process, glob, stderr, env-variable", "url": "https://github.com/cpython/cpython/issues/2707", "author": "contributor_7877", "date_published": "18-01-2024", "votes_or_stars": 464, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:c1a92ff2-0cde-4dc1-93fd-b439e43111ee": {"content": "PERF: concurrent.futures.ProcessPoolExecutor slow startup on Windows\nProblem: ProcessPoolExecutor takes 3-5 seconds to start on Windows due to process spawning overhead.\n\nFix/Discussion: Windows uses 'spawn' start method (vs 'fork' on Linux). Mitigations: (1) Reuse executor across calls — don't create/destroy in loops. (2) Use 'forkserver' or 'fork' on Linux/macOS. (3) For short tasks, ThreadPoolExecutor may be faster. (4) Use `initializer` param to pre-load modules.\nCode signature: status:closed", "file_path": "https://github.com/cpython/cpython/issues/6647", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "c1a92ff2-0cde-4dc1-93fd-b439e43111ee", "source_type": "github_issue", "domain": "Asyncio/Threading", "library": "cpython", "title": "PERF: concurrent.futures.ProcessPoolExecutor slow startup on Windows", "content": "Problem: ProcessPoolExecutor takes 3-5 seconds to start on Windows due to process spawning overhead.\n\nFix/Discussion: Windows uses 'spawn' start method (vs 'fork' on Linux). Mitigations: (1) Reuse executor across calls — don't create/destroy in loops. (2) Use 'forkserver' or 'fork' on Linux/macOS. (3) For short tasks, ThreadPoolExecutor may be faster. (4) Use `initializer` param to pre-load modules.", "code_signature": "status:closed", "tags": "event-loop, await, race-condition, executor, threading", "url": "https://github.com/cpython/cpython/issues/6647", "author": "contributor_4097", "date_published": "24-07-2019", "votes_or_stars": 256, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:a039419f-ae49-4c3a-8e64-118b9c9b1a52": {"content": "How to run background tasks in FastAPI?\nFastAPI's `BackgroundTasks`: add `background_tasks: BackgroundTasks` as a parameter, then `background_tasks.add_task(func, *args)`. Runs after response is sent. For heavy workloads (email, ML inference), use Celery with Redis/RabbitMQ broker. For async background work, `asyncio.create_task()` works within async endpoints.\nCode signature: Use BackgroundTasks for lightweight tasks or Celery for heavy/distributed work.", "file_path": "https://stackoverflow.com/questions/8077999", "function_name": "Use BackgroundTasks for lightweight tasks or Celery for heavy/distributed work.", "type": "stack_overflow", "metadata": {"record_id": "a039419f-ae49-4c3a-8e64-118b9c9b1a52", "source_type": "stack_overflow", "domain": "FastAPI/Flask/Django", "library": "django", "title": "How to run background tasks in FastAPI?", "content": "FastAPI's `BackgroundTasks`: add `background_tasks: BackgroundTasks` as a parameter, then `background_tasks.add_task(func, *args)`. Runs after response is sent. For heavy workloads (email, ML inference), use Celery with Redis/RabbitMQ broker. For async background work, `asyncio.create_task()` works within async endpoints.", "code_signature": "Use BackgroundTasks for lightweight tasks or Celery for heavy/distributed work.", "tags": "request, middleware, template, fastapi, flask", "url": "https://stackoverflow.com/questions/8077999", "author": "user_202401", "date_published": "03-05-2023", "votes_or_stars": 1847, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:90a9e36a-50b8-4f26-a908-6ea347a7625a": {"content": "How do I handle CORS in FastAPI?\n```python\nfrom fastapi.middleware.cors import CORSMiddleware\napp.add_middleware(CORSMiddleware, allow_origins=['*'], allow_methods=['*'], allow_headers=['*'])\n```\nIn production, replace `'*'` with your actual frontend domain. For credentials (cookies), set `allow_credentials=True` and specify exact origins.\nCode signature: Add CORSMiddleware from starlette to your FastAPI app.", "file_path": "https://stackoverflow.com/questions/1527021", "function_name": "Add CORSMiddleware from starlette to your FastAPI app.", "type": "stack_overflow", "metadata": {"record_id": "90a9e36a-50b8-4f26-a908-6ea347a7625a", "source_type": "stack_overflow", "domain": "FastAPI/Flask/Django", "library": "flask", "title": "How do I handle CORS in FastAPI?", "content": "```python\nfrom fastapi.middleware.cors import CORSMiddleware\napp.add_middleware(CORSMiddleware, allow_origins=['*'], allow_methods=['*'], allow_headers=['*'])\n```\nIn production, replace `'*'` with your actual frontend domain. For credentials (cookies), set `allow_credentials=True` and specify exact origins.", "code_signature": "Add CORSMiddleware from starlette to your FastAPI app.", "tags": "request, routing, template", "url": "https://stackoverflow.com/questions/1527021", "author": "user_911393", "date_published": "11-03-2020", "votes_or_stars": 929, "relevance_label": "low", "difficulty_level": "advanced"}}, "kb:62128db7-5d89-4644-ad92-8a78068e1234": {"content": "Why is my Pandas merge creating duplicate rows?\nDuplicate rows after merge usually mean the join key is not unique. Diagnose with `df.duplicated(subset=['key']).sum()`. Use `how='left'` carefully when right table has multiple matches. Set `validate='one_to_many'` or `'many_to_one'` to enforce cardinality. Consider deduplicating with `df.drop_duplicates(subset=['key'])` before merging.\nCode signature: Your merge key has duplicates in one or both DataFrames. Use validate='one_to_one' to catch this early.", "file_path": "https://stackoverflow.com/questions/2321324", "function_name": "Your merge key has duplicates in one or both DataFrames. Use validate='one_to_one' to catch this early.", "type": "stack_overflow", "metadata": {"record_id": "62128db7-5d89-4644-ad92-8a78068e1234", "source_type": "stack_overflow", "domain": "NumPy/Pandas", "library": "array", "title": "Why is my Pandas merge creating duplicate rows?", "content": "Duplicate rows after merge usually mean the join key is not unique. Diagnose with `df.duplicated(subset=['key']).sum()`. Use `how='left'` carefully when right table has multiple matches. Set `validate='one_to_many'` or `'many_to_one'` to enforce cardinality. Consider deduplicating with `df.drop_duplicates(subset=['key'])` before merging.", "code_signature": "Your merge key has duplicates in one or both DataFrames. Use validate='one_to_one' to catch this early.", "tags": "numpy, broadcasting, array, nan, vectorization", "url": "https://stackoverflow.com/questions/2321324", "author": "user_99814", "date_published": "09-12-2022", "votes_or_stars": 229, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:d4855603-15e0-43bb-a2da-5d9ad6879db4": {"content": "Why is my Pandas merge creating duplicate rows?\nDuplicate rows after merge usually mean the join key is not unique. Diagnose with `df.duplicated(subset=['key']).sum()`. Use `how='left'` carefully when right table has multiple matches. Set `validate='one_to_many'` or `'many_to_one'` to enforce cardinality. Consider deduplicating with `df.drop_duplicates(subset=['key'])` before merging.\nCode signature: Your merge key has duplicates in one or both DataFrames. Use validate='one_to_one' to catch this early.", "file_path": "https://stackoverflow.com/questions/2321324", "function_name": "Your merge key has duplicates in one or both DataFrames. Use validate='one_to_one' to catch this early.", "type": "stack_overflow", "metadata": {"record_id": "d4855603-15e0-43bb-a2da-5d9ad6879db4", "source_type": "stack_overflow", "domain": "NumPy/Pandas", "library": "array", "title": "Why is my Pandas merge creating duplicate rows?", "content": "Duplicate rows after merge usually mean the join key is not unique. Diagnose with `df.duplicated(subset=['key']).sum()`. Use `how='left'` carefully when right table has multiple matches. Set `validate='one_to_many'` or `'many_to_one'` to enforce cardinality. Consider deduplicating with `df.drop_duplicates(subset=['key'])` before merging.", "code_signature": "Your merge key has duplicates in one or both DataFrames. Use validate='one_to_one' to catch this early.", "tags": "nan, dataframe, numpy, loc, groupby, array", "url": "https://stackoverflow.com/questions/2321324", "author": "user_99814", "date_published": "14-12-2020", "votes_or_stars": 251, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:9533f064-e714-485b-bc0d-3f869243fc41": {"content": "How to read a large file line by line in Python without loading it all into memory?\n```python\nwith open('large.txt', 'r') as f:\n    for line in f:\n        process(line.strip())\n```\nThis reads one line at a time. For binary files: use `f.read(chunk_size)` in a loop. For CSV: use `pd.read_csv(file, chunksize=10000)`. Never use `f.readlines()` on large files.\nCode signature: Iterate over the file object directly — it yields lines lazily.", "file_path": "https://stackoverflow.com/questions/2737893", "function_name": "Iterate over the file object directly — it yields lines lazily.", "type": "stack_overflow", "metadata": {"record_id": "9533f064-e714-485b-bc0d-3f869243fc41", "source_type": "stack_overflow", "domain": "OS/Subprocess/File I/O", "library": "shutil", "title": "How to read a large file line by line in Python without loading it all into memory?", "content": "```python\nwith open('large.txt', 'r') as f:\n    for line in f:\n        process(line.strip())\n```\nThis reads one line at a time. For binary files: use `f.read(chunk_size)` in a loop. For CSV: use `pd.read_csv(file, chunksize=10000)`. Never use `f.readlines()` on large files.", "code_signature": "Iterate over the file object directly — it yields lines lazily.", "tags": "file-io, process, shutil, pathlib, env-variable", "url": "https://stackoverflow.com/questions/2737893", "author": "user_994539", "date_published": "07-05-2018", "votes_or_stars": 2301, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:e931b945-3f7e-471e-a107-52cf67d12ad1": {"content": "How to run asyncio code from synchronous Python?\n`asyncio.run(main())` creates an event loop, runs the coroutine, closes the loop. Never call asyncio.run() from inside a running event loop (use await instead). In Jupyter notebooks (which have a running loop), use `await coroutine()` directly or `nest_asyncio.apply()`. For mixing sync/async, use `loop.run_until_complete()`.\nCode signature: Use asyncio.run() in Python 3.7+ to execute a coroutine from sync code.", "file_path": "https://stackoverflow.com/questions/2140194", "function_name": "Use asyncio.run() in Python 3.7+ to execute a coroutine from sync code.", "type": "stack_overflow", "metadata": {"record_id": "e931b945-3f7e-471e-a107-52cf67d12ad1", "source_type": "stack_overflow", "domain": "Asyncio/Threading", "library": "gather", "title": "How to run asyncio code from synchronous Python?", "content": "`asyncio.run(main())` creates an event loop, runs the coroutine, closes the loop. Never call asyncio.run() from inside a running event loop (use await instead). In Jupyter notebooks (which have a running loop), use `await coroutine()` directly or `nest_asyncio.apply()`. For mixing sync/async, use `loop.run_until_complete()`.", "code_signature": "Use asyncio.run() in Python 3.7+ to execute a coroutine from sync code.", "tags": "threading, deadlock, await", "url": "https://stackoverflow.com/questions/2140194", "author": "user_718011", "date_published": "29-08-2024", "votes_or_stars": 264, "relevance_label": "low", "difficulty_level": "advanced"}}, "kb:c65271fc-4f64-45b4-823f-bd2477df0ea6": {"content": "How to read a large file line by line in Python without loading it all into memory?\n```python\nwith open('large.txt', 'r') as f:\n    for line in f:\n        process(line.strip())\n```\nThis reads one line at a time. For binary files: use `f.read(chunk_size)` in a loop. For CSV: use `pd.read_csv(file, chunksize=10000)`. Never use `f.readlines()` on large files.\nCode signature: Iterate over the file object directly — it yields lines lazily.", "file_path": "https://stackoverflow.com/questions/2737893", "function_name": "Iterate over the file object directly — it yields lines lazily.", "type": "stack_overflow", "metadata": {"record_id": "c65271fc-4f64-45b4-823f-bd2477df0ea6", "source_type": "stack_overflow", "domain": "OS/Subprocess/File I/O", "library": "shutil", "title": "How to read a large file line by line in Python without loading it all into memory?", "content": "```python\nwith open('large.txt', 'r') as f:\n    for line in f:\n        process(line.strip())\n```\nThis reads one line at a time. For binary files: use `f.read(chunk_size)` in a loop. For CSV: use `pd.read_csv(file, chunksize=10000)`. Never use `f.readlines()` on large files.", "code_signature": "Iterate over the file object directly — it yields lines lazily.", "tags": "os, pathlib, env-variable", "url": "https://stackoverflow.com/questions/2737893", "author": "user_994539", "date_published": "14-11-2021", "votes_or_stars": 2254, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:ad3fb0e8-ae80-4ef2-a33d-ada1cb2dbdd9": {"content": "How to handle rate limiting with the requests library?\n```python\nfrom tenacity import retry, wait_exponential, stop_after_attempt\n@retry(wait=wait_exponential(min=1, max=60), stop=stop_after_attempt(5))\ndef call_api(url):\n    r = requests.get(url)\n    r.raise_for_status()\n    return r.json()\n```\nCheck `Retry-After` header in 429 responses. For async: use `aiohttp` with tenacity.\nCode signature: Implement exponential backoff with the tenacity or backoff library.", "file_path": "https://stackoverflow.com/questions/1604451", "function_name": "Implement exponential backoff with the tenacity or backoff library.", "type": "stack_overflow", "metadata": {"record_id": "ad3fb0e8-ae80-4ef2-a33d-ada1cb2dbdd9", "source_type": "stack_overflow", "domain": "Requests/HTTP/APIs", "library": "retry", "title": "How to handle rate limiting with the requests library?", "content": "```python\nfrom tenacity import retry, wait_exponential, stop_after_attempt\n@retry(wait=wait_exponential(min=1, max=60), stop=stop_after_attempt(5))\ndef call_api(url):\n    r = requests.get(url)\n    r.raise_for_status()\n    return r.json()\n```\nCheck `Retry-After` header in 429 responses. For async: use `aiohttp` with tenacity.", "code_signature": "Implement exponential backoff with the tenacity or backoff library.", "tags": "websocket, timeout, requests, json, authentication", "url": "https://stackoverflow.com/questions/1604451", "author": "user_885136", "date_published": "02-02-2019", "votes_or_stars": 647, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:e39aef2c-afe2-4617-9f56-27b972268fc8": {"content": "How to implement a thread pool in Python?\n```python\nfrom concurrent.futures import ThreadPoolExecutor\nwith ThreadPoolExecutor(max_workers=8) as ex:\n    futures = [ex.submit(task, arg) for arg in args]\n    results = [f.result() for f in futures]\n```\nFor CPU-bound: use ProcessPoolExecutor instead. For async integration: `await loop.run_in_executor(executor, func, *args)`.\nCode signature: Use concurrent.futures.ThreadPoolExecutor for I/O-bound tasks.", "file_path": "https://stackoverflow.com/questions/3646552", "function_name": "Use concurrent.futures.ThreadPoolExecutor for I/O-bound tasks.", "type": "stack_overflow", "metadata": {"record_id": "e39aef2c-afe2-4617-9f56-27b972268fc8", "source_type": "stack_overflow", "domain": "Asyncio/Threading", "library": "coroutine", "title": "How to implement a thread pool in Python?", "content": "```python\nfrom concurrent.futures import ThreadPoolExecutor\nwith ThreadPoolExecutor(max_workers=8) as ex:\n    futures = [ex.submit(task, arg) for arg in args]\n    results = [f.result() for f in futures]\n```\nFor CPU-bound: use ProcessPoolExecutor instead. For async integration: `await loop.run_in_executor(executor, func, *args)`.", "code_signature": "Use concurrent.futures.ThreadPoolExecutor for I/O-bound tasks.", "tags": "race-condition, gather, await, executor", "url": "https://stackoverflow.com/questions/3646552", "author": "user_469469", "date_published": "08-11-2022", "votes_or_stars": 1546, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:2d651606-6f39-4586-91ae-f5b2482b6d50": {"content": "SQLAlchemy — ORM Session Query\nThe `session.query` function in SQLAlchemy constructs a SELECT query against mapped entities. It accepts `*entities` as a parameter and returns Query object. Common use cases include retrieving ORM-mapped objects from the database. Note that prefer session.execute(select(...)) in SQLAlchemy 2.0+.\nCode signature: session.query(*entities) -> Query object", "file_path": "https://docs.sqlalchemy.org/en/stable/session/query.html", "function_name": "session.query(*entities) -> Query object", "type": "documentation", "metadata": {"record_id": "2d651606-6f39-4586-91ae-f5b2482b6d50", "source_type": "documentation", "domain": "SQLAlchemy/Databases", "library": "SQLAlchemy", "title": "SQLAlchemy — ORM Session Query", "content": "The `session.query` function in SQLAlchemy constructs a SELECT query against mapped entities. It accepts `*entities` as a parameter and returns Query object. Common use cases include retrieving ORM-mapped objects from the database. Note that prefer session.execute(select(...)) in SQLAlchemy 2.0+.", "code_signature": "session.query(*entities) -> Query object", "tags": "postgresql, migration, sqlalchemy, session, query, connection-pool", "url": "https://docs.sqlalchemy.org/en/stable/session/query.html", "author": "Official Docs", "date_published": "16-02-2020", "votes_or_stars": 3026, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:9e932dd2-25db-4f20-982b-5fe53a6e0122": {"content": "BUG: httpx: SSL certificate error on Windows despite valid cert\nProblem: httpx raises SSLCertVerificationError on Windows when system CA store has the cert.\n\nFix/Discussion: httpx uses its own CA bundle (certifi) by default, not the system store. Fix: `httpx.Client(verify=certifi.where())` — already default. For custom certs: `httpx.Client(verify='/path/to/ca-bundle.pem')`. Install `pip install truststore` and use `ssl.create_default_context(ssl.Purpose.SERVER_AUTH)` to use system store.\nCode signature: status:closed", "file_path": "https://github.com/httpx/httpx/issues/8949", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "9e932dd2-25db-4f20-982b-5fe53a6e0122", "source_type": "github_issue", "domain": "Requests/HTTP/APIs", "library": "httpx", "title": "BUG: httpx: SSL certificate error on Windows despite valid cert", "content": "Problem: httpx raises SSLCertVerificationError on Windows when system CA store has the cert.\n\nFix/Discussion: httpx uses its own CA bundle (certifi) by default, not the system store. Fix: `httpx.Client(verify=certifi.where())` — already default. For custom certs: `httpx.Client(verify='/path/to/ca-bundle.pem')`. Install `pip install truststore` and use `ssl.create_default_context(ssl.Purpose.SERVER_AUTH)` to use system store.", "code_signature": "status:closed", "tags": "json, rest-api, oauth, aiohttp, requests, retry", "url": "https://github.com/httpx/httpx/issues/8949", "author": "contributor_1420", "date_published": "23-03-2024", "votes_or_stars": 460, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:e9294156-651d-499c-8fd7-2755235b3547": {"content": "How to handle database migrations with Alembic?\nWorkflow: (1) Change SQLAlchemy models. (2) `alembic revision --autogenerate -m 'description'`. (3) Review generated migration script. (4) `alembic upgrade head` to apply. Always review autogenerated migrations — they miss some changes (indexes, constraints). Use `alembic downgrade -1` to rollback. Store migrations in version control.\nCode signature: Use alembic revision --autogenerate to create migrations from model changes.", "file_path": "https://stackoverflow.com/questions/9436429", "function_name": "Use alembic revision --autogenerate to create migrations from model changes.", "type": "stack_overflow", "metadata": {"record_id": "e9294156-651d-499c-8fd7-2755235b3547", "source_type": "stack_overflow", "domain": "SQLAlchemy/Databases", "library": "connection-pool", "title": "How to handle database migrations with Alembic?", "content": "Workflow: (1) Change SQLAlchemy models. (2) `alembic revision --autogenerate -m 'description'`. (3) Review generated migration script. (4) `alembic upgrade head` to apply. Always review autogenerated migrations — they miss some changes (indexes, constraints). Use `alembic downgrade -1` to rollback. Store migrations in version control.", "code_signature": "Use alembic revision --autogenerate to create migrations from model changes.", "tags": "query, alembic, migration, relationship, sqlite, session", "url": "https://stackoverflow.com/questions/9436429", "author": "user_974047", "date_published": "16-11-2024", "votes_or_stars": 2393, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:ff31d71a-33ea-4414-a3e7-cc243cc6bc28": {"content": "SQLAlchemy bulk insert: what's the fastest way?\n`session.bulk_insert_mappings(Model, list_of_dicts)` skips ORM overhead. Even faster: `session.execute(insert(Model), data)`. Avoid `session.add()` in loops. For PostgreSQL, use `insert().on_conflict_do_nothing()` for upserts. Batch in chunks of 1000-5000 rows for optimal performance.\nCode signature: Use session.bulk_insert_mappings() or execute(insert(), list_of_dicts) for maximum speed.", "file_path": "https://stackoverflow.com/questions/5977931", "function_name": "Use session.bulk_insert_mappings() or execute(insert(), list_of_dicts) for maximum speed.", "type": "stack_overflow", "metadata": {"record_id": "ff31d71a-33ea-4414-a3e7-cc243cc6bc28", "source_type": "stack_overflow", "domain": "SQLAlchemy/Databases", "library": "query", "title": "SQLAlchemy bulk insert: what's the fastest way?", "content": "`session.bulk_insert_mappings(Model, list_of_dicts)` skips ORM overhead. Even faster: `session.execute(insert(Model), data)`. Avoid `session.add()` in loops. For PostgreSQL, use `insert().on_conflict_do_nothing()` for upserts. Batch in chunks of 1000-5000 rows for optimal performance.", "code_signature": "Use session.bulk_insert_mappings() or execute(insert(), list_of_dicts) for maximum speed.", "tags": "foreign-key, orm, postgresql, sqlite, transaction, session", "url": "https://stackoverflow.com/questions/5977931", "author": "user_238275", "date_published": "13-07-2022", "votes_or_stars": 650, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:e58ded68-d9ab-4139-b3df-f8bdc2641fe3": {"content": "requests — HTTP Session Management\nThe `requests.Session` function in requests creates a persistent HTTP session with connection pooling. It accepts `` as a parameter and returns Session object. Common use cases include making multiple requests to the same host efficiently. Note that always close or use Session as context manager to release connections.\nCode signature: requests.Session() -> Session object", "file_path": "https://docs.requests.org/en/stable/requests/Session.html", "function_name": "requests.Session() -> Session object", "type": "documentation", "metadata": {"record_id": "e58ded68-d9ab-4139-b3df-f8bdc2641fe3", "source_type": "documentation", "domain": "Requests/HTTP/APIs", "library": "requests", "title": "requests — HTTP Session Management", "content": "The `requests.Session` function in requests creates a persistent HTTP session with connection pooling. It accepts `` as a parameter and returns Session object. Common use cases include making multiple requests to the same host efficiently. Note that always close or use Session as context manager to release connections.", "code_signature": "requests.Session() -> Session object", "tags": "websocket, retry, headers, timeout", "url": "https://docs.requests.org/en/stable/requests/Session.html", "author": "Official Docs", "date_published": "01-05-2019", "votes_or_stars": 81, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:c66b9658-792d-46dd-8609-8dd6b79bb0b3": {"content": "How to use create_engine in SQLAlchemy\n`SQLAlchemy.create_engine` creates a database engine representing a connection pool. Example usage:\n```python\nengine = create_engine('postgresql://user:pass@host/db', echo=True)\n```\nThis is useful when establishing the central database connection for an application. Performance tip: set pool_size and max_overflow based on expected concurrency.\nCode signature: create_engine", "file_path": "https://docs.sqlalchemy.org/en/stable/create_engine.html", "function_name": "create_engine", "type": "documentation", "metadata": {"record_id": "c66b9658-792d-46dd-8609-8dd6b79bb0b3", "source_type": "documentation", "domain": "SQLAlchemy/Databases", "library": "SQLAlchemy", "title": "How to use create_engine in SQLAlchemy", "content": "`SQLAlchemy.create_engine` creates a database engine representing a connection pool. Example usage:\n```python\nengine = create_engine('postgresql://user:pass@host/db', echo=True)\n```\nThis is useful when establishing the central database connection for an application. Performance tip: set pool_size and max_overflow based on expected concurrency.", "code_signature": "create_engine", "tags": "alembic, orm, postgresql, migration, foreign-key", "url": "https://docs.sqlalchemy.org/en/stable/create_engine.html", "author": "Official Docs", "date_published": "09-02-2022", "votes_or_stars": 2047, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:ba498d78-b3cf-4a85-9bbb-6931e684a788": {"content": "How to run background tasks in FastAPI?\nFastAPI's `BackgroundTasks`: add `background_tasks: BackgroundTasks` as a parameter, then `background_tasks.add_task(func, *args)`. Runs after response is sent. For heavy workloads (email, ML inference), use Celery with Redis/RabbitMQ broker. For async background work, `asyncio.create_task()` works within async endpoints.\nCode signature: Use BackgroundTasks for lightweight tasks or Celery for heavy/distributed work.", "file_path": "https://stackoverflow.com/questions/8077999", "function_name": "Use BackgroundTasks for lightweight tasks or Celery for heavy/distributed work.", "type": "stack_overflow", "metadata": {"record_id": "ba498d78-b3cf-4a85-9bbb-6931e684a788", "source_type": "stack_overflow", "domain": "FastAPI/Flask/Django", "library": "django", "title": "How to run background tasks in FastAPI?", "content": "FastAPI's `BackgroundTasks`: add `background_tasks: BackgroundTasks` as a parameter, then `background_tasks.add_task(func, *args)`. Runs after response is sent. For heavy workloads (email, ML inference), use Celery with Redis/RabbitMQ broker. For async background work, `asyncio.create_task()` works within async endpoints.", "code_signature": "Use BackgroundTasks for lightweight tasks or Celery for heavy/distributed work.", "tags": "flask, request, pydantic, dependency-injection, response", "url": "https://stackoverflow.com/questions/8077999", "author": "user_202401", "date_published": "17-10-2021", "votes_or_stars": 1860, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:7851db22-a1fc-4ac6-be68-ff7c0ac40cf0": {"content": "BUG: os.path.join ignores prefix when component starts with /\nProblem: os.path.join('base', '/absolute') returns '/absolute', ignoring 'base'.\n\nFix/Discussion: This is documented behavior: os.path.join discards previous components when it encounters an absolute path. Fix: use `pathlib.Path(base) / component.lstrip('/')`. Validate user-provided paths with `Path(path).resolve().is_relative_to(base_dir)` to prevent path traversal.\nCode signature: status:closed", "file_path": "https://github.com/cpython/cpython/issues/7712", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "7851db22-a1fc-4ac6-be68-ff7c0ac40cf0", "source_type": "github_issue", "domain": "OS/Subprocess/File I/O", "library": "cpython", "title": "BUG: os.path.join ignores prefix when component starts with /", "content": "Problem: os.path.join('base', '/absolute') returns '/absolute', ignoring 'base'.\n\nFix/Discussion: This is documented behavior: os.path.join discards previous components when it encounters an absolute path. Fix: use `pathlib.Path(base) / component.lstrip('/')`. Validate user-provided paths with `Path(path).resolve().is_relative_to(base_dir)` to prevent path traversal.", "code_signature": "status:closed", "tags": "stdin, env-variable, subprocess, process, stderr, pathlib", "url": "https://github.com/cpython/cpython/issues/7712", "author": "contributor_8802", "date_published": "12-08-2018", "votes_or_stars": 429, "relevance_label": "low", "difficulty_level": "advanced"}}, "kb:bd7ca042-1abd-4433-9f22-d3c02159c2d9": {"content": "How to run background tasks in FastAPI?\nFastAPI's `BackgroundTasks`: add `background_tasks: BackgroundTasks` as a parameter, then `background_tasks.add_task(func, *args)`. Runs after response is sent. For heavy workloads (email, ML inference), use Celery with Redis/RabbitMQ broker. For async background work, `asyncio.create_task()` works within async endpoints.\nCode signature: Use BackgroundTasks for lightweight tasks or Celery for heavy/distributed work.", "file_path": "https://stackoverflow.com/questions/8077999", "function_name": "Use BackgroundTasks for lightweight tasks or Celery for heavy/distributed work.", "type": "stack_overflow", "metadata": {"record_id": "bd7ca042-1abd-4433-9f22-d3c02159c2d9", "source_type": "stack_overflow", "domain": "FastAPI/Flask/Django", "library": "django", "title": "How to run background tasks in FastAPI?", "content": "FastAPI's `BackgroundTasks`: add `background_tasks: BackgroundTasks` as a parameter, then `background_tasks.add_task(func, *args)`. Runs after response is sent. For heavy workloads (email, ML inference), use Celery with Redis/RabbitMQ broker. For async background work, `asyncio.create_task()` works within async endpoints.", "code_signature": "Use BackgroundTasks for lightweight tasks or Celery for heavy/distributed work.", "tags": "fastapi, response, template, flask, dependency-injection", "url": "https://stackoverflow.com/questions/8077999", "author": "user_202401", "date_published": "09-05-2022", "votes_or_stars": 1845, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:da966da5-21ba-414b-b0b2-9fe51dda51fb": {"content": "asyncio RuntimeError: This event loop is already running\nThis error is common in Jupyter and FastAPI. Solutions: (1) In async context: just use `await coroutine()`. (2) In Jupyter: `import nest_asyncio; nest_asyncio.apply()`. (3) Run in thread: `asyncio.get_event_loop().run_in_executor(None, sync_func)`. (4) Use `asyncio.ensure_future()` to schedule from within async code.\nCode signature: You're calling asyncio.run() inside an already running event loop. Use await instead.", "file_path": "https://stackoverflow.com/questions/2229110", "function_name": "You're calling asyncio.run() inside an already running event loop. Use await instead.", "type": "stack_overflow", "metadata": {"record_id": "da966da5-21ba-414b-b0b2-9fe51dda51fb", "source_type": "stack_overflow", "domain": "Asyncio/Threading", "library": "async", "title": "asyncio RuntimeError: This event loop is already running", "content": "This error is common in Jupyter and FastAPI. Solutions: (1) In async context: just use `await coroutine()`. (2) In Jupyter: `import nest_asyncio; nest_asyncio.apply()`. (3) Run in thread: `asyncio.get_event_loop().run_in_executor(None, sync_func)`. (4) Use `asyncio.ensure_future()` to schedule from within async code.", "code_signature": "You're calling asyncio.run() inside an already running event loop. Use await instead.", "tags": "lock, async, executor, event-loop", "url": "https://stackoverflow.com/questions/2229110", "author": "user_573750", "date_published": "20-06-2018", "votes_or_stars": 2334, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:077ed16e-3b01-4f2a-8526-872bfcb00d4b": {"content": "BUG: Alembic autogenerate misses index changes\nProblem: Alembic --autogenerate does not detect changes to indexes or constraints on existing tables.\n\nFix/Discussion: Alembic autogenerate compares metadata to database state but has limitations. Workarounds: (1) Manually add `op.create_index()` to migration. (2) Use `include_schemas=True` in env.py. (3) Run `alembic check` to see detected changes. (4) Always review autogenerated migrations before applying.\nCode signature: status:open", "file_path": "https://github.com/alembic/alembic/issues/7344", "function_name": "status:open", "type": "github_issue", "metadata": {"record_id": "077ed16e-3b01-4f2a-8526-872bfcb00d4b", "source_type": "github_issue", "domain": "SQLAlchemy/Databases", "library": "alembic", "title": "BUG: Alembic autogenerate misses index changes", "content": "Problem: Alembic --autogenerate does not detect changes to indexes or constraints on existing tables.\n\nFix/Discussion: Alembic autogenerate compares metadata to database state but has limitations. Workarounds: (1) Manually add `op.create_index()` to migration. (2) Use `include_schemas=True` in env.py. (3) Run `alembic check` to see detected changes. (4) Always review autogenerated migrations before applying.", "code_signature": "status:open", "tags": "connection-pool, sqlalchemy, session, postgresql, relationship, migration", "url": "https://github.com/alembic/alembic/issues/7344", "author": "contributor_3601", "date_published": "21-12-2019", "votes_or_stars": 213, "relevance_label": "low", "difficulty_level": "intermediate"}}, "kb:b89328e3-36a5-43c6-a16e-636ea3d4db62": {"content": "How to apply a function to each row in Pandas without using iterrows?\n`iterrows()` is slow because it creates a Series per row. Prefer `df.apply(func, axis=1)` for row-wise operations, or better yet, vectorize using NumPy: `np.where(condition, a, b)`. For complex logic, `df.assign()` with vectorized expressions is cleanest. Benchmark: vectorized > apply > itertuples > iterrows.\nCode signature: Use df.apply(func, axis=1) or vectorized NumPy operations for best performance.", "file_path": "https://stackoverflow.com/questions/5446912", "function_name": "Use df.apply(func, axis=1) or vectorized NumPy operations for best performance.", "type": "stack_overflow", "metadata": {"record_id": "b89328e3-36a5-43c6-a16e-636ea3d4db62", "source_type": "stack_overflow", "domain": "NumPy/Pandas", "library": "dataframe", "title": "How to apply a function to each row in Pandas without using iterrows?", "content": "`iterrows()` is slow because it creates a Series per row. Prefer `df.apply(func, axis=1)` for row-wise operations, or better yet, vectorize using NumPy: `np.where(condition, a, b)`. For complex logic, `df.assign()` with vectorized expressions is cleanest. Benchmark: vectorized > apply > itertuples > iterrows.", "code_signature": "Use df.apply(func, axis=1) or vectorized NumPy operations for best performance.", "tags": "loc, array, vectorization, nan", "url": "https://stackoverflow.com/questions/5446912", "author": "user_563306", "date_published": "04-11-2019", "votes_or_stars": 2277, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:2f45f53f-a0e4-44a5-94b9-5594dbe2c5e9": {"content": "How to use df.groupby in Pandas\n`Pandas.df.groupby` groups DataFrame rows by column values. Example usage:\n```python\ndf.groupby('category')['value'].mean()\n```\nThis is useful when aggregation, transformation, filtering by group. Performance tip: avoid apply() on large DataFrames; prefer agg().\nCode signature: df.groupby", "file_path": "https://docs.pandas.org/en/stable/df/groupby.html", "function_name": "df.groupby", "type": "documentation", "metadata": {"record_id": "2f45f53f-a0e4-44a5-94b9-5594dbe2c5e9", "source_type": "documentation", "domain": "NumPy/Pandas", "library": "Pandas", "title": "How to use df.groupby in Pandas", "content": "`Pandas.df.groupby` groups DataFrame rows by column values. Example usage:\n```python\ndf.groupby('category')['value'].mean()\n```\nThis is useful when aggregation, transformation, filtering by group. Performance tip: avoid apply() on large DataFrames; prefer agg().", "code_signature": "df.groupby", "tags": "groupby, pandas, vectorization", "url": "https://docs.pandas.org/en/stable/df/groupby.html", "author": "Official Docs", "date_published": "06-01-2020", "votes_or_stars": 815, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:fc07bf70-3462-483d-a37a-2771e3db3f18": {"content": "threading Thread Synchronization — official reference\nOfficial documentation for threading Thread Synchronization. The method signature is `threading.Lock()`. creates a mutual-exclusion lock for thread-safe access. Raises `RuntimeError` when lock is released without being acquired. See also: threading.RLock, threading.Semaphore, threading.Event.\nCode signature: threading.Lock()", "file_path": "https://docs.threading.org/en/stable/threading/Lock.html", "function_name": "threading.Lock()", "type": "documentation", "metadata": {"record_id": "fc07bf70-3462-483d-a37a-2771e3db3f18", "source_type": "documentation", "domain": "Asyncio/Threading", "library": "threading", "title": "threading Thread Synchronization — official reference", "content": "Official documentation for threading Thread Synchronization. The method signature is `threading.Lock()`. creates a mutual-exclusion lock for thread-safe access. Raises `RuntimeError` when lock is released without being acquired. See also: threading.RLock, threading.Semaphore, threading.Event.", "code_signature": "threading.Lock()", "tags": "gather, lock, task, threading, coroutine", "url": "https://docs.threading.org/en/stable/threading/Lock.html", "author": "Official Docs", "date_published": "22-09-2020", "votes_or_stars": 4816, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:12a2917e-5816-41b3-8df1-66bd0877da5a": {"content": "FEAT: Django: add support for async ORM queries\nProblem: Django ORM is synchronous; using it in async views requires sync_to_async wrapper.\n\nFix/Discussion: Django 4.1+ added native async ORM support. Use `await Model.objects.filter().afirst()`, `async for obj in qs:`. For older versions: wrap with `sync_to_async`: `get_user = sync_to_async(User.objects.get)\nuser = await get_user(pk=1)`.\nCode signature: status:closed", "file_path": "https://github.com/django/django/issues/8618", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "12a2917e-5816-41b3-8df1-66bd0877da5a", "source_type": "github_issue", "domain": "FastAPI/Flask/Django", "library": "django", "title": "FEAT: Django: add support for async ORM queries", "content": "Problem: Django ORM is synchronous; using it in async views requires sync_to_async wrapper.\n\nFix/Discussion: Django 4.1+ added native async ORM support. Use `await Model.objects.filter().afirst()`, `async for obj in qs:`. For older versions: wrap with `sync_to_async`: `get_user = sync_to_async(User.objects.get)\nuser = await get_user(pk=1)`.", "code_signature": "status:closed", "tags": "orm, django, blueprint, fastapi, routing, dependency-injection", "url": "https://github.com/django/django/issues/8618", "author": "contributor_8921", "date_published": "02-06-2021", "votes_or_stars": 1, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:81620b22-0123-4f64-8732-c62f90827e6a": {"content": "How to run background tasks in FastAPI?\nFastAPI's `BackgroundTasks`: add `background_tasks: BackgroundTasks` as a parameter, then `background_tasks.add_task(func, *args)`. Runs after response is sent. For heavy workloads (email, ML inference), use Celery with Redis/RabbitMQ broker. For async background work, `asyncio.create_task()` works within async endpoints.\nCode signature: Use BackgroundTasks for lightweight tasks or Celery for heavy/distributed work.", "file_path": "https://stackoverflow.com/questions/8077999", "function_name": "Use BackgroundTasks for lightweight tasks or Celery for heavy/distributed work.", "type": "stack_overflow", "metadata": {"record_id": "81620b22-0123-4f64-8732-c62f90827e6a", "source_type": "stack_overflow", "domain": "FastAPI/Flask/Django", "library": "django", "title": "How to run background tasks in FastAPI?", "content": "FastAPI's `BackgroundTasks`: add `background_tasks: BackgroundTasks` as a parameter, then `background_tasks.add_task(func, *args)`. Runs after response is sent. For heavy workloads (email, ML inference), use Celery with Redis/RabbitMQ broker. For async background work, `asyncio.create_task()` works within async endpoints.", "code_signature": "Use BackgroundTasks for lightweight tasks or Celery for heavy/distributed work.", "tags": "middleware, fastapi, response, routing, orm, request", "url": "https://stackoverflow.com/questions/8077999", "author": "user_202401", "date_published": "08-11-2023", "votes_or_stars": 1848, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:7403b976-871c-4016-9cc1-3a2225329bbe": {"content": "requests — HTTP Session Management\nThe `requests.Session` function in requests creates a persistent HTTP session with connection pooling. It accepts `` as a parameter and returns Session object. Common use cases include making multiple requests to the same host efficiently. Note that always close or use Session as context manager to release connections.\nCode signature: requests.Session() -> Session object", "file_path": "https://docs.requests.org/en/stable/requests/Session.html", "function_name": "requests.Session() -> Session object", "type": "documentation", "metadata": {"record_id": "7403b976-871c-4016-9cc1-3a2225329bbe", "source_type": "documentation", "domain": "Requests/HTTP/APIs", "library": "requests", "title": "requests — HTTP Session Management", "content": "The `requests.Session` function in requests creates a persistent HTTP session with connection pooling. It accepts `` as a parameter and returns Session object. Common use cases include making multiple requests to the same host efficiently. Note that always close or use Session as context manager to release connections.", "code_signature": "requests.Session() -> Session object", "tags": "rest-api, headers, aiohttp", "url": "https://docs.requests.org/en/stable/requests/Session.html", "author": "Official Docs", "date_published": "18-07-2022", "votes_or_stars": 76, "relevance_label": "low", "difficulty_level": "advanced"}}, "kb:704075e7-573c-42a3-a066-64b8a190f256": {"content": "How to handle rate limiting with the requests library?\n```python\nfrom tenacity import retry, wait_exponential, stop_after_attempt\n@retry(wait=wait_exponential(min=1, max=60), stop=stop_after_attempt(5))\ndef call_api(url):\n    r = requests.get(url)\n    r.raise_for_status()\n    return r.json()\n```\nCheck `Retry-After` header in 429 responses. For async: use `aiohttp` with tenacity.\nCode signature: Implement exponential backoff with the tenacity or backoff library.", "file_path": "https://stackoverflow.com/questions/1604451", "function_name": "Implement exponential backoff with the tenacity or backoff library.", "type": "stack_overflow", "metadata": {"record_id": "704075e7-573c-42a3-a066-64b8a190f256", "source_type": "stack_overflow", "domain": "Requests/HTTP/APIs", "library": "retry", "title": "How to handle rate limiting with the requests library?", "content": "```python\nfrom tenacity import retry, wait_exponential, stop_after_attempt\n@retry(wait=wait_exponential(min=1, max=60), stop=stop_after_attempt(5))\ndef call_api(url):\n    r = requests.get(url)\n    r.raise_for_status()\n    return r.json()\n```\nCheck `Retry-After` header in 429 responses. For async: use `aiohttp` with tenacity.", "code_signature": "Implement exponential backoff with the tenacity or backoff library.", "tags": "rate-limiting, http, websocket, timeout, requests, authentication", "url": "https://stackoverflow.com/questions/1604451", "author": "user_885136", "date_published": "17-02-2022", "votes_or_stars": 634, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:d1d42f99-aa5c-46f3-b266-a76c63d7a08f": {"content": "Why is my Pandas merge creating duplicate rows?\nDuplicate rows after merge usually mean the join key is not unique. Diagnose with `df.duplicated(subset=['key']).sum()`. Use `how='left'` carefully when right table has multiple matches. Set `validate='one_to_many'` or `'many_to_one'` to enforce cardinality. Consider deduplicating with `df.drop_duplicates(subset=['key'])` before merging.\nCode signature: Your merge key has duplicates in one or both DataFrames. Use validate='one_to_one' to catch this early.", "file_path": "https://stackoverflow.com/questions/2321324", "function_name": "Your merge key has duplicates in one or both DataFrames. Use validate='one_to_one' to catch this early.", "type": "stack_overflow", "metadata": {"record_id": "d1d42f99-aa5c-46f3-b266-a76c63d7a08f", "source_type": "stack_overflow", "domain": "NumPy/Pandas", "library": "array", "title": "Why is my Pandas merge creating duplicate rows?", "content": "Duplicate rows after merge usually mean the join key is not unique. Diagnose with `df.duplicated(subset=['key']).sum()`. Use `how='left'` carefully when right table has multiple matches. Set `validate='one_to_many'` or `'many_to_one'` to enforce cardinality. Consider deduplicating with `df.drop_duplicates(subset=['key'])` before merging.", "code_signature": "Your merge key has duplicates in one or both DataFrames. Use validate='one_to_one' to catch this early.", "tags": "pivot, dataframe, merge, vectorization, loc", "url": "https://stackoverflow.com/questions/2321324", "author": "user_99814", "date_published": "06-12-2022", "votes_or_stars": 242, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:bb236202-9931-4c82-a481-dea14bbedead": {"content": "requests — HTTP Session Management\nThe `requests.Session` function in requests creates a persistent HTTP session with connection pooling. It accepts `` as a parameter and returns Session object. Common use cases include making multiple requests to the same host efficiently. Note that always close or use Session as context manager to release connections.\nCode signature: requests.Session() -> Session object", "file_path": "https://docs.requests.org/en/stable/requests/Session.html", "function_name": "requests.Session() -> Session object", "type": "documentation", "metadata": {"record_id": "bb236202-9931-4c82-a481-dea14bbedead", "source_type": "documentation", "domain": "Requests/HTTP/APIs", "library": "requests", "title": "requests — HTTP Session Management", "content": "The `requests.Session` function in requests creates a persistent HTTP session with connection pooling. It accepts `` as a parameter and returns Session object. Common use cases include making multiple requests to the same host efficiently. Note that always close or use Session as context manager to release connections.", "code_signature": "requests.Session() -> Session object", "tags": "headers, http, httpx, timeout, json, retry", "url": "https://docs.requests.org/en/stable/requests/Session.html", "author": "Official Docs", "date_published": "17-09-2021", "votes_or_stars": 63, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:4326d01d-2f75-40de-849d-ac222b20bd6a": {"content": "How to use create_engine in SQLAlchemy\n`SQLAlchemy.create_engine` creates a database engine representing a connection pool. Example usage:\n```python\nengine = create_engine('postgresql://user:pass@host/db', echo=True)\n```\nThis is useful when establishing the central database connection for an application. Performance tip: set pool_size and max_overflow based on expected concurrency.\nCode signature: create_engine", "file_path": "https://docs.sqlalchemy.org/en/stable/create_engine.html", "function_name": "create_engine", "type": "documentation", "metadata": {"record_id": "4326d01d-2f75-40de-849d-ac222b20bd6a", "source_type": "documentation", "domain": "SQLAlchemy/Databases", "library": "SQLAlchemy", "title": "How to use create_engine in SQLAlchemy", "content": "`SQLAlchemy.create_engine` creates a database engine representing a connection pool. Example usage:\n```python\nengine = create_engine('postgresql://user:pass@host/db', echo=True)\n```\nThis is useful when establishing the central database connection for an application. Performance tip: set pool_size and max_overflow based on expected concurrency.", "code_signature": "create_engine", "tags": "foreign-key, sqlalchemy, query, session, postgresql", "url": "https://docs.sqlalchemy.org/en/stable/create_engine.html", "author": "Official Docs", "date_published": "19-03-2023", "votes_or_stars": 2057, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:47fc6444-b233-45ba-a392-d581a74fd352": {"content": "FastAPI — Path Operations\nThe `@app.get` function in FastAPI registers a GET endpoint at the specified path. It accepts `path, response_model` as a parameter and returns Response. Common use cases include building REST API endpoints that return JSON data. Note that use response_model to enforce output schema validation.\nCode signature: @app.get(path, response_model) -> Response", "file_path": "https://docs.fastapi.org/en/stable/@app/get.html", "function_name": "@app.get(path, response_model) -> Response", "type": "documentation", "metadata": {"record_id": "47fc6444-b233-45ba-a392-d581a74fd352", "source_type": "documentation", "domain": "FastAPI/Flask/Django", "library": "FastAPI", "title": "FastAPI — Path Operations", "content": "The `@app.get` function in FastAPI registers a GET endpoint at the specified path. It accepts `path, response_model` as a parameter and returns Response. Common use cases include building REST API endpoints that return JSON data. Note that use response_model to enforce output schema validation.", "code_signature": "@app.get(path, response_model) -> Response", "tags": "rest, orm, blueprint, request", "url": "https://docs.fastapi.org/en/stable/@app/get.html", "author": "Official Docs", "date_published": "05-04-2021", "votes_or_stars": 2879, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:e1bb0026-2f96-4c86-b9c8-0a2b65fab017": {"content": "PERF: concurrent.futures.ProcessPoolExecutor slow startup on Windows\nProblem: ProcessPoolExecutor takes 3-5 seconds to start on Windows due to process spawning overhead.\n\nFix/Discussion: Windows uses 'spawn' start method (vs 'fork' on Linux). Mitigations: (1) Reuse executor across calls — don't create/destroy in loops. (2) Use 'forkserver' or 'fork' on Linux/macOS. (3) For short tasks, ThreadPoolExecutor may be faster. (4) Use `initializer` param to pre-load modules.\nCode signature: status:closed", "file_path": "https://github.com/cpython/cpython/issues/6647", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "e1bb0026-2f96-4c86-b9c8-0a2b65fab017", "source_type": "github_issue", "domain": "Asyncio/Threading", "library": "cpython", "title": "PERF: concurrent.futures.ProcessPoolExecutor slow startup on Windows", "content": "Problem: ProcessPoolExecutor takes 3-5 seconds to start on Windows due to process spawning overhead.\n\nFix/Discussion: Windows uses 'spawn' start method (vs 'fork' on Linux). Mitigations: (1) Reuse executor across calls — don't create/destroy in loops. (2) Use 'forkserver' or 'fork' on Linux/macOS. (3) For short tasks, ThreadPoolExecutor may be faster. (4) Use `initializer` param to pre-load modules.", "code_signature": "status:closed", "tags": "await, lock, threading, deadlock, semaphore, event-loop", "url": "https://github.com/cpython/cpython/issues/6647", "author": "contributor_4097", "date_published": "21-11-2022", "votes_or_stars": 237, "relevance_label": "low", "difficulty_level": "beginner"}}, "kb:d10fe05b-ceaf-4040-bda9-4089576c7cf1": {"content": "How do I efficiently fill NaN values in a Pandas DataFrame?\nFor large DataFrames, avoid looping. Use `df.fillna({'col': val})` to fill specific columns. `df.ffill()` propagates last valid observation forward. For time-series data, `df.interpolate()` provides smoother results. Always check `df.isna().sum()` before and after to verify.\nCode signature: Use df.fillna(value) or df.ffill()/df.bfill() for forward/backward fill.", "file_path": "https://stackoverflow.com/questions/6438436", "function_name": "Use df.fillna(value) or df.ffill()/df.bfill() for forward/backward fill.", "type": "stack_overflow", "metadata": {"record_id": "d10fe05b-ceaf-4040-bda9-4089576c7cf1", "source_type": "stack_overflow", "domain": "NumPy/Pandas", "library": "loc", "title": "How do I efficiently fill NaN values in a Pandas DataFrame?", "content": "For large DataFrames, avoid looping. Use `df.fillna({'col': val})` to fill specific columns. `df.ffill()` propagates last valid observation forward. For time-series data, `df.interpolate()` provides smoother results. Always check `df.isna().sum()` before and after to verify.", "code_signature": "Use df.fillna(value) or df.ffill()/df.bfill() for forward/backward fill.", "tags": "pivot, broadcasting, groupby, vectorization, reshape, iloc", "url": "https://stackoverflow.com/questions/6438436", "author": "user_522340", "date_published": "12-01-2022", "votes_or_stars": 1, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:885d4710-a6f7-4d22-ab63-12c33e767977": {"content": "NumPy broadcasting error: operands could not be broadcast with shapes\nNumPy broadcasts shapes right-to-left. Arrays (3,4) and (4,) are compatible. Arrays (3,4) and (3,) are not — reshape to (3,1). Use `arr.reshape(-1,1)` to add a dimension. Always print `a.shape, b.shape` before operations to debug. `np.expand_dims(arr, axis=1)` is cleaner than reshape for adding dimensions.\nCode signature: Shapes must be compatible: dimensions must match or one of them must be 1.", "file_path": "https://stackoverflow.com/questions/4860684", "function_name": "Shapes must be compatible: dimensions must match or one of them must be 1.", "type": "stack_overflow", "metadata": {"record_id": "885d4710-a6f7-4d22-ab63-12c33e767977", "source_type": "stack_overflow", "domain": "NumPy/Pandas", "library": "numpy", "title": "NumPy broadcasting error: operands could not be broadcast with shapes", "content": "NumPy broadcasts shapes right-to-left. Arrays (3,4) and (4,) are compatible. Arrays (3,4) and (3,) are not — reshape to (3,1). Use `arr.reshape(-1,1)` to add a dimension. Always print `a.shape, b.shape` before operations to debug. `np.expand_dims(arr, axis=1)` is cleaner than reshape for adding dimensions.", "code_signature": "Shapes must be compatible: dimensions must match or one of them must be 1.", "tags": "merge, nan, pivot, vectorization, broadcasting", "url": "https://stackoverflow.com/questions/4860684", "author": "user_627024", "date_published": "08-08-2021", "votes_or_stars": 1406, "relevance_label": "low", "difficulty_level": "beginner"}}, "kb:4ea0d6fc-cf89-4a94-adb3-713e98b104a6": {"content": "BUG: httpx: SSL certificate error on Windows despite valid cert\nProblem: httpx raises SSLCertVerificationError on Windows when system CA store has the cert.\n\nFix/Discussion: httpx uses its own CA bundle (certifi) by default, not the system store. Fix: `httpx.Client(verify=certifi.where())` — already default. For custom certs: `httpx.Client(verify='/path/to/ca-bundle.pem')`. Install `pip install truststore` and use `ssl.create_default_context(ssl.Purpose.SERVER_AUTH)` to use system store.\nCode signature: status:closed", "file_path": "https://github.com/httpx/httpx/issues/8949", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "4ea0d6fc-cf89-4a94-adb3-713e98b104a6", "source_type": "github_issue", "domain": "Requests/HTTP/APIs", "library": "httpx", "title": "BUG: httpx: SSL certificate error on Windows despite valid cert", "content": "Problem: httpx raises SSLCertVerificationError on Windows when system CA store has the cert.\n\nFix/Discussion: httpx uses its own CA bundle (certifi) by default, not the system store. Fix: `httpx.Client(verify=certifi.where())` — already default. For custom certs: `httpx.Client(verify='/path/to/ca-bundle.pem')`. Install `pip install truststore` and use `ssl.create_default_context(ssl.Purpose.SERVER_AUTH)` to use system store.", "code_signature": "status:closed", "tags": "requests, aiohttp, timeout", "url": "https://github.com/httpx/httpx/issues/8949", "author": "contributor_1420", "date_published": "10-04-2021", "votes_or_stars": 467, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:35766141-70ed-4cb8-bfff-736f8ed7f5b0": {"content": "SQLAlchemy: how to avoid DetachedInstanceError?\nDetachedInstanceError occurs when accessing a relationship after the session is closed. Solutions: (1) Use `joinedload()` or `subqueryload()` to eagerly load relationships. (2) Keep session open while accessing objects. (3) Use `expire_on_commit=False` on sessionmaker. (4) Convert to dict/DTO before closing session.\nCode signature: Access lazy-loaded relationships within the session scope, or use eager loading.", "file_path": "https://stackoverflow.com/questions/1247595", "function_name": "Access lazy-loaded relationships within the session scope, or use eager loading.", "type": "stack_overflow", "metadata": {"record_id": "35766141-70ed-4cb8-bfff-736f8ed7f5b0", "source_type": "stack_overflow", "domain": "SQLAlchemy/Databases", "library": "sqlalchemy", "title": "SQLAlchemy: how to avoid DetachedInstanceError?", "content": "DetachedInstanceError occurs when accessing a relationship after the session is closed. Solutions: (1) Use `joinedload()` or `subqueryload()` to eagerly load relationships. (2) Keep session open while accessing objects. (3) Use `expire_on_commit=False` on sessionmaker. (4) Convert to dict/DTO before closing session.", "code_signature": "Access lazy-loaded relationships within the session scope, or use eager loading.", "tags": "alembic, orm, connection-pool, sqlalchemy", "url": "https://stackoverflow.com/questions/1247595", "author": "user_107793", "date_published": "09-07-2023", "votes_or_stars": 393, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:f48e34fa-a29b-4c76-914e-2a031ab96e12": {"content": "How do I efficiently fill NaN values in a Pandas DataFrame?\nFor large DataFrames, avoid looping. Use `df.fillna({'col': val})` to fill specific columns. `df.ffill()` propagates last valid observation forward. For time-series data, `df.interpolate()` provides smoother results. Always check `df.isna().sum()` before and after to verify.\nCode signature: Use df.fillna(value) or df.ffill()/df.bfill() for forward/backward fill.", "file_path": "https://stackoverflow.com/questions/6438436", "function_name": "Use df.fillna(value) or df.ffill()/df.bfill() for forward/backward fill.", "type": "stack_overflow", "metadata": {"record_id": "f48e34fa-a29b-4c76-914e-2a031ab96e12", "source_type": "stack_overflow", "domain": "NumPy/Pandas", "library": "loc", "title": "How do I efficiently fill NaN values in a Pandas DataFrame?", "content": "For large DataFrames, avoid looping. Use `df.fillna({'col': val})` to fill specific columns. `df.ffill()` propagates last valid observation forward. For time-series data, `df.interpolate()` provides smoother results. Always check `df.isna().sum()` before and after to verify.", "code_signature": "Use df.fillna(value) or df.ffill()/df.bfill() for forward/backward fill.", "tags": "pandas, pivot, dtype", "url": "https://stackoverflow.com/questions/6438436", "author": "user_522340", "date_published": "29-12-2022", "votes_or_stars": 5, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:169c9468-e055-4213-90b1-f68385df366a": {"content": "PERF: requests connection not reused between calls — new TCP connection each time\nProblem: Each requests.get() call opens a new TCP connection, causing high latency.\n\nFix/Discussion: requests.get/post etc. create a new Session per call. Fix: use a persistent Session: `session = requests.Session()` and reuse it. Sessions pool connections via urllib3. For thread-safety: create one Session per thread. For async: use httpx.AsyncClient or aiohttp.ClientSession for async connection pooling.\nCode signature: status:closed", "file_path": "https://github.com/requests/requests/issues/3605", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "169c9468-e055-4213-90b1-f68385df366a", "source_type": "github_issue", "domain": "Requests/HTTP/APIs", "library": "requests", "title": "PERF: requests connection not reused between calls — new TCP connection each time", "content": "Problem: Each requests.get() call opens a new TCP connection, causing high latency.\n\nFix/Discussion: requests.get/post etc. create a new Session per call. Fix: use a persistent Session: `session = requests.Session()` and reuse it. Sessions pool connections via urllib3. For thread-safety: create one Session per thread. For async: use httpx.AsyncClient or aiohttp.ClientSession for async connection pooling.", "code_signature": "status:closed", "tags": "json, timeout, rest-api", "url": "https://github.com/requests/requests/issues/3605", "author": "contributor_1152", "date_published": "03-09-2022", "votes_or_stars": 373, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:dd9a7f69-5a7d-4c30-8231-edb0d1afac3b": {"content": "Django N+1 query problem: how to fix?\nN+1 occurs when accessing related objects in a loop. Diagnose with `django-debug-toolbar` or `connection.queries`. Fix ForeignKey: `User.objects.select_related('profile').all()`. Fix M2M: `Post.objects.prefetch_related('tags').all()`. Use `only()` to fetch specific fields and reduce data transfer.\nCode signature: Use select_related() for ForeignKey and prefetch_related() for ManyToMany relationships.", "file_path": "https://stackoverflow.com/questions/8106470", "function_name": "Use select_related() for ForeignKey and prefetch_related() for ManyToMany relationships.", "type": "stack_overflow", "metadata": {"record_id": "dd9a7f69-5a7d-4c30-8231-edb0d1afac3b", "source_type": "stack_overflow", "domain": "FastAPI/Flask/Django", "library": "request", "title": "Django N+1 query problem: how to fix?", "content": "N+1 occurs when accessing related objects in a loop. Diagnose with `django-debug-toolbar` or `connection.queries`. Fix ForeignKey: `User.objects.select_related('profile').all()`. Fix M2M: `Post.objects.prefetch_related('tags').all()`. Use `only()` to fetch specific fields and reduce data transfer.", "code_signature": "Use select_related() for ForeignKey and prefetch_related() for ManyToMany relationships.", "tags": "middleware, request, orm, routing", "url": "https://stackoverflow.com/questions/8106470", "author": "user_441071", "date_published": "11-09-2021", "votes_or_stars": 1773, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:0afbce9d-6fc4-4a71-9a5a-a854dd9127fc": {"content": "How to apply a function to each row in Pandas without using iterrows?\n`iterrows()` is slow because it creates a Series per row. Prefer `df.apply(func, axis=1)` for row-wise operations, or better yet, vectorize using NumPy: `np.where(condition, a, b)`. For complex logic, `df.assign()` with vectorized expressions is cleanest. Benchmark: vectorized > apply > itertuples > iterrows.\nCode signature: Use df.apply(func, axis=1) or vectorized NumPy operations for best performance.", "file_path": "https://stackoverflow.com/questions/5446912", "function_name": "Use df.apply(func, axis=1) or vectorized NumPy operations for best performance.", "type": "stack_overflow", "metadata": {"record_id": "0afbce9d-6fc4-4a71-9a5a-a854dd9127fc", "source_type": "stack_overflow", "domain": "NumPy/Pandas", "library": "dataframe", "title": "How to apply a function to each row in Pandas without using iterrows?", "content": "`iterrows()` is slow because it creates a Series per row. Prefer `df.apply(func, axis=1)` for row-wise operations, or better yet, vectorize using NumPy: `np.where(condition, a, b)`. For complex logic, `df.assign()` with vectorized expressions is cleanest. Benchmark: vectorized > apply > itertuples > iterrows.", "code_signature": "Use df.apply(func, axis=1) or vectorized NumPy operations for best performance.", "tags": "pandas, pivot, dtype, iloc, array, loc", "url": "https://stackoverflow.com/questions/5446912", "author": "user_563306", "date_published": "26-02-2024", "votes_or_stars": 2286, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:7eaec7ff-ab39-4507-9f69-f4a05c8a424b": {"content": "How to add retry logic to requests?\n```python\nfrom requests.adapters import HTTPAdapter\nfrom urllib3.util.retry import Retry\nretry = Retry(total=3, backoff_factor=1, status_forcelist=[500,502,503,504])\nadapter = HTTPAdapter(max_retries=retry)\nsession.mount('https://', adapter)\n```\nCode signature: Mount an HTTPAdapter with urllib3 Retry on the Session.", "file_path": "https://stackoverflow.com/questions/1669324", "function_name": "Mount an HTTPAdapter with urllib3 Retry on the Session.", "type": "stack_overflow", "metadata": {"record_id": "7eaec7ff-ab39-4507-9f69-f4a05c8a424b", "source_type": "stack_overflow", "domain": "Requests/HTTP/APIs", "library": "websocket", "title": "How to add retry logic to requests?", "content": "```python\nfrom requests.adapters import HTTPAdapter\nfrom urllib3.util.retry import Retry\nretry = Retry(total=3, backoff_factor=1, status_forcelist=[500,502,503,504])\nadapter = HTTPAdapter(max_retries=retry)\nsession.mount('https://', adapter)\n```", "code_signature": "Mount an HTTPAdapter with urllib3 Retry on the Session.", "tags": "rest-api, aiohttp, http, oauth, json, requests", "url": "https://stackoverflow.com/questions/1669324", "author": "user_952590", "date_published": "02-11-2019", "votes_or_stars": 1509, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:852b858f-2b1c-4c0d-a175-cbff2887eb05": {"content": "Django ORM Queries — official reference\nOfficial documentation for Django ORM Queries. The method signature is `QuerySet.filter(**kwargs)`. filters QuerySet rows matching given field lookups. Raises `FieldError` when an unknown field lookup is provided. See also: QuerySet.exclude, QuerySet.annotate, Q objects.\nCode signature: QuerySet.filter(**kwargs)", "file_path": "https://docs.django.org/en/stable/QuerySet/filter.html", "function_name": "QuerySet.filter(**kwargs)", "type": "documentation", "metadata": {"record_id": "852b858f-2b1c-4c0d-a175-cbff2887eb05", "source_type": "documentation", "domain": "FastAPI/Flask/Django", "library": "Django", "title": "Django ORM Queries — official reference", "content": "Official documentation for Django ORM Queries. The method signature is `QuerySet.filter(**kwargs)`. filters QuerySet rows matching given field lookups. Raises `FieldError` when an unknown field lookup is provided. See also: QuerySet.exclude, QuerySet.annotate, Q objects.", "code_signature": "QuerySet.filter(**kwargs)", "tags": "fastapi, template, routing", "url": "https://docs.django.org/en/stable/QuerySet/filter.html", "author": "Official Docs", "date_published": "02-05-2024", "votes_or_stars": 2420, "relevance_label": "low", "difficulty_level": "beginner"}}, "kb:d557af26-28a3-45ce-99aa-888711a2fbab": {"content": "requests — HTTP Session Management\nThe `requests.Session` function in requests creates a persistent HTTP session with connection pooling. It accepts `` as a parameter and returns Session object. Common use cases include making multiple requests to the same host efficiently. Note that always close or use Session as context manager to release connections.\nCode signature: requests.Session() -> Session object", "file_path": "https://docs.requests.org/en/stable/requests/Session.html", "function_name": "requests.Session() -> Session object", "type": "documentation", "metadata": {"record_id": "d557af26-28a3-45ce-99aa-888711a2fbab", "source_type": "documentation", "domain": "Requests/HTTP/APIs", "library": "requests", "title": "requests — HTTP Session Management", "content": "The `requests.Session` function in requests creates a persistent HTTP session with connection pooling. It accepts `` as a parameter and returns Session object. Common use cases include making multiple requests to the same host efficiently. Note that always close or use Session as context manager to release connections.", "code_signature": "requests.Session() -> Session object", "tags": "websocket, http, json", "url": "https://docs.requests.org/en/stable/requests/Session.html", "author": "Official Docs", "date_published": "14-09-2020", "votes_or_stars": 63, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:4d534a57-0a5f-4299-87f3-afcc4d968e0d": {"content": "PERF: Django template rendering bottleneck in production\nProblem: Template rendering takes 500ms+ for complex pages with many database queries.\n\nFix/Discussion: Primary cause is N+1 queries in template. Solutions: (1) Use select_related/prefetch_related. (2) Cache rendered templates with `cache` template tag. (3) Use `django-cachalot` for query caching. (4) Move heavy computation to views, pass pre-computed context. (5) Consider server-side caching with Redis.\nCode signature: status:closed", "file_path": "https://github.com/django/django/issues/3243", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "4d534a57-0a5f-4299-87f3-afcc4d968e0d", "source_type": "github_issue", "domain": "FastAPI/Flask/Django", "library": "django", "title": "PERF: Django template rendering bottleneck in production", "content": "Problem: Template rendering takes 500ms+ for complex pages with many database queries.\n\nFix/Discussion: Primary cause is N+1 queries in template. Solutions: (1) Use select_related/prefetch_related. (2) Cache rendered templates with `cache` template tag. (3) Use `django-cachalot` for query caching. (4) Move heavy computation to views, pass pre-computed context. (5) Consider server-side caching with Redis.", "code_signature": "status:closed", "tags": "flask, fastapi, dependency-injection, request, blueprint, routing", "url": "https://github.com/django/django/issues/3243", "author": "contributor_6988", "date_published": "17-02-2022", "votes_or_stars": 79, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:736c8f5f-cc91-465c-a6fd-19ad85bb92f6": {"content": "How to read a large file line by line in Python without loading it all into memory?\n```python\nwith open('large.txt', 'r') as f:\n    for line in f:\n        process(line.strip())\n```\nThis reads one line at a time. For binary files: use `f.read(chunk_size)` in a loop. For CSV: use `pd.read_csv(file, chunksize=10000)`. Never use `f.readlines()` on large files.\nCode signature: Iterate over the file object directly — it yields lines lazily.", "file_path": "https://stackoverflow.com/questions/2737893", "function_name": "Iterate over the file object directly — it yields lines lazily.", "type": "stack_overflow", "metadata": {"record_id": "736c8f5f-cc91-465c-a6fd-19ad85bb92f6", "source_type": "stack_overflow", "domain": "OS/Subprocess/File I/O", "library": "shutil", "title": "How to read a large file line by line in Python without loading it all into memory?", "content": "```python\nwith open('large.txt', 'r') as f:\n    for line in f:\n        process(line.strip())\n```\nThis reads one line at a time. For binary files: use `f.read(chunk_size)` in a loop. For CSV: use `pd.read_csv(file, chunksize=10000)`. Never use `f.readlines()` on large files.", "code_signature": "Iterate over the file object directly — it yields lines lazily.", "tags": "subprocess, process, stdout, stdin, pipe", "url": "https://stackoverflow.com/questions/2737893", "author": "user_994539", "date_published": "03-09-2022", "votes_or_stars": 2262, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:ceaf93aa-f3cd-47d7-9f4b-8c342986c9ac": {"content": "threading Thread Synchronization — official reference\nOfficial documentation for threading Thread Synchronization. The method signature is `threading.Lock()`. creates a mutual-exclusion lock for thread-safe access. Raises `RuntimeError` when lock is released without being acquired. See also: threading.RLock, threading.Semaphore, threading.Event.\nCode signature: threading.Lock()", "file_path": "https://docs.threading.org/en/stable/threading/Lock.html", "function_name": "threading.Lock()", "type": "documentation", "metadata": {"record_id": "ceaf93aa-f3cd-47d7-9f4b-8c342986c9ac", "source_type": "documentation", "domain": "Asyncio/Threading", "library": "threading", "title": "threading Thread Synchronization — official reference", "content": "Official documentation for threading Thread Synchronization. The method signature is `threading.Lock()`. creates a mutual-exclusion lock for thread-safe access. Raises `RuntimeError` when lock is released without being acquired. See also: threading.RLock, threading.Semaphore, threading.Event.", "code_signature": "threading.Lock()", "tags": "coroutine, lock, semaphore, gather", "url": "https://docs.threading.org/en/stable/threading/Lock.html", "author": "Official Docs", "date_published": "03-04-2020", "votes_or_stars": 4811, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:0628a03d-c4fd-45b2-9404-1bd4637ff1c0": {"content": "Flask: how to return JSON responses properly?\n`return jsonify({'key': 'value'})` sets Content-Type to application/json. Flask 1.0+ automatically converts returned dicts to JSON responses. For custom status codes: `return jsonify(data), 201`. For errors: use `abort(404)` or return `({'error': 'Not found'}, 404)`.\nCode signature: Use jsonify() or return a dict directly (Flask 1.0+).", "file_path": "https://stackoverflow.com/questions/8930103", "function_name": "Use jsonify() or return a dict directly (Flask 1.0+).", "type": "stack_overflow", "metadata": {"record_id": "0628a03d-c4fd-45b2-9404-1bd4637ff1c0", "source_type": "stack_overflow", "domain": "FastAPI/Flask/Django", "library": "django", "title": "Flask: how to return JSON responses properly?", "content": "`return jsonify({'key': 'value'})` sets Content-Type to application/json. Flask 1.0+ automatically converts returned dicts to JSON responses. For custom status codes: `return jsonify(data), 201`. For errors: use `abort(404)` or return `({'error': 'Not found'}, 404)`.", "code_signature": "Use jsonify() or return a dict directly (Flask 1.0+).", "tags": "blueprint, fastapi, template", "url": "https://stackoverflow.com/questions/8930103", "author": "user_264801", "date_published": "03-12-2020", "votes_or_stars": 2225, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:9796a1a1-3399-4c54-a137-ada1edc7126a": {"content": "PERF: Django template rendering bottleneck in production\nProblem: Template rendering takes 500ms+ for complex pages with many database queries.\n\nFix/Discussion: Primary cause is N+1 queries in template. Solutions: (1) Use select_related/prefetch_related. (2) Cache rendered templates with `cache` template tag. (3) Use `django-cachalot` for query caching. (4) Move heavy computation to views, pass pre-computed context. (5) Consider server-side caching with Redis.\nCode signature: status:closed", "file_path": "https://github.com/django/django/issues/3243", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "9796a1a1-3399-4c54-a137-ada1edc7126a", "source_type": "github_issue", "domain": "FastAPI/Flask/Django", "library": "django", "title": "PERF: Django template rendering bottleneck in production", "content": "Problem: Template rendering takes 500ms+ for complex pages with many database queries.\n\nFix/Discussion: Primary cause is N+1 queries in template. Solutions: (1) Use select_related/prefetch_related. (2) Cache rendered templates with `cache` template tag. (3) Use `django-cachalot` for query caching. (4) Move heavy computation to views, pass pre-computed context. (5) Consider server-side caching with Redis.", "code_signature": "status:closed", "tags": "dependency-injection, fastapi, blueprint, template, pydantic", "url": "https://github.com/django/django/issues/3243", "author": "contributor_6988", "date_published": "02-07-2024", "votes_or_stars": 59, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:f5edd406-63ab-46c6-a4b3-890fe25e5b47": {"content": "How to handle database migrations with Alembic?\nWorkflow: (1) Change SQLAlchemy models. (2) `alembic revision --autogenerate -m 'description'`. (3) Review generated migration script. (4) `alembic upgrade head` to apply. Always review autogenerated migrations — they miss some changes (indexes, constraints). Use `alembic downgrade -1` to rollback. Store migrations in version control.\nCode signature: Use alembic revision --autogenerate to create migrations from model changes.", "file_path": "https://stackoverflow.com/questions/9436429", "function_name": "Use alembic revision --autogenerate to create migrations from model changes.", "type": "stack_overflow", "metadata": {"record_id": "f5edd406-63ab-46c6-a4b3-890fe25e5b47", "source_type": "stack_overflow", "domain": "SQLAlchemy/Databases", "library": "connection-pool", "title": "How to handle database migrations with Alembic?", "content": "Workflow: (1) Change SQLAlchemy models. (2) `alembic revision --autogenerate -m 'description'`. (3) Review generated migration script. (4) `alembic upgrade head` to apply. Always review autogenerated migrations — they miss some changes (indexes, constraints). Use `alembic downgrade -1` to rollback. Store migrations in version control.", "code_signature": "Use alembic revision --autogenerate to create migrations from model changes.", "tags": "transaction, query, sqlalchemy, relationship, orm, postgresql", "url": "https://stackoverflow.com/questions/9436429", "author": "user_974047", "date_published": "10-08-2020", "votes_or_stars": 2430, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:dc5cc6b1-788e-4235-8c5d-618d86bfde6b": {"content": "os Environment Variables — official reference\nOfficial documentation for os Environment Variables. The method signature is `os.environ(key)`. provides a mapping of the current environment variables. Raises `KeyError` when key is accessed with [] notation and is not set. See also: dotenv, os.getenv, os.putenv.\nCode signature: os.environ(key)", "file_path": "https://docs.os.org/en/stable/os/environ.html", "function_name": "os.environ(key)", "type": "documentation", "metadata": {"record_id": "dc5cc6b1-788e-4235-8c5d-618d86bfde6b", "source_type": "documentation", "domain": "OS/Subprocess/File I/O", "library": "os", "title": "os Environment Variables — official reference", "content": "Official documentation for os Environment Variables. The method signature is `os.environ(key)`. provides a mapping of the current environment variables. Raises `KeyError` when key is accessed with [] notation and is not set. See also: dotenv, os.getenv, os.putenv.", "code_signature": "os.environ(key)", "tags": "pathlib, process, stderr, os", "url": "https://docs.os.org/en/stable/os/environ.html", "author": "Official Docs", "date_published": "10-10-2020", "votes_or_stars": 2252, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:fd80ac7b-665f-43e8-87cd-a1869dcacf39": {"content": "PERF: Django template rendering bottleneck in production\nProblem: Template rendering takes 500ms+ for complex pages with many database queries.\n\nFix/Discussion: Primary cause is N+1 queries in template. Solutions: (1) Use select_related/prefetch_related. (2) Cache rendered templates with `cache` template tag. (3) Use `django-cachalot` for query caching. (4) Move heavy computation to views, pass pre-computed context. (5) Consider server-side caching with Redis.\nCode signature: status:closed", "file_path": "https://github.com/django/django/issues/3243", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "fd80ac7b-665f-43e8-87cd-a1869dcacf39", "source_type": "github_issue", "domain": "FastAPI/Flask/Django", "library": "django", "title": "PERF: Django template rendering bottleneck in production", "content": "Problem: Template rendering takes 500ms+ for complex pages with many database queries.\n\nFix/Discussion: Primary cause is N+1 queries in template. Solutions: (1) Use select_related/prefetch_related. (2) Cache rendered templates with `cache` template tag. (3) Use `django-cachalot` for query caching. (4) Move heavy computation to views, pass pre-computed context. (5) Consider server-side caching with Redis.", "code_signature": "status:closed", "tags": "request, routing, fastapi, orm", "url": "https://github.com/django/django/issues/3243", "author": "contributor_6988", "date_published": "05-07-2022", "votes_or_stars": 60, "relevance_label": "low", "difficulty_level": "beginner"}}, "kb:99162d2a-cb22-40ec-a3df-297e998e5089": {"content": "How do I efficiently fill NaN values in a Pandas DataFrame?\nFor large DataFrames, avoid looping. Use `df.fillna({'col': val})` to fill specific columns. `df.ffill()` propagates last valid observation forward. For time-series data, `df.interpolate()` provides smoother results. Always check `df.isna().sum()` before and after to verify.\nCode signature: Use df.fillna(value) or df.ffill()/df.bfill() for forward/backward fill.", "file_path": "https://stackoverflow.com/questions/6438436", "function_name": "Use df.fillna(value) or df.ffill()/df.bfill() for forward/backward fill.", "type": "stack_overflow", "metadata": {"record_id": "99162d2a-cb22-40ec-a3df-297e998e5089", "source_type": "stack_overflow", "domain": "NumPy/Pandas", "library": "loc", "title": "How do I efficiently fill NaN values in a Pandas DataFrame?", "content": "For large DataFrames, avoid looping. Use `df.fillna({'col': val})` to fill specific columns. `df.ffill()` propagates last valid observation forward. For time-series data, `df.interpolate()` provides smoother results. Always check `df.isna().sum()` before and after to verify.", "code_signature": "Use df.fillna(value) or df.ffill()/df.bfill() for forward/backward fill.", "tags": "reshape, dataframe, iloc, groupby", "url": "https://stackoverflow.com/questions/6438436", "author": "user_522340", "date_published": "09-12-2018", "votes_or_stars": 1, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:3ad14c68-c949-489c-85fd-012b94d986f7": {"content": "How to read a large file line by line in Python without loading it all into memory?\n```python\nwith open('large.txt', 'r') as f:\n    for line in f:\n        process(line.strip())\n```\nThis reads one line at a time. For binary files: use `f.read(chunk_size)` in a loop. For CSV: use `pd.read_csv(file, chunksize=10000)`. Never use `f.readlines()` on large files.\nCode signature: Iterate over the file object directly — it yields lines lazily.", "file_path": "https://stackoverflow.com/questions/2737893", "function_name": "Iterate over the file object directly — it yields lines lazily.", "type": "stack_overflow", "metadata": {"record_id": "3ad14c68-c949-489c-85fd-012b94d986f7", "source_type": "stack_overflow", "domain": "OS/Subprocess/File I/O", "library": "shutil", "title": "How to read a large file line by line in Python without loading it all into memory?", "content": "```python\nwith open('large.txt', 'r') as f:\n    for line in f:\n        process(line.strip())\n```\nThis reads one line at a time. For binary files: use `f.read(chunk_size)` in a loop. For CSV: use `pd.read_csv(file, chunksize=10000)`. Never use `f.readlines()` on large files.", "code_signature": "Iterate over the file object directly — it yields lines lazily.", "tags": "pipe, stdin, stderr, stdout, glob", "url": "https://stackoverflow.com/questions/2737893", "author": "user_994539", "date_published": "12-05-2018", "votes_or_stars": 2310, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:23a3985d-0aa7-403c-8719-94d7425f62f0": {"content": "Python: how to safely write to a file atomically?\n```python\nimport tempfile, os\nwith tempfile.NamedTemporaryFile('w', delete=False, dir='.') as tmp:\n    tmp.write(data)\n    tmp.flush()\n    os.fsync(tmp.fileno())\nos.replace(tmp.name, target_path)\n```\n`os.replace()` is atomic on POSIX. This prevents partial writes from corrupting files.\nCode signature: Write to a temp file then rename — rename is atomic on POSIX systems.", "file_path": "https://stackoverflow.com/questions/5394880", "function_name": "Write to a temp file then rename — rename is atomic on POSIX systems.", "type": "stack_overflow", "metadata": {"record_id": "23a3985d-0aa7-403c-8719-94d7425f62f0", "source_type": "stack_overflow", "domain": "OS/Subprocess/File I/O", "library": "stdin", "title": "Python: how to safely write to a file atomically?", "content": "```python\nimport tempfile, os\nwith tempfile.NamedTemporaryFile('w', delete=False, dir='.') as tmp:\n    tmp.write(data)\n    tmp.flush()\n    os.fsync(tmp.fileno())\nos.replace(tmp.name, target_path)\n```\n`os.replace()` is atomic on POSIX. This prevents partial writes from corrupting files.", "code_signature": "Write to a temp file then rename — rename is atomic on POSIX systems.", "tags": "process, stdout, pipe", "url": "https://stackoverflow.com/questions/5394880", "author": "user_179430", "date_published": "16-10-2024", "votes_or_stars": 559, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:59854682-75a5-4dd2-955d-787dc7e25a69": {"content": "SQLAlchemy — ORM Session Query\nThe `session.query` function in SQLAlchemy constructs a SELECT query against mapped entities. It accepts `*entities` as a parameter and returns Query object. Common use cases include retrieving ORM-mapped objects from the database. Note that prefer session.execute(select(...)) in SQLAlchemy 2.0+.\nCode signature: session.query(*entities) -> Query object", "file_path": "https://docs.sqlalchemy.org/en/stable/session/query.html", "function_name": "session.query(*entities) -> Query object", "type": "documentation", "metadata": {"record_id": "59854682-75a5-4dd2-955d-787dc7e25a69", "source_type": "documentation", "domain": "SQLAlchemy/Databases", "library": "SQLAlchemy", "title": "SQLAlchemy — ORM Session Query", "content": "The `session.query` function in SQLAlchemy constructs a SELECT query against mapped entities. It accepts `*entities` as a parameter and returns Query object. Common use cases include retrieving ORM-mapped objects from the database. Note that prefer session.execute(select(...)) in SQLAlchemy 2.0+.", "code_signature": "session.query(*entities) -> Query object", "tags": "orm, foreign-key, transaction, sqlalchemy", "url": "https://docs.sqlalchemy.org/en/stable/session/query.html", "author": "Official Docs", "date_published": "10-06-2018", "votes_or_stars": 3016, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:97c394f6-9858-481b-a6f6-a441455615ad": {"content": "requests.exceptions.SSLError: certificate verify failed\nNever use `verify=False` in production — it disables SSL verification entirely. Solutions: (1) Update certifi: `pip install --upgrade certifi`. (2) Provide CA bundle: `requests.get(url, verify='/path/to/ca-bundle.crt')`. (3) For self-signed certs in dev: `verify=False` only, and never commit this. (4) Check system time — expired system clock causes SSL failures.\nCode signature: Your SSL certificate chain is incomplete or self-signed. Fix the cert, don't disable verification.", "file_path": "https://stackoverflow.com/questions/7907400", "function_name": "Your SSL certificate chain is incomplete or self-signed. Fix the cert, don't disable verification.", "type": "stack_overflow", "metadata": {"record_id": "97c394f6-9858-481b-a6f6-a441455615ad", "source_type": "stack_overflow", "domain": "Requests/HTTP/APIs", "library": "websocket", "title": "requests.exceptions.SSLError: certificate verify failed", "content": "Never use `verify=False` in production — it disables SSL verification entirely. Solutions: (1) Update certifi: `pip install --upgrade certifi`. (2) Provide CA bundle: `requests.get(url, verify='/path/to/ca-bundle.crt')`. (3) For self-signed certs in dev: `verify=False` only, and never commit this. (4) Check system time — expired system clock causes SSL failures.", "code_signature": "Your SSL certificate chain is incomplete or self-signed. Fix the cert, don't disable verification.", "tags": "httpx, websocket, retry, requests, json", "url": "https://stackoverflow.com/questions/7907400", "author": "user_851204", "date_published": "19-05-2020", "votes_or_stars": 1385, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:06b833dd-3caa-4888-b204-a5c98da8e197": {"content": "How to use create_engine in SQLAlchemy\n`SQLAlchemy.create_engine` creates a database engine representing a connection pool. Example usage:\n```python\nengine = create_engine('postgresql://user:pass@host/db', echo=True)\n```\nThis is useful when establishing the central database connection for an application. Performance tip: set pool_size and max_overflow based on expected concurrency.\nCode signature: create_engine", "file_path": "https://docs.sqlalchemy.org/en/stable/create_engine.html", "function_name": "create_engine", "type": "documentation", "metadata": {"record_id": "06b833dd-3caa-4888-b204-a5c98da8e197", "source_type": "documentation", "domain": "SQLAlchemy/Databases", "library": "SQLAlchemy", "title": "How to use create_engine in SQLAlchemy", "content": "`SQLAlchemy.create_engine` creates a database engine representing a connection pool. Example usage:\n```python\nengine = create_engine('postgresql://user:pass@host/db', echo=True)\n```\nThis is useful when establishing the central database connection for an application. Performance tip: set pool_size and max_overflow based on expected concurrency.", "code_signature": "create_engine", "tags": "migration, foreign-key, orm", "url": "https://docs.sqlalchemy.org/en/stable/create_engine.html", "author": "Official Docs", "date_published": "12-09-2023", "votes_or_stars": 2031, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:7b6b1c36-5784-4a0e-82b3-e8a55bb5bca2": {"content": "requests.exceptions.SSLError: certificate verify failed\nNever use `verify=False` in production — it disables SSL verification entirely. Solutions: (1) Update certifi: `pip install --upgrade certifi`. (2) Provide CA bundle: `requests.get(url, verify='/path/to/ca-bundle.crt')`. (3) For self-signed certs in dev: `verify=False` only, and never commit this. (4) Check system time — expired system clock causes SSL failures.\nCode signature: Your SSL certificate chain is incomplete or self-signed. Fix the cert, don't disable verification.", "file_path": "https://stackoverflow.com/questions/7907400", "function_name": "Your SSL certificate chain is incomplete or self-signed. Fix the cert, don't disable verification.", "type": "stack_overflow", "metadata": {"record_id": "7b6b1c36-5784-4a0e-82b3-e8a55bb5bca2", "source_type": "stack_overflow", "domain": "Requests/HTTP/APIs", "library": "websocket", "title": "requests.exceptions.SSLError: certificate verify failed", "content": "Never use `verify=False` in production — it disables SSL verification entirely. Solutions: (1) Update certifi: `pip install --upgrade certifi`. (2) Provide CA bundle: `requests.get(url, verify='/path/to/ca-bundle.crt')`. (3) For self-signed certs in dev: `verify=False` only, and never commit this. (4) Check system time — expired system clock causes SSL failures.", "code_signature": "Your SSL certificate chain is incomplete or self-signed. Fix the cert, don't disable verification.", "tags": "json, http, oauth, httpx, timeout", "url": "https://stackoverflow.com/questions/7907400", "author": "user_851204", "date_published": "15-08-2020", "votes_or_stars": 1370, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:756717ac-2fc0-4378-b5bf-053a4567dc93": {"content": "How to add retry logic to requests?\n```python\nfrom requests.adapters import HTTPAdapter\nfrom urllib3.util.retry import Retry\nretry = Retry(total=3, backoff_factor=1, status_forcelist=[500,502,503,504])\nadapter = HTTPAdapter(max_retries=retry)\nsession.mount('https://', adapter)\n```\nCode signature: Mount an HTTPAdapter with urllib3 Retry on the Session.", "file_path": "https://stackoverflow.com/questions/1669324", "function_name": "Mount an HTTPAdapter with urllib3 Retry on the Session.", "type": "stack_overflow", "metadata": {"record_id": "756717ac-2fc0-4378-b5bf-053a4567dc93", "source_type": "stack_overflow", "domain": "Requests/HTTP/APIs", "library": "websocket", "title": "How to add retry logic to requests?", "content": "```python\nfrom requests.adapters import HTTPAdapter\nfrom urllib3.util.retry import Retry\nretry = Retry(total=3, backoff_factor=1, status_forcelist=[500,502,503,504])\nadapter = HTTPAdapter(max_retries=retry)\nsession.mount('https://', adapter)\n```", "code_signature": "Mount an HTTPAdapter with urllib3 Retry on the Session.", "tags": "rate-limiting, authentication, oauth, headers, http, rest-api", "url": "https://stackoverflow.com/questions/1669324", "author": "user_952590", "date_published": "20-04-2020", "votes_or_stars": 1529, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:1a3aac30-7d9b-4f29-a7fc-f936bafd3c77": {"content": "threading Thread Synchronization — official reference\nOfficial documentation for threading Thread Synchronization. The method signature is `threading.Lock()`. creates a mutual-exclusion lock for thread-safe access. Raises `RuntimeError` when lock is released without being acquired. See also: threading.RLock, threading.Semaphore, threading.Event.\nCode signature: threading.Lock()", "file_path": "https://docs.threading.org/en/stable/threading/Lock.html", "function_name": "threading.Lock()", "type": "documentation", "metadata": {"record_id": "1a3aac30-7d9b-4f29-a7fc-f936bafd3c77", "source_type": "documentation", "domain": "Asyncio/Threading", "library": "threading", "title": "threading Thread Synchronization — official reference", "content": "Official documentation for threading Thread Synchronization. The method signature is `threading.Lock()`. creates a mutual-exclusion lock for thread-safe access. Raises `RuntimeError` when lock is released without being acquired. See also: threading.RLock, threading.Semaphore, threading.Event.", "code_signature": "threading.Lock()", "tags": "semaphore, task, asyncio", "url": "https://docs.threading.org/en/stable/threading/Lock.html", "author": "Official Docs", "date_published": "05-04-2019", "votes_or_stars": 4831, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:373e98bd-124b-41ba-9e51-de34299523f2": {"content": "BUG: threading.Timer not cancellable after it fires\nProblem: Calling cancel() on a Timer that has already executed does not raise an error but has no effect.\n\nFix/Discussion: threading.Timer.cancel() only works before the timer fires. Store Timer reference and check `timer.finished.is_set()` to know if it's already run. For repeating timers, use a loop with `threading.Event` for clean cancellation: `stop_event = threading.Event(); stop_event.wait(interval)`.\nCode signature: status:closed", "file_path": "https://github.com/cpython/cpython/issues/4111", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "373e98bd-124b-41ba-9e51-de34299523f2", "source_type": "github_issue", "domain": "Asyncio/Threading", "library": "cpython", "title": "BUG: threading.Timer not cancellable after it fires", "content": "Problem: Calling cancel() on a Timer that has already executed does not raise an error but has no effect.\n\nFix/Discussion: threading.Timer.cancel() only works before the timer fires. Store Timer reference and check `timer.finished.is_set()` to know if it's already run. For repeating timers, use a loop with `threading.Event` for clean cancellation: `stop_event = threading.Event(); stop_event.wait(interval)`.", "code_signature": "status:closed", "tags": "race-condition, semaphore, task, deadlock, async", "url": "https://github.com/cpython/cpython/issues/4111", "author": "contributor_7884", "date_published": "21-04-2020", "votes_or_stars": 56, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:2c03853d-ea98-4404-b40e-b8605d05842d": {"content": "Django ORM Queries — official reference\nOfficial documentation for Django ORM Queries. The method signature is `QuerySet.filter(**kwargs)`. filters QuerySet rows matching given field lookups. Raises `FieldError` when an unknown field lookup is provided. See also: QuerySet.exclude, QuerySet.annotate, Q objects.\nCode signature: QuerySet.filter(**kwargs)", "file_path": "https://docs.django.org/en/stable/QuerySet/filter.html", "function_name": "QuerySet.filter(**kwargs)", "type": "documentation", "metadata": {"record_id": "2c03853d-ea98-4404-b40e-b8605d05842d", "source_type": "documentation", "domain": "FastAPI/Flask/Django", "library": "Django", "title": "Django ORM Queries — official reference", "content": "Official documentation for Django ORM Queries. The method signature is `QuerySet.filter(**kwargs)`. filters QuerySet rows matching given field lookups. Raises `FieldError` when an unknown field lookup is provided. See also: QuerySet.exclude, QuerySet.annotate, Q objects.", "code_signature": "QuerySet.filter(**kwargs)", "tags": "pydantic, blueprint, fastapi, rest, django", "url": "https://docs.django.org/en/stable/QuerySet/filter.html", "author": "Official Docs", "date_published": "23-05-2018", "votes_or_stars": 2394, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:ac757187-869b-4ad2-934c-e1b930f427d5": {"content": "How to implement database connection pooling with SQLAlchemy?\n`create_engine(url, pool_size=10, max_overflow=20, pool_timeout=30, pool_recycle=1800)`. Use `pool_pre_ping=True` to detect stale connections. For async: use `AsyncEngine` with `create_async_engine()`. Monitor with `engine.pool.status()`. NullPool disables pooling for scripts/serverless environments.\nCode signature: create_engine() has built-in pooling. Configure pool_size and max_overflow.", "file_path": "https://stackoverflow.com/questions/7358113", "function_name": "create_engine() has built-in pooling. Configure pool_size and max_overflow.", "type": "stack_overflow", "metadata": {"record_id": "ac757187-869b-4ad2-934c-e1b930f427d5", "source_type": "stack_overflow", "domain": "SQLAlchemy/Databases", "library": "session", "title": "How to implement database connection pooling with SQLAlchemy?", "content": "`create_engine(url, pool_size=10, max_overflow=20, pool_timeout=30, pool_recycle=1800)`. Use `pool_pre_ping=True` to detect stale connections. For async: use `AsyncEngine` with `create_async_engine()`. Monitor with `engine.pool.status()`. NullPool disables pooling for scripts/serverless environments.", "code_signature": "create_engine() has built-in pooling. Configure pool_size and max_overflow.", "tags": "query, foreign-key, orm, migration, alembic", "url": "https://stackoverflow.com/questions/7358113", "author": "user_12260", "date_published": "03-10-2022", "votes_or_stars": 251, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:86f74c08-2727-487e-ba4b-bbdba43c06f9": {"content": "Django ORM Queries — official reference\nOfficial documentation for Django ORM Queries. The method signature is `QuerySet.filter(**kwargs)`. filters QuerySet rows matching given field lookups. Raises `FieldError` when an unknown field lookup is provided. See also: QuerySet.exclude, QuerySet.annotate, Q objects.\nCode signature: QuerySet.filter(**kwargs)", "file_path": "https://docs.django.org/en/stable/QuerySet/filter.html", "function_name": "QuerySet.filter(**kwargs)", "type": "documentation", "metadata": {"record_id": "86f74c08-2727-487e-ba4b-bbdba43c06f9", "source_type": "documentation", "domain": "FastAPI/Flask/Django", "library": "Django", "title": "Django ORM Queries — official reference", "content": "Official documentation for Django ORM Queries. The method signature is `QuerySet.filter(**kwargs)`. filters QuerySet rows matching given field lookups. Raises `FieldError` when an unknown field lookup is provided. See also: QuerySet.exclude, QuerySet.annotate, Q objects.", "code_signature": "QuerySet.filter(**kwargs)", "tags": "django, dependency-injection, routing, middleware, flask", "url": "https://docs.django.org/en/stable/QuerySet/filter.html", "author": "Official Docs", "date_published": "21-12-2019", "votes_or_stars": 2431, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:36228d3c-aafb-4b32-8e21-7809e224524d": {"content": "How to use create_engine in SQLAlchemy\n`SQLAlchemy.create_engine` creates a database engine representing a connection pool. Example usage:\n```python\nengine = create_engine('postgresql://user:pass@host/db', echo=True)\n```\nThis is useful when establishing the central database connection for an application. Performance tip: set pool_size and max_overflow based on expected concurrency.\nCode signature: create_engine", "file_path": "https://docs.sqlalchemy.org/en/stable/create_engine.html", "function_name": "create_engine", "type": "documentation", "metadata": {"record_id": "36228d3c-aafb-4b32-8e21-7809e224524d", "source_type": "documentation", "domain": "SQLAlchemy/Databases", "library": "SQLAlchemy", "title": "How to use create_engine in SQLAlchemy", "content": "`SQLAlchemy.create_engine` creates a database engine representing a connection pool. Example usage:\n```python\nengine = create_engine('postgresql://user:pass@host/db', echo=True)\n```\nThis is useful when establishing the central database connection for an application. Performance tip: set pool_size and max_overflow based on expected concurrency.", "code_signature": "create_engine", "tags": "orm, connection-pool, relationship, session, postgresql", "url": "https://docs.sqlalchemy.org/en/stable/create_engine.html", "author": "Official Docs", "date_published": "28-02-2018", "votes_or_stars": 2036, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:e0017c5a-9f51-4480-b8a8-da78c65eaf44": {"content": "BUG: Flask session not persisting between requests in tests\nProblem: Session data set in one request is not available in the next request during testing.\n\nFix/Discussion: Use Flask test client's session_transaction() context manager: `with client.session_transaction() as sess:\n    sess['user_id'] = 1`. Ensure SECRET_KEY is set. For testing: `app.config['TESTING'] = True; app.config['SECRET_KEY'] = 'test'`.\nCode signature: status:closed", "file_path": "https://github.com/flask/flask/issues/2141", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "e0017c5a-9f51-4480-b8a8-da78c65eaf44", "source_type": "github_issue", "domain": "FastAPI/Flask/Django", "library": "flask", "title": "BUG: Flask session not persisting between requests in tests", "content": "Problem: Session data set in one request is not available in the next request during testing.\n\nFix/Discussion: Use Flask test client's session_transaction() context manager: `with client.session_transaction() as sess:\n    sess['user_id'] = 1`. Ensure SECRET_KEY is set. For testing: `app.config['TESTING'] = True; app.config['SECRET_KEY'] = 'test'`.", "code_signature": "status:closed", "tags": "rest, response, request", "url": "https://github.com/flask/flask/issues/2141", "author": "contributor_5020", "date_published": "23-11-2021", "votes_or_stars": 186, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:a6f576e7-1645-4401-bfa4-8ffefabf8f24": {"content": "asyncio — Concurrent Coroutines\nThe `asyncio.gather` function in asyncio runs multiple coroutines concurrently and collects results. It accepts `*coros, return_exceptions=False` as a parameter and returns list of results. Common use cases include parallel I/O operations such as multiple API calls. Note that set return_exceptions=True to prevent one failure from cancelling all tasks.\nCode signature: asyncio.gather(*coros, return_exceptions=False) -> list of results", "file_path": "https://docs.asyncio.org/en/stable/asyncio/gather.html", "function_name": "asyncio.gather(*coros, return_exceptions=False) -> list of results", "type": "documentation", "metadata": {"record_id": "a6f576e7-1645-4401-bfa4-8ffefabf8f24", "source_type": "documentation", "domain": "Asyncio/Threading", "library": "asyncio", "title": "asyncio — Concurrent Coroutines", "content": "The `asyncio.gather` function in asyncio runs multiple coroutines concurrently and collects results. It accepts `*coros, return_exceptions=False` as a parameter and returns list of results. Common use cases include parallel I/O operations such as multiple API calls. Note that set return_exceptions=True to prevent one failure from cancelling all tasks.", "code_signature": "asyncio.gather(*coros, return_exceptions=False) -> list of results", "tags": "await, executor, async, threading, semaphore", "url": "https://docs.asyncio.org/en/stable/asyncio/gather.html", "author": "Official Docs", "date_published": "18-09-2021", "votes_or_stars": 525, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:ea287a7f-4fdf-4e4b-a7d4-5f7dd91cbd9e": {"content": "How to run background tasks in FastAPI?\nFastAPI's `BackgroundTasks`: add `background_tasks: BackgroundTasks` as a parameter, then `background_tasks.add_task(func, *args)`. Runs after response is sent. For heavy workloads (email, ML inference), use Celery with Redis/RabbitMQ broker. For async background work, `asyncio.create_task()` works within async endpoints.\nCode signature: Use BackgroundTasks for lightweight tasks or Celery for heavy/distributed work.", "file_path": "https://stackoverflow.com/questions/8077999", "function_name": "Use BackgroundTasks for lightweight tasks or Celery for heavy/distributed work.", "type": "stack_overflow", "metadata": {"record_id": "ea287a7f-4fdf-4e4b-a7d4-5f7dd91cbd9e", "source_type": "stack_overflow", "domain": "FastAPI/Flask/Django", "library": "django", "title": "How to run background tasks in FastAPI?", "content": "FastAPI's `BackgroundTasks`: add `background_tasks: BackgroundTasks` as a parameter, then `background_tasks.add_task(func, *args)`. Runs after response is sent. For heavy workloads (email, ML inference), use Celery with Redis/RabbitMQ broker. For async background work, `asyncio.create_task()` works within async endpoints.", "code_signature": "Use BackgroundTasks for lightweight tasks or Celery for heavy/distributed work.", "tags": "template, middleware, orm, flask", "url": "https://stackoverflow.com/questions/8077999", "author": "user_202401", "date_published": "09-12-2021", "votes_or_stars": 1820, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:9e4f793f-0466-453e-9ae6-acad6841146b": {"content": "SQLAlchemy — ORM Session Query\nThe `session.query` function in SQLAlchemy constructs a SELECT query against mapped entities. It accepts `*entities` as a parameter and returns Query object. Common use cases include retrieving ORM-mapped objects from the database. Note that prefer session.execute(select(...)) in SQLAlchemy 2.0+.\nCode signature: session.query(*entities) -> Query object", "file_path": "https://docs.sqlalchemy.org/en/stable/session/query.html", "function_name": "session.query(*entities) -> Query object", "type": "documentation", "metadata": {"record_id": "9e4f793f-0466-453e-9ae6-acad6841146b", "source_type": "documentation", "domain": "SQLAlchemy/Databases", "library": "SQLAlchemy", "title": "SQLAlchemy — ORM Session Query", "content": "The `session.query` function in SQLAlchemy constructs a SELECT query against mapped entities. It accepts `*entities` as a parameter and returns Query object. Common use cases include retrieving ORM-mapped objects from the database. Note that prefer session.execute(select(...)) in SQLAlchemy 2.0+.", "code_signature": "session.query(*entities) -> Query object", "tags": "postgresql, foreign-key, relationship, session, alembic, transaction", "url": "https://docs.sqlalchemy.org/en/stable/session/query.html", "author": "Official Docs", "date_published": "15-01-2023", "votes_or_stars": 3048, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:dfbca5b7-a7d7-4de9-ad31-a414aa846590": {"content": "How to run asyncio code from synchronous Python?\n`asyncio.run(main())` creates an event loop, runs the coroutine, closes the loop. Never call asyncio.run() from inside a running event loop (use await instead). In Jupyter notebooks (which have a running loop), use `await coroutine()` directly or `nest_asyncio.apply()`. For mixing sync/async, use `loop.run_until_complete()`.\nCode signature: Use asyncio.run() in Python 3.7+ to execute a coroutine from sync code.", "file_path": "https://stackoverflow.com/questions/2140194", "function_name": "Use asyncio.run() in Python 3.7+ to execute a coroutine from sync code.", "type": "stack_overflow", "metadata": {"record_id": "dfbca5b7-a7d7-4de9-ad31-a414aa846590", "source_type": "stack_overflow", "domain": "Asyncio/Threading", "library": "gather", "title": "How to run asyncio code from synchronous Python?", "content": "`asyncio.run(main())` creates an event loop, runs the coroutine, closes the loop. Never call asyncio.run() from inside a running event loop (use await instead). In Jupyter notebooks (which have a running loop), use `await coroutine()` directly or `nest_asyncio.apply()`. For mixing sync/async, use `loop.run_until_complete()`.", "code_signature": "Use asyncio.run() in Python 3.7+ to execute a coroutine from sync code.", "tags": "deadlock, race-condition, task, coroutine, threading, asyncio", "url": "https://stackoverflow.com/questions/2140194", "author": "user_718011", "date_published": "23-12-2021", "votes_or_stars": 281, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:4268d402-d595-4a2b-88c3-8a79837db961": {"content": "Python: how to safely write to a file atomically?\n```python\nimport tempfile, os\nwith tempfile.NamedTemporaryFile('w', delete=False, dir='.') as tmp:\n    tmp.write(data)\n    tmp.flush()\n    os.fsync(tmp.fileno())\nos.replace(tmp.name, target_path)\n```\n`os.replace()` is atomic on POSIX. This prevents partial writes from corrupting files.\nCode signature: Write to a temp file then rename — rename is atomic on POSIX systems.", "file_path": "https://stackoverflow.com/questions/5394880", "function_name": "Write to a temp file then rename — rename is atomic on POSIX systems.", "type": "stack_overflow", "metadata": {"record_id": "4268d402-d595-4a2b-88c3-8a79837db961", "source_type": "stack_overflow", "domain": "OS/Subprocess/File I/O", "library": "stdin", "title": "Python: how to safely write to a file atomically?", "content": "```python\nimport tempfile, os\nwith tempfile.NamedTemporaryFile('w', delete=False, dir='.') as tmp:\n    tmp.write(data)\n    tmp.flush()\n    os.fsync(tmp.fileno())\nos.replace(tmp.name, target_path)\n```\n`os.replace()` is atomic on POSIX. This prevents partial writes from corrupting files.", "code_signature": "Write to a temp file then rename — rename is atomic on POSIX systems.", "tags": "glob, shutil, os, stdin, tempfile", "url": "https://stackoverflow.com/questions/5394880", "author": "user_179430", "date_published": "15-12-2022", "votes_or_stars": 545, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:e466f864-ebc9-44a4-bd2a-7fb8b9d53cff": {"content": "requests — HTTP Session Management\nThe `requests.Session` function in requests creates a persistent HTTP session with connection pooling. It accepts `` as a parameter and returns Session object. Common use cases include making multiple requests to the same host efficiently. Note that always close or use Session as context manager to release connections.\nCode signature: requests.Session() -> Session object", "file_path": "https://docs.requests.org/en/stable/requests/Session.html", "function_name": "requests.Session() -> Session object", "type": "documentation", "metadata": {"record_id": "e466f864-ebc9-44a4-bd2a-7fb8b9d53cff", "source_type": "documentation", "domain": "Requests/HTTP/APIs", "library": "requests", "title": "requests — HTTP Session Management", "content": "The `requests.Session` function in requests creates a persistent HTTP session with connection pooling. It accepts `` as a parameter and returns Session object. Common use cases include making multiple requests to the same host efficiently. Note that always close or use Session as context manager to release connections.", "code_signature": "requests.Session() -> Session object", "tags": "timeout, httpx, json, oauth, requests, http", "url": "https://docs.requests.org/en/stable/requests/Session.html", "author": "Official Docs", "date_published": "07-06-2020", "votes_or_stars": 83, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:190031d5-d9b8-4f97-a998-a17770886846": {"content": "How to run background tasks in FastAPI?\nFastAPI's `BackgroundTasks`: add `background_tasks: BackgroundTasks` as a parameter, then `background_tasks.add_task(func, *args)`. Runs after response is sent. For heavy workloads (email, ML inference), use Celery with Redis/RabbitMQ broker. For async background work, `asyncio.create_task()` works within async endpoints.\nCode signature: Use BackgroundTasks for lightweight tasks or Celery for heavy/distributed work.", "file_path": "https://stackoverflow.com/questions/8077999", "function_name": "Use BackgroundTasks for lightweight tasks or Celery for heavy/distributed work.", "type": "stack_overflow", "metadata": {"record_id": "190031d5-d9b8-4f97-a998-a17770886846", "source_type": "stack_overflow", "domain": "FastAPI/Flask/Django", "library": "django", "title": "How to run background tasks in FastAPI?", "content": "FastAPI's `BackgroundTasks`: add `background_tasks: BackgroundTasks` as a parameter, then `background_tasks.add_task(func, *args)`. Runs after response is sent. For heavy workloads (email, ML inference), use Celery with Redis/RabbitMQ broker. For async background work, `asyncio.create_task()` works within async endpoints.", "code_signature": "Use BackgroundTasks for lightweight tasks or Celery for heavy/distributed work.", "tags": "middleware, flask, orm, request, rest", "url": "https://stackoverflow.com/questions/8077999", "author": "user_202401", "date_published": "08-07-2020", "votes_or_stars": 1837, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:cf6b3fc2-f8f0-43b8-9bb9-a7e27a6b5148": {"content": "How do I reshape a wide DataFrame to long format?\n`pd.melt(df, id_vars=['id'], value_vars=['col1','col2'])` unpivots columns to rows. `df.stack()` moves column index to row index. For the reverse, use `df.pivot()` or `df.unstack()`. Always reset_index() after stack/unstack to get a clean DataFrame.\nCode signature: Use pd.melt() or df.stack() depending on your structure.", "file_path": "https://stackoverflow.com/questions/6229731", "function_name": "Use pd.melt() or df.stack() depending on your structure.", "type": "stack_overflow", "metadata": {"record_id": "cf6b3fc2-f8f0-43b8-9bb9-a7e27a6b5148", "source_type": "stack_overflow", "domain": "NumPy/Pandas", "library": "nan", "title": "How do I reshape a wide DataFrame to long format?", "content": "`pd.melt(df, id_vars=['id'], value_vars=['col1','col2'])` unpivots columns to rows. `df.stack()` moves column index to row index. For the reverse, use `df.pivot()` or `df.unstack()`. Always reset_index() after stack/unstack to get a clean DataFrame.", "code_signature": "Use pd.melt() or df.stack() depending on your structure.", "tags": "groupby, broadcasting, array, reshape, dtype, numpy", "url": "https://stackoverflow.com/questions/6229731", "author": "user_428373", "date_published": "17-06-2024", "votes_or_stars": 808, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:e648d739-b396-41c4-8636-eaba5192bc28": {"content": "How to implement JWT authentication in FastAPI?\n```python\nfrom fastapi.security import OAuth2PasswordBearer\nfrom jose import jwt\noauth2_scheme = OAuth2PasswordBearer(tokenUrl='token')\n```\nCreate access tokens with `jwt.encode(payload, SECRET_KEY, algorithm='HS256')`. Verify with `jwt.decode(token, SECRET_KEY, algorithms=['HS256'])`. Store SECRET_KEY in environment variables, never in code.\nCode signature: Use python-jose for JWT and passlib for password hashing with OAuth2PasswordBearer.", "file_path": "https://stackoverflow.com/questions/7754864", "function_name": "Use python-jose for JWT and passlib for password hashing with OAuth2PasswordBearer.", "type": "stack_overflow", "metadata": {"record_id": "e648d739-b396-41c4-8636-eaba5192bc28", "source_type": "stack_overflow", "domain": "FastAPI/Flask/Django", "library": "fastapi", "title": "How to implement JWT authentication in FastAPI?", "content": "```python\nfrom fastapi.security import OAuth2PasswordBearer\nfrom jose import jwt\noauth2_scheme = OAuth2PasswordBearer(tokenUrl='token')\n```\nCreate access tokens with `jwt.encode(payload, SECRET_KEY, algorithm='HS256')`. Verify with `jwt.decode(token, SECRET_KEY, algorithms=['HS256'])`. Store SECRET_KEY in environment variables, never in code.", "code_signature": "Use python-jose for JWT and passlib for password hashing with OAuth2PasswordBearer.", "tags": "rest, response, dependency-injection", "url": "https://stackoverflow.com/questions/7754864", "author": "user_773587", "date_published": "26-02-2018", "votes_or_stars": 422, "relevance_label": "low", "difficulty_level": "beginner"}}, "kb:2ffb96a9-c072-46e3-8a7f-7713cadad658": {"content": "BUG: asyncio task leaks when exception is not handled\nProblem: Unhandled exceptions in fire-and-forget asyncio tasks are silently ignored, causing task leaks.\n\nFix/Discussion: Always await tasks or add exception handlers. Use `task.add_done_callback()` to log exceptions. In Python 3.11+, use TaskGroup for structured concurrency — exceptions propagate automatically. Set `asyncio.get_event_loop().set_exception_handler()` for global unhandled task exception logging.\nCode signature: status:closed", "file_path": "https://github.com/cpython/cpython/issues/5591", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "2ffb96a9-c072-46e3-8a7f-7713cadad658", "source_type": "github_issue", "domain": "Asyncio/Threading", "library": "cpython", "title": "BUG: asyncio task leaks when exception is not handled", "content": "Problem: Unhandled exceptions in fire-and-forget asyncio tasks are silently ignored, causing task leaks.\n\nFix/Discussion: Always await tasks or add exception handlers. Use `task.add_done_callback()` to log exceptions. In Python 3.11+, use TaskGroup for structured concurrency — exceptions propagate automatically. Set `asyncio.get_event_loop().set_exception_handler()` for global unhandled task exception logging.", "code_signature": "status:closed", "tags": "deadlock, race-condition, threading, gather, asyncio", "url": "https://github.com/cpython/cpython/issues/5591", "author": "contributor_1630", "date_published": "10-05-2022", "votes_or_stars": 336, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:1f2c1c67-f5d9-4742-bbc4-c13e21d26917": {"content": "How to handle database migrations with Alembic?\nWorkflow: (1) Change SQLAlchemy models. (2) `alembic revision --autogenerate -m 'description'`. (3) Review generated migration script. (4) `alembic upgrade head` to apply. Always review autogenerated migrations — they miss some changes (indexes, constraints). Use `alembic downgrade -1` to rollback. Store migrations in version control.\nCode signature: Use alembic revision --autogenerate to create migrations from model changes.", "file_path": "https://stackoverflow.com/questions/9436429", "function_name": "Use alembic revision --autogenerate to create migrations from model changes.", "type": "stack_overflow", "metadata": {"record_id": "1f2c1c67-f5d9-4742-bbc4-c13e21d26917", "source_type": "stack_overflow", "domain": "SQLAlchemy/Databases", "library": "connection-pool", "title": "How to handle database migrations with Alembic?", "content": "Workflow: (1) Change SQLAlchemy models. (2) `alembic revision --autogenerate -m 'description'`. (3) Review generated migration script. (4) `alembic upgrade head` to apply. Always review autogenerated migrations — they miss some changes (indexes, constraints). Use `alembic downgrade -1` to rollback. Store migrations in version control.", "code_signature": "Use alembic revision --autogenerate to create migrations from model changes.", "tags": "alembic, foreign-key, sqlite, query, connection-pool, orm", "url": "https://stackoverflow.com/questions/9436429", "author": "user_974047", "date_published": "25-01-2018", "votes_or_stars": 2432, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:6366c6a4-931a-441f-af06-0043acb5b0c6": {"content": "How to use asyncio.create_task in asyncio\n`asyncio.asyncio.create_task` schedules a coroutine as a Task in the event loop. Example usage:\n```python\ntask = asyncio.create_task(send_email(user))\nawait task\n```\nThis is useful when fire-and-forget background tasks, concurrent task management. Performance tip: use TaskGroup (Python 3.11+) for structured concurrency.\nCode signature: asyncio.create_task", "file_path": "https://docs.asyncio.org/en/stable/asyncio/create_task.html", "function_name": "asyncio.create_task", "type": "documentation", "metadata": {"record_id": "6366c6a4-931a-441f-af06-0043acb5b0c6", "source_type": "documentation", "domain": "Asyncio/Threading", "library": "asyncio", "title": "How to use asyncio.create_task in asyncio", "content": "`asyncio.asyncio.create_task` schedules a coroutine as a Task in the event loop. Example usage:\n```python\ntask = asyncio.create_task(send_email(user))\nawait task\n```\nThis is useful when fire-and-forget background tasks, concurrent task management. Performance tip: use TaskGroup (Python 3.11+) for structured concurrency.", "code_signature": "asyncio.create_task", "tags": "gather, deadlock, executor, asyncio, coroutine, async", "url": "https://docs.asyncio.org/en/stable/asyncio/create_task.html", "author": "Official Docs", "date_published": "23-11-2020", "votes_or_stars": 4168, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:9b4471ef-315d-44f1-98f6-91d6d1bb019f": {"content": "How to stream large HTTP responses with requests?\n```python\nwith requests.get(url, stream=True) as r:\n    r.raise_for_status()\n    with open('file', 'wb') as f:\n        for chunk in r.iter_content(chunk_size=8192):\n            f.write(chunk)\n```\nAlways use as context manager with stream=True to ensure connection is released.\nCode signature: Use stream=True and iterate over response.iter_content() or iter_lines().", "file_path": "https://stackoverflow.com/questions/3592974", "function_name": "Use stream=True and iterate over response.iter_content() or iter_lines().", "type": "stack_overflow", "metadata": {"record_id": "9b4471ef-315d-44f1-98f6-91d6d1bb019f", "source_type": "stack_overflow", "domain": "Requests/HTTP/APIs", "library": "json", "title": "How to stream large HTTP responses with requests?", "content": "```python\nwith requests.get(url, stream=True) as r:\n    r.raise_for_status()\n    with open('file', 'wb') as f:\n        for chunk in r.iter_content(chunk_size=8192):\n            f.write(chunk)\n```\nAlways use as context manager with stream=True to ensure connection is released.", "code_signature": "Use stream=True and iterate over response.iter_content() or iter_lines().", "tags": "json, timeout, oauth, rate-limiting, requests", "url": "https://stackoverflow.com/questions/3592974", "author": "user_980733", "date_published": "01-02-2024", "votes_or_stars": 1673, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:c3d59b69-3ac4-4d62-b37f-3d4d789ce8c0": {"content": "BUG: httpx: SSL certificate error on Windows despite valid cert\nProblem: httpx raises SSLCertVerificationError on Windows when system CA store has the cert.\n\nFix/Discussion: httpx uses its own CA bundle (certifi) by default, not the system store. Fix: `httpx.Client(verify=certifi.where())` — already default. For custom certs: `httpx.Client(verify='/path/to/ca-bundle.pem')`. Install `pip install truststore` and use `ssl.create_default_context(ssl.Purpose.SERVER_AUTH)` to use system store.\nCode signature: status:closed", "file_path": "https://github.com/httpx/httpx/issues/8949", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "c3d59b69-3ac4-4d62-b37f-3d4d789ce8c0", "source_type": "github_issue", "domain": "Requests/HTTP/APIs", "library": "httpx", "title": "BUG: httpx: SSL certificate error on Windows despite valid cert", "content": "Problem: httpx raises SSLCertVerificationError on Windows when system CA store has the cert.\n\nFix/Discussion: httpx uses its own CA bundle (certifi) by default, not the system store. Fix: `httpx.Client(verify=certifi.where())` — already default. For custom certs: `httpx.Client(verify='/path/to/ca-bundle.pem')`. Install `pip install truststore` and use `ssl.create_default_context(ssl.Purpose.SERVER_AUTH)` to use system store.", "code_signature": "status:closed", "tags": "headers, http, retry, authentication", "url": "https://github.com/httpx/httpx/issues/8949", "author": "contributor_1420", "date_published": "26-04-2019", "votes_or_stars": 479, "relevance_label": "low", "difficulty_level": "intermediate"}}, "kb:95d9a3f6-bfc1-442a-93d2-4f3ff912c385": {"content": "How to read a large file line by line in Python without loading it all into memory?\n```python\nwith open('large.txt', 'r') as f:\n    for line in f:\n        process(line.strip())\n```\nThis reads one line at a time. For binary files: use `f.read(chunk_size)` in a loop. For CSV: use `pd.read_csv(file, chunksize=10000)`. Never use `f.readlines()` on large files.\nCode signature: Iterate over the file object directly — it yields lines lazily.", "file_path": "https://stackoverflow.com/questions/2737893", "function_name": "Iterate over the file object directly — it yields lines lazily.", "type": "stack_overflow", "metadata": {"record_id": "95d9a3f6-bfc1-442a-93d2-4f3ff912c385", "source_type": "stack_overflow", "domain": "OS/Subprocess/File I/O", "library": "shutil", "title": "How to read a large file line by line in Python without loading it all into memory?", "content": "```python\nwith open('large.txt', 'r') as f:\n    for line in f:\n        process(line.strip())\n```\nThis reads one line at a time. For binary files: use `f.read(chunk_size)` in a loop. For CSV: use `pd.read_csv(file, chunksize=10000)`. Never use `f.readlines()` on large files.", "code_signature": "Iterate over the file object directly — it yields lines lazily.", "tags": "shutil, pathlib, file-io, tempfile, stdin, glob", "url": "https://stackoverflow.com/questions/2737893", "author": "user_994539", "date_published": "25-06-2020", "votes_or_stars": 2298, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:4f762aba-b1a9-455f-8cf0-c7872ab3860d": {"content": "How to run background tasks in FastAPI?\nFastAPI's `BackgroundTasks`: add `background_tasks: BackgroundTasks` as a parameter, then `background_tasks.add_task(func, *args)`. Runs after response is sent. For heavy workloads (email, ML inference), use Celery with Redis/RabbitMQ broker. For async background work, `asyncio.create_task()` works within async endpoints.\nCode signature: Use BackgroundTasks for lightweight tasks or Celery for heavy/distributed work.", "file_path": "https://stackoverflow.com/questions/8077999", "function_name": "Use BackgroundTasks for lightweight tasks or Celery for heavy/distributed work.", "type": "stack_overflow", "metadata": {"record_id": "4f762aba-b1a9-455f-8cf0-c7872ab3860d", "source_type": "stack_overflow", "domain": "FastAPI/Flask/Django", "library": "django", "title": "How to run background tasks in FastAPI?", "content": "FastAPI's `BackgroundTasks`: add `background_tasks: BackgroundTasks` as a parameter, then `background_tasks.add_task(func, *args)`. Runs after response is sent. For heavy workloads (email, ML inference), use Celery with Redis/RabbitMQ broker. For async background work, `asyncio.create_task()` works within async endpoints.", "code_signature": "Use BackgroundTasks for lightweight tasks or Celery for heavy/distributed work.", "tags": "routing, django, blueprint, dependency-injection, template", "url": "https://stackoverflow.com/questions/8077999", "author": "user_202401", "date_published": "13-10-2020", "votes_or_stars": 1872, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:aa0e0913-874d-47cb-9209-46f352cfe2ee": {"content": "Django N+1 query problem: how to fix?\nN+1 occurs when accessing related objects in a loop. Diagnose with `django-debug-toolbar` or `connection.queries`. Fix ForeignKey: `User.objects.select_related('profile').all()`. Fix M2M: `Post.objects.prefetch_related('tags').all()`. Use `only()` to fetch specific fields and reduce data transfer.\nCode signature: Use select_related() for ForeignKey and prefetch_related() for ManyToMany relationships.", "file_path": "https://stackoverflow.com/questions/8106470", "function_name": "Use select_related() for ForeignKey and prefetch_related() for ManyToMany relationships.", "type": "stack_overflow", "metadata": {"record_id": "aa0e0913-874d-47cb-9209-46f352cfe2ee", "source_type": "stack_overflow", "domain": "FastAPI/Flask/Django", "library": "request", "title": "Django N+1 query problem: how to fix?", "content": "N+1 occurs when accessing related objects in a loop. Diagnose with `django-debug-toolbar` or `connection.queries`. Fix ForeignKey: `User.objects.select_related('profile').all()`. Fix M2M: `Post.objects.prefetch_related('tags').all()`. Use `only()` to fetch specific fields and reduce data transfer.", "code_signature": "Use select_related() for ForeignKey and prefetch_related() for ManyToMany relationships.", "tags": "middleware, rest, django, blueprint", "url": "https://stackoverflow.com/questions/8106470", "author": "user_441071", "date_published": "24-01-2020", "votes_or_stars": 1793, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:cff7d83b-5bf3-4f59-bba9-fc78e5696f9a": {"content": "How to use create_engine in SQLAlchemy\n`SQLAlchemy.create_engine` creates a database engine representing a connection pool. Example usage:\n```python\nengine = create_engine('postgresql://user:pass@host/db', echo=True)\n```\nThis is useful when establishing the central database connection for an application. Performance tip: set pool_size and max_overflow based on expected concurrency.\nCode signature: create_engine", "file_path": "https://docs.sqlalchemy.org/en/stable/create_engine.html", "function_name": "create_engine", "type": "documentation", "metadata": {"record_id": "cff7d83b-5bf3-4f59-bba9-fc78e5696f9a", "source_type": "documentation", "domain": "SQLAlchemy/Databases", "library": "SQLAlchemy", "title": "How to use create_engine in SQLAlchemy", "content": "`SQLAlchemy.create_engine` creates a database engine representing a connection pool. Example usage:\n```python\nengine = create_engine('postgresql://user:pass@host/db', echo=True)\n```\nThis is useful when establishing the central database connection for an application. Performance tip: set pool_size and max_overflow based on expected concurrency.", "code_signature": "create_engine", "tags": "foreign-key, alembic, migration, sqlalchemy, sqlite, orm", "url": "https://docs.sqlalchemy.org/en/stable/create_engine.html", "author": "Official Docs", "date_published": "07-10-2024", "votes_or_stars": 2059, "relevance_label": "low", "difficulty_level": "beginner"}}, "kb:ec3caf73-5c41-497f-a34e-3723e9453d4a": {"content": "FEAT: Django: add support for async ORM queries\nProblem: Django ORM is synchronous; using it in async views requires sync_to_async wrapper.\n\nFix/Discussion: Django 4.1+ added native async ORM support. Use `await Model.objects.filter().afirst()`, `async for obj in qs:`. For older versions: wrap with `sync_to_async`: `get_user = sync_to_async(User.objects.get)\nuser = await get_user(pk=1)`.\nCode signature: status:closed", "file_path": "https://github.com/django/django/issues/8618", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "ec3caf73-5c41-497f-a34e-3723e9453d4a", "source_type": "github_issue", "domain": "FastAPI/Flask/Django", "library": "django", "title": "FEAT: Django: add support for async ORM queries", "content": "Problem: Django ORM is synchronous; using it in async views requires sync_to_async wrapper.\n\nFix/Discussion: Django 4.1+ added native async ORM support. Use `await Model.objects.filter().afirst()`, `async for obj in qs:`. For older versions: wrap with `sync_to_async`: `get_user = sync_to_async(User.objects.get)\nuser = await get_user(pk=1)`.", "code_signature": "status:closed", "tags": "flask, routing, django", "url": "https://github.com/django/django/issues/8618", "author": "contributor_8921", "date_published": "02-04-2022", "votes_or_stars": 1, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:e7047478-24d0-4d4c-a8c8-19f6cd3e7b00": {"content": "BUG: SQLAlchemy connection pool exhaustion under load\nProblem: Application hangs under concurrent load — all connections are checked out and pool is exhausted.\n\nFix/Discussion: Increase `pool_size` and `max_overflow`. Ensure sessions are always closed: use `contextmanager` or `with Session() as session:`. Set `pool_timeout` to fail fast. Enable `pool_pre_ping=True`. Monitor with `engine.pool.checkedout()`. In async: use `AsyncSession` with `async_scoped_session`.\nCode signature: status:closed", "file_path": "https://github.com/sqlalchemy/sqlalchemy/issues/106", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "e7047478-24d0-4d4c-a8c8-19f6cd3e7b00", "source_type": "github_issue", "domain": "SQLAlchemy/Databases", "library": "sqlalchemy", "title": "BUG: SQLAlchemy connection pool exhaustion under load", "content": "Problem: Application hangs under concurrent load — all connections are checked out and pool is exhausted.\n\nFix/Discussion: Increase `pool_size` and `max_overflow`. Ensure sessions are always closed: use `contextmanager` or `with Session() as session:`. Set `pool_timeout` to fail fast. Enable `pool_pre_ping=True`. Monitor with `engine.pool.checkedout()`. In async: use `AsyncSession` with `async_scoped_session`.", "code_signature": "status:closed", "tags": "transaction, session, foreign-key, sqlalchemy", "url": "https://github.com/sqlalchemy/sqlalchemy/issues/106", "author": "contributor_5078", "date_published": "26-06-2024", "votes_or_stars": 303, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:b5da7911-7536-4146-9b30-5e0b9616c2dd": {"content": "asyncio RuntimeError: This event loop is already running\nThis error is common in Jupyter and FastAPI. Solutions: (1) In async context: just use `await coroutine()`. (2) In Jupyter: `import nest_asyncio; nest_asyncio.apply()`. (3) Run in thread: `asyncio.get_event_loop().run_in_executor(None, sync_func)`. (4) Use `asyncio.ensure_future()` to schedule from within async code.\nCode signature: You're calling asyncio.run() inside an already running event loop. Use await instead.", "file_path": "https://stackoverflow.com/questions/2229110", "function_name": "You're calling asyncio.run() inside an already running event loop. Use await instead.", "type": "stack_overflow", "metadata": {"record_id": "b5da7911-7536-4146-9b30-5e0b9616c2dd", "source_type": "stack_overflow", "domain": "Asyncio/Threading", "library": "async", "title": "asyncio RuntimeError: This event loop is already running", "content": "This error is common in Jupyter and FastAPI. Solutions: (1) In async context: just use `await coroutine()`. (2) In Jupyter: `import nest_asyncio; nest_asyncio.apply()`. (3) Run in thread: `asyncio.get_event_loop().run_in_executor(None, sync_func)`. (4) Use `asyncio.ensure_future()` to schedule from within async code.", "code_signature": "You're calling asyncio.run() inside an already running event loop. Use await instead.", "tags": "threading, semaphore, lock, deadlock, await", "url": "https://stackoverflow.com/questions/2229110", "author": "user_573750", "date_published": "03-07-2024", "votes_or_stars": 2308, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:277e8c3f-988b-4f46-82b3-cd6ef8299ec6": {"content": "BUG: pd.read_csv() silently truncates large integers\nProblem: Integer columns with values > 2^53 are silently corrupted when read from CSV.\n\nFix/Discussion: Floating point cannot represent integers > 2^53 exactly. Fix: `pd.read_csv(f, dtype={'col': str})` then convert: `df['col'] = df['col'].astype('Int64')`. Use `dtype_backend='numpy_nullable'` in Pandas 2.0+ for better integer handling.\nCode signature: status:closed", "file_path": "https://github.com/pandas/pandas/issues/4449", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "277e8c3f-988b-4f46-82b3-cd6ef8299ec6", "source_type": "github_issue", "domain": "NumPy/Pandas", "library": "pandas", "title": "BUG: pd.read_csv() silently truncates large integers", "content": "Problem: Integer columns with values > 2^53 are silently corrupted when read from CSV.\n\nFix/Discussion: Floating point cannot represent integers > 2^53 exactly. Fix: `pd.read_csv(f, dtype={'col': str})` then convert: `df['col'] = df['col'].astype('Int64')`. Use `dtype_backend='numpy_nullable'` in Pandas 2.0+ for better integer handling.", "code_signature": "status:closed", "tags": "dtype, pivot, nan, broadcasting, merge", "url": "https://github.com/pandas/pandas/issues/4449", "author": "contributor_726", "date_published": "05-04-2024", "votes_or_stars": 493, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:115785be-d045-4427-a02f-9d064ee878ed": {"content": "NumPy — Linear Algebra\nThe `np.einsum` function in NumPy evaluates Einstein summation convention on operands. It accepts `subscripts, *operands` as a parameter and returns ndarray. Common use cases include matrix multiplication, dot products, tensor contractions. Note that prefer np.einsum over loops for large arrays.\nCode signature: np.einsum(subscripts, *operands) -> ndarray", "file_path": "https://docs.numpy.org/en/stable/np/einsum.html", "function_name": "np.einsum(subscripts, *operands) -> ndarray", "type": "documentation", "metadata": {"record_id": "115785be-d045-4427-a02f-9d064ee878ed", "source_type": "documentation", "domain": "NumPy/Pandas", "library": "NumPy", "title": "NumPy — Linear Algebra", "content": "The `np.einsum` function in NumPy evaluates Einstein summation convention on operands. It accepts `subscripts, *operands` as a parameter and returns ndarray. Common use cases include matrix multiplication, dot products, tensor contractions. Note that prefer np.einsum over loops for large arrays.", "code_signature": "np.einsum(subscripts, *operands) -> ndarray", "tags": "broadcasting, dtype, nan, loc, pivot, vectorization", "url": "https://docs.numpy.org/en/stable/np/einsum.html", "author": "Official Docs", "date_published": "05-12-2019", "votes_or_stars": 1889, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:5dbbb342-6dab-43c9-b221-99695d41d321": {"content": "How do I reshape a wide DataFrame to long format?\n`pd.melt(df, id_vars=['id'], value_vars=['col1','col2'])` unpivots columns to rows. `df.stack()` moves column index to row index. For the reverse, use `df.pivot()` or `df.unstack()`. Always reset_index() after stack/unstack to get a clean DataFrame.\nCode signature: Use pd.melt() or df.stack() depending on your structure.", "file_path": "https://stackoverflow.com/questions/6229731", "function_name": "Use pd.melt() or df.stack() depending on your structure.", "type": "stack_overflow", "metadata": {"record_id": "5dbbb342-6dab-43c9-b221-99695d41d321", "source_type": "stack_overflow", "domain": "NumPy/Pandas", "library": "nan", "title": "How do I reshape a wide DataFrame to long format?", "content": "`pd.melt(df, id_vars=['id'], value_vars=['col1','col2'])` unpivots columns to rows. `df.stack()` moves column index to row index. For the reverse, use `df.pivot()` or `df.unstack()`. Always reset_index() after stack/unstack to get a clean DataFrame.", "code_signature": "Use pd.melt() or df.stack() depending on your structure.", "tags": "dtype, nan, reshape, groupby, pandas", "url": "https://stackoverflow.com/questions/6229731", "author": "user_428373", "date_published": "19-07-2021", "votes_or_stars": 855, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:4453bf1e-b374-4576-a807-279809854651": {"content": "How to apply a function to each row in Pandas without using iterrows?\n`iterrows()` is slow because it creates a Series per row. Prefer `df.apply(func, axis=1)` for row-wise operations, or better yet, vectorize using NumPy: `np.where(condition, a, b)`. For complex logic, `df.assign()` with vectorized expressions is cleanest. Benchmark: vectorized > apply > itertuples > iterrows.\nCode signature: Use df.apply(func, axis=1) or vectorized NumPy operations for best performance.", "file_path": "https://stackoverflow.com/questions/5446912", "function_name": "Use df.apply(func, axis=1) or vectorized NumPy operations for best performance.", "type": "stack_overflow", "metadata": {"record_id": "4453bf1e-b374-4576-a807-279809854651", "source_type": "stack_overflow", "domain": "NumPy/Pandas", "library": "dataframe", "title": "How to apply a function to each row in Pandas without using iterrows?", "content": "`iterrows()` is slow because it creates a Series per row. Prefer `df.apply(func, axis=1)` for row-wise operations, or better yet, vectorize using NumPy: `np.where(condition, a, b)`. For complex logic, `df.assign()` with vectorized expressions is cleanest. Benchmark: vectorized > apply > itertuples > iterrows.", "code_signature": "Use df.apply(func, axis=1) or vectorized NumPy operations for best performance.", "tags": "pandas, reshape, pivot, dataframe", "url": "https://stackoverflow.com/questions/5446912", "author": "user_563306", "date_published": "20-11-2018", "votes_or_stars": 2256, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:6d16a39f-dafb-4eab-b47d-f192e0345aae": {"content": "requests.exceptions.SSLError: certificate verify failed\nNever use `verify=False` in production — it disables SSL verification entirely. Solutions: (1) Update certifi: `pip install --upgrade certifi`. (2) Provide CA bundle: `requests.get(url, verify='/path/to/ca-bundle.crt')`. (3) For self-signed certs in dev: `verify=False` only, and never commit this. (4) Check system time — expired system clock causes SSL failures.\nCode signature: Your SSL certificate chain is incomplete or self-signed. Fix the cert, don't disable verification.", "file_path": "https://stackoverflow.com/questions/7907400", "function_name": "Your SSL certificate chain is incomplete or self-signed. Fix the cert, don't disable verification.", "type": "stack_overflow", "metadata": {"record_id": "6d16a39f-dafb-4eab-b47d-f192e0345aae", "source_type": "stack_overflow", "domain": "Requests/HTTP/APIs", "library": "websocket", "title": "requests.exceptions.SSLError: certificate verify failed", "content": "Never use `verify=False` in production — it disables SSL verification entirely. Solutions: (1) Update certifi: `pip install --upgrade certifi`. (2) Provide CA bundle: `requests.get(url, verify='/path/to/ca-bundle.crt')`. (3) For self-signed certs in dev: `verify=False` only, and never commit this. (4) Check system time — expired system clock causes SSL failures.", "code_signature": "Your SSL certificate chain is incomplete or self-signed. Fix the cert, don't disable verification.", "tags": "aiohttp, retry, headers", "url": "https://stackoverflow.com/questions/7907400", "author": "user_851204", "date_published": "18-01-2024", "votes_or_stars": 1348, "relevance_label": "low", "difficulty_level": "advanced"}}, "kb:ad34200c-726f-4f54-a61f-43076df4c24d": {"content": "BUG: requests hangs indefinitely on slow server responses\nProblem: requests.get() hangs forever when server accepts connection but never sends response.\n\nFix/Discussion: Always set both connect and read timeouts: `requests.get(url, timeout=(3.05, 27))`. Tuple: (connect_timeout, read_timeout). Or single value for both. Mount a Session with default timeouts using a custom HTTPAdapter subclass. Never use `timeout=None` in production code.\nCode signature: status:closed", "file_path": "https://github.com/requests/requests/issues/4019", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "ad34200c-726f-4f54-a61f-43076df4c24d", "source_type": "github_issue", "domain": "Requests/HTTP/APIs", "library": "requests", "title": "BUG: requests hangs indefinitely on slow server responses", "content": "Problem: requests.get() hangs forever when server accepts connection but never sends response.\n\nFix/Discussion: Always set both connect and read timeouts: `requests.get(url, timeout=(3.05, 27))`. Tuple: (connect_timeout, read_timeout). Or single value for both. Mount a Session with default timeouts using a custom HTTPAdapter subclass. Never use `timeout=None` in production code.", "code_signature": "status:closed", "tags": "rate-limiting, timeout, headers, json, websocket", "url": "https://github.com/requests/requests/issues/4019", "author": "contributor_4599", "date_published": "25-04-2019", "votes_or_stars": 274, "relevance_label": "low", "difficulty_level": "advanced"}}, "kb:79ef6666-6342-4b08-91d0-5defb62354df": {"content": "Why is my Pandas merge creating duplicate rows?\nDuplicate rows after merge usually mean the join key is not unique. Diagnose with `df.duplicated(subset=['key']).sum()`. Use `how='left'` carefully when right table has multiple matches. Set `validate='one_to_many'` or `'many_to_one'` to enforce cardinality. Consider deduplicating with `df.drop_duplicates(subset=['key'])` before merging.\nCode signature: Your merge key has duplicates in one or both DataFrames. Use validate='one_to_one' to catch this early.", "file_path": "https://stackoverflow.com/questions/2321324", "function_name": "Your merge key has duplicates in one or both DataFrames. Use validate='one_to_one' to catch this early.", "type": "stack_overflow", "metadata": {"record_id": "79ef6666-6342-4b08-91d0-5defb62354df", "source_type": "stack_overflow", "domain": "NumPy/Pandas", "library": "array", "title": "Why is my Pandas merge creating duplicate rows?", "content": "Duplicate rows after merge usually mean the join key is not unique. Diagnose with `df.duplicated(subset=['key']).sum()`. Use `how='left'` carefully when right table has multiple matches. Set `validate='one_to_many'` or `'many_to_one'` to enforce cardinality. Consider deduplicating with `df.drop_duplicates(subset=['key'])` before merging.", "code_signature": "Your merge key has duplicates in one or both DataFrames. Use validate='one_to_one' to catch this early.", "tags": "dtype, groupby, dataframe, numpy, pivot, pandas", "url": "https://stackoverflow.com/questions/2321324", "author": "user_99814", "date_published": "19-10-2022", "votes_or_stars": 229, "relevance_label": "low", "difficulty_level": "beginner"}}, "kb:f26a2848-1c39-4b7d-9aca-8b7d02b20f5b": {"content": "How to implement a thread pool in Python?\n```python\nfrom concurrent.futures import ThreadPoolExecutor\nwith ThreadPoolExecutor(max_workers=8) as ex:\n    futures = [ex.submit(task, arg) for arg in args]\n    results = [f.result() for f in futures]\n```\nFor CPU-bound: use ProcessPoolExecutor instead. For async integration: `await loop.run_in_executor(executor, func, *args)`.\nCode signature: Use concurrent.futures.ThreadPoolExecutor for I/O-bound tasks.", "file_path": "https://stackoverflow.com/questions/3646552", "function_name": "Use concurrent.futures.ThreadPoolExecutor for I/O-bound tasks.", "type": "stack_overflow", "metadata": {"record_id": "f26a2848-1c39-4b7d-9aca-8b7d02b20f5b", "source_type": "stack_overflow", "domain": "Asyncio/Threading", "library": "coroutine", "title": "How to implement a thread pool in Python?", "content": "```python\nfrom concurrent.futures import ThreadPoolExecutor\nwith ThreadPoolExecutor(max_workers=8) as ex:\n    futures = [ex.submit(task, arg) for arg in args]\n    results = [f.result() for f in futures]\n```\nFor CPU-bound: use ProcessPoolExecutor instead. For async integration: `await loop.run_in_executor(executor, func, *args)`.", "code_signature": "Use concurrent.futures.ThreadPoolExecutor for I/O-bound tasks.", "tags": "gather, async, race-condition, coroutine", "url": "https://stackoverflow.com/questions/3646552", "author": "user_469469", "date_published": "02-06-2024", "votes_or_stars": 1516, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:6d808b52-996c-45ca-8365-fbb5860ff999": {"content": "subprocess — Process Execution\nThe `subprocess.run` function in subprocess runs a subprocess and waits for it to complete. It accepts `args, capture_output=False, timeout=None` as a parameter and returns CompletedProcess. Common use cases include executing shell commands, scripts, or external programs from Python. Note that use shell=False (default) to avoid shell injection vulnerabilities.\nCode signature: subprocess.run(args, capture_output=False, timeout=None) -> CompletedProcess", "file_path": "https://docs.subprocess.org/en/stable/subprocess/run.html", "function_name": "subprocess.run(args, capture_output=False, timeout=None) -> CompletedProcess", "type": "documentation", "metadata": {"record_id": "6d808b52-996c-45ca-8365-fbb5860ff999", "source_type": "documentation", "domain": "OS/Subprocess/File I/O", "library": "subprocess", "title": "subprocess — Process Execution", "content": "The `subprocess.run` function in subprocess runs a subprocess and waits for it to complete. It accepts `args, capture_output=False, timeout=None` as a parameter and returns CompletedProcess. Common use cases include executing shell commands, scripts, or external programs from Python. Note that use shell=False (default) to avoid shell injection vulnerabilities.", "code_signature": "subprocess.run(args, capture_output=False, timeout=None) -> CompletedProcess", "tags": "os, file-io, env-variable, pathlib", "url": "https://docs.subprocess.org/en/stable/subprocess/run.html", "author": "Official Docs", "date_published": "04-04-2019", "votes_or_stars": 457, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:69611d5e-9db3-4c4c-9458-e95467c651b1": {"content": "How to use create_engine in SQLAlchemy\n`SQLAlchemy.create_engine` creates a database engine representing a connection pool. Example usage:\n```python\nengine = create_engine('postgresql://user:pass@host/db', echo=True)\n```\nThis is useful when establishing the central database connection for an application. Performance tip: set pool_size and max_overflow based on expected concurrency.\nCode signature: create_engine", "file_path": "https://docs.sqlalchemy.org/en/stable/create_engine.html", "function_name": "create_engine", "type": "documentation", "metadata": {"record_id": "69611d5e-9db3-4c4c-9458-e95467c651b1", "source_type": "documentation", "domain": "SQLAlchemy/Databases", "library": "SQLAlchemy", "title": "How to use create_engine in SQLAlchemy", "content": "`SQLAlchemy.create_engine` creates a database engine representing a connection pool. Example usage:\n```python\nengine = create_engine('postgresql://user:pass@host/db', echo=True)\n```\nThis is useful when establishing the central database connection for an application. Performance tip: set pool_size and max_overflow based on expected concurrency.", "code_signature": "create_engine", "tags": "sqlite, transaction, orm, session", "url": "https://docs.sqlalchemy.org/en/stable/create_engine.html", "author": "Official Docs", "date_published": "29-12-2022", "votes_or_stars": 2055, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:064a6425-60d2-4601-a6d5-4593b85c98b0": {"content": "Python threading: how to share data between threads safely?\nFor simple counters: use `threading.Lock()` as context manager. For complex shared state: prefer `queue.Queue` (thread-safe by design). Avoid global variables without locks. `threading.local()` gives each thread its own data copy. For read-heavy workloads, `threading.RLock` allows re-entrant acquisition.\nCode signature: Use threading.Lock for mutual exclusion or queue.Queue for producer-consumer patterns.", "file_path": "https://stackoverflow.com/questions/3195773", "function_name": "Use threading.Lock for mutual exclusion or queue.Queue for producer-consumer patterns.", "type": "stack_overflow", "metadata": {"record_id": "064a6425-60d2-4601-a6d5-4593b85c98b0", "source_type": "stack_overflow", "domain": "Asyncio/Threading", "library": "lock", "title": "Python threading: how to share data between threads safely?", "content": "For simple counters: use `threading.Lock()` as context manager. For complex shared state: prefer `queue.Queue` (thread-safe by design). Avoid global variables without locks. `threading.local()` gives each thread its own data copy. For read-heavy workloads, `threading.RLock` allows re-entrant acquisition.", "code_signature": "Use threading.Lock for mutual exclusion or queue.Queue for producer-consumer patterns.", "tags": "event-loop, coroutine, await", "url": "https://stackoverflow.com/questions/3195773", "author": "user_714318", "date_published": "13-03-2019", "votes_or_stars": 1068, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:6f875f67-f282-4f9f-8d90-7f43df5dbb4c": {"content": "PERF: groupby().apply() extremely slow on large DataFrames\nProblem: groupby().apply() with a custom function is 100x slower than expected on 10M row DataFrames.\n\nFix/Discussion: groupby().apply() serializes each group and has high overhead. Fix: vectorize with numpy, use groupby().transform() for same-shape output, or groupby().agg() with named aggregations. For complex logic, consider Polars or Dask. Numba's @jit can accelerate custom group functions.\nCode signature: status:closed", "file_path": "https://github.com/pandas/pandas/issues/8446", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "6f875f67-f282-4f9f-8d90-7f43df5dbb4c", "source_type": "github_issue", "domain": "NumPy/Pandas", "library": "pandas", "title": "PERF: groupby().apply() extremely slow on large DataFrames", "content": "Problem: groupby().apply() with a custom function is 100x slower than expected on 10M row DataFrames.\n\nFix/Discussion: groupby().apply() serializes each group and has high overhead. Fix: vectorize with numpy, use groupby().transform() for same-shape output, or groupby().agg() with named aggregations. For complex logic, consider Polars or Dask. Numba's @jit can accelerate custom group functions.", "code_signature": "status:closed", "tags": "iloc, loc, dataframe, merge, pivot", "url": "https://github.com/pandas/pandas/issues/8446", "author": "contributor_6648", "date_published": "08-07-2021", "votes_or_stars": 361, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:01225355-e23c-4540-89e6-519b33741d75": {"content": "FEAT: pathlib: add support for reading/writing with encoding shorthand\nProblem: pathlib.Path.read_text() does not default to UTF-8, causing issues across platforms.\n\nFix/Discussion: Always pass encoding explicitly: `Path('file').read_text(encoding='utf-8')`. Python 3.10+ added `encoding='locale'` default change proposal. Best practice: always specify encoding in all file I/O operations for portability.\nCode signature: status:closed", "file_path": "https://github.com/cpython/cpython/issues/2707", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "01225355-e23c-4540-89e6-519b33741d75", "source_type": "github_issue", "domain": "OS/Subprocess/File I/O", "library": "cpython", "title": "FEAT: pathlib: add support for reading/writing with encoding shorthand", "content": "Problem: pathlib.Path.read_text() does not default to UTF-8, causing issues across platforms.\n\nFix/Discussion: Always pass encoding explicitly: `Path('file').read_text(encoding='utf-8')`. Python 3.10+ added `encoding='locale'` default change proposal. Best practice: always specify encoding in all file I/O operations for portability.", "code_signature": "status:closed", "tags": "stdin, pathlib, process, pipe, glob", "url": "https://github.com/cpython/cpython/issues/2707", "author": "contributor_7877", "date_published": "11-05-2020", "votes_or_stars": 453, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:fa0c957f-9abd-4ffd-b77b-e4e702b4928d": {"content": "BUG: requests hangs indefinitely on slow server responses\nProblem: requests.get() hangs forever when server accepts connection but never sends response.\n\nFix/Discussion: Always set both connect and read timeouts: `requests.get(url, timeout=(3.05, 27))`. Tuple: (connect_timeout, read_timeout). Or single value for both. Mount a Session with default timeouts using a custom HTTPAdapter subclass. Never use `timeout=None` in production code.\nCode signature: status:closed", "file_path": "https://github.com/requests/requests/issues/4019", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "fa0c957f-9abd-4ffd-b77b-e4e702b4928d", "source_type": "github_issue", "domain": "Requests/HTTP/APIs", "library": "requests", "title": "BUG: requests hangs indefinitely on slow server responses", "content": "Problem: requests.get() hangs forever when server accepts connection but never sends response.\n\nFix/Discussion: Always set both connect and read timeouts: `requests.get(url, timeout=(3.05, 27))`. Tuple: (connect_timeout, read_timeout). Or single value for both. Mount a Session with default timeouts using a custom HTTPAdapter subclass. Never use `timeout=None` in production code.", "code_signature": "status:closed", "tags": "headers, aiohttp, rest-api", "url": "https://github.com/requests/requests/issues/4019", "author": "contributor_4599", "date_published": "29-05-2023", "votes_or_stars": 273, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:704996b6-0321-4a46-8285-0c5951468e00": {"content": "How to use asyncio.create_task in asyncio\n`asyncio.asyncio.create_task` schedules a coroutine as a Task in the event loop. Example usage:\n```python\ntask = asyncio.create_task(send_email(user))\nawait task\n```\nThis is useful when fire-and-forget background tasks, concurrent task management. Performance tip: use TaskGroup (Python 3.11+) for structured concurrency.\nCode signature: asyncio.create_task", "file_path": "https://docs.asyncio.org/en/stable/asyncio/create_task.html", "function_name": "asyncio.create_task", "type": "documentation", "metadata": {"record_id": "704996b6-0321-4a46-8285-0c5951468e00", "source_type": "documentation", "domain": "Asyncio/Threading", "library": "asyncio", "title": "How to use asyncio.create_task in asyncio", "content": "`asyncio.asyncio.create_task` schedules a coroutine as a Task in the event loop. Example usage:\n```python\ntask = asyncio.create_task(send_email(user))\nawait task\n```\nThis is useful when fire-and-forget background tasks, concurrent task management. Performance tip: use TaskGroup (Python 3.11+) for structured concurrency.", "code_signature": "asyncio.create_task", "tags": "gather, coroutine, lock, async, deadlock, threading", "url": "https://docs.asyncio.org/en/stable/asyncio/create_task.html", "author": "Official Docs", "date_published": "15-10-2022", "votes_or_stars": 4125, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:8b1a95ce-ac98-48b9-a2a9-55c7581b3138": {"content": "requests.exceptions.SSLError: certificate verify failed\nNever use `verify=False` in production — it disables SSL verification entirely. Solutions: (1) Update certifi: `pip install --upgrade certifi`. (2) Provide CA bundle: `requests.get(url, verify='/path/to/ca-bundle.crt')`. (3) For self-signed certs in dev: `verify=False` only, and never commit this. (4) Check system time — expired system clock causes SSL failures.\nCode signature: Your SSL certificate chain is incomplete or self-signed. Fix the cert, don't disable verification.", "file_path": "https://stackoverflow.com/questions/7907400", "function_name": "Your SSL certificate chain is incomplete or self-signed. Fix the cert, don't disable verification.", "type": "stack_overflow", "metadata": {"record_id": "8b1a95ce-ac98-48b9-a2a9-55c7581b3138", "source_type": "stack_overflow", "domain": "Requests/HTTP/APIs", "library": "websocket", "title": "requests.exceptions.SSLError: certificate verify failed", "content": "Never use `verify=False` in production — it disables SSL verification entirely. Solutions: (1) Update certifi: `pip install --upgrade certifi`. (2) Provide CA bundle: `requests.get(url, verify='/path/to/ca-bundle.crt')`. (3) For self-signed certs in dev: `verify=False` only, and never commit this. (4) Check system time — expired system clock causes SSL failures.", "code_signature": "Your SSL certificate chain is incomplete or self-signed. Fix the cert, don't disable verification.", "tags": "rate-limiting, requests, oauth, retry", "url": "https://stackoverflow.com/questions/7907400", "author": "user_851204", "date_published": "24-01-2021", "votes_or_stars": 1349, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:9d3a026a-373e-4a1f-a91a-fe6c137d618f": {"content": "How to limit concurrency with asyncio?\n```python\nsem = asyncio.Semaphore(10)\nasync def limited(url):\n    async with sem:\n        return await fetch(url)\nresults = await asyncio.gather(*[limited(u) for u in urls])\n```\nThis prevents overwhelming APIs or databases with too many simultaneous connections.\nCode signature: Use asyncio.Semaphore to cap the number of concurrent coroutines.", "file_path": "https://stackoverflow.com/questions/2375453", "function_name": "Use asyncio.Semaphore to cap the number of concurrent coroutines.", "type": "stack_overflow", "metadata": {"record_id": "9d3a026a-373e-4a1f-a91a-fe6c137d618f", "source_type": "stack_overflow", "domain": "Asyncio/Threading", "library": "asyncio", "title": "How to limit concurrency with asyncio?", "content": "```python\nsem = asyncio.Semaphore(10)\nasync def limited(url):\n    async with sem:\n        return await fetch(url)\nresults = await asyncio.gather(*[limited(u) for u in urls])\n```\nThis prevents overwhelming APIs or databases with too many simultaneous connections.", "code_signature": "Use asyncio.Semaphore to cap the number of concurrent coroutines.", "tags": "task, event-loop, asyncio", "url": "https://stackoverflow.com/questions/2375453", "author": "user_449589", "date_published": "03-07-2023", "votes_or_stars": 2445, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:295fa90c-4f66-4d4e-9c50-e808dae01306": {"content": "threading Thread Synchronization — official reference\nOfficial documentation for threading Thread Synchronization. The method signature is `threading.Lock()`. creates a mutual-exclusion lock for thread-safe access. Raises `RuntimeError` when lock is released without being acquired. See also: threading.RLock, threading.Semaphore, threading.Event.\nCode signature: threading.Lock()", "file_path": "https://docs.threading.org/en/stable/threading/Lock.html", "function_name": "threading.Lock()", "type": "documentation", "metadata": {"record_id": "295fa90c-4f66-4d4e-9c50-e808dae01306", "source_type": "documentation", "domain": "Asyncio/Threading", "library": "threading", "title": "threading Thread Synchronization — official reference", "content": "Official documentation for threading Thread Synchronization. The method signature is `threading.Lock()`. creates a mutual-exclusion lock for thread-safe access. Raises `RuntimeError` when lock is released without being acquired. See also: threading.RLock, threading.Semaphore, threading.Event.", "code_signature": "threading.Lock()", "tags": "coroutine, lock, event-loop", "url": "https://docs.threading.org/en/stable/threading/Lock.html", "author": "Official Docs", "date_published": "30-07-2019", "votes_or_stars": 4838, "relevance_label": "low", "difficulty_level": "intermediate"}}, "kb:c65c6afc-83ff-4623-b00c-9fac8e31a0f1": {"content": "FEAT: pathlib: add support for reading/writing with encoding shorthand\nProblem: pathlib.Path.read_text() does not default to UTF-8, causing issues across platforms.\n\nFix/Discussion: Always pass encoding explicitly: `Path('file').read_text(encoding='utf-8')`. Python 3.10+ added `encoding='locale'` default change proposal. Best practice: always specify encoding in all file I/O operations for portability.\nCode signature: status:closed", "file_path": "https://github.com/cpython/cpython/issues/2707", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "c65c6afc-83ff-4623-b00c-9fac8e31a0f1", "source_type": "github_issue", "domain": "OS/Subprocess/File I/O", "library": "cpython", "title": "FEAT: pathlib: add support for reading/writing with encoding shorthand", "content": "Problem: pathlib.Path.read_text() does not default to UTF-8, causing issues across platforms.\n\nFix/Discussion: Always pass encoding explicitly: `Path('file').read_text(encoding='utf-8')`. Python 3.10+ added `encoding='locale'` default change proposal. Best practice: always specify encoding in all file I/O operations for portability.", "code_signature": "status:closed", "tags": "process, os, stdout, shutil, file-io", "url": "https://github.com/cpython/cpython/issues/2707", "author": "contributor_7877", "date_published": "12-04-2022", "votes_or_stars": 458, "relevance_label": "low", "difficulty_level": "intermediate"}}, "kb:84fb110e-9949-4176-b151-64620b1e095f": {"content": "How to run asyncio code from synchronous Python?\n`asyncio.run(main())` creates an event loop, runs the coroutine, closes the loop. Never call asyncio.run() from inside a running event loop (use await instead). In Jupyter notebooks (which have a running loop), use `await coroutine()` directly or `nest_asyncio.apply()`. For mixing sync/async, use `loop.run_until_complete()`.\nCode signature: Use asyncio.run() in Python 3.7+ to execute a coroutine from sync code.", "file_path": "https://stackoverflow.com/questions/2140194", "function_name": "Use asyncio.run() in Python 3.7+ to execute a coroutine from sync code.", "type": "stack_overflow", "metadata": {"record_id": "84fb110e-9949-4176-b151-64620b1e095f", "source_type": "stack_overflow", "domain": "Asyncio/Threading", "library": "gather", "title": "How to run asyncio code from synchronous Python?", "content": "`asyncio.run(main())` creates an event loop, runs the coroutine, closes the loop. Never call asyncio.run() from inside a running event loop (use await instead). In Jupyter notebooks (which have a running loop), use `await coroutine()` directly or `nest_asyncio.apply()`. For mixing sync/async, use `loop.run_until_complete()`.", "code_signature": "Use asyncio.run() in Python 3.7+ to execute a coroutine from sync code.", "tags": "threading, race-condition, asyncio, gather, coroutine, executor", "url": "https://stackoverflow.com/questions/2140194", "author": "user_718011", "date_published": "24-11-2018", "votes_or_stars": 289, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:534f606f-2269-49e8-afb9-7243984f59a1": {"content": "Flask: how to return JSON responses properly?\n`return jsonify({'key': 'value'})` sets Content-Type to application/json. Flask 1.0+ automatically converts returned dicts to JSON responses. For custom status codes: `return jsonify(data), 201`. For errors: use `abort(404)` or return `({'error': 'Not found'}, 404)`.\nCode signature: Use jsonify() or return a dict directly (Flask 1.0+).", "file_path": "https://stackoverflow.com/questions/8930103", "function_name": "Use jsonify() or return a dict directly (Flask 1.0+).", "type": "stack_overflow", "metadata": {"record_id": "534f606f-2269-49e8-afb9-7243984f59a1", "source_type": "stack_overflow", "domain": "FastAPI/Flask/Django", "library": "django", "title": "Flask: how to return JSON responses properly?", "content": "`return jsonify({'key': 'value'})` sets Content-Type to application/json. Flask 1.0+ automatically converts returned dicts to JSON responses. For custom status codes: `return jsonify(data), 201`. For errors: use `abort(404)` or return `({'error': 'Not found'}, 404)`.", "code_signature": "Use jsonify() or return a dict directly (Flask 1.0+).", "tags": "orm, template, dependency-injection, middleware, fastapi, django", "url": "https://stackoverflow.com/questions/8930103", "author": "user_264801", "date_published": "24-11-2018", "votes_or_stars": 2226, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:d2e3eda8-aa7c-423e-8f8a-63c21646b432": {"content": "How to implement JWT authentication in FastAPI?\n```python\nfrom fastapi.security import OAuth2PasswordBearer\nfrom jose import jwt\noauth2_scheme = OAuth2PasswordBearer(tokenUrl='token')\n```\nCreate access tokens with `jwt.encode(payload, SECRET_KEY, algorithm='HS256')`. Verify with `jwt.decode(token, SECRET_KEY, algorithms=['HS256'])`. Store SECRET_KEY in environment variables, never in code.\nCode signature: Use python-jose for JWT and passlib for password hashing with OAuth2PasswordBearer.", "file_path": "https://stackoverflow.com/questions/7754864", "function_name": "Use python-jose for JWT and passlib for password hashing with OAuth2PasswordBearer.", "type": "stack_overflow", "metadata": {"record_id": "d2e3eda8-aa7c-423e-8f8a-63c21646b432", "source_type": "stack_overflow", "domain": "FastAPI/Flask/Django", "library": "fastapi", "title": "How to implement JWT authentication in FastAPI?", "content": "```python\nfrom fastapi.security import OAuth2PasswordBearer\nfrom jose import jwt\noauth2_scheme = OAuth2PasswordBearer(tokenUrl='token')\n```\nCreate access tokens with `jwt.encode(payload, SECRET_KEY, algorithm='HS256')`. Verify with `jwt.decode(token, SECRET_KEY, algorithms=['HS256'])`. Store SECRET_KEY in environment variables, never in code.", "code_signature": "Use python-jose for JWT and passlib for password hashing with OAuth2PasswordBearer.", "tags": "template, pydantic, rest", "url": "https://stackoverflow.com/questions/7754864", "author": "user_773587", "date_published": "21-10-2021", "votes_or_stars": 413, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:0755c5a7-5037-435c-8bcc-11202b01108b": {"content": "SQLAlchemy: how to avoid DetachedInstanceError?\nDetachedInstanceError occurs when accessing a relationship after the session is closed. Solutions: (1) Use `joinedload()` or `subqueryload()` to eagerly load relationships. (2) Keep session open while accessing objects. (3) Use `expire_on_commit=False` on sessionmaker. (4) Convert to dict/DTO before closing session.\nCode signature: Access lazy-loaded relationships within the session scope, or use eager loading.", "file_path": "https://stackoverflow.com/questions/1247595", "function_name": "Access lazy-loaded relationships within the session scope, or use eager loading.", "type": "stack_overflow", "metadata": {"record_id": "0755c5a7-5037-435c-8bcc-11202b01108b", "source_type": "stack_overflow", "domain": "SQLAlchemy/Databases", "library": "sqlalchemy", "title": "SQLAlchemy: how to avoid DetachedInstanceError?", "content": "DetachedInstanceError occurs when accessing a relationship after the session is closed. Solutions: (1) Use `joinedload()` or `subqueryload()` to eagerly load relationships. (2) Keep session open while accessing objects. (3) Use `expire_on_commit=False` on sessionmaker. (4) Convert to dict/DTO before closing session.", "code_signature": "Access lazy-loaded relationships within the session scope, or use eager loading.", "tags": "postgresql, migration, query, sqlite, session", "url": "https://stackoverflow.com/questions/1247595", "author": "user_107793", "date_published": "09-08-2024", "votes_or_stars": 406, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:27fb96c6-5970-4a02-b2c0-749ec350a5aa": {"content": "Why is my Pandas merge creating duplicate rows?\nDuplicate rows after merge usually mean the join key is not unique. Diagnose with `df.duplicated(subset=['key']).sum()`. Use `how='left'` carefully when right table has multiple matches. Set `validate='one_to_many'` or `'many_to_one'` to enforce cardinality. Consider deduplicating with `df.drop_duplicates(subset=['key'])` before merging.\nCode signature: Your merge key has duplicates in one or both DataFrames. Use validate='one_to_one' to catch this early.", "file_path": "https://stackoverflow.com/questions/2321324", "function_name": "Your merge key has duplicates in one or both DataFrames. Use validate='one_to_one' to catch this early.", "type": "stack_overflow", "metadata": {"record_id": "27fb96c6-5970-4a02-b2c0-749ec350a5aa", "source_type": "stack_overflow", "domain": "NumPy/Pandas", "library": "array", "title": "Why is my Pandas merge creating duplicate rows?", "content": "Duplicate rows after merge usually mean the join key is not unique. Diagnose with `df.duplicated(subset=['key']).sum()`. Use `how='left'` carefully when right table has multiple matches. Set `validate='one_to_many'` or `'many_to_one'` to enforce cardinality. Consider deduplicating with `df.drop_duplicates(subset=['key'])` before merging.", "code_signature": "Your merge key has duplicates in one or both DataFrames. Use validate='one_to_one' to catch this early.", "tags": "loc, nan, broadcasting", "url": "https://stackoverflow.com/questions/2321324", "author": "user_99814", "date_published": "28-11-2020", "votes_or_stars": 275, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:36139519-9a25-463e-a350-8dec574bb57e": {"content": "NumPy — Vectorization\nThe `np.vectorize` function in NumPy generalizes a scalar function to operate on arrays. It accepts `pyfunc` as a parameter and returns vectorized function. Common use cases include applying Python functions element-wise without explicit loops. Note that np.vectorize is a convenience wrapper; it does not improve performance like true ufuncs.\nCode signature: np.vectorize(pyfunc) -> vectorized function", "file_path": "https://docs.numpy.org/en/stable/np/vectorize.html", "function_name": "np.vectorize(pyfunc) -> vectorized function", "type": "documentation", "metadata": {"record_id": "36139519-9a25-463e-a350-8dec574bb57e", "source_type": "documentation", "domain": "NumPy/Pandas", "library": "NumPy", "title": "NumPy — Vectorization", "content": "The `np.vectorize` function in NumPy generalizes a scalar function to operate on arrays. It accepts `pyfunc` as a parameter and returns vectorized function. Common use cases include applying Python functions element-wise without explicit loops. Note that np.vectorize is a convenience wrapper; it does not improve performance like true ufuncs.", "code_signature": "np.vectorize(pyfunc) -> vectorized function", "tags": "groupby, iloc, nan, array", "url": "https://docs.numpy.org/en/stable/np/vectorize.html", "author": "Official Docs", "date_published": "02-06-2021", "votes_or_stars": 1326, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:f55ab10b-eba4-410c-8cba-0cc9f7ef7eeb": {"content": "Python: how to safely write to a file atomically?\n```python\nimport tempfile, os\nwith tempfile.NamedTemporaryFile('w', delete=False, dir='.') as tmp:\n    tmp.write(data)\n    tmp.flush()\n    os.fsync(tmp.fileno())\nos.replace(tmp.name, target_path)\n```\n`os.replace()` is atomic on POSIX. This prevents partial writes from corrupting files.\nCode signature: Write to a temp file then rename — rename is atomic on POSIX systems.", "file_path": "https://stackoverflow.com/questions/5394880", "function_name": "Write to a temp file then rename — rename is atomic on POSIX systems.", "type": "stack_overflow", "metadata": {"record_id": "f55ab10b-eba4-410c-8cba-0cc9f7ef7eeb", "source_type": "stack_overflow", "domain": "OS/Subprocess/File I/O", "library": "stdin", "title": "Python: how to safely write to a file atomically?", "content": "```python\nimport tempfile, os\nwith tempfile.NamedTemporaryFile('w', delete=False, dir='.') as tmp:\n    tmp.write(data)\n    tmp.flush()\n    os.fsync(tmp.fileno())\nos.replace(tmp.name, target_path)\n```\n`os.replace()` is atomic on POSIX. This prevents partial writes from corrupting files.", "code_signature": "Write to a temp file then rename — rename is atomic on POSIX systems.", "tags": "stdin, tempfile, os", "url": "https://stackoverflow.com/questions/5394880", "author": "user_179430", "date_published": "01-05-2023", "votes_or_stars": 547, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:10b36a39-8ca1-4486-88ab-4279d3bf1565": {"content": "How to watch a directory for file changes in Python?\n```python\nfrom watchdog.observers import Observer\nfrom watchdog.events import FileSystemEventHandler\nhandler = FileSystemEventHandler()\nobserver = Observer()\nobserver.schedule(handler, path='.', recursive=True)\nobserver.start()\n```\nFor simple polling: `os.stat(file).st_mtime`. Linux-native: `inotify`. macOS-native: `FSEvents`.\nCode signature: Use the watchdog library for cross-platform directory monitoring.", "file_path": "https://stackoverflow.com/questions/5213115", "function_name": "Use the watchdog library for cross-platform directory monitoring.", "type": "stack_overflow", "metadata": {"record_id": "10b36a39-8ca1-4486-88ab-4279d3bf1565", "source_type": "stack_overflow", "domain": "OS/Subprocess/File I/O", "library": "env-variable", "title": "How to watch a directory for file changes in Python?", "content": "```python\nfrom watchdog.observers import Observer\nfrom watchdog.events import FileSystemEventHandler\nhandler = FileSystemEventHandler()\nobserver = Observer()\nobserver.schedule(handler, path='.', recursive=True)\nobserver.start()\n```\nFor simple polling: `os.stat(file).st_mtime`. Linux-native: `inotify`. macOS-native: `FSEvents`.", "code_signature": "Use the watchdog library for cross-platform directory monitoring.", "tags": "subprocess, env-variable, stdin", "url": "https://stackoverflow.com/questions/5213115", "author": "user_959314", "date_published": "09-06-2018", "votes_or_stars": 2106, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:bc232e1d-968d-4a96-8463-e872cfcd38df": {"content": "PERF: requests connection not reused between calls — new TCP connection each time\nProblem: Each requests.get() call opens a new TCP connection, causing high latency.\n\nFix/Discussion: requests.get/post etc. create a new Session per call. Fix: use a persistent Session: `session = requests.Session()` and reuse it. Sessions pool connections via urllib3. For thread-safety: create one Session per thread. For async: use httpx.AsyncClient or aiohttp.ClientSession for async connection pooling.\nCode signature: status:closed", "file_path": "https://github.com/requests/requests/issues/3605", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "bc232e1d-968d-4a96-8463-e872cfcd38df", "source_type": "github_issue", "domain": "Requests/HTTP/APIs", "library": "requests", "title": "PERF: requests connection not reused between calls — new TCP connection each time", "content": "Problem: Each requests.get() call opens a new TCP connection, causing high latency.\n\nFix/Discussion: requests.get/post etc. create a new Session per call. Fix: use a persistent Session: `session = requests.Session()` and reuse it. Sessions pool connections via urllib3. For thread-safety: create one Session per thread. For async: use httpx.AsyncClient or aiohttp.ClientSession for async connection pooling.", "code_signature": "status:closed", "tags": "retry, authentication, aiohttp, http, websocket", "url": "https://github.com/requests/requests/issues/3605", "author": "contributor_1152", "date_published": "01-02-2019", "votes_or_stars": 361, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:70b879bc-7a9e-430c-903a-bb00f882914e": {"content": "SQLAlchemy bulk insert: what's the fastest way?\n`session.bulk_insert_mappings(Model, list_of_dicts)` skips ORM overhead. Even faster: `session.execute(insert(Model), data)`. Avoid `session.add()` in loops. For PostgreSQL, use `insert().on_conflict_do_nothing()` for upserts. Batch in chunks of 1000-5000 rows for optimal performance.\nCode signature: Use session.bulk_insert_mappings() or execute(insert(), list_of_dicts) for maximum speed.", "file_path": "https://stackoverflow.com/questions/5977931", "function_name": "Use session.bulk_insert_mappings() or execute(insert(), list_of_dicts) for maximum speed.", "type": "stack_overflow", "metadata": {"record_id": "70b879bc-7a9e-430c-903a-bb00f882914e", "source_type": "stack_overflow", "domain": "SQLAlchemy/Databases", "library": "query", "title": "SQLAlchemy bulk insert: what's the fastest way?", "content": "`session.bulk_insert_mappings(Model, list_of_dicts)` skips ORM overhead. Even faster: `session.execute(insert(Model), data)`. Avoid `session.add()` in loops. For PostgreSQL, use `insert().on_conflict_do_nothing()` for upserts. Batch in chunks of 1000-5000 rows for optimal performance.", "code_signature": "Use session.bulk_insert_mappings() or execute(insert(), list_of_dicts) for maximum speed.", "tags": "sqlalchemy, connection-pool, relationship, postgresql, migration", "url": "https://stackoverflow.com/questions/5977931", "author": "user_238275", "date_published": "27-09-2022", "votes_or_stars": 650, "relevance_label": "low", "difficulty_level": "advanced"}}, "kb:1fa2e98e-142d-4a7a-b502-02e7732c58e9": {"content": "How to implement database connection pooling with SQLAlchemy?\n`create_engine(url, pool_size=10, max_overflow=20, pool_timeout=30, pool_recycle=1800)`. Use `pool_pre_ping=True` to detect stale connections. For async: use `AsyncEngine` with `create_async_engine()`. Monitor with `engine.pool.status()`. NullPool disables pooling for scripts/serverless environments.\nCode signature: create_engine() has built-in pooling. Configure pool_size and max_overflow.", "file_path": "https://stackoverflow.com/questions/7358113", "function_name": "create_engine() has built-in pooling. Configure pool_size and max_overflow.", "type": "stack_overflow", "metadata": {"record_id": "1fa2e98e-142d-4a7a-b502-02e7732c58e9", "source_type": "stack_overflow", "domain": "SQLAlchemy/Databases", "library": "session", "title": "How to implement database connection pooling with SQLAlchemy?", "content": "`create_engine(url, pool_size=10, max_overflow=20, pool_timeout=30, pool_recycle=1800)`. Use `pool_pre_ping=True` to detect stale connections. For async: use `AsyncEngine` with `create_async_engine()`. Monitor with `engine.pool.status()`. NullPool disables pooling for scripts/serverless environments.", "code_signature": "create_engine() has built-in pooling. Configure pool_size and max_overflow.", "tags": "postgresql, sqlite, foreign-key", "url": "https://stackoverflow.com/questions/7358113", "author": "user_12260", "date_published": "08-07-2018", "votes_or_stars": 224, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:20f56d6c-8507-4378-9406-e776e30bb00b": {"content": "PERF: concurrent.futures.ProcessPoolExecutor slow startup on Windows\nProblem: ProcessPoolExecutor takes 3-5 seconds to start on Windows due to process spawning overhead.\n\nFix/Discussion: Windows uses 'spawn' start method (vs 'fork' on Linux). Mitigations: (1) Reuse executor across calls — don't create/destroy in loops. (2) Use 'forkserver' or 'fork' on Linux/macOS. (3) For short tasks, ThreadPoolExecutor may be faster. (4) Use `initializer` param to pre-load modules.\nCode signature: status:closed", "file_path": "https://github.com/cpython/cpython/issues/6647", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "20f56d6c-8507-4378-9406-e776e30bb00b", "source_type": "github_issue", "domain": "Asyncio/Threading", "library": "cpython", "title": "PERF: concurrent.futures.ProcessPoolExecutor slow startup on Windows", "content": "Problem: ProcessPoolExecutor takes 3-5 seconds to start on Windows due to process spawning overhead.\n\nFix/Discussion: Windows uses 'spawn' start method (vs 'fork' on Linux). Mitigations: (1) Reuse executor across calls — don't create/destroy in loops. (2) Use 'forkserver' or 'fork' on Linux/macOS. (3) For short tasks, ThreadPoolExecutor may be faster. (4) Use `initializer` param to pre-load modules.", "code_signature": "status:closed", "tags": "executor, await, task, gather, semaphore", "url": "https://github.com/cpython/cpython/issues/6647", "author": "contributor_4097", "date_published": "03-03-2019", "votes_or_stars": 238, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:0d3823d0-f083-412a-accd-2c0533995d30": {"content": "How to run background tasks in FastAPI?\nFastAPI's `BackgroundTasks`: add `background_tasks: BackgroundTasks` as a parameter, then `background_tasks.add_task(func, *args)`. Runs after response is sent. For heavy workloads (email, ML inference), use Celery with Redis/RabbitMQ broker. For async background work, `asyncio.create_task()` works within async endpoints.\nCode signature: Use BackgroundTasks for lightweight tasks or Celery for heavy/distributed work.", "file_path": "https://stackoverflow.com/questions/8077999", "function_name": "Use BackgroundTasks for lightweight tasks or Celery for heavy/distributed work.", "type": "stack_overflow", "metadata": {"record_id": "0d3823d0-f083-412a-accd-2c0533995d30", "source_type": "stack_overflow", "domain": "FastAPI/Flask/Django", "library": "django", "title": "How to run background tasks in FastAPI?", "content": "FastAPI's `BackgroundTasks`: add `background_tasks: BackgroundTasks` as a parameter, then `background_tasks.add_task(func, *args)`. Runs after response is sent. For heavy workloads (email, ML inference), use Celery with Redis/RabbitMQ broker. For async background work, `asyncio.create_task()` works within async endpoints.", "code_signature": "Use BackgroundTasks for lightweight tasks or Celery for heavy/distributed work.", "tags": "pydantic, flask, request, routing", "url": "https://stackoverflow.com/questions/8077999", "author": "user_202401", "date_published": "25-03-2020", "votes_or_stars": 1864, "relevance_label": "low", "difficulty_level": "advanced"}}, "kb:57c3a341-0848-42bb-bcb3-54e0b38cb6ed": {"content": "How to implement JWT authentication in FastAPI?\n```python\nfrom fastapi.security import OAuth2PasswordBearer\nfrom jose import jwt\noauth2_scheme = OAuth2PasswordBearer(tokenUrl='token')\n```\nCreate access tokens with `jwt.encode(payload, SECRET_KEY, algorithm='HS256')`. Verify with `jwt.decode(token, SECRET_KEY, algorithms=['HS256'])`. Store SECRET_KEY in environment variables, never in code.\nCode signature: Use python-jose for JWT and passlib for password hashing with OAuth2PasswordBearer.", "file_path": "https://stackoverflow.com/questions/7754864", "function_name": "Use python-jose for JWT and passlib for password hashing with OAuth2PasswordBearer.", "type": "stack_overflow", "metadata": {"record_id": "57c3a341-0848-42bb-bcb3-54e0b38cb6ed", "source_type": "stack_overflow", "domain": "FastAPI/Flask/Django", "library": "fastapi", "title": "How to implement JWT authentication in FastAPI?", "content": "```python\nfrom fastapi.security import OAuth2PasswordBearer\nfrom jose import jwt\noauth2_scheme = OAuth2PasswordBearer(tokenUrl='token')\n```\nCreate access tokens with `jwt.encode(payload, SECRET_KEY, algorithm='HS256')`. Verify with `jwt.decode(token, SECRET_KEY, algorithms=['HS256'])`. Store SECRET_KEY in environment variables, never in code.", "code_signature": "Use python-jose for JWT and passlib for password hashing with OAuth2PasswordBearer.", "tags": "django, response, rest", "url": "https://stackoverflow.com/questions/7754864", "author": "user_773587", "date_published": "30-11-2022", "votes_or_stars": 416, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:1616156a-e195-4ee5-b937-62ee88d9e4e6": {"content": "Pandas Merging DataFrames — official reference\nOfficial documentation for Pandas Merging DataFrames. The method signature is `pd.merge(left, right, on, how='inner')`. merges two DataFrames using SQL-style joins. Raises `MergeError` when merge keys produce duplicate columns. See also: df.join, df.concat.\nCode signature: pd.merge(left, right, on, how='inner')", "file_path": "https://docs.pandas.org/en/stable/pd/merge.html", "function_name": "pd.merge(left, right, on, how='inner')", "type": "documentation", "metadata": {"record_id": "1616156a-e195-4ee5-b937-62ee88d9e4e6", "source_type": "documentation", "domain": "NumPy/Pandas", "library": "Pandas", "title": "Pandas Merging DataFrames — official reference", "content": "Official documentation for Pandas Merging DataFrames. The method signature is `pd.merge(left, right, on, how='inner')`. merges two DataFrames using SQL-style joins. Raises `MergeError` when merge keys produce duplicate columns. See also: df.join, df.concat.", "code_signature": "pd.merge(left, right, on, how='inner')", "tags": "dataframe, broadcasting, reshape, loc, nan, pivot", "url": "https://docs.pandas.org/en/stable/pd/merge.html", "author": "Official Docs", "date_published": "11-10-2021", "votes_or_stars": 4540, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:01770363-bca2-41c6-a35f-2ee605776015": {"content": "PERF: concurrent.futures.ProcessPoolExecutor slow startup on Windows\nProblem: ProcessPoolExecutor takes 3-5 seconds to start on Windows due to process spawning overhead.\n\nFix/Discussion: Windows uses 'spawn' start method (vs 'fork' on Linux). Mitigations: (1) Reuse executor across calls — don't create/destroy in loops. (2) Use 'forkserver' or 'fork' on Linux/macOS. (3) For short tasks, ThreadPoolExecutor may be faster. (4) Use `initializer` param to pre-load modules.\nCode signature: status:closed", "file_path": "https://github.com/cpython/cpython/issues/6647", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "01770363-bca2-41c6-a35f-2ee605776015", "source_type": "github_issue", "domain": "Asyncio/Threading", "library": "cpython", "title": "PERF: concurrent.futures.ProcessPoolExecutor slow startup on Windows", "content": "Problem: ProcessPoolExecutor takes 3-5 seconds to start on Windows due to process spawning overhead.\n\nFix/Discussion: Windows uses 'spawn' start method (vs 'fork' on Linux). Mitigations: (1) Reuse executor across calls — don't create/destroy in loops. (2) Use 'forkserver' or 'fork' on Linux/macOS. (3) For short tasks, ThreadPoolExecutor may be faster. (4) Use `initializer` param to pre-load modules.", "code_signature": "status:closed", "tags": "lock, threading, gather", "url": "https://github.com/cpython/cpython/issues/6647", "author": "contributor_4097", "date_published": "08-09-2021", "votes_or_stars": 229, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:5a540e47-67a5-495a-b401-4600a0653e0e": {"content": "How to apply a function to each row in Pandas without using iterrows?\n`iterrows()` is slow because it creates a Series per row. Prefer `df.apply(func, axis=1)` for row-wise operations, or better yet, vectorize using NumPy: `np.where(condition, a, b)`. For complex logic, `df.assign()` with vectorized expressions is cleanest. Benchmark: vectorized > apply > itertuples > iterrows.\nCode signature: Use df.apply(func, axis=1) or vectorized NumPy operations for best performance.", "file_path": "https://stackoverflow.com/questions/5446912", "function_name": "Use df.apply(func, axis=1) or vectorized NumPy operations for best performance.", "type": "stack_overflow", "metadata": {"record_id": "5a540e47-67a5-495a-b401-4600a0653e0e", "source_type": "stack_overflow", "domain": "NumPy/Pandas", "library": "dataframe", "title": "How to apply a function to each row in Pandas without using iterrows?", "content": "`iterrows()` is slow because it creates a Series per row. Prefer `df.apply(func, axis=1)` for row-wise operations, or better yet, vectorize using NumPy: `np.where(condition, a, b)`. For complex logic, `df.assign()` with vectorized expressions is cleanest. Benchmark: vectorized > apply > itertuples > iterrows.", "code_signature": "Use df.apply(func, axis=1) or vectorized NumPy operations for best performance.", "tags": "nan, reshape, pandas, broadcasting, numpy, pivot", "url": "https://stackoverflow.com/questions/5446912", "author": "user_563306", "date_published": "29-05-2019", "votes_or_stars": 2244, "relevance_label": "low", "difficulty_level": "beginner"}}, "kb:633bddff-7e7f-4fd3-934e-039c4fe7a5c9": {"content": "How to stream large HTTP responses with requests?\n```python\nwith requests.get(url, stream=True) as r:\n    r.raise_for_status()\n    with open('file', 'wb') as f:\n        for chunk in r.iter_content(chunk_size=8192):\n            f.write(chunk)\n```\nAlways use as context manager with stream=True to ensure connection is released.\nCode signature: Use stream=True and iterate over response.iter_content() or iter_lines().", "file_path": "https://stackoverflow.com/questions/3592974", "function_name": "Use stream=True and iterate over response.iter_content() or iter_lines().", "type": "stack_overflow", "metadata": {"record_id": "633bddff-7e7f-4fd3-934e-039c4fe7a5c9", "source_type": "stack_overflow", "domain": "Requests/HTTP/APIs", "library": "json", "title": "How to stream large HTTP responses with requests?", "content": "```python\nwith requests.get(url, stream=True) as r:\n    r.raise_for_status()\n    with open('file', 'wb') as f:\n        for chunk in r.iter_content(chunk_size=8192):\n            f.write(chunk)\n```\nAlways use as context manager with stream=True to ensure connection is released.", "code_signature": "Use stream=True and iterate over response.iter_content() or iter_lines().", "tags": "rate-limiting, timeout, aiohttp, websocket", "url": "https://stackoverflow.com/questions/3592974", "author": "user_980733", "date_published": "17-03-2023", "votes_or_stars": 1675, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:109edd02-f4ce-42bd-bccd-7f43657878d6": {"content": "How do I reshape a wide DataFrame to long format?\n`pd.melt(df, id_vars=['id'], value_vars=['col1','col2'])` unpivots columns to rows. `df.stack()` moves column index to row index. For the reverse, use `df.pivot()` or `df.unstack()`. Always reset_index() after stack/unstack to get a clean DataFrame.\nCode signature: Use pd.melt() or df.stack() depending on your structure.", "file_path": "https://stackoverflow.com/questions/6229731", "function_name": "Use pd.melt() or df.stack() depending on your structure.", "type": "stack_overflow", "metadata": {"record_id": "109edd02-f4ce-42bd-bccd-7f43657878d6", "source_type": "stack_overflow", "domain": "NumPy/Pandas", "library": "nan", "title": "How do I reshape a wide DataFrame to long format?", "content": "`pd.melt(df, id_vars=['id'], value_vars=['col1','col2'])` unpivots columns to rows. `df.stack()` moves column index to row index. For the reverse, use `df.pivot()` or `df.unstack()`. Always reset_index() after stack/unstack to get a clean DataFrame.", "code_signature": "Use pd.melt() or df.stack() depending on your structure.", "tags": "merge, vectorization, pivot, dtype, reshape, dataframe", "url": "https://stackoverflow.com/questions/6229731", "author": "user_428373", "date_published": "20-08-2019", "votes_or_stars": 849, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:260743f4-8779-4fde-aecd-a6c912c296fc": {"content": "PERF: groupby().apply() extremely slow on large DataFrames\nProblem: groupby().apply() with a custom function is 100x slower than expected on 10M row DataFrames.\n\nFix/Discussion: groupby().apply() serializes each group and has high overhead. Fix: vectorize with numpy, use groupby().transform() for same-shape output, or groupby().agg() with named aggregations. For complex logic, consider Polars or Dask. Numba's @jit can accelerate custom group functions.\nCode signature: status:closed", "file_path": "https://github.com/pandas/pandas/issues/8446", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "260743f4-8779-4fde-aecd-a6c912c296fc", "source_type": "github_issue", "domain": "NumPy/Pandas", "library": "pandas", "title": "PERF: groupby().apply() extremely slow on large DataFrames", "content": "Problem: groupby().apply() with a custom function is 100x slower than expected on 10M row DataFrames.\n\nFix/Discussion: groupby().apply() serializes each group and has high overhead. Fix: vectorize with numpy, use groupby().transform() for same-shape output, or groupby().agg() with named aggregations. For complex logic, consider Polars or Dask. Numba's @jit can accelerate custom group functions.", "code_signature": "status:closed", "tags": "vectorization, numpy, broadcasting, array", "url": "https://github.com/pandas/pandas/issues/8446", "author": "contributor_6648", "date_published": "22-05-2018", "votes_or_stars": 321, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:0e6f7fb5-8690-4db5-8d66-2ee8ad6e149a": {"content": "How to use httpx.AsyncClient in httpx\n`httpx.httpx.AsyncClient` provides an async HTTP client for non-blocking requests. Example usage:\n```python\nasync with httpx.AsyncClient() as client:\n    r = await client.get(url)\n    data = r.json()\n```\nThis is useful when making concurrent HTTP requests in async applications. Performance tip: set timeout=httpx.Timeout(10.0) to prevent hanging requests.\nCode signature: httpx.AsyncClient", "file_path": "https://docs.httpx.org/en/stable/httpx/AsyncClient.html", "function_name": "httpx.AsyncClient", "type": "documentation", "metadata": {"record_id": "0e6f7fb5-8690-4db5-8d66-2ee8ad6e149a", "source_type": "documentation", "domain": "Requests/HTTP/APIs", "library": "httpx", "title": "How to use httpx.AsyncClient in httpx", "content": "`httpx.httpx.AsyncClient` provides an async HTTP client for non-blocking requests. Example usage:\n```python\nasync with httpx.AsyncClient() as client:\n    r = await client.get(url)\n    data = r.json()\n```\nThis is useful when making concurrent HTTP requests in async applications. Performance tip: set timeout=httpx.Timeout(10.0) to prevent hanging requests.", "code_signature": "httpx.AsyncClient", "tags": "aiohttp, httpx, authentication, retry, headers, websocket", "url": "https://docs.httpx.org/en/stable/httpx/AsyncClient.html", "author": "Official Docs", "date_published": "27-11-2019", "votes_or_stars": 4210, "relevance_label": "low", "difficulty_level": "intermediate"}}, "kb:aa10f4e3-139b-4b68-8d33-5eee1ae0a379": {"content": "How to limit concurrency with asyncio?\n```python\nsem = asyncio.Semaphore(10)\nasync def limited(url):\n    async with sem:\n        return await fetch(url)\nresults = await asyncio.gather(*[limited(u) for u in urls])\n```\nThis prevents overwhelming APIs or databases with too many simultaneous connections.\nCode signature: Use asyncio.Semaphore to cap the number of concurrent coroutines.", "file_path": "https://stackoverflow.com/questions/2375453", "function_name": "Use asyncio.Semaphore to cap the number of concurrent coroutines.", "type": "stack_overflow", "metadata": {"record_id": "aa10f4e3-139b-4b68-8d33-5eee1ae0a379", "source_type": "stack_overflow", "domain": "Asyncio/Threading", "library": "asyncio", "title": "How to limit concurrency with asyncio?", "content": "```python\nsem = asyncio.Semaphore(10)\nasync def limited(url):\n    async with sem:\n        return await fetch(url)\nresults = await asyncio.gather(*[limited(u) for u in urls])\n```\nThis prevents overwhelming APIs or databases with too many simultaneous connections.", "code_signature": "Use asyncio.Semaphore to cap the number of concurrent coroutines.", "tags": "threading, gather, race-condition", "url": "https://stackoverflow.com/questions/2375453", "author": "user_449589", "date_published": "17-11-2021", "votes_or_stars": 2418, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:abaa3a98-d860-460d-8b87-d9456df87783": {"content": "asyncio — Concurrent Coroutines\nThe `asyncio.gather` function in asyncio runs multiple coroutines concurrently and collects results. It accepts `*coros, return_exceptions=False` as a parameter and returns list of results. Common use cases include parallel I/O operations such as multiple API calls. Note that set return_exceptions=True to prevent one failure from cancelling all tasks.\nCode signature: asyncio.gather(*coros, return_exceptions=False) -> list of results", "file_path": "https://docs.asyncio.org/en/stable/asyncio/gather.html", "function_name": "asyncio.gather(*coros, return_exceptions=False) -> list of results", "type": "documentation", "metadata": {"record_id": "abaa3a98-d860-460d-8b87-d9456df87783", "source_type": "documentation", "domain": "Asyncio/Threading", "library": "asyncio", "title": "asyncio — Concurrent Coroutines", "content": "The `asyncio.gather` function in asyncio runs multiple coroutines concurrently and collects results. It accepts `*coros, return_exceptions=False` as a parameter and returns list of results. Common use cases include parallel I/O operations such as multiple API calls. Note that set return_exceptions=True to prevent one failure from cancelling all tasks.", "code_signature": "asyncio.gather(*coros, return_exceptions=False) -> list of results", "tags": "executor, threading, deadlock, await, asyncio, semaphore", "url": "https://docs.asyncio.org/en/stable/asyncio/gather.html", "author": "Official Docs", "date_published": "20-10-2023", "votes_or_stars": 526, "relevance_label": "low", "difficulty_level": "beginner"}}, "kb:841e5e8c-5993-46dc-81c4-7a234d5b3c64": {"content": "How to run background tasks in FastAPI?\nFastAPI's `BackgroundTasks`: add `background_tasks: BackgroundTasks` as a parameter, then `background_tasks.add_task(func, *args)`. Runs after response is sent. For heavy workloads (email, ML inference), use Celery with Redis/RabbitMQ broker. For async background work, `asyncio.create_task()` works within async endpoints.\nCode signature: Use BackgroundTasks for lightweight tasks or Celery for heavy/distributed work.", "file_path": "https://stackoverflow.com/questions/8077999", "function_name": "Use BackgroundTasks for lightweight tasks or Celery for heavy/distributed work.", "type": "stack_overflow", "metadata": {"record_id": "841e5e8c-5993-46dc-81c4-7a234d5b3c64", "source_type": "stack_overflow", "domain": "FastAPI/Flask/Django", "library": "django", "title": "How to run background tasks in FastAPI?", "content": "FastAPI's `BackgroundTasks`: add `background_tasks: BackgroundTasks` as a parameter, then `background_tasks.add_task(func, *args)`. Runs after response is sent. For heavy workloads (email, ML inference), use Celery with Redis/RabbitMQ broker. For async background work, `asyncio.create_task()` works within async endpoints.", "code_signature": "Use BackgroundTasks for lightweight tasks or Celery for heavy/distributed work.", "tags": "dependency-injection, response, pydantic, blueprint, routing, orm", "url": "https://stackoverflow.com/questions/8077999", "author": "user_202401", "date_published": "12-08-2024", "votes_or_stars": 1831, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:5b873dcb-fcb2-4bb9-9f01-baf211dc81f8": {"content": "Why is my Pandas merge creating duplicate rows?\nDuplicate rows after merge usually mean the join key is not unique. Diagnose with `df.duplicated(subset=['key']).sum()`. Use `how='left'` carefully when right table has multiple matches. Set `validate='one_to_many'` or `'many_to_one'` to enforce cardinality. Consider deduplicating with `df.drop_duplicates(subset=['key'])` before merging.\nCode signature: Your merge key has duplicates in one or both DataFrames. Use validate='one_to_one' to catch this early.", "file_path": "https://stackoverflow.com/questions/2321324", "function_name": "Your merge key has duplicates in one or both DataFrames. Use validate='one_to_one' to catch this early.", "type": "stack_overflow", "metadata": {"record_id": "5b873dcb-fcb2-4bb9-9f01-baf211dc81f8", "source_type": "stack_overflow", "domain": "NumPy/Pandas", "library": "array", "title": "Why is my Pandas merge creating duplicate rows?", "content": "Duplicate rows after merge usually mean the join key is not unique. Diagnose with `df.duplicated(subset=['key']).sum()`. Use `how='left'` carefully when right table has multiple matches. Set `validate='one_to_many'` or `'many_to_one'` to enforce cardinality. Consider deduplicating with `df.drop_duplicates(subset=['key'])` before merging.", "code_signature": "Your merge key has duplicates in one or both DataFrames. Use validate='one_to_one' to catch this early.", "tags": "broadcasting, merge, pandas, array", "url": "https://stackoverflow.com/questions/2321324", "author": "user_99814", "date_published": "10-12-2018", "votes_or_stars": 265, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:1f331420-41c0-4fd2-9f4a-172ca5637429": {"content": "How to handle rate limiting with the requests library?\n```python\nfrom tenacity import retry, wait_exponential, stop_after_attempt\n@retry(wait=wait_exponential(min=1, max=60), stop=stop_after_attempt(5))\ndef call_api(url):\n    r = requests.get(url)\n    r.raise_for_status()\n    return r.json()\n```\nCheck `Retry-After` header in 429 responses. For async: use `aiohttp` with tenacity.\nCode signature: Implement exponential backoff with the tenacity or backoff library.", "file_path": "https://stackoverflow.com/questions/1604451", "function_name": "Implement exponential backoff with the tenacity or backoff library.", "type": "stack_overflow", "metadata": {"record_id": "1f331420-41c0-4fd2-9f4a-172ca5637429", "source_type": "stack_overflow", "domain": "Requests/HTTP/APIs", "library": "retry", "title": "How to handle rate limiting with the requests library?", "content": "```python\nfrom tenacity import retry, wait_exponential, stop_after_attempt\n@retry(wait=wait_exponential(min=1, max=60), stop=stop_after_attempt(5))\ndef call_api(url):\n    r = requests.get(url)\n    r.raise_for_status()\n    return r.json()\n```\nCheck `Retry-After` header in 429 responses. For async: use `aiohttp` with tenacity.", "code_signature": "Implement exponential backoff with the tenacity or backoff library.", "tags": "httpx, timeout, rest-api, http, aiohttp", "url": "https://stackoverflow.com/questions/1604451", "author": "user_885136", "date_published": "07-01-2019", "votes_or_stars": 613, "relevance_label": "low", "difficulty_level": "advanced"}}, "kb:ad1697ef-ff08-4b93-9b51-c94cf0bc60af": {"content": "BUG: asyncio task leaks when exception is not handled\nProblem: Unhandled exceptions in fire-and-forget asyncio tasks are silently ignored, causing task leaks.\n\nFix/Discussion: Always await tasks or add exception handlers. Use `task.add_done_callback()` to log exceptions. In Python 3.11+, use TaskGroup for structured concurrency — exceptions propagate automatically. Set `asyncio.get_event_loop().set_exception_handler()` for global unhandled task exception logging.\nCode signature: status:closed", "file_path": "https://github.com/cpython/cpython/issues/5591", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "ad1697ef-ff08-4b93-9b51-c94cf0bc60af", "source_type": "github_issue", "domain": "Asyncio/Threading", "library": "cpython", "title": "BUG: asyncio task leaks when exception is not handled", "content": "Problem: Unhandled exceptions in fire-and-forget asyncio tasks are silently ignored, causing task leaks.\n\nFix/Discussion: Always await tasks or add exception handlers. Use `task.add_done_callback()` to log exceptions. In Python 3.11+, use TaskGroup for structured concurrency — exceptions propagate automatically. Set `asyncio.get_event_loop().set_exception_handler()` for global unhandled task exception logging.", "code_signature": "status:closed", "tags": "event-loop, lock, gather", "url": "https://github.com/cpython/cpython/issues/5591", "author": "contributor_1630", "date_published": "22-05-2021", "votes_or_stars": 326, "relevance_label": "low", "difficulty_level": "intermediate"}}, "kb:32d687d3-3358-4a56-942a-35760e1cebe2": {"content": "How to handle database migrations with Alembic?\nWorkflow: (1) Change SQLAlchemy models. (2) `alembic revision --autogenerate -m 'description'`. (3) Review generated migration script. (4) `alembic upgrade head` to apply. Always review autogenerated migrations — they miss some changes (indexes, constraints). Use `alembic downgrade -1` to rollback. Store migrations in version control.\nCode signature: Use alembic revision --autogenerate to create migrations from model changes.", "file_path": "https://stackoverflow.com/questions/9436429", "function_name": "Use alembic revision --autogenerate to create migrations from model changes.", "type": "stack_overflow", "metadata": {"record_id": "32d687d3-3358-4a56-942a-35760e1cebe2", "source_type": "stack_overflow", "domain": "SQLAlchemy/Databases", "library": "connection-pool", "title": "How to handle database migrations with Alembic?", "content": "Workflow: (1) Change SQLAlchemy models. (2) `alembic revision --autogenerate -m 'description'`. (3) Review generated migration script. (4) `alembic upgrade head` to apply. Always review autogenerated migrations — they miss some changes (indexes, constraints). Use `alembic downgrade -1` to rollback. Store migrations in version control.", "code_signature": "Use alembic revision --autogenerate to create migrations from model changes.", "tags": "relationship, transaction, alembic, migration, session", "url": "https://stackoverflow.com/questions/9436429", "author": "user_974047", "date_published": "30-03-2023", "votes_or_stars": 2410, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:e4d184cc-fc0e-4d6d-a7c1-b448c9dfe451": {"content": "How to read a large file line by line in Python without loading it all into memory?\n```python\nwith open('large.txt', 'r') as f:\n    for line in f:\n        process(line.strip())\n```\nThis reads one line at a time. For binary files: use `f.read(chunk_size)` in a loop. For CSV: use `pd.read_csv(file, chunksize=10000)`. Never use `f.readlines()` on large files.\nCode signature: Iterate over the file object directly — it yields lines lazily.", "file_path": "https://stackoverflow.com/questions/2737893", "function_name": "Iterate over the file object directly — it yields lines lazily.", "type": "stack_overflow", "metadata": {"record_id": "e4d184cc-fc0e-4d6d-a7c1-b448c9dfe451", "source_type": "stack_overflow", "domain": "OS/Subprocess/File I/O", "library": "shutil", "title": "How to read a large file line by line in Python without loading it all into memory?", "content": "```python\nwith open('large.txt', 'r') as f:\n    for line in f:\n        process(line.strip())\n```\nThis reads one line at a time. For binary files: use `f.read(chunk_size)` in a loop. For CSV: use `pd.read_csv(file, chunksize=10000)`. Never use `f.readlines()` on large files.", "code_signature": "Iterate over the file object directly — it yields lines lazily.", "tags": "stdin, subprocess, file-io, env-variable", "url": "https://stackoverflow.com/questions/2737893", "author": "user_994539", "date_published": "11-11-2021", "votes_or_stars": 2300, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:2fc08a05-fb9b-4bcd-8849-75e8f5ee52ef": {"content": "BUG: Flask session not persisting between requests in tests\nProblem: Session data set in one request is not available in the next request during testing.\n\nFix/Discussion: Use Flask test client's session_transaction() context manager: `with client.session_transaction() as sess:\n    sess['user_id'] = 1`. Ensure SECRET_KEY is set. For testing: `app.config['TESTING'] = True; app.config['SECRET_KEY'] = 'test'`.\nCode signature: status:closed", "file_path": "https://github.com/flask/flask/issues/2141", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "2fc08a05-fb9b-4bcd-8849-75e8f5ee52ef", "source_type": "github_issue", "domain": "FastAPI/Flask/Django", "library": "flask", "title": "BUG: Flask session not persisting between requests in tests", "content": "Problem: Session data set in one request is not available in the next request during testing.\n\nFix/Discussion: Use Flask test client's session_transaction() context manager: `with client.session_transaction() as sess:\n    sess['user_id'] = 1`. Ensure SECRET_KEY is set. For testing: `app.config['TESTING'] = True; app.config['SECRET_KEY'] = 'test'`.", "code_signature": "status:closed", "tags": "rest, django, blueprint, template, fastapi, flask", "url": "https://github.com/flask/flask/issues/2141", "author": "contributor_5020", "date_published": "22-08-2019", "votes_or_stars": 143, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:85e1ef11-f0c8-408c-b38c-e2de84d60f22": {"content": "How to run background tasks in FastAPI?\nFastAPI's `BackgroundTasks`: add `background_tasks: BackgroundTasks` as a parameter, then `background_tasks.add_task(func, *args)`. Runs after response is sent. For heavy workloads (email, ML inference), use Celery with Redis/RabbitMQ broker. For async background work, `asyncio.create_task()` works within async endpoints.\nCode signature: Use BackgroundTasks for lightweight tasks or Celery for heavy/distributed work.", "file_path": "https://stackoverflow.com/questions/8077999", "function_name": "Use BackgroundTasks for lightweight tasks or Celery for heavy/distributed work.", "type": "stack_overflow", "metadata": {"record_id": "85e1ef11-f0c8-408c-b38c-e2de84d60f22", "source_type": "stack_overflow", "domain": "FastAPI/Flask/Django", "library": "django", "title": "How to run background tasks in FastAPI?", "content": "FastAPI's `BackgroundTasks`: add `background_tasks: BackgroundTasks` as a parameter, then `background_tasks.add_task(func, *args)`. Runs after response is sent. For heavy workloads (email, ML inference), use Celery with Redis/RabbitMQ broker. For async background work, `asyncio.create_task()` works within async endpoints.", "code_signature": "Use BackgroundTasks for lightweight tasks or Celery for heavy/distributed work.", "tags": "rest, template, blueprint, orm", "url": "https://stackoverflow.com/questions/8077999", "author": "user_202401", "date_published": "12-05-2021", "votes_or_stars": 1847, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:86add8fd-0283-43a4-98e8-c9c6b65a73b2": {"content": "BUG: asyncio task leaks when exception is not handled\nProblem: Unhandled exceptions in fire-and-forget asyncio tasks are silently ignored, causing task leaks.\n\nFix/Discussion: Always await tasks or add exception handlers. Use `task.add_done_callback()` to log exceptions. In Python 3.11+, use TaskGroup for structured concurrency — exceptions propagate automatically. Set `asyncio.get_event_loop().set_exception_handler()` for global unhandled task exception logging.\nCode signature: status:closed", "file_path": "https://github.com/cpython/cpython/issues/5591", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "86add8fd-0283-43a4-98e8-c9c6b65a73b2", "source_type": "github_issue", "domain": "Asyncio/Threading", "library": "cpython", "title": "BUG: asyncio task leaks when exception is not handled", "content": "Problem: Unhandled exceptions in fire-and-forget asyncio tasks are silently ignored, causing task leaks.\n\nFix/Discussion: Always await tasks or add exception handlers. Use `task.add_done_callback()` to log exceptions. In Python 3.11+, use TaskGroup for structured concurrency — exceptions propagate automatically. Set `asyncio.get_event_loop().set_exception_handler()` for global unhandled task exception logging.", "code_signature": "status:closed", "tags": "executor, task, semaphore, asyncio, lock", "url": "https://github.com/cpython/cpython/issues/5591", "author": "contributor_1630", "date_published": "05-04-2018", "votes_or_stars": 358, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:adf195be-9453-4e7f-843e-fa8473ba647c": {"content": "How do I reshape a wide DataFrame to long format?\n`pd.melt(df, id_vars=['id'], value_vars=['col1','col2'])` unpivots columns to rows. `df.stack()` moves column index to row index. For the reverse, use `df.pivot()` or `df.unstack()`. Always reset_index() after stack/unstack to get a clean DataFrame.\nCode signature: Use pd.melt() or df.stack() depending on your structure.", "file_path": "https://stackoverflow.com/questions/6229731", "function_name": "Use pd.melt() or df.stack() depending on your structure.", "type": "stack_overflow", "metadata": {"record_id": "adf195be-9453-4e7f-843e-fa8473ba647c", "source_type": "stack_overflow", "domain": "NumPy/Pandas", "library": "nan", "title": "How do I reshape a wide DataFrame to long format?", "content": "`pd.melt(df, id_vars=['id'], value_vars=['col1','col2'])` unpivots columns to rows. `df.stack()` moves column index to row index. For the reverse, use `df.pivot()` or `df.unstack()`. Always reset_index() after stack/unstack to get a clean DataFrame.", "code_signature": "Use pd.melt() or df.stack() depending on your structure.", "tags": "broadcasting, vectorization, iloc, pivot, numpy", "url": "https://stackoverflow.com/questions/6229731", "author": "user_428373", "date_published": "02-10-2018", "votes_or_stars": 808, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:d5d1bcc7-8f43-42ad-ac18-f6724b4f22e0": {"content": "NumPy — Linear Algebra\nThe `np.einsum` function in NumPy evaluates Einstein summation convention on operands. It accepts `subscripts, *operands` as a parameter and returns ndarray. Common use cases include matrix multiplication, dot products, tensor contractions. Note that prefer np.einsum over loops for large arrays.\nCode signature: np.einsum(subscripts, *operands) -> ndarray", "file_path": "https://docs.numpy.org/en/stable/np/einsum.html", "function_name": "np.einsum(subscripts, *operands) -> ndarray", "type": "documentation", "metadata": {"record_id": "d5d1bcc7-8f43-42ad-ac18-f6724b4f22e0", "source_type": "documentation", "domain": "NumPy/Pandas", "library": "NumPy", "title": "NumPy — Linear Algebra", "content": "The `np.einsum` function in NumPy evaluates Einstein summation convention on operands. It accepts `subscripts, *operands` as a parameter and returns ndarray. Common use cases include matrix multiplication, dot products, tensor contractions. Note that prefer np.einsum over loops for large arrays.", "code_signature": "np.einsum(subscripts, *operands) -> ndarray", "tags": "dataframe, pandas, iloc, reshape, dtype, vectorization", "url": "https://docs.numpy.org/en/stable/np/einsum.html", "author": "Official Docs", "date_published": "31-12-2018", "votes_or_stars": 1906, "relevance_label": "low", "difficulty_level": "beginner"}}, "kb:422705f6-854d-4870-b129-c5828faf6ba2": {"content": "How to watch a directory for file changes in Python?\n```python\nfrom watchdog.observers import Observer\nfrom watchdog.events import FileSystemEventHandler\nhandler = FileSystemEventHandler()\nobserver = Observer()\nobserver.schedule(handler, path='.', recursive=True)\nobserver.start()\n```\nFor simple polling: `os.stat(file).st_mtime`. Linux-native: `inotify`. macOS-native: `FSEvents`.\nCode signature: Use the watchdog library for cross-platform directory monitoring.", "file_path": "https://stackoverflow.com/questions/5213115", "function_name": "Use the watchdog library for cross-platform directory monitoring.", "type": "stack_overflow", "metadata": {"record_id": "422705f6-854d-4870-b129-c5828faf6ba2", "source_type": "stack_overflow", "domain": "OS/Subprocess/File I/O", "library": "env-variable", "title": "How to watch a directory for file changes in Python?", "content": "```python\nfrom watchdog.observers import Observer\nfrom watchdog.events import FileSystemEventHandler\nhandler = FileSystemEventHandler()\nobserver = Observer()\nobserver.schedule(handler, path='.', recursive=True)\nobserver.start()\n```\nFor simple polling: `os.stat(file).st_mtime`. Linux-native: `inotify`. macOS-native: `FSEvents`.", "code_signature": "Use the watchdog library for cross-platform directory monitoring.", "tags": "file-io, process, pipe, stdout, stderr", "url": "https://stackoverflow.com/questions/5213115", "author": "user_959314", "date_published": "24-10-2024", "votes_or_stars": 2068, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:5a3dc02d-b349-4144-b07d-448f4c29b454": {"content": "How to add retry logic to requests?\n```python\nfrom requests.adapters import HTTPAdapter\nfrom urllib3.util.retry import Retry\nretry = Retry(total=3, backoff_factor=1, status_forcelist=[500,502,503,504])\nadapter = HTTPAdapter(max_retries=retry)\nsession.mount('https://', adapter)\n```\nCode signature: Mount an HTTPAdapter with urllib3 Retry on the Session.", "file_path": "https://stackoverflow.com/questions/1669324", "function_name": "Mount an HTTPAdapter with urllib3 Retry on the Session.", "type": "stack_overflow", "metadata": {"record_id": "5a3dc02d-b349-4144-b07d-448f4c29b454", "source_type": "stack_overflow", "domain": "Requests/HTTP/APIs", "library": "websocket", "title": "How to add retry logic to requests?", "content": "```python\nfrom requests.adapters import HTTPAdapter\nfrom urllib3.util.retry import Retry\nretry = Retry(total=3, backoff_factor=1, status_forcelist=[500,502,503,504])\nadapter = HTTPAdapter(max_retries=retry)\nsession.mount('https://', adapter)\n```", "code_signature": "Mount an HTTPAdapter with urllib3 Retry on the Session.", "tags": "requests, aiohttp, rest-api", "url": "https://stackoverflow.com/questions/1669324", "author": "user_952590", "date_published": "03-09-2019", "votes_or_stars": 1531, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:d0d8cdc3-c5ea-419a-b084-3e2568109e15": {"content": "Django ORM Queries — official reference\nOfficial documentation for Django ORM Queries. The method signature is `QuerySet.filter(**kwargs)`. filters QuerySet rows matching given field lookups. Raises `FieldError` when an unknown field lookup is provided. See also: QuerySet.exclude, QuerySet.annotate, Q objects.\nCode signature: QuerySet.filter(**kwargs)", "file_path": "https://docs.django.org/en/stable/QuerySet/filter.html", "function_name": "QuerySet.filter(**kwargs)", "type": "documentation", "metadata": {"record_id": "d0d8cdc3-c5ea-419a-b084-3e2568109e15", "source_type": "documentation", "domain": "FastAPI/Flask/Django", "library": "Django", "title": "Django ORM Queries — official reference", "content": "Official documentation for Django ORM Queries. The method signature is `QuerySet.filter(**kwargs)`. filters QuerySet rows matching given field lookups. Raises `FieldError` when an unknown field lookup is provided. See also: QuerySet.exclude, QuerySet.annotate, Q objects.", "code_signature": "QuerySet.filter(**kwargs)", "tags": "rest, orm, pydantic, blueprint", "url": "https://docs.django.org/en/stable/QuerySet/filter.html", "author": "Official Docs", "date_published": "21-04-2022", "votes_or_stars": 2393, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:5e7bf536-5ada-45f1-b889-6495abcc22b9": {"content": "asyncio RuntimeError: This event loop is already running\nThis error is common in Jupyter and FastAPI. Solutions: (1) In async context: just use `await coroutine()`. (2) In Jupyter: `import nest_asyncio; nest_asyncio.apply()`. (3) Run in thread: `asyncio.get_event_loop().run_in_executor(None, sync_func)`. (4) Use `asyncio.ensure_future()` to schedule from within async code.\nCode signature: You're calling asyncio.run() inside an already running event loop. Use await instead.", "file_path": "https://stackoverflow.com/questions/2229110", "function_name": "You're calling asyncio.run() inside an already running event loop. Use await instead.", "type": "stack_overflow", "metadata": {"record_id": "5e7bf536-5ada-45f1-b889-6495abcc22b9", "source_type": "stack_overflow", "domain": "Asyncio/Threading", "library": "async", "title": "asyncio RuntimeError: This event loop is already running", "content": "This error is common in Jupyter and FastAPI. Solutions: (1) In async context: just use `await coroutine()`. (2) In Jupyter: `import nest_asyncio; nest_asyncio.apply()`. (3) Run in thread: `asyncio.get_event_loop().run_in_executor(None, sync_func)`. (4) Use `asyncio.ensure_future()` to schedule from within async code.", "code_signature": "You're calling asyncio.run() inside an already running event loop. Use await instead.", "tags": "gather, semaphore, event-loop, executor, coroutine", "url": "https://stackoverflow.com/questions/2229110", "author": "user_573750", "date_published": "22-12-2021", "votes_or_stars": 2329, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:c5796659-1d45-4058-8543-be5c5524f349": {"content": "BUG: Alembic autogenerate misses index changes\nProblem: Alembic --autogenerate does not detect changes to indexes or constraints on existing tables.\n\nFix/Discussion: Alembic autogenerate compares metadata to database state but has limitations. Workarounds: (1) Manually add `op.create_index()` to migration. (2) Use `include_schemas=True` in env.py. (3) Run `alembic check` to see detected changes. (4) Always review autogenerated migrations before applying.\nCode signature: status:open", "file_path": "https://github.com/alembic/alembic/issues/7344", "function_name": "status:open", "type": "github_issue", "metadata": {"record_id": "c5796659-1d45-4058-8543-be5c5524f349", "source_type": "github_issue", "domain": "SQLAlchemy/Databases", "library": "alembic", "title": "BUG: Alembic autogenerate misses index changes", "content": "Problem: Alembic --autogenerate does not detect changes to indexes or constraints on existing tables.\n\nFix/Discussion: Alembic autogenerate compares metadata to database state but has limitations. Workarounds: (1) Manually add `op.create_index()` to migration. (2) Use `include_schemas=True` in env.py. (3) Run `alembic check` to see detected changes. (4) Always review autogenerated migrations before applying.", "code_signature": "status:open", "tags": "alembic, query, session, orm", "url": "https://github.com/alembic/alembic/issues/7344", "author": "contributor_3601", "date_published": "21-07-2024", "votes_or_stars": 248, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:d622515a-72bc-4c20-8fb2-5e78ca492104": {"content": "PERF: Django template rendering bottleneck in production\nProblem: Template rendering takes 500ms+ for complex pages with many database queries.\n\nFix/Discussion: Primary cause is N+1 queries in template. Solutions: (1) Use select_related/prefetch_related. (2) Cache rendered templates with `cache` template tag. (3) Use `django-cachalot` for query caching. (4) Move heavy computation to views, pass pre-computed context. (5) Consider server-side caching with Redis.\nCode signature: status:closed", "file_path": "https://github.com/django/django/issues/3243", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "d622515a-72bc-4c20-8fb2-5e78ca492104", "source_type": "github_issue", "domain": "FastAPI/Flask/Django", "library": "django", "title": "PERF: Django template rendering bottleneck in production", "content": "Problem: Template rendering takes 500ms+ for complex pages with many database queries.\n\nFix/Discussion: Primary cause is N+1 queries in template. Solutions: (1) Use select_related/prefetch_related. (2) Cache rendered templates with `cache` template tag. (3) Use `django-cachalot` for query caching. (4) Move heavy computation to views, pass pre-computed context. (5) Consider server-side caching with Redis.", "code_signature": "status:closed", "tags": "response, request, blueprint, middleware, routing, template", "url": "https://github.com/django/django/issues/3243", "author": "contributor_6988", "date_published": "05-06-2021", "votes_or_stars": 85, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:624a866d-7f0c-4a6c-baa8-16e557e4d6d6": {"content": "BUG: SettingWithCopyWarning when modifying sliced DataFrame\nProblem: Modifying a slice of a DataFrame raises SettingWithCopyWarning and may not update original.\n\nFix/Discussion: Always use .loc or .copy() when slicing. Use `df.loc[mask, 'col'] = val` for assignment. Create explicit copies: `subset = df[mask].copy()`. In Pandas 2.0+, Copy-on-Write makes this behavior consistent — enable with `pd.options.mode.copy_on_write = True`.\nCode signature: status:closed", "file_path": "https://github.com/pandas/pandas/issues/3264", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "624a866d-7f0c-4a6c-baa8-16e557e4d6d6", "source_type": "github_issue", "domain": "NumPy/Pandas", "library": "pandas", "title": "BUG: SettingWithCopyWarning when modifying sliced DataFrame", "content": "Problem: Modifying a slice of a DataFrame raises SettingWithCopyWarning and may not update original.\n\nFix/Discussion: Always use .loc or .copy() when slicing. Use `df.loc[mask, 'col'] = val` for assignment. Create explicit copies: `subset = df[mask].copy()`. In Pandas 2.0+, Copy-on-Write makes this behavior consistent — enable with `pd.options.mode.copy_on_write = True`.", "code_signature": "status:closed", "tags": "pandas, reshape, loc", "url": "https://github.com/pandas/pandas/issues/3264", "author": "contributor_6628", "date_published": "26-07-2018", "votes_or_stars": 46, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:37a56d46-1011-4796-b288-4a0005b2850a": {"content": "BUG: threading.Timer not cancellable after it fires\nProblem: Calling cancel() on a Timer that has already executed does not raise an error but has no effect.\n\nFix/Discussion: threading.Timer.cancel() only works before the timer fires. Store Timer reference and check `timer.finished.is_set()` to know if it's already run. For repeating timers, use a loop with `threading.Event` for clean cancellation: `stop_event = threading.Event(); stop_event.wait(interval)`.\nCode signature: status:closed", "file_path": "https://github.com/cpython/cpython/issues/4111", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "37a56d46-1011-4796-b288-4a0005b2850a", "source_type": "github_issue", "domain": "Asyncio/Threading", "library": "cpython", "title": "BUG: threading.Timer not cancellable after it fires", "content": "Problem: Calling cancel() on a Timer that has already executed does not raise an error but has no effect.\n\nFix/Discussion: threading.Timer.cancel() only works before the timer fires. Store Timer reference and check `timer.finished.is_set()` to know if it's already run. For repeating timers, use a loop with `threading.Event` for clean cancellation: `stop_event = threading.Event(); stop_event.wait(interval)`.", "code_signature": "status:closed", "tags": "gather, race-condition, lock", "url": "https://github.com/cpython/cpython/issues/4111", "author": "contributor_7884", "date_published": "08-06-2020", "votes_or_stars": 3, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:cba34db0-69b0-4746-a74d-da4f661a1a9c": {"content": "Why is my Pandas merge creating duplicate rows?\nDuplicate rows after merge usually mean the join key is not unique. Diagnose with `df.duplicated(subset=['key']).sum()`. Use `how='left'` carefully when right table has multiple matches. Set `validate='one_to_many'` or `'many_to_one'` to enforce cardinality. Consider deduplicating with `df.drop_duplicates(subset=['key'])` before merging.\nCode signature: Your merge key has duplicates in one or both DataFrames. Use validate='one_to_one' to catch this early.", "file_path": "https://stackoverflow.com/questions/2321324", "function_name": "Your merge key has duplicates in one or both DataFrames. Use validate='one_to_one' to catch this early.", "type": "stack_overflow", "metadata": {"record_id": "cba34db0-69b0-4746-a74d-da4f661a1a9c", "source_type": "stack_overflow", "domain": "NumPy/Pandas", "library": "array", "title": "Why is my Pandas merge creating duplicate rows?", "content": "Duplicate rows after merge usually mean the join key is not unique. Diagnose with `df.duplicated(subset=['key']).sum()`. Use `how='left'` carefully when right table has multiple matches. Set `validate='one_to_many'` or `'many_to_one'` to enforce cardinality. Consider deduplicating with `df.drop_duplicates(subset=['key'])` before merging.", "code_signature": "Your merge key has duplicates in one or both DataFrames. Use validate='one_to_one' to catch this early.", "tags": "dtype, groupby, vectorization, loc", "url": "https://stackoverflow.com/questions/2321324", "author": "user_99814", "date_published": "17-05-2022", "votes_or_stars": 256, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:e7d8bb8b-90db-4295-9e4f-dd4fdde8d023": {"content": "Flask: how to return JSON responses properly?\n`return jsonify({'key': 'value'})` sets Content-Type to application/json. Flask 1.0+ automatically converts returned dicts to JSON responses. For custom status codes: `return jsonify(data), 201`. For errors: use `abort(404)` or return `({'error': 'Not found'}, 404)`.\nCode signature: Use jsonify() or return a dict directly (Flask 1.0+).", "file_path": "https://stackoverflow.com/questions/8930103", "function_name": "Use jsonify() or return a dict directly (Flask 1.0+).", "type": "stack_overflow", "metadata": {"record_id": "e7d8bb8b-90db-4295-9e4f-dd4fdde8d023", "source_type": "stack_overflow", "domain": "FastAPI/Flask/Django", "library": "django", "title": "Flask: how to return JSON responses properly?", "content": "`return jsonify({'key': 'value'})` sets Content-Type to application/json. Flask 1.0+ automatically converts returned dicts to JSON responses. For custom status codes: `return jsonify(data), 201`. For errors: use `abort(404)` or return `({'error': 'Not found'}, 404)`.", "code_signature": "Use jsonify() or return a dict directly (Flask 1.0+).", "tags": "response, template, flask, rest", "url": "https://stackoverflow.com/questions/8930103", "author": "user_264801", "date_published": "09-11-2019", "votes_or_stars": 2191, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:01b31f53-78e6-4439-9dfa-ac45e99aa8c6": {"content": "PERF: groupby().apply() extremely slow on large DataFrames\nProblem: groupby().apply() with a custom function is 100x slower than expected on 10M row DataFrames.\n\nFix/Discussion: groupby().apply() serializes each group and has high overhead. Fix: vectorize with numpy, use groupby().transform() for same-shape output, or groupby().agg() with named aggregations. For complex logic, consider Polars or Dask. Numba's @jit can accelerate custom group functions.\nCode signature: status:closed", "file_path": "https://github.com/pandas/pandas/issues/8446", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "01b31f53-78e6-4439-9dfa-ac45e99aa8c6", "source_type": "github_issue", "domain": "NumPy/Pandas", "library": "pandas", "title": "PERF: groupby().apply() extremely slow on large DataFrames", "content": "Problem: groupby().apply() with a custom function is 100x slower than expected on 10M row DataFrames.\n\nFix/Discussion: groupby().apply() serializes each group and has high overhead. Fix: vectorize with numpy, use groupby().transform() for same-shape output, or groupby().agg() with named aggregations. For complex logic, consider Polars or Dask. Numba's @jit can accelerate custom group functions.", "code_signature": "status:closed", "tags": "broadcasting, loc, reshape", "url": "https://github.com/pandas/pandas/issues/8446", "author": "contributor_6648", "date_published": "05-11-2018", "votes_or_stars": 360, "relevance_label": "low", "difficulty_level": "beginner"}}, "kb:987a397b-86c9-4044-ba92-058e3dbc4985": {"content": "How to use Path.glob in pathlib\n`pathlib.Path.glob` yields all paths matching the given glob pattern. Example usage:\n```python\npy_files = list(Path('src').glob('**/*.py'))\n```\nThis is useful when finding files recursively, batch file processing. Performance tip: iterate the generator lazily rather than converting to list for large directories.\nCode signature: Path.glob", "file_path": "https://docs.pathlib.org/en/stable/Path/glob.html", "function_name": "Path.glob", "type": "documentation", "metadata": {"record_id": "987a397b-86c9-4044-ba92-058e3dbc4985", "source_type": "documentation", "domain": "OS/Subprocess/File I/O", "library": "pathlib", "title": "How to use Path.glob in pathlib", "content": "`pathlib.Path.glob` yields all paths matching the given glob pattern. Example usage:\n```python\npy_files = list(Path('src').glob('**/*.py'))\n```\nThis is useful when finding files recursively, batch file processing. Performance tip: iterate the generator lazily rather than converting to list for large directories.", "code_signature": "Path.glob", "tags": "tempfile, env-variable, os, stderr, subprocess, pathlib", "url": "https://docs.pathlib.org/en/stable/Path/glob.html", "author": "Official Docs", "date_published": "01-11-2021", "votes_or_stars": 3198, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:e5f96a2d-d30e-444f-bcb5-9e5426d9a6ff": {"content": "How to add retry logic to requests?\n```python\nfrom requests.adapters import HTTPAdapter\nfrom urllib3.util.retry import Retry\nretry = Retry(total=3, backoff_factor=1, status_forcelist=[500,502,503,504])\nadapter = HTTPAdapter(max_retries=retry)\nsession.mount('https://', adapter)\n```\nCode signature: Mount an HTTPAdapter with urllib3 Retry on the Session.", "file_path": "https://stackoverflow.com/questions/1669324", "function_name": "Mount an HTTPAdapter with urllib3 Retry on the Session.", "type": "stack_overflow", "metadata": {"record_id": "e5f96a2d-d30e-444f-bcb5-9e5426d9a6ff", "source_type": "stack_overflow", "domain": "Requests/HTTP/APIs", "library": "websocket", "title": "How to add retry logic to requests?", "content": "```python\nfrom requests.adapters import HTTPAdapter\nfrom urllib3.util.retry import Retry\nretry = Retry(total=3, backoff_factor=1, status_forcelist=[500,502,503,504])\nadapter = HTTPAdapter(max_retries=retry)\nsession.mount('https://', adapter)\n```", "code_signature": "Mount an HTTPAdapter with urllib3 Retry on the Session.", "tags": "rest-api, websocket, oauth", "url": "https://stackoverflow.com/questions/1669324", "author": "user_952590", "date_published": "01-06-2018", "votes_or_stars": 1485, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:759fbcf5-c0a0-4952-963a-dce9773c899d": {"content": "How to read a large file line by line in Python without loading it all into memory?\n```python\nwith open('large.txt', 'r') as f:\n    for line in f:\n        process(line.strip())\n```\nThis reads one line at a time. For binary files: use `f.read(chunk_size)` in a loop. For CSV: use `pd.read_csv(file, chunksize=10000)`. Never use `f.readlines()` on large files.\nCode signature: Iterate over the file object directly — it yields lines lazily.", "file_path": "https://stackoverflow.com/questions/2737893", "function_name": "Iterate over the file object directly — it yields lines lazily.", "type": "stack_overflow", "metadata": {"record_id": "759fbcf5-c0a0-4952-963a-dce9773c899d", "source_type": "stack_overflow", "domain": "OS/Subprocess/File I/O", "library": "shutil", "title": "How to read a large file line by line in Python without loading it all into memory?", "content": "```python\nwith open('large.txt', 'r') as f:\n    for line in f:\n        process(line.strip())\n```\nThis reads one line at a time. For binary files: use `f.read(chunk_size)` in a loop. For CSV: use `pd.read_csv(file, chunksize=10000)`. Never use `f.readlines()` on large files.", "code_signature": "Iterate over the file object directly — it yields lines lazily.", "tags": "os, subprocess, stdin", "url": "https://stackoverflow.com/questions/2737893", "author": "user_994539", "date_published": "11-06-2023", "votes_or_stars": 2301, "relevance_label": "low", "difficulty_level": "intermediate"}}, "kb:9a94e037-cbd9-4e33-ac0c-94f0ec72bb76": {"content": "PERF: groupby().apply() extremely slow on large DataFrames\nProblem: groupby().apply() with a custom function is 100x slower than expected on 10M row DataFrames.\n\nFix/Discussion: groupby().apply() serializes each group and has high overhead. Fix: vectorize with numpy, use groupby().transform() for same-shape output, or groupby().agg() with named aggregations. For complex logic, consider Polars or Dask. Numba's @jit can accelerate custom group functions.\nCode signature: status:closed", "file_path": "https://github.com/pandas/pandas/issues/8446", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "9a94e037-cbd9-4e33-ac0c-94f0ec72bb76", "source_type": "github_issue", "domain": "NumPy/Pandas", "library": "pandas", "title": "PERF: groupby().apply() extremely slow on large DataFrames", "content": "Problem: groupby().apply() with a custom function is 100x slower than expected on 10M row DataFrames.\n\nFix/Discussion: groupby().apply() serializes each group and has high overhead. Fix: vectorize with numpy, use groupby().transform() for same-shape output, or groupby().agg() with named aggregations. For complex logic, consider Polars or Dask. Numba's @jit can accelerate custom group functions.", "code_signature": "status:closed", "tags": "groupby, pivot, merge, nan, pandas, broadcasting", "url": "https://github.com/pandas/pandas/issues/8446", "author": "contributor_6648", "date_published": "30-05-2020", "votes_or_stars": 330, "relevance_label": "low", "difficulty_level": "intermediate"}}, "kb:ad0ef856-8018-4e11-8110-074050bf98bc": {"content": "How to run asyncio code from synchronous Python?\n`asyncio.run(main())` creates an event loop, runs the coroutine, closes the loop. Never call asyncio.run() from inside a running event loop (use await instead). In Jupyter notebooks (which have a running loop), use `await coroutine()` directly or `nest_asyncio.apply()`. For mixing sync/async, use `loop.run_until_complete()`.\nCode signature: Use asyncio.run() in Python 3.7+ to execute a coroutine from sync code.", "file_path": "https://stackoverflow.com/questions/2140194", "function_name": "Use asyncio.run() in Python 3.7+ to execute a coroutine from sync code.", "type": "stack_overflow", "metadata": {"record_id": "ad0ef856-8018-4e11-8110-074050bf98bc", "source_type": "stack_overflow", "domain": "Asyncio/Threading", "library": "gather", "title": "How to run asyncio code from synchronous Python?", "content": "`asyncio.run(main())` creates an event loop, runs the coroutine, closes the loop. Never call asyncio.run() from inside a running event loop (use await instead). In Jupyter notebooks (which have a running loop), use `await coroutine()` directly or `nest_asyncio.apply()`. For mixing sync/async, use `loop.run_until_complete()`.", "code_signature": "Use asyncio.run() in Python 3.7+ to execute a coroutine from sync code.", "tags": "race-condition, lock, deadlock, await, coroutine", "url": "https://stackoverflow.com/questions/2140194", "author": "user_718011", "date_published": "12-07-2018", "votes_or_stars": 282, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:767628da-e024-45cc-ac69-2c24085a9593": {"content": "BUG: asyncio task leaks when exception is not handled\nProblem: Unhandled exceptions in fire-and-forget asyncio tasks are silently ignored, causing task leaks.\n\nFix/Discussion: Always await tasks or add exception handlers. Use `task.add_done_callback()` to log exceptions. In Python 3.11+, use TaskGroup for structured concurrency — exceptions propagate automatically. Set `asyncio.get_event_loop().set_exception_handler()` for global unhandled task exception logging.\nCode signature: status:closed", "file_path": "https://github.com/cpython/cpython/issues/5591", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "767628da-e024-45cc-ac69-2c24085a9593", "source_type": "github_issue", "domain": "Asyncio/Threading", "library": "cpython", "title": "BUG: asyncio task leaks when exception is not handled", "content": "Problem: Unhandled exceptions in fire-and-forget asyncio tasks are silently ignored, causing task leaks.\n\nFix/Discussion: Always await tasks or add exception handlers. Use `task.add_done_callback()` to log exceptions. In Python 3.11+, use TaskGroup for structured concurrency — exceptions propagate automatically. Set `asyncio.get_event_loop().set_exception_handler()` for global unhandled task exception logging.", "code_signature": "status:closed", "tags": "lock, executor, race-condition, await, semaphore, asyncio", "url": "https://github.com/cpython/cpython/issues/5591", "author": "contributor_1630", "date_published": "10-08-2022", "votes_or_stars": 316, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:57579c67-ac1a-4d94-8bad-6e138581d0ea": {"content": "BUG: Alembic autogenerate misses index changes\nProblem: Alembic --autogenerate does not detect changes to indexes or constraints on existing tables.\n\nFix/Discussion: Alembic autogenerate compares metadata to database state but has limitations. Workarounds: (1) Manually add `op.create_index()` to migration. (2) Use `include_schemas=True` in env.py. (3) Run `alembic check` to see detected changes. (4) Always review autogenerated migrations before applying.\nCode signature: status:open", "file_path": "https://github.com/alembic/alembic/issues/7344", "function_name": "status:open", "type": "github_issue", "metadata": {"record_id": "57579c67-ac1a-4d94-8bad-6e138581d0ea", "source_type": "github_issue", "domain": "SQLAlchemy/Databases", "library": "alembic", "title": "BUG: Alembic autogenerate misses index changes", "content": "Problem: Alembic --autogenerate does not detect changes to indexes or constraints on existing tables.\n\nFix/Discussion: Alembic autogenerate compares metadata to database state but has limitations. Workarounds: (1) Manually add `op.create_index()` to migration. (2) Use `include_schemas=True` in env.py. (3) Run `alembic check` to see detected changes. (4) Always review autogenerated migrations before applying.", "code_signature": "status:open", "tags": "session, connection-pool, orm, sqlalchemy", "url": "https://github.com/alembic/alembic/issues/7344", "author": "contributor_3601", "date_published": "18-05-2023", "votes_or_stars": 251, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:7d869120-37e9-4db5-8c2e-58b866b2df17": {"content": "How to implement a thread pool in Python?\n```python\nfrom concurrent.futures import ThreadPoolExecutor\nwith ThreadPoolExecutor(max_workers=8) as ex:\n    futures = [ex.submit(task, arg) for arg in args]\n    results = [f.result() for f in futures]\n```\nFor CPU-bound: use ProcessPoolExecutor instead. For async integration: `await loop.run_in_executor(executor, func, *args)`.\nCode signature: Use concurrent.futures.ThreadPoolExecutor for I/O-bound tasks.", "file_path": "https://stackoverflow.com/questions/3646552", "function_name": "Use concurrent.futures.ThreadPoolExecutor for I/O-bound tasks.", "type": "stack_overflow", "metadata": {"record_id": "7d869120-37e9-4db5-8c2e-58b866b2df17", "source_type": "stack_overflow", "domain": "Asyncio/Threading", "library": "coroutine", "title": "How to implement a thread pool in Python?", "content": "```python\nfrom concurrent.futures import ThreadPoolExecutor\nwith ThreadPoolExecutor(max_workers=8) as ex:\n    futures = [ex.submit(task, arg) for arg in args]\n    results = [f.result() for f in futures]\n```\nFor CPU-bound: use ProcessPoolExecutor instead. For async integration: `await loop.run_in_executor(executor, func, *args)`.", "code_signature": "Use concurrent.futures.ThreadPoolExecutor for I/O-bound tasks.", "tags": "async, executor, threading, semaphore, deadlock, gather", "url": "https://stackoverflow.com/questions/3646552", "author": "user_469469", "date_published": "11-06-2018", "votes_or_stars": 1517, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:0336c308-7214-4661-a594-f94d47416a22": {"content": "FEAT: pathlib: add support for reading/writing with encoding shorthand\nProblem: pathlib.Path.read_text() does not default to UTF-8, causing issues across platforms.\n\nFix/Discussion: Always pass encoding explicitly: `Path('file').read_text(encoding='utf-8')`. Python 3.10+ added `encoding='locale'` default change proposal. Best practice: always specify encoding in all file I/O operations for portability.\nCode signature: status:closed", "file_path": "https://github.com/cpython/cpython/issues/2707", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "0336c308-7214-4661-a594-f94d47416a22", "source_type": "github_issue", "domain": "OS/Subprocess/File I/O", "library": "cpython", "title": "FEAT: pathlib: add support for reading/writing with encoding shorthand", "content": "Problem: pathlib.Path.read_text() does not default to UTF-8, causing issues across platforms.\n\nFix/Discussion: Always pass encoding explicitly: `Path('file').read_text(encoding='utf-8')`. Python 3.10+ added `encoding='locale'` default change proposal. Best practice: always specify encoding in all file I/O operations for portability.", "code_signature": "status:closed", "tags": "stderr, file-io, subprocess, pathlib", "url": "https://github.com/cpython/cpython/issues/2707", "author": "contributor_7877", "date_published": "10-01-2021", "votes_or_stars": 494, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:214f91b6-77a2-4245-a75c-1fa3125f2e44": {"content": "How to use df.groupby in Pandas\n`Pandas.df.groupby` groups DataFrame rows by column values. Example usage:\n```python\ndf.groupby('category')['value'].mean()\n```\nThis is useful when aggregation, transformation, filtering by group. Performance tip: avoid apply() on large DataFrames; prefer agg().\nCode signature: df.groupby", "file_path": "https://docs.pandas.org/en/stable/df/groupby.html", "function_name": "df.groupby", "type": "documentation", "metadata": {"record_id": "214f91b6-77a2-4245-a75c-1fa3125f2e44", "source_type": "documentation", "domain": "NumPy/Pandas", "library": "Pandas", "title": "How to use df.groupby in Pandas", "content": "`Pandas.df.groupby` groups DataFrame rows by column values. Example usage:\n```python\ndf.groupby('category')['value'].mean()\n```\nThis is useful when aggregation, transformation, filtering by group. Performance tip: avoid apply() on large DataFrames; prefer agg().", "code_signature": "df.groupby", "tags": "dataframe, reshape, groupby, dtype, pivot, numpy", "url": "https://docs.pandas.org/en/stable/df/groupby.html", "author": "Official Docs", "date_published": "07-02-2023", "votes_or_stars": 816, "relevance_label": "low", "difficulty_level": "beginner"}}, "kb:98c75e08-c89d-4ee9-ab43-7f65280fe3b5": {"content": "How to use asyncio.create_task in asyncio\n`asyncio.asyncio.create_task` schedules a coroutine as a Task in the event loop. Example usage:\n```python\ntask = asyncio.create_task(send_email(user))\nawait task\n```\nThis is useful when fire-and-forget background tasks, concurrent task management. Performance tip: use TaskGroup (Python 3.11+) for structured concurrency.\nCode signature: asyncio.create_task", "file_path": "https://docs.asyncio.org/en/stable/asyncio/create_task.html", "function_name": "asyncio.create_task", "type": "documentation", "metadata": {"record_id": "98c75e08-c89d-4ee9-ab43-7f65280fe3b5", "source_type": "documentation", "domain": "Asyncio/Threading", "library": "asyncio", "title": "How to use asyncio.create_task in asyncio", "content": "`asyncio.asyncio.create_task` schedules a coroutine as a Task in the event loop. Example usage:\n```python\ntask = asyncio.create_task(send_email(user))\nawait task\n```\nThis is useful when fire-and-forget background tasks, concurrent task management. Performance tip: use TaskGroup (Python 3.11+) for structured concurrency.", "code_signature": "asyncio.create_task", "tags": "executor, deadlock, event-loop, lock", "url": "https://docs.asyncio.org/en/stable/asyncio/create_task.html", "author": "Official Docs", "date_published": "13-01-2024", "votes_or_stars": 4143, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:62a04145-367a-4408-b28e-cafe26d0f43e": {"content": "How do I efficiently fill NaN values in a Pandas DataFrame?\nFor large DataFrames, avoid looping. Use `df.fillna({'col': val})` to fill specific columns. `df.ffill()` propagates last valid observation forward. For time-series data, `df.interpolate()` provides smoother results. Always check `df.isna().sum()` before and after to verify.\nCode signature: Use df.fillna(value) or df.ffill()/df.bfill() for forward/backward fill.", "file_path": "https://stackoverflow.com/questions/6438436", "function_name": "Use df.fillna(value) or df.ffill()/df.bfill() for forward/backward fill.", "type": "stack_overflow", "metadata": {"record_id": "62a04145-367a-4408-b28e-cafe26d0f43e", "source_type": "stack_overflow", "domain": "NumPy/Pandas", "library": "loc", "title": "How do I efficiently fill NaN values in a Pandas DataFrame?", "content": "For large DataFrames, avoid looping. Use `df.fillna({'col': val})` to fill specific columns. `df.ffill()` propagates last valid observation forward. For time-series data, `df.interpolate()` provides smoother results. Always check `df.isna().sum()` before and after to verify.", "code_signature": "Use df.fillna(value) or df.ffill()/df.bfill() for forward/backward fill.", "tags": "broadcasting, array, dtype, merge, pivot", "url": "https://stackoverflow.com/questions/6438436", "author": "user_522340", "date_published": "04-06-2019", "votes_or_stars": 26, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:c4ca29b3-090b-473f-95f0-cf8afbc26ac7": {"content": "PERF: groupby().apply() extremely slow on large DataFrames\nProblem: groupby().apply() with a custom function is 100x slower than expected on 10M row DataFrames.\n\nFix/Discussion: groupby().apply() serializes each group and has high overhead. Fix: vectorize with numpy, use groupby().transform() for same-shape output, or groupby().agg() with named aggregations. For complex logic, consider Polars or Dask. Numba's @jit can accelerate custom group functions.\nCode signature: status:closed", "file_path": "https://github.com/pandas/pandas/issues/8446", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "c4ca29b3-090b-473f-95f0-cf8afbc26ac7", "source_type": "github_issue", "domain": "NumPy/Pandas", "library": "pandas", "title": "PERF: groupby().apply() extremely slow on large DataFrames", "content": "Problem: groupby().apply() with a custom function is 100x slower than expected on 10M row DataFrames.\n\nFix/Discussion: groupby().apply() serializes each group and has high overhead. Fix: vectorize with numpy, use groupby().transform() for same-shape output, or groupby().agg() with named aggregations. For complex logic, consider Polars or Dask. Numba's @jit can accelerate custom group functions.", "code_signature": "status:closed", "tags": "dtype, groupby, broadcasting, vectorization, pivot", "url": "https://github.com/pandas/pandas/issues/8446", "author": "contributor_6648", "date_published": "29-03-2024", "votes_or_stars": 320, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:825cdb0e-9281-4303-b060-4089665950d2": {"content": "SQLAlchemy — ORM Session Query\nThe `session.query` function in SQLAlchemy constructs a SELECT query against mapped entities. It accepts `*entities` as a parameter and returns Query object. Common use cases include retrieving ORM-mapped objects from the database. Note that prefer session.execute(select(...)) in SQLAlchemy 2.0+.\nCode signature: session.query(*entities) -> Query object", "file_path": "https://docs.sqlalchemy.org/en/stable/session/query.html", "function_name": "session.query(*entities) -> Query object", "type": "documentation", "metadata": {"record_id": "825cdb0e-9281-4303-b060-4089665950d2", "source_type": "documentation", "domain": "SQLAlchemy/Databases", "library": "SQLAlchemy", "title": "SQLAlchemy — ORM Session Query", "content": "The `session.query` function in SQLAlchemy constructs a SELECT query against mapped entities. It accepts `*entities` as a parameter and returns Query object. Common use cases include retrieving ORM-mapped objects from the database. Note that prefer session.execute(select(...)) in SQLAlchemy 2.0+.", "code_signature": "session.query(*entities) -> Query object", "tags": "transaction, alembic, relationship", "url": "https://docs.sqlalchemy.org/en/stable/session/query.html", "author": "Official Docs", "date_published": "31-12-2020", "votes_or_stars": 3036, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:6fa7497c-f8df-4e8a-850e-f8588a42fdb6": {"content": "BUG: httpx: SSL certificate error on Windows despite valid cert\nProblem: httpx raises SSLCertVerificationError on Windows when system CA store has the cert.\n\nFix/Discussion: httpx uses its own CA bundle (certifi) by default, not the system store. Fix: `httpx.Client(verify=certifi.where())` — already default. For custom certs: `httpx.Client(verify='/path/to/ca-bundle.pem')`. Install `pip install truststore` and use `ssl.create_default_context(ssl.Purpose.SERVER_AUTH)` to use system store.\nCode signature: status:closed", "file_path": "https://github.com/httpx/httpx/issues/8949", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "6fa7497c-f8df-4e8a-850e-f8588a42fdb6", "source_type": "github_issue", "domain": "Requests/HTTP/APIs", "library": "httpx", "title": "BUG: httpx: SSL certificate error on Windows despite valid cert", "content": "Problem: httpx raises SSLCertVerificationError on Windows when system CA store has the cert.\n\nFix/Discussion: httpx uses its own CA bundle (certifi) by default, not the system store. Fix: `httpx.Client(verify=certifi.where())` — already default. For custom certs: `httpx.Client(verify='/path/to/ca-bundle.pem')`. Install `pip install truststore` and use `ssl.create_default_context(ssl.Purpose.SERVER_AUTH)` to use system store.", "code_signature": "status:closed", "tags": "rest-api, oauth, httpx, timeout, rate-limiting", "url": "https://github.com/httpx/httpx/issues/8949", "author": "contributor_1420", "date_published": "13-03-2022", "votes_or_stars": 467, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:03ee52ef-ac0a-4da3-bbc1-97667ac549af": {"content": "PERF: concurrent.futures.ProcessPoolExecutor slow startup on Windows\nProblem: ProcessPoolExecutor takes 3-5 seconds to start on Windows due to process spawning overhead.\n\nFix/Discussion: Windows uses 'spawn' start method (vs 'fork' on Linux). Mitigations: (1) Reuse executor across calls — don't create/destroy in loops. (2) Use 'forkserver' or 'fork' on Linux/macOS. (3) For short tasks, ThreadPoolExecutor may be faster. (4) Use `initializer` param to pre-load modules.\nCode signature: status:closed", "file_path": "https://github.com/cpython/cpython/issues/6647", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "03ee52ef-ac0a-4da3-bbc1-97667ac549af", "source_type": "github_issue", "domain": "Asyncio/Threading", "library": "cpython", "title": "PERF: concurrent.futures.ProcessPoolExecutor slow startup on Windows", "content": "Problem: ProcessPoolExecutor takes 3-5 seconds to start on Windows due to process spawning overhead.\n\nFix/Discussion: Windows uses 'spawn' start method (vs 'fork' on Linux). Mitigations: (1) Reuse executor across calls — don't create/destroy in loops. (2) Use 'forkserver' or 'fork' on Linux/macOS. (3) For short tasks, ThreadPoolExecutor may be faster. (4) Use `initializer` param to pre-load modules.", "code_signature": "status:closed", "tags": "lock, await, race-condition, asyncio, executor, event-loop", "url": "https://github.com/cpython/cpython/issues/6647", "author": "contributor_4097", "date_published": "28-12-2022", "votes_or_stars": 277, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:fabe47be-aa5d-4e27-bd0e-9aa9e75ee381": {"content": "PERF: requests connection not reused between calls — new TCP connection each time\nProblem: Each requests.get() call opens a new TCP connection, causing high latency.\n\nFix/Discussion: requests.get/post etc. create a new Session per call. Fix: use a persistent Session: `session = requests.Session()` and reuse it. Sessions pool connections via urllib3. For thread-safety: create one Session per thread. For async: use httpx.AsyncClient or aiohttp.ClientSession for async connection pooling.\nCode signature: status:closed", "file_path": "https://github.com/requests/requests/issues/3605", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "fabe47be-aa5d-4e27-bd0e-9aa9e75ee381", "source_type": "github_issue", "domain": "Requests/HTTP/APIs", "library": "requests", "title": "PERF: requests connection not reused between calls — new TCP connection each time", "content": "Problem: Each requests.get() call opens a new TCP connection, causing high latency.\n\nFix/Discussion: requests.get/post etc. create a new Session per call. Fix: use a persistent Session: `session = requests.Session()` and reuse it. Sessions pool connections via urllib3. For thread-safety: create one Session per thread. For async: use httpx.AsyncClient or aiohttp.ClientSession for async connection pooling.", "code_signature": "status:closed", "tags": "aiohttp, json, httpx, rest-api, authentication, requests", "url": "https://github.com/requests/requests/issues/3605", "author": "contributor_1152", "date_published": "17-03-2022", "votes_or_stars": 376, "relevance_label": "low", "difficulty_level": "advanced"}}, "kb:156a4c2f-7191-4546-a8fa-3be0e4a3a545": {"content": "NumPy — Linear Algebra\nThe `np.einsum` function in NumPy evaluates Einstein summation convention on operands. It accepts `subscripts, *operands` as a parameter and returns ndarray. Common use cases include matrix multiplication, dot products, tensor contractions. Note that prefer np.einsum over loops for large arrays.\nCode signature: np.einsum(subscripts, *operands) -> ndarray", "file_path": "https://docs.numpy.org/en/stable/np/einsum.html", "function_name": "np.einsum(subscripts, *operands) -> ndarray", "type": "documentation", "metadata": {"record_id": "156a4c2f-7191-4546-a8fa-3be0e4a3a545", "source_type": "documentation", "domain": "NumPy/Pandas", "library": "NumPy", "title": "NumPy — Linear Algebra", "content": "The `np.einsum` function in NumPy evaluates Einstein summation convention on operands. It accepts `subscripts, *operands` as a parameter and returns ndarray. Common use cases include matrix multiplication, dot products, tensor contractions. Note that prefer np.einsum over loops for large arrays.", "code_signature": "np.einsum(subscripts, *operands) -> ndarray", "tags": "numpy, nan, vectorization", "url": "https://docs.numpy.org/en/stable/np/einsum.html", "author": "Official Docs", "date_published": "16-05-2021", "votes_or_stars": 1878, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:683ea517-a7ab-4115-8d46-cde07a1fe04d": {"content": "BUG: threading.Timer not cancellable after it fires\nProblem: Calling cancel() on a Timer that has already executed does not raise an error but has no effect.\n\nFix/Discussion: threading.Timer.cancel() only works before the timer fires. Store Timer reference and check `timer.finished.is_set()` to know if it's already run. For repeating timers, use a loop with `threading.Event` for clean cancellation: `stop_event = threading.Event(); stop_event.wait(interval)`.\nCode signature: status:closed", "file_path": "https://github.com/cpython/cpython/issues/4111", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "683ea517-a7ab-4115-8d46-cde07a1fe04d", "source_type": "github_issue", "domain": "Asyncio/Threading", "library": "cpython", "title": "BUG: threading.Timer not cancellable after it fires", "content": "Problem: Calling cancel() on a Timer that has already executed does not raise an error but has no effect.\n\nFix/Discussion: threading.Timer.cancel() only works before the timer fires. Store Timer reference and check `timer.finished.is_set()` to know if it's already run. For repeating timers, use a loop with `threading.Event` for clean cancellation: `stop_event = threading.Event(); stop_event.wait(interval)`.", "code_signature": "status:closed", "tags": "threading, race-condition, asyncio, executor, event-loop, lock", "url": "https://github.com/cpython/cpython/issues/4111", "author": "contributor_7884", "date_published": "17-12-2019", "votes_or_stars": 27, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:9d05eea0-9b49-4559-bdfc-dce07a9b6eba": {"content": "How to implement database connection pooling with SQLAlchemy?\n`create_engine(url, pool_size=10, max_overflow=20, pool_timeout=30, pool_recycle=1800)`. Use `pool_pre_ping=True` to detect stale connections. For async: use `AsyncEngine` with `create_async_engine()`. Monitor with `engine.pool.status()`. NullPool disables pooling for scripts/serverless environments.\nCode signature: create_engine() has built-in pooling. Configure pool_size and max_overflow.", "file_path": "https://stackoverflow.com/questions/7358113", "function_name": "create_engine() has built-in pooling. Configure pool_size and max_overflow.", "type": "stack_overflow", "metadata": {"record_id": "9d05eea0-9b49-4559-bdfc-dce07a9b6eba", "source_type": "stack_overflow", "domain": "SQLAlchemy/Databases", "library": "session", "title": "How to implement database connection pooling with SQLAlchemy?", "content": "`create_engine(url, pool_size=10, max_overflow=20, pool_timeout=30, pool_recycle=1800)`. Use `pool_pre_ping=True` to detect stale connections. For async: use `AsyncEngine` with `create_async_engine()`. Monitor with `engine.pool.status()`. NullPool disables pooling for scripts/serverless environments.", "code_signature": "create_engine() has built-in pooling. Configure pool_size and max_overflow.", "tags": "sqlalchemy, orm, connection-pool, alembic, migration, relationship", "url": "https://stackoverflow.com/questions/7358113", "author": "user_12260", "date_published": "07-11-2019", "votes_or_stars": 236, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:cb953775-b89f-4ba4-bc1d-df5b934f14fc": {"content": "How to apply a function to each row in Pandas without using iterrows?\n`iterrows()` is slow because it creates a Series per row. Prefer `df.apply(func, axis=1)` for row-wise operations, or better yet, vectorize using NumPy: `np.where(condition, a, b)`. For complex logic, `df.assign()` with vectorized expressions is cleanest. Benchmark: vectorized > apply > itertuples > iterrows.\nCode signature: Use df.apply(func, axis=1) or vectorized NumPy operations for best performance.", "file_path": "https://stackoverflow.com/questions/5446912", "function_name": "Use df.apply(func, axis=1) or vectorized NumPy operations for best performance.", "type": "stack_overflow", "metadata": {"record_id": "cb953775-b89f-4ba4-bc1d-df5b934f14fc", "source_type": "stack_overflow", "domain": "NumPy/Pandas", "library": "dataframe", "title": "How to apply a function to each row in Pandas without using iterrows?", "content": "`iterrows()` is slow because it creates a Series per row. Prefer `df.apply(func, axis=1)` for row-wise operations, or better yet, vectorize using NumPy: `np.where(condition, a, b)`. For complex logic, `df.assign()` with vectorized expressions is cleanest. Benchmark: vectorized > apply > itertuples > iterrows.", "code_signature": "Use df.apply(func, axis=1) or vectorized NumPy operations for best performance.", "tags": "numpy, iloc, dtype", "url": "https://stackoverflow.com/questions/5446912", "author": "user_563306", "date_published": "03-11-2023", "votes_or_stars": 2283, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:1742731b-3f46-478d-ab4f-ae3c305b9acb": {"content": "How to use create_engine in SQLAlchemy\n`SQLAlchemy.create_engine` creates a database engine representing a connection pool. Example usage:\n```python\nengine = create_engine('postgresql://user:pass@host/db', echo=True)\n```\nThis is useful when establishing the central database connection for an application. Performance tip: set pool_size and max_overflow based on expected concurrency.\nCode signature: create_engine", "file_path": "https://docs.sqlalchemy.org/en/stable/create_engine.html", "function_name": "create_engine", "type": "documentation", "metadata": {"record_id": "1742731b-3f46-478d-ab4f-ae3c305b9acb", "source_type": "documentation", "domain": "SQLAlchemy/Databases", "library": "SQLAlchemy", "title": "How to use create_engine in SQLAlchemy", "content": "`SQLAlchemy.create_engine` creates a database engine representing a connection pool. Example usage:\n```python\nengine = create_engine('postgresql://user:pass@host/db', echo=True)\n```\nThis is useful when establishing the central database connection for an application. Performance tip: set pool_size and max_overflow based on expected concurrency.", "code_signature": "create_engine", "tags": "transaction, postgresql, sqlalchemy, orm, session", "url": "https://docs.sqlalchemy.org/en/stable/create_engine.html", "author": "Official Docs", "date_published": "02-09-2023", "votes_or_stars": 2029, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:2ea78e60-1220-4cd7-b487-afefdf7ca380": {"content": "Pandas Merging DataFrames — official reference\nOfficial documentation for Pandas Merging DataFrames. The method signature is `pd.merge(left, right, on, how='inner')`. merges two DataFrames using SQL-style joins. Raises `MergeError` when merge keys produce duplicate columns. See also: df.join, df.concat.\nCode signature: pd.merge(left, right, on, how='inner')", "file_path": "https://docs.pandas.org/en/stable/pd/merge.html", "function_name": "pd.merge(left, right, on, how='inner')", "type": "documentation", "metadata": {"record_id": "2ea78e60-1220-4cd7-b487-afefdf7ca380", "source_type": "documentation", "domain": "NumPy/Pandas", "library": "Pandas", "title": "Pandas Merging DataFrames — official reference", "content": "Official documentation for Pandas Merging DataFrames. The method signature is `pd.merge(left, right, on, how='inner')`. merges two DataFrames using SQL-style joins. Raises `MergeError` when merge keys produce duplicate columns. See also: df.join, df.concat.", "code_signature": "pd.merge(left, right, on, how='inner')", "tags": "loc, vectorization, nan, pivot, array, iloc", "url": "https://docs.pandas.org/en/stable/pd/merge.html", "author": "Official Docs", "date_published": "10-08-2022", "votes_or_stars": 4507, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:5389eeae-d33c-491a-abf9-fcce4ba63127": {"content": "How to implement JWT authentication in FastAPI?\n```python\nfrom fastapi.security import OAuth2PasswordBearer\nfrom jose import jwt\noauth2_scheme = OAuth2PasswordBearer(tokenUrl='token')\n```\nCreate access tokens with `jwt.encode(payload, SECRET_KEY, algorithm='HS256')`. Verify with `jwt.decode(token, SECRET_KEY, algorithms=['HS256'])`. Store SECRET_KEY in environment variables, never in code.\nCode signature: Use python-jose for JWT and passlib for password hashing with OAuth2PasswordBearer.", "file_path": "https://stackoverflow.com/questions/7754864", "function_name": "Use python-jose for JWT and passlib for password hashing with OAuth2PasswordBearer.", "type": "stack_overflow", "metadata": {"record_id": "5389eeae-d33c-491a-abf9-fcce4ba63127", "source_type": "stack_overflow", "domain": "FastAPI/Flask/Django", "library": "fastapi", "title": "How to implement JWT authentication in FastAPI?", "content": "```python\nfrom fastapi.security import OAuth2PasswordBearer\nfrom jose import jwt\noauth2_scheme = OAuth2PasswordBearer(tokenUrl='token')\n```\nCreate access tokens with `jwt.encode(payload, SECRET_KEY, algorithm='HS256')`. Verify with `jwt.decode(token, SECRET_KEY, algorithms=['HS256'])`. Store SECRET_KEY in environment variables, never in code.", "code_signature": "Use python-jose for JWT and passlib for password hashing with OAuth2PasswordBearer.", "tags": "django, request, template", "url": "https://stackoverflow.com/questions/7754864", "author": "user_773587", "date_published": "15-05-2023", "votes_or_stars": 416, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:94771d31-9e79-4164-8395-8bb680b614f1": {"content": "How to run background tasks in FastAPI?\nFastAPI's `BackgroundTasks`: add `background_tasks: BackgroundTasks` as a parameter, then `background_tasks.add_task(func, *args)`. Runs after response is sent. For heavy workloads (email, ML inference), use Celery with Redis/RabbitMQ broker. For async background work, `asyncio.create_task()` works within async endpoints.\nCode signature: Use BackgroundTasks for lightweight tasks or Celery for heavy/distributed work.", "file_path": "https://stackoverflow.com/questions/8077999", "function_name": "Use BackgroundTasks for lightweight tasks or Celery for heavy/distributed work.", "type": "stack_overflow", "metadata": {"record_id": "94771d31-9e79-4164-8395-8bb680b614f1", "source_type": "stack_overflow", "domain": "FastAPI/Flask/Django", "library": "django", "title": "How to run background tasks in FastAPI?", "content": "FastAPI's `BackgroundTasks`: add `background_tasks: BackgroundTasks` as a parameter, then `background_tasks.add_task(func, *args)`. Runs after response is sent. For heavy workloads (email, ML inference), use Celery with Redis/RabbitMQ broker. For async background work, `asyncio.create_task()` works within async endpoints.", "code_signature": "Use BackgroundTasks for lightweight tasks or Celery for heavy/distributed work.", "tags": "middleware, template, django, flask, rest, routing", "url": "https://stackoverflow.com/questions/8077999", "author": "user_202401", "date_published": "15-06-2021", "votes_or_stars": 1836, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:e95dfdbb-6fbe-4930-a370-903674241474": {"content": "FEAT: pathlib: add support for reading/writing with encoding shorthand\nProblem: pathlib.Path.read_text() does not default to UTF-8, causing issues across platforms.\n\nFix/Discussion: Always pass encoding explicitly: `Path('file').read_text(encoding='utf-8')`. Python 3.10+ added `encoding='locale'` default change proposal. Best practice: always specify encoding in all file I/O operations for portability.\nCode signature: status:closed", "file_path": "https://github.com/cpython/cpython/issues/2707", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "e95dfdbb-6fbe-4930-a370-903674241474", "source_type": "github_issue", "domain": "OS/Subprocess/File I/O", "library": "cpython", "title": "FEAT: pathlib: add support for reading/writing with encoding shorthand", "content": "Problem: pathlib.Path.read_text() does not default to UTF-8, causing issues across platforms.\n\nFix/Discussion: Always pass encoding explicitly: `Path('file').read_text(encoding='utf-8')`. Python 3.10+ added `encoding='locale'` default change proposal. Best practice: always specify encoding in all file I/O operations for portability.", "code_signature": "status:closed", "tags": "env-variable, stdout, stdin", "url": "https://github.com/cpython/cpython/issues/2707", "author": "contributor_7877", "date_published": "11-03-2019", "votes_or_stars": 487, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:374bc038-fef6-44c6-bc45-88eee2a13e45": {"content": "BUG: httpx: SSL certificate error on Windows despite valid cert\nProblem: httpx raises SSLCertVerificationError on Windows when system CA store has the cert.\n\nFix/Discussion: httpx uses its own CA bundle (certifi) by default, not the system store. Fix: `httpx.Client(verify=certifi.where())` — already default. For custom certs: `httpx.Client(verify='/path/to/ca-bundle.pem')`. Install `pip install truststore` and use `ssl.create_default_context(ssl.Purpose.SERVER_AUTH)` to use system store.\nCode signature: status:closed", "file_path": "https://github.com/httpx/httpx/issues/8949", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "374bc038-fef6-44c6-bc45-88eee2a13e45", "source_type": "github_issue", "domain": "Requests/HTTP/APIs", "library": "httpx", "title": "BUG: httpx: SSL certificate error on Windows despite valid cert", "content": "Problem: httpx raises SSLCertVerificationError on Windows when system CA store has the cert.\n\nFix/Discussion: httpx uses its own CA bundle (certifi) by default, not the system store. Fix: `httpx.Client(verify=certifi.where())` — already default. For custom certs: `httpx.Client(verify='/path/to/ca-bundle.pem')`. Install `pip install truststore` and use `ssl.create_default_context(ssl.Purpose.SERVER_AUTH)` to use system store.", "code_signature": "status:closed", "tags": "headers, rate-limiting, timeout, oauth, aiohttp, httpx", "url": "https://github.com/httpx/httpx/issues/8949", "author": "contributor_1420", "date_published": "19-04-2024", "votes_or_stars": 464, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:f374a546-3254-4dc2-823a-a2ec682b66c2": {"content": "How to add retry logic to requests?\n```python\nfrom requests.adapters import HTTPAdapter\nfrom urllib3.util.retry import Retry\nretry = Retry(total=3, backoff_factor=1, status_forcelist=[500,502,503,504])\nadapter = HTTPAdapter(max_retries=retry)\nsession.mount('https://', adapter)\n```\nCode signature: Mount an HTTPAdapter with urllib3 Retry on the Session.", "file_path": "https://stackoverflow.com/questions/1669324", "function_name": "Mount an HTTPAdapter with urllib3 Retry on the Session.", "type": "stack_overflow", "metadata": {"record_id": "f374a546-3254-4dc2-823a-a2ec682b66c2", "source_type": "stack_overflow", "domain": "Requests/HTTP/APIs", "library": "websocket", "title": "How to add retry logic to requests?", "content": "```python\nfrom requests.adapters import HTTPAdapter\nfrom urllib3.util.retry import Retry\nretry = Retry(total=3, backoff_factor=1, status_forcelist=[500,502,503,504])\nadapter = HTTPAdapter(max_retries=retry)\nsession.mount('https://', adapter)\n```", "code_signature": "Mount an HTTPAdapter with urllib3 Retry on the Session.", "tags": "requests, rate-limiting, authentication, http, timeout", "url": "https://stackoverflow.com/questions/1669324", "author": "user_952590", "date_published": "10-02-2019", "votes_or_stars": 1526, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:e9852464-bc6a-4ca0-b858-25060d90f5a1": {"content": "SQLAlchemy — ORM Session Query\nThe `session.query` function in SQLAlchemy constructs a SELECT query against mapped entities. It accepts `*entities` as a parameter and returns Query object. Common use cases include retrieving ORM-mapped objects from the database. Note that prefer session.execute(select(...)) in SQLAlchemy 2.0+.\nCode signature: session.query(*entities) -> Query object", "file_path": "https://docs.sqlalchemy.org/en/stable/session/query.html", "function_name": "session.query(*entities) -> Query object", "type": "documentation", "metadata": {"record_id": "e9852464-bc6a-4ca0-b858-25060d90f5a1", "source_type": "documentation", "domain": "SQLAlchemy/Databases", "library": "SQLAlchemy", "title": "SQLAlchemy — ORM Session Query", "content": "The `session.query` function in SQLAlchemy constructs a SELECT query against mapped entities. It accepts `*entities` as a parameter and returns Query object. Common use cases include retrieving ORM-mapped objects from the database. Note that prefer session.execute(select(...)) in SQLAlchemy 2.0+.", "code_signature": "session.query(*entities) -> Query object", "tags": "transaction, foreign-key, query, migration", "url": "https://docs.sqlalchemy.org/en/stable/session/query.html", "author": "Official Docs", "date_published": "03-03-2021", "votes_or_stars": 3015, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:5fee7364-2e4f-41dc-a57a-24bc67392792": {"content": "How do I handle CORS in FastAPI?\n```python\nfrom fastapi.middleware.cors import CORSMiddleware\napp.add_middleware(CORSMiddleware, allow_origins=['*'], allow_methods=['*'], allow_headers=['*'])\n```\nIn production, replace `'*'` with your actual frontend domain. For credentials (cookies), set `allow_credentials=True` and specify exact origins.\nCode signature: Add CORSMiddleware from starlette to your FastAPI app.", "file_path": "https://stackoverflow.com/questions/1527021", "function_name": "Add CORSMiddleware from starlette to your FastAPI app.", "type": "stack_overflow", "metadata": {"record_id": "5fee7364-2e4f-41dc-a57a-24bc67392792", "source_type": "stack_overflow", "domain": "FastAPI/Flask/Django", "library": "flask", "title": "How do I handle CORS in FastAPI?", "content": "```python\nfrom fastapi.middleware.cors import CORSMiddleware\napp.add_middleware(CORSMiddleware, allow_origins=['*'], allow_methods=['*'], allow_headers=['*'])\n```\nIn production, replace `'*'` with your actual frontend domain. For credentials (cookies), set `allow_credentials=True` and specify exact origins.", "code_signature": "Add CORSMiddleware from starlette to your FastAPI app.", "tags": "middleware, fastapi, orm, rest, blueprint, pydantic", "url": "https://stackoverflow.com/questions/1527021", "author": "user_911393", "date_published": "12-06-2024", "votes_or_stars": 935, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:2ddeb9fb-f5d9-45d5-9107-83e1b0b63183": {"content": "BUG: os.path.join ignores prefix when component starts with /\nProblem: os.path.join('base', '/absolute') returns '/absolute', ignoring 'base'.\n\nFix/Discussion: This is documented behavior: os.path.join discards previous components when it encounters an absolute path. Fix: use `pathlib.Path(base) / component.lstrip('/')`. Validate user-provided paths with `Path(path).resolve().is_relative_to(base_dir)` to prevent path traversal.\nCode signature: status:closed", "file_path": "https://github.com/cpython/cpython/issues/7712", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "2ddeb9fb-f5d9-45d5-9107-83e1b0b63183", "source_type": "github_issue", "domain": "OS/Subprocess/File I/O", "library": "cpython", "title": "BUG: os.path.join ignores prefix when component starts with /", "content": "Problem: os.path.join('base', '/absolute') returns '/absolute', ignoring 'base'.\n\nFix/Discussion: This is documented behavior: os.path.join discards previous components when it encounters an absolute path. Fix: use `pathlib.Path(base) / component.lstrip('/')`. Validate user-provided paths with `Path(path).resolve().is_relative_to(base_dir)` to prevent path traversal.", "code_signature": "status:closed", "tags": "tempfile, shutil, file-io, pipe, stdin", "url": "https://github.com/cpython/cpython/issues/7712", "author": "contributor_8802", "date_published": "05-01-2019", "votes_or_stars": 420, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:ea9dfe1a-1a32-45f7-9819-59737cb95777": {"content": "Django ORM Queries — official reference\nOfficial documentation for Django ORM Queries. The method signature is `QuerySet.filter(**kwargs)`. filters QuerySet rows matching given field lookups. Raises `FieldError` when an unknown field lookup is provided. See also: QuerySet.exclude, QuerySet.annotate, Q objects.\nCode signature: QuerySet.filter(**kwargs)", "file_path": "https://docs.django.org/en/stable/QuerySet/filter.html", "function_name": "QuerySet.filter(**kwargs)", "type": "documentation", "metadata": {"record_id": "ea9dfe1a-1a32-45f7-9819-59737cb95777", "source_type": "documentation", "domain": "FastAPI/Flask/Django", "library": "Django", "title": "Django ORM Queries — official reference", "content": "Official documentation for Django ORM Queries. The method signature is `QuerySet.filter(**kwargs)`. filters QuerySet rows matching given field lookups. Raises `FieldError` when an unknown field lookup is provided. See also: QuerySet.exclude, QuerySet.annotate, Q objects.", "code_signature": "QuerySet.filter(**kwargs)", "tags": "template, routing, rest", "url": "https://docs.django.org/en/stable/QuerySet/filter.html", "author": "Official Docs", "date_published": "04-02-2023", "votes_or_stars": 2446, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:511d02f8-d5fe-48f2-8476-1c7692ff5c04": {"content": "How to stream large HTTP responses with requests?\n```python\nwith requests.get(url, stream=True) as r:\n    r.raise_for_status()\n    with open('file', 'wb') as f:\n        for chunk in r.iter_content(chunk_size=8192):\n            f.write(chunk)\n```\nAlways use as context manager with stream=True to ensure connection is released.\nCode signature: Use stream=True and iterate over response.iter_content() or iter_lines().", "file_path": "https://stackoverflow.com/questions/3592974", "function_name": "Use stream=True and iterate over response.iter_content() or iter_lines().", "type": "stack_overflow", "metadata": {"record_id": "511d02f8-d5fe-48f2-8476-1c7692ff5c04", "source_type": "stack_overflow", "domain": "Requests/HTTP/APIs", "library": "json", "title": "How to stream large HTTP responses with requests?", "content": "```python\nwith requests.get(url, stream=True) as r:\n    r.raise_for_status()\n    with open('file', 'wb') as f:\n        for chunk in r.iter_content(chunk_size=8192):\n            f.write(chunk)\n```\nAlways use as context manager with stream=True to ensure connection is released.", "code_signature": "Use stream=True and iterate over response.iter_content() or iter_lines().", "tags": "rate-limiting, rest-api, retry, aiohttp", "url": "https://stackoverflow.com/questions/3592974", "author": "user_980733", "date_published": "05-04-2024", "votes_or_stars": 1655, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:e28ac6c8-42d3-49a7-93ab-84e0d1cc7542": {"content": "Python threading: how to share data between threads safely?\nFor simple counters: use `threading.Lock()` as context manager. For complex shared state: prefer `queue.Queue` (thread-safe by design). Avoid global variables without locks. `threading.local()` gives each thread its own data copy. For read-heavy workloads, `threading.RLock` allows re-entrant acquisition.\nCode signature: Use threading.Lock for mutual exclusion or queue.Queue for producer-consumer patterns.", "file_path": "https://stackoverflow.com/questions/3195773", "function_name": "Use threading.Lock for mutual exclusion or queue.Queue for producer-consumer patterns.", "type": "stack_overflow", "metadata": {"record_id": "e28ac6c8-42d3-49a7-93ab-84e0d1cc7542", "source_type": "stack_overflow", "domain": "Asyncio/Threading", "library": "lock", "title": "Python threading: how to share data between threads safely?", "content": "For simple counters: use `threading.Lock()` as context manager. For complex shared state: prefer `queue.Queue` (thread-safe by design). Avoid global variables without locks. `threading.local()` gives each thread its own data copy. For read-heavy workloads, `threading.RLock` allows re-entrant acquisition.", "code_signature": "Use threading.Lock for mutual exclusion or queue.Queue for producer-consumer patterns.", "tags": "coroutine, semaphore, executor", "url": "https://stackoverflow.com/questions/3195773", "author": "user_714318", "date_published": "06-06-2019", "votes_or_stars": 1113, "relevance_label": "low", "difficulty_level": "beginner"}}, "kb:705cb82f-fc84-4563-b52c-ffd25a2bf85a": {"content": "How do I efficiently fill NaN values in a Pandas DataFrame?\nFor large DataFrames, avoid looping. Use `df.fillna({'col': val})` to fill specific columns. `df.ffill()` propagates last valid observation forward. For time-series data, `df.interpolate()` provides smoother results. Always check `df.isna().sum()` before and after to verify.\nCode signature: Use df.fillna(value) or df.ffill()/df.bfill() for forward/backward fill.", "file_path": "https://stackoverflow.com/questions/6438436", "function_name": "Use df.fillna(value) or df.ffill()/df.bfill() for forward/backward fill.", "type": "stack_overflow", "metadata": {"record_id": "705cb82f-fc84-4563-b52c-ffd25a2bf85a", "source_type": "stack_overflow", "domain": "NumPy/Pandas", "library": "loc", "title": "How do I efficiently fill NaN values in a Pandas DataFrame?", "content": "For large DataFrames, avoid looping. Use `df.fillna({'col': val})` to fill specific columns. `df.ffill()` propagates last valid observation forward. For time-series data, `df.interpolate()` provides smoother results. Always check `df.isna().sum()` before and after to verify.", "code_signature": "Use df.fillna(value) or df.ffill()/df.bfill() for forward/backward fill.", "tags": "iloc, numpy, nan, pandas", "url": "https://stackoverflow.com/questions/6438436", "author": "user_522340", "date_published": "25-06-2022", "votes_or_stars": 4, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:c9600e52-05d8-420f-8167-bff0e41d21bf": {"content": "How to use httpx.AsyncClient in httpx\n`httpx.httpx.AsyncClient` provides an async HTTP client for non-blocking requests. Example usage:\n```python\nasync with httpx.AsyncClient() as client:\n    r = await client.get(url)\n    data = r.json()\n```\nThis is useful when making concurrent HTTP requests in async applications. Performance tip: set timeout=httpx.Timeout(10.0) to prevent hanging requests.\nCode signature: httpx.AsyncClient", "file_path": "https://docs.httpx.org/en/stable/httpx/AsyncClient.html", "function_name": "httpx.AsyncClient", "type": "documentation", "metadata": {"record_id": "c9600e52-05d8-420f-8167-bff0e41d21bf", "source_type": "documentation", "domain": "Requests/HTTP/APIs", "library": "httpx", "title": "How to use httpx.AsyncClient in httpx", "content": "`httpx.httpx.AsyncClient` provides an async HTTP client for non-blocking requests. Example usage:\n```python\nasync with httpx.AsyncClient() as client:\n    r = await client.get(url)\n    data = r.json()\n```\nThis is useful when making concurrent HTTP requests in async applications. Performance tip: set timeout=httpx.Timeout(10.0) to prevent hanging requests.", "code_signature": "httpx.AsyncClient", "tags": "websocket, aiohttp, timeout, retry", "url": "https://docs.httpx.org/en/stable/httpx/AsyncClient.html", "author": "Official Docs", "date_published": "15-12-2018", "votes_or_stars": 4203, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:9864d55e-8695-4ca2-a268-c4bab601b7e1": {"content": "Python threading: how to share data between threads safely?\nFor simple counters: use `threading.Lock()` as context manager. For complex shared state: prefer `queue.Queue` (thread-safe by design). Avoid global variables without locks. `threading.local()` gives each thread its own data copy. For read-heavy workloads, `threading.RLock` allows re-entrant acquisition.\nCode signature: Use threading.Lock for mutual exclusion or queue.Queue for producer-consumer patterns.", "file_path": "https://stackoverflow.com/questions/3195773", "function_name": "Use threading.Lock for mutual exclusion or queue.Queue for producer-consumer patterns.", "type": "stack_overflow", "metadata": {"record_id": "9864d55e-8695-4ca2-a268-c4bab601b7e1", "source_type": "stack_overflow", "domain": "Asyncio/Threading", "library": "lock", "title": "Python threading: how to share data between threads safely?", "content": "For simple counters: use `threading.Lock()` as context manager. For complex shared state: prefer `queue.Queue` (thread-safe by design). Avoid global variables without locks. `threading.local()` gives each thread its own data copy. For read-heavy workloads, `threading.RLock` allows re-entrant acquisition.", "code_signature": "Use threading.Lock for mutual exclusion or queue.Queue for producer-consumer patterns.", "tags": "lock, gather, await", "url": "https://stackoverflow.com/questions/3195773", "author": "user_714318", "date_published": "28-12-2020", "votes_or_stars": 1118, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:e55dab4c-c680-4d5f-85fa-18de7bf48abe": {"content": "How to limit concurrency with asyncio?\n```python\nsem = asyncio.Semaphore(10)\nasync def limited(url):\n    async with sem:\n        return await fetch(url)\nresults = await asyncio.gather(*[limited(u) for u in urls])\n```\nThis prevents overwhelming APIs or databases with too many simultaneous connections.\nCode signature: Use asyncio.Semaphore to cap the number of concurrent coroutines.", "file_path": "https://stackoverflow.com/questions/2375453", "function_name": "Use asyncio.Semaphore to cap the number of concurrent coroutines.", "type": "stack_overflow", "metadata": {"record_id": "e55dab4c-c680-4d5f-85fa-18de7bf48abe", "source_type": "stack_overflow", "domain": "Asyncio/Threading", "library": "asyncio", "title": "How to limit concurrency with asyncio?", "content": "```python\nsem = asyncio.Semaphore(10)\nasync def limited(url):\n    async with sem:\n        return await fetch(url)\nresults = await asyncio.gather(*[limited(u) for u in urls])\n```\nThis prevents overwhelming APIs or databases with too many simultaneous connections.", "code_signature": "Use asyncio.Semaphore to cap the number of concurrent coroutines.", "tags": "lock, async, semaphore", "url": "https://stackoverflow.com/questions/2375453", "author": "user_449589", "date_published": "04-08-2018", "votes_or_stars": 2426, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:70d80e68-b2c8-4d30-bb2a-d1dcb9a2c0b9": {"content": "BUG: Alembic autogenerate misses index changes\nProblem: Alembic --autogenerate does not detect changes to indexes or constraints on existing tables.\n\nFix/Discussion: Alembic autogenerate compares metadata to database state but has limitations. Workarounds: (1) Manually add `op.create_index()` to migration. (2) Use `include_schemas=True` in env.py. (3) Run `alembic check` to see detected changes. (4) Always review autogenerated migrations before applying.\nCode signature: status:open", "file_path": "https://github.com/alembic/alembic/issues/7344", "function_name": "status:open", "type": "github_issue", "metadata": {"record_id": "70d80e68-b2c8-4d30-bb2a-d1dcb9a2c0b9", "source_type": "github_issue", "domain": "SQLAlchemy/Databases", "library": "alembic", "title": "BUG: Alembic autogenerate misses index changes", "content": "Problem: Alembic --autogenerate does not detect changes to indexes or constraints on existing tables.\n\nFix/Discussion: Alembic autogenerate compares metadata to database state but has limitations. Workarounds: (1) Manually add `op.create_index()` to migration. (2) Use `include_schemas=True` in env.py. (3) Run `alembic check` to see detected changes. (4) Always review autogenerated migrations before applying.", "code_signature": "status:open", "tags": "postgresql, query, transaction, migration, connection-pool", "url": "https://github.com/alembic/alembic/issues/7344", "author": "contributor_3601", "date_published": "25-08-2018", "votes_or_stars": 216, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:411e198a-8dd7-4c68-94b2-bd3225c6cf62": {"content": "How to implement JWT authentication in FastAPI?\n```python\nfrom fastapi.security import OAuth2PasswordBearer\nfrom jose import jwt\noauth2_scheme = OAuth2PasswordBearer(tokenUrl='token')\n```\nCreate access tokens with `jwt.encode(payload, SECRET_KEY, algorithm='HS256')`. Verify with `jwt.decode(token, SECRET_KEY, algorithms=['HS256'])`. Store SECRET_KEY in environment variables, never in code.\nCode signature: Use python-jose for JWT and passlib for password hashing with OAuth2PasswordBearer.", "file_path": "https://stackoverflow.com/questions/7754864", "function_name": "Use python-jose for JWT and passlib for password hashing with OAuth2PasswordBearer.", "type": "stack_overflow", "metadata": {"record_id": "411e198a-8dd7-4c68-94b2-bd3225c6cf62", "source_type": "stack_overflow", "domain": "FastAPI/Flask/Django", "library": "fastapi", "title": "How to implement JWT authentication in FastAPI?", "content": "```python\nfrom fastapi.security import OAuth2PasswordBearer\nfrom jose import jwt\noauth2_scheme = OAuth2PasswordBearer(tokenUrl='token')\n```\nCreate access tokens with `jwt.encode(payload, SECRET_KEY, algorithm='HS256')`. Verify with `jwt.decode(token, SECRET_KEY, algorithms=['HS256'])`. Store SECRET_KEY in environment variables, never in code.", "code_signature": "Use python-jose for JWT and passlib for password hashing with OAuth2PasswordBearer.", "tags": "flask, dependency-injection, orm, rest, pydantic, routing", "url": "https://stackoverflow.com/questions/7754864", "author": "user_773587", "date_published": "27-03-2024", "votes_or_stars": 435, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:9d095a36-20f2-482a-ab01-f694dbfad5a9": {"content": "NumPy — Vectorization\nThe `np.vectorize` function in NumPy generalizes a scalar function to operate on arrays. It accepts `pyfunc` as a parameter and returns vectorized function. Common use cases include applying Python functions element-wise without explicit loops. Note that np.vectorize is a convenience wrapper; it does not improve performance like true ufuncs.\nCode signature: np.vectorize(pyfunc) -> vectorized function", "file_path": "https://docs.numpy.org/en/stable/np/vectorize.html", "function_name": "np.vectorize(pyfunc) -> vectorized function", "type": "documentation", "metadata": {"record_id": "9d095a36-20f2-482a-ab01-f694dbfad5a9", "source_type": "documentation", "domain": "NumPy/Pandas", "library": "NumPy", "title": "NumPy — Vectorization", "content": "The `np.vectorize` function in NumPy generalizes a scalar function to operate on arrays. It accepts `pyfunc` as a parameter and returns vectorized function. Common use cases include applying Python functions element-wise without explicit loops. Note that np.vectorize is a convenience wrapper; it does not improve performance like true ufuncs.", "code_signature": "np.vectorize(pyfunc) -> vectorized function", "tags": "dataframe, array, vectorization, merge, numpy, pandas", "url": "https://docs.numpy.org/en/stable/np/vectorize.html", "author": "Official Docs", "date_published": "27-12-2020", "votes_or_stars": 1342, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:e8f9c554-f2ce-47d6-a024-b3242f05119f": {"content": "How to handle rate limiting with the requests library?\n```python\nfrom tenacity import retry, wait_exponential, stop_after_attempt\n@retry(wait=wait_exponential(min=1, max=60), stop=stop_after_attempt(5))\ndef call_api(url):\n    r = requests.get(url)\n    r.raise_for_status()\n    return r.json()\n```\nCheck `Retry-After` header in 429 responses. For async: use `aiohttp` with tenacity.\nCode signature: Implement exponential backoff with the tenacity or backoff library.", "file_path": "https://stackoverflow.com/questions/1604451", "function_name": "Implement exponential backoff with the tenacity or backoff library.", "type": "stack_overflow", "metadata": {"record_id": "e8f9c554-f2ce-47d6-a024-b3242f05119f", "source_type": "stack_overflow", "domain": "Requests/HTTP/APIs", "library": "retry", "title": "How to handle rate limiting with the requests library?", "content": "```python\nfrom tenacity import retry, wait_exponential, stop_after_attempt\n@retry(wait=wait_exponential(min=1, max=60), stop=stop_after_attempt(5))\ndef call_api(url):\n    r = requests.get(url)\n    r.raise_for_status()\n    return r.json()\n```\nCheck `Retry-After` header in 429 responses. For async: use `aiohttp` with tenacity.", "code_signature": "Implement exponential backoff with the tenacity or backoff library.", "tags": "headers, requests, authentication, oauth, retry, http", "url": "https://stackoverflow.com/questions/1604451", "author": "user_885136", "date_published": "29-04-2019", "votes_or_stars": 637, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:fc4019f2-0b39-4271-b476-34da16810337": {"content": "BUG: asyncio task leaks when exception is not handled\nProblem: Unhandled exceptions in fire-and-forget asyncio tasks are silently ignored, causing task leaks.\n\nFix/Discussion: Always await tasks or add exception handlers. Use `task.add_done_callback()` to log exceptions. In Python 3.11+, use TaskGroup for structured concurrency — exceptions propagate automatically. Set `asyncio.get_event_loop().set_exception_handler()` for global unhandled task exception logging.\nCode signature: status:closed", "file_path": "https://github.com/cpython/cpython/issues/5591", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "fc4019f2-0b39-4271-b476-34da16810337", "source_type": "github_issue", "domain": "Asyncio/Threading", "library": "cpython", "title": "BUG: asyncio task leaks when exception is not handled", "content": "Problem: Unhandled exceptions in fire-and-forget asyncio tasks are silently ignored, causing task leaks.\n\nFix/Discussion: Always await tasks or add exception handlers. Use `task.add_done_callback()` to log exceptions. In Python 3.11+, use TaskGroup for structured concurrency — exceptions propagate automatically. Set `asyncio.get_event_loop().set_exception_handler()` for global unhandled task exception logging.", "code_signature": "status:closed", "tags": "executor, coroutine, await", "url": "https://github.com/cpython/cpython/issues/5591", "author": "contributor_1630", "date_published": "03-01-2024", "votes_or_stars": 332, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:1bac8e53-e9ad-4be4-bafb-b1a9b2056cd0": {"content": "Python threading: how to share data between threads safely?\nFor simple counters: use `threading.Lock()` as context manager. For complex shared state: prefer `queue.Queue` (thread-safe by design). Avoid global variables without locks. `threading.local()` gives each thread its own data copy. For read-heavy workloads, `threading.RLock` allows re-entrant acquisition.\nCode signature: Use threading.Lock for mutual exclusion or queue.Queue for producer-consumer patterns.", "file_path": "https://stackoverflow.com/questions/3195773", "function_name": "Use threading.Lock for mutual exclusion or queue.Queue for producer-consumer patterns.", "type": "stack_overflow", "metadata": {"record_id": "1bac8e53-e9ad-4be4-bafb-b1a9b2056cd0", "source_type": "stack_overflow", "domain": "Asyncio/Threading", "library": "lock", "title": "Python threading: how to share data between threads safely?", "content": "For simple counters: use `threading.Lock()` as context manager. For complex shared state: prefer `queue.Queue` (thread-safe by design). Avoid global variables without locks. `threading.local()` gives each thread its own data copy. For read-heavy workloads, `threading.RLock` allows re-entrant acquisition.", "code_signature": "Use threading.Lock for mutual exclusion or queue.Queue for producer-consumer patterns.", "tags": "threading, race-condition, event-loop, await, lock, semaphore", "url": "https://stackoverflow.com/questions/3195773", "author": "user_714318", "date_published": "26-04-2024", "votes_or_stars": 1067, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:bd59a931-8e1c-421e-a943-67c7f4fd5336": {"content": "How to handle database migrations with Alembic?\nWorkflow: (1) Change SQLAlchemy models. (2) `alembic revision --autogenerate -m 'description'`. (3) Review generated migration script. (4) `alembic upgrade head` to apply. Always review autogenerated migrations — they miss some changes (indexes, constraints). Use `alembic downgrade -1` to rollback. Store migrations in version control.\nCode signature: Use alembic revision --autogenerate to create migrations from model changes.", "file_path": "https://stackoverflow.com/questions/9436429", "function_name": "Use alembic revision --autogenerate to create migrations from model changes.", "type": "stack_overflow", "metadata": {"record_id": "bd59a931-8e1c-421e-a943-67c7f4fd5336", "source_type": "stack_overflow", "domain": "SQLAlchemy/Databases", "library": "connection-pool", "title": "How to handle database migrations with Alembic?", "content": "Workflow: (1) Change SQLAlchemy models. (2) `alembic revision --autogenerate -m 'description'`. (3) Review generated migration script. (4) `alembic upgrade head` to apply. Always review autogenerated migrations — they miss some changes (indexes, constraints). Use `alembic downgrade -1` to rollback. Store migrations in version control.", "code_signature": "Use alembic revision --autogenerate to create migrations from model changes.", "tags": "transaction, query, orm, connection-pool, sqlite", "url": "https://stackoverflow.com/questions/9436429", "author": "user_974047", "date_published": "05-11-2022", "votes_or_stars": 2423, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:22e08b61-6acd-4a77-bcbd-824f992f64ba": {"content": "FastAPI — Path Operations\nThe `@app.get` function in FastAPI registers a GET endpoint at the specified path. It accepts `path, response_model` as a parameter and returns Response. Common use cases include building REST API endpoints that return JSON data. Note that use response_model to enforce output schema validation.\nCode signature: @app.get(path, response_model) -> Response", "file_path": "https://docs.fastapi.org/en/stable/@app/get.html", "function_name": "@app.get(path, response_model) -> Response", "type": "documentation", "metadata": {"record_id": "22e08b61-6acd-4a77-bcbd-824f992f64ba", "source_type": "documentation", "domain": "FastAPI/Flask/Django", "library": "FastAPI", "title": "FastAPI — Path Operations", "content": "The `@app.get` function in FastAPI registers a GET endpoint at the specified path. It accepts `path, response_model` as a parameter and returns Response. Common use cases include building REST API endpoints that return JSON data. Note that use response_model to enforce output schema validation.", "code_signature": "@app.get(path, response_model) -> Response", "tags": "flask, request, pydantic, fastapi", "url": "https://docs.fastapi.org/en/stable/@app/get.html", "author": "Official Docs", "date_published": "20-03-2020", "votes_or_stars": 2894, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:bc5a6dea-cac6-4529-84bc-870f70f224e0": {"content": "How to implement database connection pooling with SQLAlchemy?\n`create_engine(url, pool_size=10, max_overflow=20, pool_timeout=30, pool_recycle=1800)`. Use `pool_pre_ping=True` to detect stale connections. For async: use `AsyncEngine` with `create_async_engine()`. Monitor with `engine.pool.status()`. NullPool disables pooling for scripts/serverless environments.\nCode signature: create_engine() has built-in pooling. Configure pool_size and max_overflow.", "file_path": "https://stackoverflow.com/questions/7358113", "function_name": "create_engine() has built-in pooling. Configure pool_size and max_overflow.", "type": "stack_overflow", "metadata": {"record_id": "bc5a6dea-cac6-4529-84bc-870f70f224e0", "source_type": "stack_overflow", "domain": "SQLAlchemy/Databases", "library": "session", "title": "How to implement database connection pooling with SQLAlchemy?", "content": "`create_engine(url, pool_size=10, max_overflow=20, pool_timeout=30, pool_recycle=1800)`. Use `pool_pre_ping=True` to detect stale connections. For async: use `AsyncEngine` with `create_async_engine()`. Monitor with `engine.pool.status()`. NullPool disables pooling for scripts/serverless environments.", "code_signature": "create_engine() has built-in pooling. Configure pool_size and max_overflow.", "tags": "migration, sqlalchemy, connection-pool, orm", "url": "https://stackoverflow.com/questions/7358113", "author": "user_12260", "date_published": "13-03-2022", "votes_or_stars": 245, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:88c4a0fe-a7a2-4ff3-938a-8d53e7cb4a59": {"content": "BUG: Flask session not persisting between requests in tests\nProblem: Session data set in one request is not available in the next request during testing.\n\nFix/Discussion: Use Flask test client's session_transaction() context manager: `with client.session_transaction() as sess:\n    sess['user_id'] = 1`. Ensure SECRET_KEY is set. For testing: `app.config['TESTING'] = True; app.config['SECRET_KEY'] = 'test'`.\nCode signature: status:closed", "file_path": "https://github.com/flask/flask/issues/2141", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "88c4a0fe-a7a2-4ff3-938a-8d53e7cb4a59", "source_type": "github_issue", "domain": "FastAPI/Flask/Django", "library": "flask", "title": "BUG: Flask session not persisting between requests in tests", "content": "Problem: Session data set in one request is not available in the next request during testing.\n\nFix/Discussion: Use Flask test client's session_transaction() context manager: `with client.session_transaction() as sess:\n    sess['user_id'] = 1`. Ensure SECRET_KEY is set. For testing: `app.config['TESTING'] = True; app.config['SECRET_KEY'] = 'test'`.", "code_signature": "status:closed", "tags": "flask, template, request, orm", "url": "https://github.com/flask/flask/issues/2141", "author": "contributor_5020", "date_published": "08-09-2024", "votes_or_stars": 165, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:4156e63e-ab72-43b4-915d-eb247feb915a": {"content": "BUG: asyncio task leaks when exception is not handled\nProblem: Unhandled exceptions in fire-and-forget asyncio tasks are silently ignored, causing task leaks.\n\nFix/Discussion: Always await tasks or add exception handlers. Use `task.add_done_callback()` to log exceptions. In Python 3.11+, use TaskGroup for structured concurrency — exceptions propagate automatically. Set `asyncio.get_event_loop().set_exception_handler()` for global unhandled task exception logging.\nCode signature: status:closed", "file_path": "https://github.com/cpython/cpython/issues/5591", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "4156e63e-ab72-43b4-915d-eb247feb915a", "source_type": "github_issue", "domain": "Asyncio/Threading", "library": "cpython", "title": "BUG: asyncio task leaks when exception is not handled", "content": "Problem: Unhandled exceptions in fire-and-forget asyncio tasks are silently ignored, causing task leaks.\n\nFix/Discussion: Always await tasks or add exception handlers. Use `task.add_done_callback()` to log exceptions. In Python 3.11+, use TaskGroup for structured concurrency — exceptions propagate automatically. Set `asyncio.get_event_loop().set_exception_handler()` for global unhandled task exception logging.", "code_signature": "status:closed", "tags": "executor, gather, event-loop, lock, deadlock", "url": "https://github.com/cpython/cpython/issues/5591", "author": "contributor_1630", "date_published": "17-08-2022", "votes_or_stars": 331, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:9d7184dc-b8a2-44ef-98ca-8a1f7885a0fe": {"content": "How to implement a thread pool in Python?\n```python\nfrom concurrent.futures import ThreadPoolExecutor\nwith ThreadPoolExecutor(max_workers=8) as ex:\n    futures = [ex.submit(task, arg) for arg in args]\n    results = [f.result() for f in futures]\n```\nFor CPU-bound: use ProcessPoolExecutor instead. For async integration: `await loop.run_in_executor(executor, func, *args)`.\nCode signature: Use concurrent.futures.ThreadPoolExecutor for I/O-bound tasks.", "file_path": "https://stackoverflow.com/questions/3646552", "function_name": "Use concurrent.futures.ThreadPoolExecutor for I/O-bound tasks.", "type": "stack_overflow", "metadata": {"record_id": "9d7184dc-b8a2-44ef-98ca-8a1f7885a0fe", "source_type": "stack_overflow", "domain": "Asyncio/Threading", "library": "coroutine", "title": "How to implement a thread pool in Python?", "content": "```python\nfrom concurrent.futures import ThreadPoolExecutor\nwith ThreadPoolExecutor(max_workers=8) as ex:\n    futures = [ex.submit(task, arg) for arg in args]\n    results = [f.result() for f in futures]\n```\nFor CPU-bound: use ProcessPoolExecutor instead. For async integration: `await loop.run_in_executor(executor, func, *args)`.", "code_signature": "Use concurrent.futures.ThreadPoolExecutor for I/O-bound tasks.", "tags": "race-condition, deadlock, semaphore, threading", "url": "https://stackoverflow.com/questions/3646552", "author": "user_469469", "date_published": "07-05-2021", "votes_or_stars": 1507, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:5f13b38c-0ba7-424f-965b-b2655a06da20": {"content": "SQLAlchemy bulk insert: what's the fastest way?\n`session.bulk_insert_mappings(Model, list_of_dicts)` skips ORM overhead. Even faster: `session.execute(insert(Model), data)`. Avoid `session.add()` in loops. For PostgreSQL, use `insert().on_conflict_do_nothing()` for upserts. Batch in chunks of 1000-5000 rows for optimal performance.\nCode signature: Use session.bulk_insert_mappings() or execute(insert(), list_of_dicts) for maximum speed.", "file_path": "https://stackoverflow.com/questions/5977931", "function_name": "Use session.bulk_insert_mappings() or execute(insert(), list_of_dicts) for maximum speed.", "type": "stack_overflow", "metadata": {"record_id": "5f13b38c-0ba7-424f-965b-b2655a06da20", "source_type": "stack_overflow", "domain": "SQLAlchemy/Databases", "library": "query", "title": "SQLAlchemy bulk insert: what's the fastest way?", "content": "`session.bulk_insert_mappings(Model, list_of_dicts)` skips ORM overhead. Even faster: `session.execute(insert(Model), data)`. Avoid `session.add()` in loops. For PostgreSQL, use `insert().on_conflict_do_nothing()` for upserts. Batch in chunks of 1000-5000 rows for optimal performance.", "code_signature": "Use session.bulk_insert_mappings() or execute(insert(), list_of_dicts) for maximum speed.", "tags": "postgresql, transaction, foreign-key, sqlalchemy, orm", "url": "https://stackoverflow.com/questions/5977931", "author": "user_238275", "date_published": "28-03-2019", "votes_or_stars": 647, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:e79be205-d183-489e-84a7-91d82d603373": {"content": "BUG: httpx: SSL certificate error on Windows despite valid cert\nProblem: httpx raises SSLCertVerificationError on Windows when system CA store has the cert.\n\nFix/Discussion: httpx uses its own CA bundle (certifi) by default, not the system store. Fix: `httpx.Client(verify=certifi.where())` — already default. For custom certs: `httpx.Client(verify='/path/to/ca-bundle.pem')`. Install `pip install truststore` and use `ssl.create_default_context(ssl.Purpose.SERVER_AUTH)` to use system store.\nCode signature: status:closed", "file_path": "https://github.com/httpx/httpx/issues/8949", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "e79be205-d183-489e-84a7-91d82d603373", "source_type": "github_issue", "domain": "Requests/HTTP/APIs", "library": "httpx", "title": "BUG: httpx: SSL certificate error on Windows despite valid cert", "content": "Problem: httpx raises SSLCertVerificationError on Windows when system CA store has the cert.\n\nFix/Discussion: httpx uses its own CA bundle (certifi) by default, not the system store. Fix: `httpx.Client(verify=certifi.where())` — already default. For custom certs: `httpx.Client(verify='/path/to/ca-bundle.pem')`. Install `pip install truststore` and use `ssl.create_default_context(ssl.Purpose.SERVER_AUTH)` to use system store.", "code_signature": "status:closed", "tags": "json, httpx, authentication, rest-api, oauth, aiohttp", "url": "https://github.com/httpx/httpx/issues/8949", "author": "contributor_1420", "date_published": "25-10-2018", "votes_or_stars": 455, "relevance_label": "low", "difficulty_level": "advanced"}}, "kb:c77dc5cb-51a0-43a6-967c-868aa70b1294": {"content": "NumPy — Vectorization\nThe `np.vectorize` function in NumPy generalizes a scalar function to operate on arrays. It accepts `pyfunc` as a parameter and returns vectorized function. Common use cases include applying Python functions element-wise without explicit loops. Note that np.vectorize is a convenience wrapper; it does not improve performance like true ufuncs.\nCode signature: np.vectorize(pyfunc) -> vectorized function", "file_path": "https://docs.numpy.org/en/stable/np/vectorize.html", "function_name": "np.vectorize(pyfunc) -> vectorized function", "type": "documentation", "metadata": {"record_id": "c77dc5cb-51a0-43a6-967c-868aa70b1294", "source_type": "documentation", "domain": "NumPy/Pandas", "library": "NumPy", "title": "NumPy — Vectorization", "content": "The `np.vectorize` function in NumPy generalizes a scalar function to operate on arrays. It accepts `pyfunc` as a parameter and returns vectorized function. Common use cases include applying Python functions element-wise without explicit loops. Note that np.vectorize is a convenience wrapper; it does not improve performance like true ufuncs.", "code_signature": "np.vectorize(pyfunc) -> vectorized function", "tags": "dtype, broadcasting, groupby, pandas, numpy", "url": "https://docs.numpy.org/en/stable/np/vectorize.html", "author": "Official Docs", "date_published": "20-03-2023", "votes_or_stars": 1334, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:7a5ef0f3-c74d-474e-b6cc-112e9df1f65e": {"content": "How to run background tasks in FastAPI?\nFastAPI's `BackgroundTasks`: add `background_tasks: BackgroundTasks` as a parameter, then `background_tasks.add_task(func, *args)`. Runs after response is sent. For heavy workloads (email, ML inference), use Celery with Redis/RabbitMQ broker. For async background work, `asyncio.create_task()` works within async endpoints.\nCode signature: Use BackgroundTasks for lightweight tasks or Celery for heavy/distributed work.", "file_path": "https://stackoverflow.com/questions/8077999", "function_name": "Use BackgroundTasks for lightweight tasks or Celery for heavy/distributed work.", "type": "stack_overflow", "metadata": {"record_id": "7a5ef0f3-c74d-474e-b6cc-112e9df1f65e", "source_type": "stack_overflow", "domain": "FastAPI/Flask/Django", "library": "django", "title": "How to run background tasks in FastAPI?", "content": "FastAPI's `BackgroundTasks`: add `background_tasks: BackgroundTasks` as a parameter, then `background_tasks.add_task(func, *args)`. Runs after response is sent. For heavy workloads (email, ML inference), use Celery with Redis/RabbitMQ broker. For async background work, `asyncio.create_task()` works within async endpoints.", "code_signature": "Use BackgroundTasks for lightweight tasks or Celery for heavy/distributed work.", "tags": "request, rest, pydantic, template", "url": "https://stackoverflow.com/questions/8077999", "author": "user_202401", "date_published": "07-06-2023", "votes_or_stars": 1846, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:06f81827-d5ad-4f29-bd5a-5034e9f6b382": {"content": "How to use create_engine in SQLAlchemy\n`SQLAlchemy.create_engine` creates a database engine representing a connection pool. Example usage:\n```python\nengine = create_engine('postgresql://user:pass@host/db', echo=True)\n```\nThis is useful when establishing the central database connection for an application. Performance tip: set pool_size and max_overflow based on expected concurrency.\nCode signature: create_engine", "file_path": "https://docs.sqlalchemy.org/en/stable/create_engine.html", "function_name": "create_engine", "type": "documentation", "metadata": {"record_id": "06f81827-d5ad-4f29-bd5a-5034e9f6b382", "source_type": "documentation", "domain": "SQLAlchemy/Databases", "library": "SQLAlchemy", "title": "How to use create_engine in SQLAlchemy", "content": "`SQLAlchemy.create_engine` creates a database engine representing a connection pool. Example usage:\n```python\nengine = create_engine('postgresql://user:pass@host/db', echo=True)\n```\nThis is useful when establishing the central database connection for an application. Performance tip: set pool_size and max_overflow based on expected concurrency.", "code_signature": "create_engine", "tags": "postgresql, transaction, connection-pool", "url": "https://docs.sqlalchemy.org/en/stable/create_engine.html", "author": "Official Docs", "date_published": "17-08-2021", "votes_or_stars": 2073, "relevance_label": "low", "difficulty_level": "advanced"}}, "kb:3aacf486-04d0-4c0b-92ca-979e7714ac78": {"content": "Flask: how to return JSON responses properly?\n`return jsonify({'key': 'value'})` sets Content-Type to application/json. Flask 1.0+ automatically converts returned dicts to JSON responses. For custom status codes: `return jsonify(data), 201`. For errors: use `abort(404)` or return `({'error': 'Not found'}, 404)`.\nCode signature: Use jsonify() or return a dict directly (Flask 1.0+).", "file_path": "https://stackoverflow.com/questions/8930103", "function_name": "Use jsonify() or return a dict directly (Flask 1.0+).", "type": "stack_overflow", "metadata": {"record_id": "3aacf486-04d0-4c0b-92ca-979e7714ac78", "source_type": "stack_overflow", "domain": "FastAPI/Flask/Django", "library": "django", "title": "Flask: how to return JSON responses properly?", "content": "`return jsonify({'key': 'value'})` sets Content-Type to application/json. Flask 1.0+ automatically converts returned dicts to JSON responses. For custom status codes: `return jsonify(data), 201`. For errors: use `abort(404)` or return `({'error': 'Not found'}, 404)`.", "code_signature": "Use jsonify() or return a dict directly (Flask 1.0+).", "tags": "response, pydantic, request, flask, routing", "url": "https://stackoverflow.com/questions/8930103", "author": "user_264801", "date_published": "10-03-2023", "votes_or_stars": 2197, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:5e2dc0aa-91ed-4417-8b9d-8137cc628179": {"content": "How to use @app.route in Flask\n`Flask.@app.route` maps a URL rule to a view function. Example usage:\n```python\n@app.route('/users/<int:uid>')\ndef get_user(uid):\n    return jsonify(user)\n```\nThis is useful when defining HTTP endpoints in a Flask application. Performance tip: use Blueprints to organize large applications.\nCode signature: @app.route", "file_path": "https://docs.flask.org/en/stable/@app/route.html", "function_name": "@app.route", "type": "documentation", "metadata": {"record_id": "5e2dc0aa-91ed-4417-8b9d-8137cc628179", "source_type": "documentation", "domain": "FastAPI/Flask/Django", "library": "Flask", "title": "How to use @app.route in Flask", "content": "`Flask.@app.route` maps a URL rule to a view function. Example usage:\n```python\n@app.route('/users/<int:uid>')\ndef get_user(uid):\n    return jsonify(user)\n```\nThis is useful when defining HTTP endpoints in a Flask application. Performance tip: use Blueprints to organize large applications.", "code_signature": "@app.route", "tags": "rest, middleware, orm, response, pydantic", "url": "https://docs.flask.org/en/stable/@app/route.html", "author": "Official Docs", "date_published": "26-10-2019", "votes_or_stars": 3003, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:b6d3296b-1303-4260-876b-674d53853829": {"content": "os Environment Variables — official reference\nOfficial documentation for os Environment Variables. The method signature is `os.environ(key)`. provides a mapping of the current environment variables. Raises `KeyError` when key is accessed with [] notation and is not set. See also: dotenv, os.getenv, os.putenv.\nCode signature: os.environ(key)", "file_path": "https://docs.os.org/en/stable/os/environ.html", "function_name": "os.environ(key)", "type": "documentation", "metadata": {"record_id": "b6d3296b-1303-4260-876b-674d53853829", "source_type": "documentation", "domain": "OS/Subprocess/File I/O", "library": "os", "title": "os Environment Variables — official reference", "content": "Official documentation for os Environment Variables. The method signature is `os.environ(key)`. provides a mapping of the current environment variables. Raises `KeyError` when key is accessed with [] notation and is not set. See also: dotenv, os.getenv, os.putenv.", "code_signature": "os.environ(key)", "tags": "subprocess, stdin, pathlib, os, process, stdout", "url": "https://docs.os.org/en/stable/os/environ.html", "author": "Official Docs", "date_published": "11-08-2020", "votes_or_stars": 2208, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:43ea60f0-f9a0-42cf-b788-23b42815a790": {"content": "How to read a large file line by line in Python without loading it all into memory?\n```python\nwith open('large.txt', 'r') as f:\n    for line in f:\n        process(line.strip())\n```\nThis reads one line at a time. For binary files: use `f.read(chunk_size)` in a loop. For CSV: use `pd.read_csv(file, chunksize=10000)`. Never use `f.readlines()` on large files.\nCode signature: Iterate over the file object directly — it yields lines lazily.", "file_path": "https://stackoverflow.com/questions/2737893", "function_name": "Iterate over the file object directly — it yields lines lazily.", "type": "stack_overflow", "metadata": {"record_id": "43ea60f0-f9a0-42cf-b788-23b42815a790", "source_type": "stack_overflow", "domain": "OS/Subprocess/File I/O", "library": "shutil", "title": "How to read a large file line by line in Python without loading it all into memory?", "content": "```python\nwith open('large.txt', 'r') as f:\n    for line in f:\n        process(line.strip())\n```\nThis reads one line at a time. For binary files: use `f.read(chunk_size)` in a loop. For CSV: use `pd.read_csv(file, chunksize=10000)`. Never use `f.readlines()` on large files.", "code_signature": "Iterate over the file object directly — it yields lines lazily.", "tags": "pipe, pathlib, glob, subprocess", "url": "https://stackoverflow.com/questions/2737893", "author": "user_994539", "date_published": "15-05-2018", "votes_or_stars": 2262, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:6cfbba59-7e33-4e1e-8b77-442824f206bf": {"content": "How to use @app.route in Flask\n`Flask.@app.route` maps a URL rule to a view function. Example usage:\n```python\n@app.route('/users/<int:uid>')\ndef get_user(uid):\n    return jsonify(user)\n```\nThis is useful when defining HTTP endpoints in a Flask application. Performance tip: use Blueprints to organize large applications.\nCode signature: @app.route", "file_path": "https://docs.flask.org/en/stable/@app/route.html", "function_name": "@app.route", "type": "documentation", "metadata": {"record_id": "6cfbba59-7e33-4e1e-8b77-442824f206bf", "source_type": "documentation", "domain": "FastAPI/Flask/Django", "library": "Flask", "title": "How to use @app.route in Flask", "content": "`Flask.@app.route` maps a URL rule to a view function. Example usage:\n```python\n@app.route('/users/<int:uid>')\ndef get_user(uid):\n    return jsonify(user)\n```\nThis is useful when defining HTTP endpoints in a Flask application. Performance tip: use Blueprints to organize large applications.", "code_signature": "@app.route", "tags": "dependency-injection, pydantic, middleware, routing", "url": "https://docs.flask.org/en/stable/@app/route.html", "author": "Official Docs", "date_published": "01-06-2018", "votes_or_stars": 3016, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:acec6be5-0d0a-4d39-8ea5-f279f6b68d44": {"content": "How to stream large HTTP responses with requests?\n```python\nwith requests.get(url, stream=True) as r:\n    r.raise_for_status()\n    with open('file', 'wb') as f:\n        for chunk in r.iter_content(chunk_size=8192):\n            f.write(chunk)\n```\nAlways use as context manager with stream=True to ensure connection is released.\nCode signature: Use stream=True and iterate over response.iter_content() or iter_lines().", "file_path": "https://stackoverflow.com/questions/3592974", "function_name": "Use stream=True and iterate over response.iter_content() or iter_lines().", "type": "stack_overflow", "metadata": {"record_id": "acec6be5-0d0a-4d39-8ea5-f279f6b68d44", "source_type": "stack_overflow", "domain": "Requests/HTTP/APIs", "library": "json", "title": "How to stream large HTTP responses with requests?", "content": "```python\nwith requests.get(url, stream=True) as r:\n    r.raise_for_status()\n    with open('file', 'wb') as f:\n        for chunk in r.iter_content(chunk_size=8192):\n            f.write(chunk)\n```\nAlways use as context manager with stream=True to ensure connection is released.", "code_signature": "Use stream=True and iterate over response.iter_content() or iter_lines().", "tags": "aiohttp, websocket, requests, json, headers", "url": "https://stackoverflow.com/questions/3592974", "author": "user_980733", "date_published": "09-08-2024", "votes_or_stars": 1696, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:7c0891fd-adf6-4983-8c48-55f101c00355": {"content": "How to read a large file line by line in Python without loading it all into memory?\n```python\nwith open('large.txt', 'r') as f:\n    for line in f:\n        process(line.strip())\n```\nThis reads one line at a time. For binary files: use `f.read(chunk_size)` in a loop. For CSV: use `pd.read_csv(file, chunksize=10000)`. Never use `f.readlines()` on large files.\nCode signature: Iterate over the file object directly — it yields lines lazily.", "file_path": "https://stackoverflow.com/questions/2737893", "function_name": "Iterate over the file object directly — it yields lines lazily.", "type": "stack_overflow", "metadata": {"record_id": "7c0891fd-adf6-4983-8c48-55f101c00355", "source_type": "stack_overflow", "domain": "OS/Subprocess/File I/O", "library": "shutil", "title": "How to read a large file line by line in Python without loading it all into memory?", "content": "```python\nwith open('large.txt', 'r') as f:\n    for line in f:\n        process(line.strip())\n```\nThis reads one line at a time. For binary files: use `f.read(chunk_size)` in a loop. For CSV: use `pd.read_csv(file, chunksize=10000)`. Never use `f.readlines()` on large files.", "code_signature": "Iterate over the file object directly — it yields lines lazily.", "tags": "stderr, env-variable, pathlib, stdout, subprocess", "url": "https://stackoverflow.com/questions/2737893", "author": "user_994539", "date_published": "26-06-2022", "votes_or_stars": 2287, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:279a9d23-2f32-4627-ab29-86d473c5e0b0": {"content": "requests — HTTP Session Management\nThe `requests.Session` function in requests creates a persistent HTTP session with connection pooling. It accepts `` as a parameter and returns Session object. Common use cases include making multiple requests to the same host efficiently. Note that always close or use Session as context manager to release connections.\nCode signature: requests.Session() -> Session object", "file_path": "https://docs.requests.org/en/stable/requests/Session.html", "function_name": "requests.Session() -> Session object", "type": "documentation", "metadata": {"record_id": "279a9d23-2f32-4627-ab29-86d473c5e0b0", "source_type": "documentation", "domain": "Requests/HTTP/APIs", "library": "requests", "title": "requests — HTTP Session Management", "content": "The `requests.Session` function in requests creates a persistent HTTP session with connection pooling. It accepts `` as a parameter and returns Session object. Common use cases include making multiple requests to the same host efficiently. Note that always close or use Session as context manager to release connections.", "code_signature": "requests.Session() -> Session object", "tags": "aiohttp, json, retry, http", "url": "https://docs.requests.org/en/stable/requests/Session.html", "author": "Official Docs", "date_published": "31-05-2018", "votes_or_stars": 58, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:cfdb6bb1-0871-4c87-bd99-7ecbd55bab63": {"content": "BUG: Alembic autogenerate misses index changes\nProblem: Alembic --autogenerate does not detect changes to indexes or constraints on existing tables.\n\nFix/Discussion: Alembic autogenerate compares metadata to database state but has limitations. Workarounds: (1) Manually add `op.create_index()` to migration. (2) Use `include_schemas=True` in env.py. (3) Run `alembic check` to see detected changes. (4) Always review autogenerated migrations before applying.\nCode signature: status:open", "file_path": "https://github.com/alembic/alembic/issues/7344", "function_name": "status:open", "type": "github_issue", "metadata": {"record_id": "cfdb6bb1-0871-4c87-bd99-7ecbd55bab63", "source_type": "github_issue", "domain": "SQLAlchemy/Databases", "library": "alembic", "title": "BUG: Alembic autogenerate misses index changes", "content": "Problem: Alembic --autogenerate does not detect changes to indexes or constraints on existing tables.\n\nFix/Discussion: Alembic autogenerate compares metadata to database state but has limitations. Workarounds: (1) Manually add `op.create_index()` to migration. (2) Use `include_schemas=True` in env.py. (3) Run `alembic check` to see detected changes. (4) Always review autogenerated migrations before applying.", "code_signature": "status:open", "tags": "alembic, session, query", "url": "https://github.com/alembic/alembic/issues/7344", "author": "contributor_3601", "date_published": "01-11-2018", "votes_or_stars": 261, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:b073ca24-a0a4-46d8-a7d8-5cf7a7505c2c": {"content": "How to upload a file with requests?\n```python\nwith open('file.pdf', 'rb') as f:\n    r = requests.post(url, files={'file': ('file.pdf', f, 'application/pdf')})\n```\nFor multiple files: pass a list of tuples. For multipart form data with fields: combine `files` and `data` parameters. Don't set Content-Type manually — requests sets it with boundary.\nCode signature: Use the files parameter with a file-like object or tuple.", "file_path": "https://stackoverflow.com/questions/8896868", "function_name": "Use the files parameter with a file-like object or tuple.", "type": "stack_overflow", "metadata": {"record_id": "b073ca24-a0a4-46d8-a7d8-5cf7a7505c2c", "source_type": "stack_overflow", "domain": "Requests/HTTP/APIs", "library": "requests", "title": "How to upload a file with requests?", "content": "```python\nwith open('file.pdf', 'rb') as f:\n    r = requests.post(url, files={'file': ('file.pdf', f, 'application/pdf')})\n```\nFor multiple files: pass a list of tuples. For multipart form data with fields: combine `files` and `data` parameters. Don't set Content-Type manually — requests sets it with boundary.", "code_signature": "Use the files parameter with a file-like object or tuple.", "tags": "websocket, rate-limiting, httpx", "url": "https://stackoverflow.com/questions/8896868", "author": "user_243238", "date_published": "20-02-2022", "votes_or_stars": 1575, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:cbb557a6-70c0-43dc-bef1-3dc71f3dbb1a": {"content": "BUG: FastAPI dependency_overrides not working in pytest\nProblem: Dependency overrides set in conftest are not applied during test execution.\n\nFix/Discussion: Set overrides on the app object before TestClient is created. Use `app.dependency_overrides[dep] = mock_dep` in a pytest fixture with function scope. Clear overrides after test: `app.dependency_overrides = {}`. Ensure TestClient is created after overrides are set.\nCode signature: status:closed", "file_path": "https://github.com/fastapi/fastapi/issues/8479", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "cbb557a6-70c0-43dc-bef1-3dc71f3dbb1a", "source_type": "github_issue", "domain": "FastAPI/Flask/Django", "library": "fastapi", "title": "BUG: FastAPI dependency_overrides not working in pytest", "content": "Problem: Dependency overrides set in conftest are not applied during test execution.\n\nFix/Discussion: Set overrides on the app object before TestClient is created. Use `app.dependency_overrides[dep] = mock_dep` in a pytest fixture with function scope. Clear overrides after test: `app.dependency_overrides = {}`. Ensure TestClient is created after overrides are set.", "code_signature": "status:closed", "tags": "blueprint, routing, dependency-injection, orm", "url": "https://github.com/fastapi/fastapi/issues/8479", "author": "contributor_1994", "date_published": "04-09-2019", "votes_or_stars": 340, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:4aaea669-bfa0-4c75-a209-101d6fa8aee0": {"content": "os Environment Variables — official reference\nOfficial documentation for os Environment Variables. The method signature is `os.environ(key)`. provides a mapping of the current environment variables. Raises `KeyError` when key is accessed with [] notation and is not set. See also: dotenv, os.getenv, os.putenv.\nCode signature: os.environ(key)", "file_path": "https://docs.os.org/en/stable/os/environ.html", "function_name": "os.environ(key)", "type": "documentation", "metadata": {"record_id": "4aaea669-bfa0-4c75-a209-101d6fa8aee0", "source_type": "documentation", "domain": "OS/Subprocess/File I/O", "library": "os", "title": "os Environment Variables — official reference", "content": "Official documentation for os Environment Variables. The method signature is `os.environ(key)`. provides a mapping of the current environment variables. Raises `KeyError` when key is accessed with [] notation and is not set. See also: dotenv, os.getenv, os.putenv.", "code_signature": "os.environ(key)", "tags": "shutil, os, stdout, process", "url": "https://docs.os.org/en/stable/os/environ.html", "author": "Official Docs", "date_published": "18-01-2022", "votes_or_stars": 2216, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:fed15f29-2177-4945-87a9-11291564d8a3": {"content": "SQLAlchemy — ORM Session Query\nThe `session.query` function in SQLAlchemy constructs a SELECT query against mapped entities. It accepts `*entities` as a parameter and returns Query object. Common use cases include retrieving ORM-mapped objects from the database. Note that prefer session.execute(select(...)) in SQLAlchemy 2.0+.\nCode signature: session.query(*entities) -> Query object", "file_path": "https://docs.sqlalchemy.org/en/stable/session/query.html", "function_name": "session.query(*entities) -> Query object", "type": "documentation", "metadata": {"record_id": "fed15f29-2177-4945-87a9-11291564d8a3", "source_type": "documentation", "domain": "SQLAlchemy/Databases", "library": "SQLAlchemy", "title": "SQLAlchemy — ORM Session Query", "content": "The `session.query` function in SQLAlchemy constructs a SELECT query against mapped entities. It accepts `*entities` as a parameter and returns Query object. Common use cases include retrieving ORM-mapped objects from the database. Note that prefer session.execute(select(...)) in SQLAlchemy 2.0+.", "code_signature": "session.query(*entities) -> Query object", "tags": "postgresql, query, migration, relationship", "url": "https://docs.sqlalchemy.org/en/stable/session/query.html", "author": "Official Docs", "date_published": "14-05-2023", "votes_or_stars": 3055, "relevance_label": "low", "difficulty_level": "advanced"}}, "kb:4eb2050e-2c96-4785-9430-134f37f20096": {"content": "BUG: FastAPI dependency_overrides not working in pytest\nProblem: Dependency overrides set in conftest are not applied during test execution.\n\nFix/Discussion: Set overrides on the app object before TestClient is created. Use `app.dependency_overrides[dep] = mock_dep` in a pytest fixture with function scope. Clear overrides after test: `app.dependency_overrides = {}`. Ensure TestClient is created after overrides are set.\nCode signature: status:closed", "file_path": "https://github.com/fastapi/fastapi/issues/8479", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "4eb2050e-2c96-4785-9430-134f37f20096", "source_type": "github_issue", "domain": "FastAPI/Flask/Django", "library": "fastapi", "title": "BUG: FastAPI dependency_overrides not working in pytest", "content": "Problem: Dependency overrides set in conftest are not applied during test execution.\n\nFix/Discussion: Set overrides on the app object before TestClient is created. Use `app.dependency_overrides[dep] = mock_dep` in a pytest fixture with function scope. Clear overrides after test: `app.dependency_overrides = {}`. Ensure TestClient is created after overrides are set.", "code_signature": "status:closed", "tags": "request, pydantic, routing, blueprint", "url": "https://github.com/fastapi/fastapi/issues/8479", "author": "contributor_1994", "date_published": "14-09-2018", "votes_or_stars": 321, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:458cd673-428f-4dc6-b0b8-dd3b39105575": {"content": "How to implement JWT authentication in FastAPI?\n```python\nfrom fastapi.security import OAuth2PasswordBearer\nfrom jose import jwt\noauth2_scheme = OAuth2PasswordBearer(tokenUrl='token')\n```\nCreate access tokens with `jwt.encode(payload, SECRET_KEY, algorithm='HS256')`. Verify with `jwt.decode(token, SECRET_KEY, algorithms=['HS256'])`. Store SECRET_KEY in environment variables, never in code.\nCode signature: Use python-jose for JWT and passlib for password hashing with OAuth2PasswordBearer.", "file_path": "https://stackoverflow.com/questions/7754864", "function_name": "Use python-jose for JWT and passlib for password hashing with OAuth2PasswordBearer.", "type": "stack_overflow", "metadata": {"record_id": "458cd673-428f-4dc6-b0b8-dd3b39105575", "source_type": "stack_overflow", "domain": "FastAPI/Flask/Django", "library": "fastapi", "title": "How to implement JWT authentication in FastAPI?", "content": "```python\nfrom fastapi.security import OAuth2PasswordBearer\nfrom jose import jwt\noauth2_scheme = OAuth2PasswordBearer(tokenUrl='token')\n```\nCreate access tokens with `jwt.encode(payload, SECRET_KEY, algorithm='HS256')`. Verify with `jwt.decode(token, SECRET_KEY, algorithms=['HS256'])`. Store SECRET_KEY in environment variables, never in code.", "code_signature": "Use python-jose for JWT and passlib for password hashing with OAuth2PasswordBearer.", "tags": "pydantic, blueprint, dependency-injection, rest, django", "url": "https://stackoverflow.com/questions/7754864", "author": "user_773587", "date_published": "11-08-2023", "votes_or_stars": 385, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:c8cc9c2e-3628-4ff3-b930-0e47758f4203": {"content": "BUG: httpx: SSL certificate error on Windows despite valid cert\nProblem: httpx raises SSLCertVerificationError on Windows when system CA store has the cert.\n\nFix/Discussion: httpx uses its own CA bundle (certifi) by default, not the system store. Fix: `httpx.Client(verify=certifi.where())` — already default. For custom certs: `httpx.Client(verify='/path/to/ca-bundle.pem')`. Install `pip install truststore` and use `ssl.create_default_context(ssl.Purpose.SERVER_AUTH)` to use system store.\nCode signature: status:closed", "file_path": "https://github.com/httpx/httpx/issues/8949", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "c8cc9c2e-3628-4ff3-b930-0e47758f4203", "source_type": "github_issue", "domain": "Requests/HTTP/APIs", "library": "httpx", "title": "BUG: httpx: SSL certificate error on Windows despite valid cert", "content": "Problem: httpx raises SSLCertVerificationError on Windows when system CA store has the cert.\n\nFix/Discussion: httpx uses its own CA bundle (certifi) by default, not the system store. Fix: `httpx.Client(verify=certifi.where())` — already default. For custom certs: `httpx.Client(verify='/path/to/ca-bundle.pem')`. Install `pip install truststore` and use `ssl.create_default_context(ssl.Purpose.SERVER_AUTH)` to use system store.", "code_signature": "status:closed", "tags": "rate-limiting, websocket, oauth, headers", "url": "https://github.com/httpx/httpx/issues/8949", "author": "contributor_1420", "date_published": "21-10-2022", "votes_or_stars": 442, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:c4651ff8-a452-480f-a04c-6a442bfcbe3b": {"content": "How to add retry logic to requests?\n```python\nfrom requests.adapters import HTTPAdapter\nfrom urllib3.util.retry import Retry\nretry = Retry(total=3, backoff_factor=1, status_forcelist=[500,502,503,504])\nadapter = HTTPAdapter(max_retries=retry)\nsession.mount('https://', adapter)\n```\nCode signature: Mount an HTTPAdapter with urllib3 Retry on the Session.", "file_path": "https://stackoverflow.com/questions/1669324", "function_name": "Mount an HTTPAdapter with urllib3 Retry on the Session.", "type": "stack_overflow", "metadata": {"record_id": "c4651ff8-a452-480f-a04c-6a442bfcbe3b", "source_type": "stack_overflow", "domain": "Requests/HTTP/APIs", "library": "websocket", "title": "How to add retry logic to requests?", "content": "```python\nfrom requests.adapters import HTTPAdapter\nfrom urllib3.util.retry import Retry\nretry = Retry(total=3, backoff_factor=1, status_forcelist=[500,502,503,504])\nadapter = HTTPAdapter(max_retries=retry)\nsession.mount('https://', adapter)\n```", "code_signature": "Mount an HTTPAdapter with urllib3 Retry on the Session.", "tags": "json, timeout, requests, httpx, oauth", "url": "https://stackoverflow.com/questions/1669324", "author": "user_952590", "date_published": "31-01-2022", "votes_or_stars": 1491, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:e0192a36-17e0-434e-baee-2cfd583be8d8": {"content": "How to implement JWT authentication in FastAPI?\n```python\nfrom fastapi.security import OAuth2PasswordBearer\nfrom jose import jwt\noauth2_scheme = OAuth2PasswordBearer(tokenUrl='token')\n```\nCreate access tokens with `jwt.encode(payload, SECRET_KEY, algorithm='HS256')`. Verify with `jwt.decode(token, SECRET_KEY, algorithms=['HS256'])`. Store SECRET_KEY in environment variables, never in code.\nCode signature: Use python-jose for JWT and passlib for password hashing with OAuth2PasswordBearer.", "file_path": "https://stackoverflow.com/questions/7754864", "function_name": "Use python-jose for JWT and passlib for password hashing with OAuth2PasswordBearer.", "type": "stack_overflow", "metadata": {"record_id": "e0192a36-17e0-434e-baee-2cfd583be8d8", "source_type": "stack_overflow", "domain": "FastAPI/Flask/Django", "library": "fastapi", "title": "How to implement JWT authentication in FastAPI?", "content": "```python\nfrom fastapi.security import OAuth2PasswordBearer\nfrom jose import jwt\noauth2_scheme = OAuth2PasswordBearer(tokenUrl='token')\n```\nCreate access tokens with `jwt.encode(payload, SECRET_KEY, algorithm='HS256')`. Verify with `jwt.decode(token, SECRET_KEY, algorithms=['HS256'])`. Store SECRET_KEY in environment variables, never in code.", "code_signature": "Use python-jose for JWT and passlib for password hashing with OAuth2PasswordBearer.", "tags": "django, fastapi, dependency-injection, template, request", "url": "https://stackoverflow.com/questions/7754864", "author": "user_773587", "date_published": "08-08-2023", "votes_or_stars": 432, "relevance_label": "low", "difficulty_level": "beginner"}}, "kb:1f218e1e-8265-45d7-8686-65ef21834920": {"content": "PERF: groupby().apply() extremely slow on large DataFrames\nProblem: groupby().apply() with a custom function is 100x slower than expected on 10M row DataFrames.\n\nFix/Discussion: groupby().apply() serializes each group and has high overhead. Fix: vectorize with numpy, use groupby().transform() for same-shape output, or groupby().agg() with named aggregations. For complex logic, consider Polars or Dask. Numba's @jit can accelerate custom group functions.\nCode signature: status:closed", "file_path": "https://github.com/pandas/pandas/issues/8446", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "1f218e1e-8265-45d7-8686-65ef21834920", "source_type": "github_issue", "domain": "NumPy/Pandas", "library": "pandas", "title": "PERF: groupby().apply() extremely slow on large DataFrames", "content": "Problem: groupby().apply() with a custom function is 100x slower than expected on 10M row DataFrames.\n\nFix/Discussion: groupby().apply() serializes each group and has high overhead. Fix: vectorize with numpy, use groupby().transform() for same-shape output, or groupby().agg() with named aggregations. For complex logic, consider Polars or Dask. Numba's @jit can accelerate custom group functions.", "code_signature": "status:closed", "tags": "nan, reshape, array, groupby, pivot", "url": "https://github.com/pandas/pandas/issues/8446", "author": "contributor_6648", "date_published": "23-07-2019", "votes_or_stars": 304, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:ee539032-6b5f-4bc8-b855-9027d8755f9e": {"content": "NumPy — Linear Algebra\nThe `np.einsum` function in NumPy evaluates Einstein summation convention on operands. It accepts `subscripts, *operands` as a parameter and returns ndarray. Common use cases include matrix multiplication, dot products, tensor contractions. Note that prefer np.einsum over loops for large arrays.\nCode signature: np.einsum(subscripts, *operands) -> ndarray", "file_path": "https://docs.numpy.org/en/stable/np/einsum.html", "function_name": "np.einsum(subscripts, *operands) -> ndarray", "type": "documentation", "metadata": {"record_id": "ee539032-6b5f-4bc8-b855-9027d8755f9e", "source_type": "documentation", "domain": "NumPy/Pandas", "library": "NumPy", "title": "NumPy — Linear Algebra", "content": "The `np.einsum` function in NumPy evaluates Einstein summation convention on operands. It accepts `subscripts, *operands` as a parameter and returns ndarray. Common use cases include matrix multiplication, dot products, tensor contractions. Note that prefer np.einsum over loops for large arrays.", "code_signature": "np.einsum(subscripts, *operands) -> ndarray", "tags": "groupby, iloc, pandas", "url": "https://docs.numpy.org/en/stable/np/einsum.html", "author": "Official Docs", "date_published": "07-12-2024", "votes_or_stars": 1852, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:718073f4-6070-460d-a5f2-e5e3a516c107": {"content": "SQLAlchemy bulk insert: what's the fastest way?\n`session.bulk_insert_mappings(Model, list_of_dicts)` skips ORM overhead. Even faster: `session.execute(insert(Model), data)`. Avoid `session.add()` in loops. For PostgreSQL, use `insert().on_conflict_do_nothing()` for upserts. Batch in chunks of 1000-5000 rows for optimal performance.\nCode signature: Use session.bulk_insert_mappings() or execute(insert(), list_of_dicts) for maximum speed.", "file_path": "https://stackoverflow.com/questions/5977931", "function_name": "Use session.bulk_insert_mappings() or execute(insert(), list_of_dicts) for maximum speed.", "type": "stack_overflow", "metadata": {"record_id": "718073f4-6070-460d-a5f2-e5e3a516c107", "source_type": "stack_overflow", "domain": "SQLAlchemy/Databases", "library": "query", "title": "SQLAlchemy bulk insert: what's the fastest way?", "content": "`session.bulk_insert_mappings(Model, list_of_dicts)` skips ORM overhead. Even faster: `session.execute(insert(Model), data)`. Avoid `session.add()` in loops. For PostgreSQL, use `insert().on_conflict_do_nothing()` for upserts. Batch in chunks of 1000-5000 rows for optimal performance.", "code_signature": "Use session.bulk_insert_mappings() or execute(insert(), list_of_dicts) for maximum speed.", "tags": "query, migration, sqlite, alembic, sqlalchemy", "url": "https://stackoverflow.com/questions/5977931", "author": "user_238275", "date_published": "08-02-2019", "votes_or_stars": 665, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:e2ae6609-491e-4f80-a546-e5fb2ceab2d4": {"content": "How to run asyncio code from synchronous Python?\n`asyncio.run(main())` creates an event loop, runs the coroutine, closes the loop. Never call asyncio.run() from inside a running event loop (use await instead). In Jupyter notebooks (which have a running loop), use `await coroutine()` directly or `nest_asyncio.apply()`. For mixing sync/async, use `loop.run_until_complete()`.\nCode signature: Use asyncio.run() in Python 3.7+ to execute a coroutine from sync code.", "file_path": "https://stackoverflow.com/questions/2140194", "function_name": "Use asyncio.run() in Python 3.7+ to execute a coroutine from sync code.", "type": "stack_overflow", "metadata": {"record_id": "e2ae6609-491e-4f80-a546-e5fb2ceab2d4", "source_type": "stack_overflow", "domain": "Asyncio/Threading", "library": "gather", "title": "How to run asyncio code from synchronous Python?", "content": "`asyncio.run(main())` creates an event loop, runs the coroutine, closes the loop. Never call asyncio.run() from inside a running event loop (use await instead). In Jupyter notebooks (which have a running loop), use `await coroutine()` directly or `nest_asyncio.apply()`. For mixing sync/async, use `loop.run_until_complete()`.", "code_signature": "Use asyncio.run() in Python 3.7+ to execute a coroutine from sync code.", "tags": "task, semaphore, event-loop, executor, threading, asyncio", "url": "https://stackoverflow.com/questions/2140194", "author": "user_718011", "date_published": "17-06-2021", "votes_or_stars": 291, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:eced8047-9c83-4e9a-bfab-9582e999abdb": {"content": "BUG: os.path.join ignores prefix when component starts with /\nProblem: os.path.join('base', '/absolute') returns '/absolute', ignoring 'base'.\n\nFix/Discussion: This is documented behavior: os.path.join discards previous components when it encounters an absolute path. Fix: use `pathlib.Path(base) / component.lstrip('/')`. Validate user-provided paths with `Path(path).resolve().is_relative_to(base_dir)` to prevent path traversal.\nCode signature: status:closed", "file_path": "https://github.com/cpython/cpython/issues/7712", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "eced8047-9c83-4e9a-bfab-9582e999abdb", "source_type": "github_issue", "domain": "OS/Subprocess/File I/O", "library": "cpython", "title": "BUG: os.path.join ignores prefix when component starts with /", "content": "Problem: os.path.join('base', '/absolute') returns '/absolute', ignoring 'base'.\n\nFix/Discussion: This is documented behavior: os.path.join discards previous components when it encounters an absolute path. Fix: use `pathlib.Path(base) / component.lstrip('/')`. Validate user-provided paths with `Path(path).resolve().is_relative_to(base_dir)` to prevent path traversal.", "code_signature": "status:closed", "tags": "pipe, stdin, shutil, tempfile, stdout, process", "url": "https://github.com/cpython/cpython/issues/7712", "author": "contributor_8802", "date_published": "17-12-2023", "votes_or_stars": 425, "relevance_label": "low", "difficulty_level": "intermediate"}}, "kb:e4bc9e33-6ba9-4f1f-a695-a2b9f155756d": {"content": "FEAT: pathlib: add support for reading/writing with encoding shorthand\nProblem: pathlib.Path.read_text() does not default to UTF-8, causing issues across platforms.\n\nFix/Discussion: Always pass encoding explicitly: `Path('file').read_text(encoding='utf-8')`. Python 3.10+ added `encoding='locale'` default change proposal. Best practice: always specify encoding in all file I/O operations for portability.\nCode signature: status:closed", "file_path": "https://github.com/cpython/cpython/issues/2707", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "e4bc9e33-6ba9-4f1f-a695-a2b9f155756d", "source_type": "github_issue", "domain": "OS/Subprocess/File I/O", "library": "cpython", "title": "FEAT: pathlib: add support for reading/writing with encoding shorthand", "content": "Problem: pathlib.Path.read_text() does not default to UTF-8, causing issues across platforms.\n\nFix/Discussion: Always pass encoding explicitly: `Path('file').read_text(encoding='utf-8')`. Python 3.10+ added `encoding='locale'` default change proposal. Best practice: always specify encoding in all file I/O operations for portability.", "code_signature": "status:closed", "tags": "shutil, file-io, subprocess", "url": "https://github.com/cpython/cpython/issues/2707", "author": "contributor_7877", "date_published": "25-06-2019", "votes_or_stars": 464, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:d5f5ddd3-3b15-402a-9770-efb76535ba6d": {"content": "How to add retry logic to requests?\n```python\nfrom requests.adapters import HTTPAdapter\nfrom urllib3.util.retry import Retry\nretry = Retry(total=3, backoff_factor=1, status_forcelist=[500,502,503,504])\nadapter = HTTPAdapter(max_retries=retry)\nsession.mount('https://', adapter)\n```\nCode signature: Mount an HTTPAdapter with urllib3 Retry on the Session.", "file_path": "https://stackoverflow.com/questions/1669324", "function_name": "Mount an HTTPAdapter with urllib3 Retry on the Session.", "type": "stack_overflow", "metadata": {"record_id": "d5f5ddd3-3b15-402a-9770-efb76535ba6d", "source_type": "stack_overflow", "domain": "Requests/HTTP/APIs", "library": "websocket", "title": "How to add retry logic to requests?", "content": "```python\nfrom requests.adapters import HTTPAdapter\nfrom urllib3.util.retry import Retry\nretry = Retry(total=3, backoff_factor=1, status_forcelist=[500,502,503,504])\nadapter = HTTPAdapter(max_retries=retry)\nsession.mount('https://', adapter)\n```", "code_signature": "Mount an HTTPAdapter with urllib3 Retry on the Session.", "tags": "aiohttp, headers, websocket, requests, rest-api", "url": "https://stackoverflow.com/questions/1669324", "author": "user_952590", "date_published": "05-01-2022", "votes_or_stars": 1503, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:42d59678-6ff2-4404-ad30-37f2be598cf8": {"content": "BUG: FastAPI dependency_overrides not working in pytest\nProblem: Dependency overrides set in conftest are not applied during test execution.\n\nFix/Discussion: Set overrides on the app object before TestClient is created. Use `app.dependency_overrides[dep] = mock_dep` in a pytest fixture with function scope. Clear overrides after test: `app.dependency_overrides = {}`. Ensure TestClient is created after overrides are set.\nCode signature: status:closed", "file_path": "https://github.com/fastapi/fastapi/issues/8479", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "42d59678-6ff2-4404-ad30-37f2be598cf8", "source_type": "github_issue", "domain": "FastAPI/Flask/Django", "library": "fastapi", "title": "BUG: FastAPI dependency_overrides not working in pytest", "content": "Problem: Dependency overrides set in conftest are not applied during test execution.\n\nFix/Discussion: Set overrides on the app object before TestClient is created. Use `app.dependency_overrides[dep] = mock_dep` in a pytest fixture with function scope. Clear overrides after test: `app.dependency_overrides = {}`. Ensure TestClient is created after overrides are set.", "code_signature": "status:closed", "tags": "fastapi, pydantic, response", "url": "https://github.com/fastapi/fastapi/issues/8479", "author": "contributor_1994", "date_published": "27-01-2019", "votes_or_stars": 325, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:42424893-77bb-4439-9673-0d67624e098f": {"content": "How to upload a file with requests?\n```python\nwith open('file.pdf', 'rb') as f:\n    r = requests.post(url, files={'file': ('file.pdf', f, 'application/pdf')})\n```\nFor multiple files: pass a list of tuples. For multipart form data with fields: combine `files` and `data` parameters. Don't set Content-Type manually — requests sets it with boundary.\nCode signature: Use the files parameter with a file-like object or tuple.", "file_path": "https://stackoverflow.com/questions/8896868", "function_name": "Use the files parameter with a file-like object or tuple.", "type": "stack_overflow", "metadata": {"record_id": "42424893-77bb-4439-9673-0d67624e098f", "source_type": "stack_overflow", "domain": "Requests/HTTP/APIs", "library": "requests", "title": "How to upload a file with requests?", "content": "```python\nwith open('file.pdf', 'rb') as f:\n    r = requests.post(url, files={'file': ('file.pdf', f, 'application/pdf')})\n```\nFor multiple files: pass a list of tuples. For multipart form data with fields: combine `files` and `data` parameters. Don't set Content-Type manually — requests sets it with boundary.", "code_signature": "Use the files parameter with a file-like object or tuple.", "tags": "oauth, httpx, http, timeout, headers", "url": "https://stackoverflow.com/questions/8896868", "author": "user_243238", "date_published": "28-06-2018", "votes_or_stars": 1561, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:dc23103f-6f43-4bd7-bc5e-ce093950df21": {"content": "How to implement a thread pool in Python?\n```python\nfrom concurrent.futures import ThreadPoolExecutor\nwith ThreadPoolExecutor(max_workers=8) as ex:\n    futures = [ex.submit(task, arg) for arg in args]\n    results = [f.result() for f in futures]\n```\nFor CPU-bound: use ProcessPoolExecutor instead. For async integration: `await loop.run_in_executor(executor, func, *args)`.\nCode signature: Use concurrent.futures.ThreadPoolExecutor for I/O-bound tasks.", "file_path": "https://stackoverflow.com/questions/3646552", "function_name": "Use concurrent.futures.ThreadPoolExecutor for I/O-bound tasks.", "type": "stack_overflow", "metadata": {"record_id": "dc23103f-6f43-4bd7-bc5e-ce093950df21", "source_type": "stack_overflow", "domain": "Asyncio/Threading", "library": "coroutine", "title": "How to implement a thread pool in Python?", "content": "```python\nfrom concurrent.futures import ThreadPoolExecutor\nwith ThreadPoolExecutor(max_workers=8) as ex:\n    futures = [ex.submit(task, arg) for arg in args]\n    results = [f.result() for f in futures]\n```\nFor CPU-bound: use ProcessPoolExecutor instead. For async integration: `await loop.run_in_executor(executor, func, *args)`.", "code_signature": "Use concurrent.futures.ThreadPoolExecutor for I/O-bound tasks.", "tags": "await, threading, async, event-loop", "url": "https://stackoverflow.com/questions/3646552", "author": "user_469469", "date_published": "04-02-2024", "votes_or_stars": 1523, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:cdc6a135-0597-4ada-b086-20f6671f6f33": {"content": "asyncio RuntimeError: This event loop is already running\nThis error is common in Jupyter and FastAPI. Solutions: (1) In async context: just use `await coroutine()`. (2) In Jupyter: `import nest_asyncio; nest_asyncio.apply()`. (3) Run in thread: `asyncio.get_event_loop().run_in_executor(None, sync_func)`. (4) Use `asyncio.ensure_future()` to schedule from within async code.\nCode signature: You're calling asyncio.run() inside an already running event loop. Use await instead.", "file_path": "https://stackoverflow.com/questions/2229110", "function_name": "You're calling asyncio.run() inside an already running event loop. Use await instead.", "type": "stack_overflow", "metadata": {"record_id": "cdc6a135-0597-4ada-b086-20f6671f6f33", "source_type": "stack_overflow", "domain": "Asyncio/Threading", "library": "async", "title": "asyncio RuntimeError: This event loop is already running", "content": "This error is common in Jupyter and FastAPI. Solutions: (1) In async context: just use `await coroutine()`. (2) In Jupyter: `import nest_asyncio; nest_asyncio.apply()`. (3) Run in thread: `asyncio.get_event_loop().run_in_executor(None, sync_func)`. (4) Use `asyncio.ensure_future()` to schedule from within async code.", "code_signature": "You're calling asyncio.run() inside an already running event loop. Use await instead.", "tags": "deadlock, task, event-loop, asyncio, gather", "url": "https://stackoverflow.com/questions/2229110", "author": "user_573750", "date_published": "27-03-2020", "votes_or_stars": 2332, "relevance_label": "low", "difficulty_level": "intermediate"}}, "kb:85e50969-d5b9-476c-9787-0c7f212c07c1": {"content": "How to stream large HTTP responses with requests?\n```python\nwith requests.get(url, stream=True) as r:\n    r.raise_for_status()\n    with open('file', 'wb') as f:\n        for chunk in r.iter_content(chunk_size=8192):\n            f.write(chunk)\n```\nAlways use as context manager with stream=True to ensure connection is released.\nCode signature: Use stream=True and iterate over response.iter_content() or iter_lines().", "file_path": "https://stackoverflow.com/questions/3592974", "function_name": "Use stream=True and iterate over response.iter_content() or iter_lines().", "type": "stack_overflow", "metadata": {"record_id": "85e50969-d5b9-476c-9787-0c7f212c07c1", "source_type": "stack_overflow", "domain": "Requests/HTTP/APIs", "library": "json", "title": "How to stream large HTTP responses with requests?", "content": "```python\nwith requests.get(url, stream=True) as r:\n    r.raise_for_status()\n    with open('file', 'wb') as f:\n        for chunk in r.iter_content(chunk_size=8192):\n            f.write(chunk)\n```\nAlways use as context manager with stream=True to ensure connection is released.", "code_signature": "Use stream=True and iterate over response.iter_content() or iter_lines().", "tags": "aiohttp, retry, json, requests, oauth, websocket", "url": "https://stackoverflow.com/questions/3592974", "author": "user_980733", "date_published": "15-10-2020", "votes_or_stars": 1674, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:7bca433f-f049-4882-ba3a-50824c7186a5": {"content": "BUG: requests hangs indefinitely on slow server responses\nProblem: requests.get() hangs forever when server accepts connection but never sends response.\n\nFix/Discussion: Always set both connect and read timeouts: `requests.get(url, timeout=(3.05, 27))`. Tuple: (connect_timeout, read_timeout). Or single value for both. Mount a Session with default timeouts using a custom HTTPAdapter subclass. Never use `timeout=None` in production code.\nCode signature: status:closed", "file_path": "https://github.com/requests/requests/issues/4019", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "7bca433f-f049-4882-ba3a-50824c7186a5", "source_type": "github_issue", "domain": "Requests/HTTP/APIs", "library": "requests", "title": "BUG: requests hangs indefinitely on slow server responses", "content": "Problem: requests.get() hangs forever when server accepts connection but never sends response.\n\nFix/Discussion: Always set both connect and read timeouts: `requests.get(url, timeout=(3.05, 27))`. Tuple: (connect_timeout, read_timeout). Or single value for both. Mount a Session with default timeouts using a custom HTTPAdapter subclass. Never use `timeout=None` in production code.", "code_signature": "status:closed", "tags": "aiohttp, http, websocket", "url": "https://github.com/requests/requests/issues/4019", "author": "contributor_4599", "date_published": "01-05-2020", "votes_or_stars": 263, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:6179cd88-c9c4-465e-9c78-dfc9022c1733": {"content": "How do I efficiently fill NaN values in a Pandas DataFrame?\nFor large DataFrames, avoid looping. Use `df.fillna({'col': val})` to fill specific columns. `df.ffill()` propagates last valid observation forward. For time-series data, `df.interpolate()` provides smoother results. Always check `df.isna().sum()` before and after to verify.\nCode signature: Use df.fillna(value) or df.ffill()/df.bfill() for forward/backward fill.", "file_path": "https://stackoverflow.com/questions/6438436", "function_name": "Use df.fillna(value) or df.ffill()/df.bfill() for forward/backward fill.", "type": "stack_overflow", "metadata": {"record_id": "6179cd88-c9c4-465e-9c78-dfc9022c1733", "source_type": "stack_overflow", "domain": "NumPy/Pandas", "library": "loc", "title": "How do I efficiently fill NaN values in a Pandas DataFrame?", "content": "For large DataFrames, avoid looping. Use `df.fillna({'col': val})` to fill specific columns. `df.ffill()` propagates last valid observation forward. For time-series data, `df.interpolate()` provides smoother results. Always check `df.isna().sum()` before and after to verify.", "code_signature": "Use df.fillna(value) or df.ffill()/df.bfill() for forward/backward fill.", "tags": "dtype, broadcasting, loc, numpy", "url": "https://stackoverflow.com/questions/6438436", "author": "user_522340", "date_published": "17-02-2020", "votes_or_stars": 1, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:fe4f322d-34b5-4a9f-b5f5-320ff224da5b": {"content": "How to limit concurrency with asyncio?\n```python\nsem = asyncio.Semaphore(10)\nasync def limited(url):\n    async with sem:\n        return await fetch(url)\nresults = await asyncio.gather(*[limited(u) for u in urls])\n```\nThis prevents overwhelming APIs or databases with too many simultaneous connections.\nCode signature: Use asyncio.Semaphore to cap the number of concurrent coroutines.", "file_path": "https://stackoverflow.com/questions/2375453", "function_name": "Use asyncio.Semaphore to cap the number of concurrent coroutines.", "type": "stack_overflow", "metadata": {"record_id": "fe4f322d-34b5-4a9f-b5f5-320ff224da5b", "source_type": "stack_overflow", "domain": "Asyncio/Threading", "library": "asyncio", "title": "How to limit concurrency with asyncio?", "content": "```python\nsem = asyncio.Semaphore(10)\nasync def limited(url):\n    async with sem:\n        return await fetch(url)\nresults = await asyncio.gather(*[limited(u) for u in urls])\n```\nThis prevents overwhelming APIs or databases with too many simultaneous connections.", "code_signature": "Use asyncio.Semaphore to cap the number of concurrent coroutines.", "tags": "executor, deadlock, task", "url": "https://stackoverflow.com/questions/2375453", "author": "user_449589", "date_published": "15-10-2024", "votes_or_stars": 2442, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:d08ad5e7-f37f-4ae4-aeca-a57045f967e9": {"content": "How to use httpx.AsyncClient in httpx\n`httpx.httpx.AsyncClient` provides an async HTTP client for non-blocking requests. Example usage:\n```python\nasync with httpx.AsyncClient() as client:\n    r = await client.get(url)\n    data = r.json()\n```\nThis is useful when making concurrent HTTP requests in async applications. Performance tip: set timeout=httpx.Timeout(10.0) to prevent hanging requests.\nCode signature: httpx.AsyncClient", "file_path": "https://docs.httpx.org/en/stable/httpx/AsyncClient.html", "function_name": "httpx.AsyncClient", "type": "documentation", "metadata": {"record_id": "d08ad5e7-f37f-4ae4-aeca-a57045f967e9", "source_type": "documentation", "domain": "Requests/HTTP/APIs", "library": "httpx", "title": "How to use httpx.AsyncClient in httpx", "content": "`httpx.httpx.AsyncClient` provides an async HTTP client for non-blocking requests. Example usage:\n```python\nasync with httpx.AsyncClient() as client:\n    r = await client.get(url)\n    data = r.json()\n```\nThis is useful when making concurrent HTTP requests in async applications. Performance tip: set timeout=httpx.Timeout(10.0) to prevent hanging requests.", "code_signature": "httpx.AsyncClient", "tags": "oauth, rate-limiting, http, authentication", "url": "https://docs.httpx.org/en/stable/httpx/AsyncClient.html", "author": "Official Docs", "date_published": "04-10-2018", "votes_or_stars": 4202, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:ac86da7d-032c-4659-a993-936393d7a1fb": {"content": "PERF: Django template rendering bottleneck in production\nProblem: Template rendering takes 500ms+ for complex pages with many database queries.\n\nFix/Discussion: Primary cause is N+1 queries in template. Solutions: (1) Use select_related/prefetch_related. (2) Cache rendered templates with `cache` template tag. (3) Use `django-cachalot` for query caching. (4) Move heavy computation to views, pass pre-computed context. (5) Consider server-side caching with Redis.\nCode signature: status:closed", "file_path": "https://github.com/django/django/issues/3243", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "ac86da7d-032c-4659-a993-936393d7a1fb", "source_type": "github_issue", "domain": "FastAPI/Flask/Django", "library": "django", "title": "PERF: Django template rendering bottleneck in production", "content": "Problem: Template rendering takes 500ms+ for complex pages with many database queries.\n\nFix/Discussion: Primary cause is N+1 queries in template. Solutions: (1) Use select_related/prefetch_related. (2) Cache rendered templates with `cache` template tag. (3) Use `django-cachalot` for query caching. (4) Move heavy computation to views, pass pre-computed context. (5) Consider server-side caching with Redis.", "code_signature": "status:closed", "tags": "middleware, template, request, flask, dependency-injection, routing", "url": "https://github.com/django/django/issues/3243", "author": "contributor_6988", "date_published": "12-08-2018", "votes_or_stars": 84, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:00a88d9b-a279-4266-bbc8-2b7fee44c520": {"content": "SQLAlchemy: how to avoid DetachedInstanceError?\nDetachedInstanceError occurs when accessing a relationship after the session is closed. Solutions: (1) Use `joinedload()` or `subqueryload()` to eagerly load relationships. (2) Keep session open while accessing objects. (3) Use `expire_on_commit=False` on sessionmaker. (4) Convert to dict/DTO before closing session.\nCode signature: Access lazy-loaded relationships within the session scope, or use eager loading.", "file_path": "https://stackoverflow.com/questions/1247595", "function_name": "Access lazy-loaded relationships within the session scope, or use eager loading.", "type": "stack_overflow", "metadata": {"record_id": "00a88d9b-a279-4266-bbc8-2b7fee44c520", "source_type": "stack_overflow", "domain": "SQLAlchemy/Databases", "library": "sqlalchemy", "title": "SQLAlchemy: how to avoid DetachedInstanceError?", "content": "DetachedInstanceError occurs when accessing a relationship after the session is closed. Solutions: (1) Use `joinedload()` or `subqueryload()` to eagerly load relationships. (2) Keep session open while accessing objects. (3) Use `expire_on_commit=False` on sessionmaker. (4) Convert to dict/DTO before closing session.", "code_signature": "Access lazy-loaded relationships within the session scope, or use eager loading.", "tags": "sqlite, postgresql, alembic, relationship", "url": "https://stackoverflow.com/questions/1247595", "author": "user_107793", "date_published": "08-03-2023", "votes_or_stars": 395, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:cb195781-c89e-4758-b0d2-c09aa58de832": {"content": "SQLAlchemy: how to avoid DetachedInstanceError?\nDetachedInstanceError occurs when accessing a relationship after the session is closed. Solutions: (1) Use `joinedload()` or `subqueryload()` to eagerly load relationships. (2) Keep session open while accessing objects. (3) Use `expire_on_commit=False` on sessionmaker. (4) Convert to dict/DTO before closing session.\nCode signature: Access lazy-loaded relationships within the session scope, or use eager loading.", "file_path": "https://stackoverflow.com/questions/1247595", "function_name": "Access lazy-loaded relationships within the session scope, or use eager loading.", "type": "stack_overflow", "metadata": {"record_id": "cb195781-c89e-4758-b0d2-c09aa58de832", "source_type": "stack_overflow", "domain": "SQLAlchemy/Databases", "library": "sqlalchemy", "title": "SQLAlchemy: how to avoid DetachedInstanceError?", "content": "DetachedInstanceError occurs when accessing a relationship after the session is closed. Solutions: (1) Use `joinedload()` or `subqueryload()` to eagerly load relationships. (2) Keep session open while accessing objects. (3) Use `expire_on_commit=False` on sessionmaker. (4) Convert to dict/DTO before closing session.", "code_signature": "Access lazy-loaded relationships within the session scope, or use eager loading.", "tags": "orm, connection-pool, foreign-key", "url": "https://stackoverflow.com/questions/1247595", "author": "user_107793", "date_published": "26-08-2020", "votes_or_stars": 411, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:444ce7b8-8744-4618-a7a8-bb8e4364b093": {"content": "How to use df.groupby in Pandas\n`Pandas.df.groupby` groups DataFrame rows by column values. Example usage:\n```python\ndf.groupby('category')['value'].mean()\n```\nThis is useful when aggregation, transformation, filtering by group. Performance tip: avoid apply() on large DataFrames; prefer agg().\nCode signature: df.groupby", "file_path": "https://docs.pandas.org/en/stable/df/groupby.html", "function_name": "df.groupby", "type": "documentation", "metadata": {"record_id": "444ce7b8-8744-4618-a7a8-bb8e4364b093", "source_type": "documentation", "domain": "NumPy/Pandas", "library": "Pandas", "title": "How to use df.groupby in Pandas", "content": "`Pandas.df.groupby` groups DataFrame rows by column values. Example usage:\n```python\ndf.groupby('category')['value'].mean()\n```\nThis is useful when aggregation, transformation, filtering by group. Performance tip: avoid apply() on large DataFrames; prefer agg().", "code_signature": "df.groupby", "tags": "broadcasting, nan, pandas, merge, pivot", "url": "https://docs.pandas.org/en/stable/df/groupby.html", "author": "Official Docs", "date_published": "06-03-2023", "votes_or_stars": 847, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:e7b2ab3c-9465-43bd-92d7-876a7105435e": {"content": "subprocess.run vs Popen: which should I use?\n`subprocess.run()` is a high-level wrapper that waits for completion — use for most cases. `subprocess.Popen()` gives full control: stream stdout/stderr in real-time, write to stdin, run processes concurrently. For capturing output: `subprocess.run(..., capture_output=True, text=True)`. Always use `shell=False` and pass args as a list to prevent shell injection.\nCode signature: Use subprocess.run() for simple cases; Popen for streaming output or interactive processes.", "file_path": "https://stackoverflow.com/questions/5727085", "function_name": "Use subprocess.run() for simple cases; Popen for streaming output or interactive processes.", "type": "stack_overflow", "metadata": {"record_id": "e7b2ab3c-9465-43bd-92d7-876a7105435e", "source_type": "stack_overflow", "domain": "OS/Subprocess/File I/O", "library": "shutil", "title": "subprocess.run vs Popen: which should I use?", "content": "`subprocess.run()` is a high-level wrapper that waits for completion — use for most cases. `subprocess.Popen()` gives full control: stream stdout/stderr in real-time, write to stdin, run processes concurrently. For capturing output: `subprocess.run(..., capture_output=True, text=True)`. Always use `shell=False` and pass args as a list to prevent shell injection.", "code_signature": "Use subprocess.run() for simple cases; Popen for streaming output or interactive processes.", "tags": "subprocess, process, shutil, stdin, glob, pathlib", "url": "https://stackoverflow.com/questions/5727085", "author": "user_644210", "date_published": "10-01-2022", "votes_or_stars": 633, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:fa434176-1d15-427c-8ef3-0123ddb010eb": {"content": "threading Thread Synchronization — official reference\nOfficial documentation for threading Thread Synchronization. The method signature is `threading.Lock()`. creates a mutual-exclusion lock for thread-safe access. Raises `RuntimeError` when lock is released without being acquired. See also: threading.RLock, threading.Semaphore, threading.Event.\nCode signature: threading.Lock()", "file_path": "https://docs.threading.org/en/stable/threading/Lock.html", "function_name": "threading.Lock()", "type": "documentation", "metadata": {"record_id": "fa434176-1d15-427c-8ef3-0123ddb010eb", "source_type": "documentation", "domain": "Asyncio/Threading", "library": "threading", "title": "threading Thread Synchronization — official reference", "content": "Official documentation for threading Thread Synchronization. The method signature is `threading.Lock()`. creates a mutual-exclusion lock for thread-safe access. Raises `RuntimeError` when lock is released without being acquired. See also: threading.RLock, threading.Semaphore, threading.Event.", "code_signature": "threading.Lock()", "tags": "coroutine, race-condition, async, deadlock", "url": "https://docs.threading.org/en/stable/threading/Lock.html", "author": "Official Docs", "date_published": "28-04-2023", "votes_or_stars": 4845, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:c88ebec2-aca1-4f0c-8042-16ab319f5919": {"content": "BUG: SettingWithCopyWarning when modifying sliced DataFrame\nProblem: Modifying a slice of a DataFrame raises SettingWithCopyWarning and may not update original.\n\nFix/Discussion: Always use .loc or .copy() when slicing. Use `df.loc[mask, 'col'] = val` for assignment. Create explicit copies: `subset = df[mask].copy()`. In Pandas 2.0+, Copy-on-Write makes this behavior consistent — enable with `pd.options.mode.copy_on_write = True`.\nCode signature: status:closed", "file_path": "https://github.com/pandas/pandas/issues/3264", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "c88ebec2-aca1-4f0c-8042-16ab319f5919", "source_type": "github_issue", "domain": "NumPy/Pandas", "library": "pandas", "title": "BUG: SettingWithCopyWarning when modifying sliced DataFrame", "content": "Problem: Modifying a slice of a DataFrame raises SettingWithCopyWarning and may not update original.\n\nFix/Discussion: Always use .loc or .copy() when slicing. Use `df.loc[mask, 'col'] = val` for assignment. Create explicit copies: `subset = df[mask].copy()`. In Pandas 2.0+, Copy-on-Write makes this behavior consistent — enable with `pd.options.mode.copy_on_write = True`.", "code_signature": "status:closed", "tags": "broadcasting, nan, dtype", "url": "https://github.com/pandas/pandas/issues/3264", "author": "contributor_6628", "date_published": "15-02-2022", "votes_or_stars": 39, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:9a01aa2a-df53-4c95-8a32-e2ff817c0a7b": {"content": "How to run asyncio code from synchronous Python?\n`asyncio.run(main())` creates an event loop, runs the coroutine, closes the loop. Never call asyncio.run() from inside a running event loop (use await instead). In Jupyter notebooks (which have a running loop), use `await coroutine()` directly or `nest_asyncio.apply()`. For mixing sync/async, use `loop.run_until_complete()`.\nCode signature: Use asyncio.run() in Python 3.7+ to execute a coroutine from sync code.", "file_path": "https://stackoverflow.com/questions/2140194", "function_name": "Use asyncio.run() in Python 3.7+ to execute a coroutine from sync code.", "type": "stack_overflow", "metadata": {"record_id": "9a01aa2a-df53-4c95-8a32-e2ff817c0a7b", "source_type": "stack_overflow", "domain": "Asyncio/Threading", "library": "gather", "title": "How to run asyncio code from synchronous Python?", "content": "`asyncio.run(main())` creates an event loop, runs the coroutine, closes the loop. Never call asyncio.run() from inside a running event loop (use await instead). In Jupyter notebooks (which have a running loop), use `await coroutine()` directly or `nest_asyncio.apply()`. For mixing sync/async, use `loop.run_until_complete()`.", "code_signature": "Use asyncio.run() in Python 3.7+ to execute a coroutine from sync code.", "tags": "task, async, await", "url": "https://stackoverflow.com/questions/2140194", "author": "user_718011", "date_published": "21-08-2020", "votes_or_stars": 290, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:2fe4282b-79f6-4b37-b369-99b5dffdf2b9": {"content": "FastAPI — Path Operations\nThe `@app.get` function in FastAPI registers a GET endpoint at the specified path. It accepts `path, response_model` as a parameter and returns Response. Common use cases include building REST API endpoints that return JSON data. Note that use response_model to enforce output schema validation.\nCode signature: @app.get(path, response_model) -> Response", "file_path": "https://docs.fastapi.org/en/stable/@app/get.html", "function_name": "@app.get(path, response_model) -> Response", "type": "documentation", "metadata": {"record_id": "2fe4282b-79f6-4b37-b369-99b5dffdf2b9", "source_type": "documentation", "domain": "FastAPI/Flask/Django", "library": "FastAPI", "title": "FastAPI — Path Operations", "content": "The `@app.get` function in FastAPI registers a GET endpoint at the specified path. It accepts `path, response_model` as a parameter and returns Response. Common use cases include building REST API endpoints that return JSON data. Note that use response_model to enforce output schema validation.", "code_signature": "@app.get(path, response_model) -> Response", "tags": "middleware, dependency-injection, orm", "url": "https://docs.fastapi.org/en/stable/@app/get.html", "author": "Official Docs", "date_published": "07-06-2023", "votes_or_stars": 2838, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:92b380dd-981d-4ff6-b418-3647c920e09f": {"content": "Flask: how to return JSON responses properly?\n`return jsonify({'key': 'value'})` sets Content-Type to application/json. Flask 1.0+ automatically converts returned dicts to JSON responses. For custom status codes: `return jsonify(data), 201`. For errors: use `abort(404)` or return `({'error': 'Not found'}, 404)`.\nCode signature: Use jsonify() or return a dict directly (Flask 1.0+).", "file_path": "https://stackoverflow.com/questions/8930103", "function_name": "Use jsonify() or return a dict directly (Flask 1.0+).", "type": "stack_overflow", "metadata": {"record_id": "92b380dd-981d-4ff6-b418-3647c920e09f", "source_type": "stack_overflow", "domain": "FastAPI/Flask/Django", "library": "django", "title": "Flask: how to return JSON responses properly?", "content": "`return jsonify({'key': 'value'})` sets Content-Type to application/json. Flask 1.0+ automatically converts returned dicts to JSON responses. For custom status codes: `return jsonify(data), 201`. For errors: use `abort(404)` or return `({'error': 'Not found'}, 404)`.", "code_signature": "Use jsonify() or return a dict directly (Flask 1.0+).", "tags": "fastapi, middleware, response, dependency-injection", "url": "https://stackoverflow.com/questions/8930103", "author": "user_264801", "date_published": "20-10-2023", "votes_or_stars": 2236, "relevance_label": "low", "difficulty_level": "beginner"}}, "kb:82087894-6123-475b-ab74-3dbe23b79e85": {"content": "How to handle rate limiting with the requests library?\n```python\nfrom tenacity import retry, wait_exponential, stop_after_attempt\n@retry(wait=wait_exponential(min=1, max=60), stop=stop_after_attempt(5))\ndef call_api(url):\n    r = requests.get(url)\n    r.raise_for_status()\n    return r.json()\n```\nCheck `Retry-After` header in 429 responses. For async: use `aiohttp` with tenacity.\nCode signature: Implement exponential backoff with the tenacity or backoff library.", "file_path": "https://stackoverflow.com/questions/1604451", "function_name": "Implement exponential backoff with the tenacity or backoff library.", "type": "stack_overflow", "metadata": {"record_id": "82087894-6123-475b-ab74-3dbe23b79e85", "source_type": "stack_overflow", "domain": "Requests/HTTP/APIs", "library": "retry", "title": "How to handle rate limiting with the requests library?", "content": "```python\nfrom tenacity import retry, wait_exponential, stop_after_attempt\n@retry(wait=wait_exponential(min=1, max=60), stop=stop_after_attempt(5))\ndef call_api(url):\n    r = requests.get(url)\n    r.raise_for_status()\n    return r.json()\n```\nCheck `Retry-After` header in 429 responses. For async: use `aiohttp` with tenacity.", "code_signature": "Implement exponential backoff with the tenacity or backoff library.", "tags": "oauth, timeout, http, json, rate-limiting", "url": "https://stackoverflow.com/questions/1604451", "author": "user_885136", "date_published": "15-01-2022", "votes_or_stars": 645, "relevance_label": "low", "difficulty_level": "beginner"}}, "kb:24af0c29-4b87-4e7a-8d94-be34d8b0d0ef": {"content": "How to use httpx.AsyncClient in httpx\n`httpx.httpx.AsyncClient` provides an async HTTP client for non-blocking requests. Example usage:\n```python\nasync with httpx.AsyncClient() as client:\n    r = await client.get(url)\n    data = r.json()\n```\nThis is useful when making concurrent HTTP requests in async applications. Performance tip: set timeout=httpx.Timeout(10.0) to prevent hanging requests.\nCode signature: httpx.AsyncClient", "file_path": "https://docs.httpx.org/en/stable/httpx/AsyncClient.html", "function_name": "httpx.AsyncClient", "type": "documentation", "metadata": {"record_id": "24af0c29-4b87-4e7a-8d94-be34d8b0d0ef", "source_type": "documentation", "domain": "Requests/HTTP/APIs", "library": "httpx", "title": "How to use httpx.AsyncClient in httpx", "content": "`httpx.httpx.AsyncClient` provides an async HTTP client for non-blocking requests. Example usage:\n```python\nasync with httpx.AsyncClient() as client:\n    r = await client.get(url)\n    data = r.json()\n```\nThis is useful when making concurrent HTTP requests in async applications. Performance tip: set timeout=httpx.Timeout(10.0) to prevent hanging requests.", "code_signature": "httpx.AsyncClient", "tags": "rate-limiting, requests, authentication", "url": "https://docs.httpx.org/en/stable/httpx/AsyncClient.html", "author": "Official Docs", "date_published": "15-09-2020", "votes_or_stars": 4231, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:6c306988-1a9a-4676-b268-8318dd16553a": {"content": "BUG: threading.Timer not cancellable after it fires\nProblem: Calling cancel() on a Timer that has already executed does not raise an error but has no effect.\n\nFix/Discussion: threading.Timer.cancel() only works before the timer fires. Store Timer reference and check `timer.finished.is_set()` to know if it's already run. For repeating timers, use a loop with `threading.Event` for clean cancellation: `stop_event = threading.Event(); stop_event.wait(interval)`.\nCode signature: status:closed", "file_path": "https://github.com/cpython/cpython/issues/4111", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "6c306988-1a9a-4676-b268-8318dd16553a", "source_type": "github_issue", "domain": "Asyncio/Threading", "library": "cpython", "title": "BUG: threading.Timer not cancellable after it fires", "content": "Problem: Calling cancel() on a Timer that has already executed does not raise an error but has no effect.\n\nFix/Discussion: threading.Timer.cancel() only works before the timer fires. Store Timer reference and check `timer.finished.is_set()` to know if it's already run. For repeating timers, use a loop with `threading.Event` for clean cancellation: `stop_event = threading.Event(); stop_event.wait(interval)`.", "code_signature": "status:closed", "tags": "threading, gather, lock, async, deadlock", "url": "https://github.com/cpython/cpython/issues/4111", "author": "contributor_7884", "date_published": "25-09-2019", "votes_or_stars": 2, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:54571af3-9310-427d-a215-c27d2099d459": {"content": "How do I handle CORS in FastAPI?\n```python\nfrom fastapi.middleware.cors import CORSMiddleware\napp.add_middleware(CORSMiddleware, allow_origins=['*'], allow_methods=['*'], allow_headers=['*'])\n```\nIn production, replace `'*'` with your actual frontend domain. For credentials (cookies), set `allow_credentials=True` and specify exact origins.\nCode signature: Add CORSMiddleware from starlette to your FastAPI app.", "file_path": "https://stackoverflow.com/questions/1527021", "function_name": "Add CORSMiddleware from starlette to your FastAPI app.", "type": "stack_overflow", "metadata": {"record_id": "54571af3-9310-427d-a215-c27d2099d459", "source_type": "stack_overflow", "domain": "FastAPI/Flask/Django", "library": "flask", "title": "How do I handle CORS in FastAPI?", "content": "```python\nfrom fastapi.middleware.cors import CORSMiddleware\napp.add_middleware(CORSMiddleware, allow_origins=['*'], allow_methods=['*'], allow_headers=['*'])\n```\nIn production, replace `'*'` with your actual frontend domain. For credentials (cookies), set `allow_credentials=True` and specify exact origins.", "code_signature": "Add CORSMiddleware from starlette to your FastAPI app.", "tags": "fastapi, response, blueprint, flask, rest", "url": "https://stackoverflow.com/questions/1527021", "author": "user_911393", "date_published": "23-12-2023", "votes_or_stars": 944, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:53b35be1-0e26-4c89-9dea-6c4d70a6db7f": {"content": "SQLAlchemy — ORM Session Query\nThe `session.query` function in SQLAlchemy constructs a SELECT query against mapped entities. It accepts `*entities` as a parameter and returns Query object. Common use cases include retrieving ORM-mapped objects from the database. Note that prefer session.execute(select(...)) in SQLAlchemy 2.0+.\nCode signature: session.query(*entities) -> Query object", "file_path": "https://docs.sqlalchemy.org/en/stable/session/query.html", "function_name": "session.query(*entities) -> Query object", "type": "documentation", "metadata": {"record_id": "53b35be1-0e26-4c89-9dea-6c4d70a6db7f", "source_type": "documentation", "domain": "SQLAlchemy/Databases", "library": "SQLAlchemy", "title": "SQLAlchemy — ORM Session Query", "content": "The `session.query` function in SQLAlchemy constructs a SELECT query against mapped entities. It accepts `*entities` as a parameter and returns Query object. Common use cases include retrieving ORM-mapped objects from the database. Note that prefer session.execute(select(...)) in SQLAlchemy 2.0+.", "code_signature": "session.query(*entities) -> Query object", "tags": "postgresql, sqlalchemy, transaction, migration", "url": "https://docs.sqlalchemy.org/en/stable/session/query.html", "author": "Official Docs", "date_published": "11-03-2023", "votes_or_stars": 3031, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:f1968080-5de3-46ef-b639-1eea3e1f5452": {"content": "How do I handle CORS in FastAPI?\n```python\nfrom fastapi.middleware.cors import CORSMiddleware\napp.add_middleware(CORSMiddleware, allow_origins=['*'], allow_methods=['*'], allow_headers=['*'])\n```\nIn production, replace `'*'` with your actual frontend domain. For credentials (cookies), set `allow_credentials=True` and specify exact origins.\nCode signature: Add CORSMiddleware from starlette to your FastAPI app.", "file_path": "https://stackoverflow.com/questions/1527021", "function_name": "Add CORSMiddleware from starlette to your FastAPI app.", "type": "stack_overflow", "metadata": {"record_id": "f1968080-5de3-46ef-b639-1eea3e1f5452", "source_type": "stack_overflow", "domain": "FastAPI/Flask/Django", "library": "flask", "title": "How do I handle CORS in FastAPI?", "content": "```python\nfrom fastapi.middleware.cors import CORSMiddleware\napp.add_middleware(CORSMiddleware, allow_origins=['*'], allow_methods=['*'], allow_headers=['*'])\n```\nIn production, replace `'*'` with your actual frontend domain. For credentials (cookies), set `allow_credentials=True` and specify exact origins.", "code_signature": "Add CORSMiddleware from starlette to your FastAPI app.", "tags": "routing, dependency-injection, orm", "url": "https://stackoverflow.com/questions/1527021", "author": "user_911393", "date_published": "25-10-2022", "votes_or_stars": 937, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:f30039ac-3ca2-4839-8205-f7bc8c633283": {"content": "How to use @app.route in Flask\n`Flask.@app.route` maps a URL rule to a view function. Example usage:\n```python\n@app.route('/users/<int:uid>')\ndef get_user(uid):\n    return jsonify(user)\n```\nThis is useful when defining HTTP endpoints in a Flask application. Performance tip: use Blueprints to organize large applications.\nCode signature: @app.route", "file_path": "https://docs.flask.org/en/stable/@app/route.html", "function_name": "@app.route", "type": "documentation", "metadata": {"record_id": "f30039ac-3ca2-4839-8205-f7bc8c633283", "source_type": "documentation", "domain": "FastAPI/Flask/Django", "library": "Flask", "title": "How to use @app.route in Flask", "content": "`Flask.@app.route` maps a URL rule to a view function. Example usage:\n```python\n@app.route('/users/<int:uid>')\ndef get_user(uid):\n    return jsonify(user)\n```\nThis is useful when defining HTTP endpoints in a Flask application. Performance tip: use Blueprints to organize large applications.", "code_signature": "@app.route", "tags": "orm, flask, dependency-injection, fastapi, routing", "url": "https://docs.flask.org/en/stable/@app/route.html", "author": "Official Docs", "date_published": "10-08-2023", "votes_or_stars": 3036, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:7c16f7e6-4ffd-4328-b54c-16d8f97de9c6": {"content": "BUG: asyncio task leaks when exception is not handled\nProblem: Unhandled exceptions in fire-and-forget asyncio tasks are silently ignored, causing task leaks.\n\nFix/Discussion: Always await tasks or add exception handlers. Use `task.add_done_callback()` to log exceptions. In Python 3.11+, use TaskGroup for structured concurrency — exceptions propagate automatically. Set `asyncio.get_event_loop().set_exception_handler()` for global unhandled task exception logging.\nCode signature: status:closed", "file_path": "https://github.com/cpython/cpython/issues/5591", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "7c16f7e6-4ffd-4328-b54c-16d8f97de9c6", "source_type": "github_issue", "domain": "Asyncio/Threading", "library": "cpython", "title": "BUG: asyncio task leaks when exception is not handled", "content": "Problem: Unhandled exceptions in fire-and-forget asyncio tasks are silently ignored, causing task leaks.\n\nFix/Discussion: Always await tasks or add exception handlers. Use `task.add_done_callback()` to log exceptions. In Python 3.11+, use TaskGroup for structured concurrency — exceptions propagate automatically. Set `asyncio.get_event_loop().set_exception_handler()` for global unhandled task exception logging.", "code_signature": "status:closed", "tags": "semaphore, gather, coroutine, async, threading", "url": "https://github.com/cpython/cpython/issues/5591", "author": "contributor_1630", "date_published": "21-07-2019", "votes_or_stars": 356, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:79a6428f-8934-4c5e-b012-323b80e3c5a8": {"content": "How to use Path.glob in pathlib\n`pathlib.Path.glob` yields all paths matching the given glob pattern. Example usage:\n```python\npy_files = list(Path('src').glob('**/*.py'))\n```\nThis is useful when finding files recursively, batch file processing. Performance tip: iterate the generator lazily rather than converting to list for large directories.\nCode signature: Path.glob", "file_path": "https://docs.pathlib.org/en/stable/Path/glob.html", "function_name": "Path.glob", "type": "documentation", "metadata": {"record_id": "79a6428f-8934-4c5e-b012-323b80e3c5a8", "source_type": "documentation", "domain": "OS/Subprocess/File I/O", "library": "pathlib", "title": "How to use Path.glob in pathlib", "content": "`pathlib.Path.glob` yields all paths matching the given glob pattern. Example usage:\n```python\npy_files = list(Path('src').glob('**/*.py'))\n```\nThis is useful when finding files recursively, batch file processing. Performance tip: iterate the generator lazily rather than converting to list for large directories.", "code_signature": "Path.glob", "tags": "os, subprocess, pathlib, shutil, tempfile", "url": "https://docs.pathlib.org/en/stable/Path/glob.html", "author": "Official Docs", "date_published": "03-08-2024", "votes_or_stars": 3229, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:568d4954-8ec7-4bd3-b016-6abbac622124": {"content": "BUG: SQLAlchemy connection pool exhaustion under load\nProblem: Application hangs under concurrent load — all connections are checked out and pool is exhausted.\n\nFix/Discussion: Increase `pool_size` and `max_overflow`. Ensure sessions are always closed: use `contextmanager` or `with Session() as session:`. Set `pool_timeout` to fail fast. Enable `pool_pre_ping=True`. Monitor with `engine.pool.checkedout()`. In async: use `AsyncSession` with `async_scoped_session`.\nCode signature: status:closed", "file_path": "https://github.com/sqlalchemy/sqlalchemy/issues/106", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "568d4954-8ec7-4bd3-b016-6abbac622124", "source_type": "github_issue", "domain": "SQLAlchemy/Databases", "library": "sqlalchemy", "title": "BUG: SQLAlchemy connection pool exhaustion under load", "content": "Problem: Application hangs under concurrent load — all connections are checked out and pool is exhausted.\n\nFix/Discussion: Increase `pool_size` and `max_overflow`. Ensure sessions are always closed: use `contextmanager` or `with Session() as session:`. Set `pool_timeout` to fail fast. Enable `pool_pre_ping=True`. Monitor with `engine.pool.checkedout()`. In async: use `AsyncSession` with `async_scoped_session`.", "code_signature": "status:closed", "tags": "relationship, query, foreign-key, connection-pool, transaction", "url": "https://github.com/sqlalchemy/sqlalchemy/issues/106", "author": "contributor_5078", "date_published": "29-04-2023", "votes_or_stars": 265, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:0ad78bc6-5f2e-4bf8-a486-1e85d3d79333": {"content": "SQLAlchemy: how to avoid DetachedInstanceError?\nDetachedInstanceError occurs when accessing a relationship after the session is closed. Solutions: (1) Use `joinedload()` or `subqueryload()` to eagerly load relationships. (2) Keep session open while accessing objects. (3) Use `expire_on_commit=False` on sessionmaker. (4) Convert to dict/DTO before closing session.\nCode signature: Access lazy-loaded relationships within the session scope, or use eager loading.", "file_path": "https://stackoverflow.com/questions/1247595", "function_name": "Access lazy-loaded relationships within the session scope, or use eager loading.", "type": "stack_overflow", "metadata": {"record_id": "0ad78bc6-5f2e-4bf8-a486-1e85d3d79333", "source_type": "stack_overflow", "domain": "SQLAlchemy/Databases", "library": "sqlalchemy", "title": "SQLAlchemy: how to avoid DetachedInstanceError?", "content": "DetachedInstanceError occurs when accessing a relationship after the session is closed. Solutions: (1) Use `joinedload()` or `subqueryload()` to eagerly load relationships. (2) Keep session open while accessing objects. (3) Use `expire_on_commit=False` on sessionmaker. (4) Convert to dict/DTO before closing session.", "code_signature": "Access lazy-loaded relationships within the session scope, or use eager loading.", "tags": "postgresql, query, sqlite, transaction", "url": "https://stackoverflow.com/questions/1247595", "author": "user_107793", "date_published": "02-02-2023", "votes_or_stars": 433, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:2f8414d9-fd05-4fb6-9be3-092f5ce8471c": {"content": "How to add retry logic to requests?\n```python\nfrom requests.adapters import HTTPAdapter\nfrom urllib3.util.retry import Retry\nretry = Retry(total=3, backoff_factor=1, status_forcelist=[500,502,503,504])\nadapter = HTTPAdapter(max_retries=retry)\nsession.mount('https://', adapter)\n```\nCode signature: Mount an HTTPAdapter with urllib3 Retry on the Session.", "file_path": "https://stackoverflow.com/questions/1669324", "function_name": "Mount an HTTPAdapter with urllib3 Retry on the Session.", "type": "stack_overflow", "metadata": {"record_id": "2f8414d9-fd05-4fb6-9be3-092f5ce8471c", "source_type": "stack_overflow", "domain": "Requests/HTTP/APIs", "library": "websocket", "title": "How to add retry logic to requests?", "content": "```python\nfrom requests.adapters import HTTPAdapter\nfrom urllib3.util.retry import Retry\nretry = Retry(total=3, backoff_factor=1, status_forcelist=[500,502,503,504])\nadapter = HTTPAdapter(max_retries=retry)\nsession.mount('https://', adapter)\n```", "code_signature": "Mount an HTTPAdapter with urllib3 Retry on the Session.", "tags": "httpx, rest-api, timeout, aiohttp", "url": "https://stackoverflow.com/questions/1669324", "author": "user_952590", "date_published": "17-09-2020", "votes_or_stars": 1500, "relevance_label": "low", "difficulty_level": "advanced"}}, "kb:5fb585c8-8767-4435-883b-109d86c1de36": {"content": "How to implement JWT authentication in FastAPI?\n```python\nfrom fastapi.security import OAuth2PasswordBearer\nfrom jose import jwt\noauth2_scheme = OAuth2PasswordBearer(tokenUrl='token')\n```\nCreate access tokens with `jwt.encode(payload, SECRET_KEY, algorithm='HS256')`. Verify with `jwt.decode(token, SECRET_KEY, algorithms=['HS256'])`. Store SECRET_KEY in environment variables, never in code.\nCode signature: Use python-jose for JWT and passlib for password hashing with OAuth2PasswordBearer.", "file_path": "https://stackoverflow.com/questions/7754864", "function_name": "Use python-jose for JWT and passlib for password hashing with OAuth2PasswordBearer.", "type": "stack_overflow", "metadata": {"record_id": "5fb585c8-8767-4435-883b-109d86c1de36", "source_type": "stack_overflow", "domain": "FastAPI/Flask/Django", "library": "fastapi", "title": "How to implement JWT authentication in FastAPI?", "content": "```python\nfrom fastapi.security import OAuth2PasswordBearer\nfrom jose import jwt\noauth2_scheme = OAuth2PasswordBearer(tokenUrl='token')\n```\nCreate access tokens with `jwt.encode(payload, SECRET_KEY, algorithm='HS256')`. Verify with `jwt.decode(token, SECRET_KEY, algorithms=['HS256'])`. Store SECRET_KEY in environment variables, never in code.", "code_signature": "Use python-jose for JWT and passlib for password hashing with OAuth2PasswordBearer.", "tags": "template, rest, flask, response, middleware, routing", "url": "https://stackoverflow.com/questions/7754864", "author": "user_773587", "date_published": "14-11-2024", "votes_or_stars": 433, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:b1fd4cd3-2fe1-4bd6-aba9-9f75660e56df": {"content": "NumPy — Linear Algebra\nThe `np.einsum` function in NumPy evaluates Einstein summation convention on operands. It accepts `subscripts, *operands` as a parameter and returns ndarray. Common use cases include matrix multiplication, dot products, tensor contractions. Note that prefer np.einsum over loops for large arrays.\nCode signature: np.einsum(subscripts, *operands) -> ndarray", "file_path": "https://docs.numpy.org/en/stable/np/einsum.html", "function_name": "np.einsum(subscripts, *operands) -> ndarray", "type": "documentation", "metadata": {"record_id": "b1fd4cd3-2fe1-4bd6-aba9-9f75660e56df", "source_type": "documentation", "domain": "NumPy/Pandas", "library": "NumPy", "title": "NumPy — Linear Algebra", "content": "The `np.einsum` function in NumPy evaluates Einstein summation convention on operands. It accepts `subscripts, *operands` as a parameter and returns ndarray. Common use cases include matrix multiplication, dot products, tensor contractions. Note that prefer np.einsum over loops for large arrays.", "code_signature": "np.einsum(subscripts, *operands) -> ndarray", "tags": "dtype, merge, dataframe, pandas, nan, reshape", "url": "https://docs.numpy.org/en/stable/np/einsum.html", "author": "Official Docs", "date_published": "21-10-2018", "votes_or_stars": 1889, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:a18b5fc5-efbd-4147-956c-134902546ef0": {"content": "subprocess.run vs Popen: which should I use?\n`subprocess.run()` is a high-level wrapper that waits for completion — use for most cases. `subprocess.Popen()` gives full control: stream stdout/stderr in real-time, write to stdin, run processes concurrently. For capturing output: `subprocess.run(..., capture_output=True, text=True)`. Always use `shell=False` and pass args as a list to prevent shell injection.\nCode signature: Use subprocess.run() for simple cases; Popen for streaming output or interactive processes.", "file_path": "https://stackoverflow.com/questions/5727085", "function_name": "Use subprocess.run() for simple cases; Popen for streaming output or interactive processes.", "type": "stack_overflow", "metadata": {"record_id": "a18b5fc5-efbd-4147-956c-134902546ef0", "source_type": "stack_overflow", "domain": "OS/Subprocess/File I/O", "library": "shutil", "title": "subprocess.run vs Popen: which should I use?", "content": "`subprocess.run()` is a high-level wrapper that waits for completion — use for most cases. `subprocess.Popen()` gives full control: stream stdout/stderr in real-time, write to stdin, run processes concurrently. For capturing output: `subprocess.run(..., capture_output=True, text=True)`. Always use `shell=False` and pass args as a list to prevent shell injection.", "code_signature": "Use subprocess.run() for simple cases; Popen for streaming output or interactive processes.", "tags": "file-io, stdout, os, process, stdin, glob", "url": "https://stackoverflow.com/questions/5727085", "author": "user_644210", "date_published": "09-11-2024", "votes_or_stars": 675, "relevance_label": "low", "difficulty_level": "advanced"}}, "kb:270f8a98-0192-455b-871f-c22a4c886dce": {"content": "How to add retry logic to requests?\n```python\nfrom requests.adapters import HTTPAdapter\nfrom urllib3.util.retry import Retry\nretry = Retry(total=3, backoff_factor=1, status_forcelist=[500,502,503,504])\nadapter = HTTPAdapter(max_retries=retry)\nsession.mount('https://', adapter)\n```\nCode signature: Mount an HTTPAdapter with urllib3 Retry on the Session.", "file_path": "https://stackoverflow.com/questions/1669324", "function_name": "Mount an HTTPAdapter with urllib3 Retry on the Session.", "type": "stack_overflow", "metadata": {"record_id": "270f8a98-0192-455b-871f-c22a4c886dce", "source_type": "stack_overflow", "domain": "Requests/HTTP/APIs", "library": "websocket", "title": "How to add retry logic to requests?", "content": "```python\nfrom requests.adapters import HTTPAdapter\nfrom urllib3.util.retry import Retry\nretry = Retry(total=3, backoff_factor=1, status_forcelist=[500,502,503,504])\nadapter = HTTPAdapter(max_retries=retry)\nsession.mount('https://', adapter)\n```", "code_signature": "Mount an HTTPAdapter with urllib3 Retry on the Session.", "tags": "json, retry, requests, timeout, http, authentication", "url": "https://stackoverflow.com/questions/1669324", "author": "user_952590", "date_published": "05-01-2019", "votes_or_stars": 1512, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:dae90fbd-954e-4f2f-94bb-7ba09b6eda19": {"content": "SQLAlchemy — ORM Session Query\nThe `session.query` function in SQLAlchemy constructs a SELECT query against mapped entities. It accepts `*entities` as a parameter and returns Query object. Common use cases include retrieving ORM-mapped objects from the database. Note that prefer session.execute(select(...)) in SQLAlchemy 2.0+.\nCode signature: session.query(*entities) -> Query object", "file_path": "https://docs.sqlalchemy.org/en/stable/session/query.html", "function_name": "session.query(*entities) -> Query object", "type": "documentation", "metadata": {"record_id": "dae90fbd-954e-4f2f-94bb-7ba09b6eda19", "source_type": "documentation", "domain": "SQLAlchemy/Databases", "library": "SQLAlchemy", "title": "SQLAlchemy — ORM Session Query", "content": "The `session.query` function in SQLAlchemy constructs a SELECT query against mapped entities. It accepts `*entities` as a parameter and returns Query object. Common use cases include retrieving ORM-mapped objects from the database. Note that prefer session.execute(select(...)) in SQLAlchemy 2.0+.", "code_signature": "session.query(*entities) -> Query object", "tags": "connection-pool, postgresql, foreign-key, alembic, relationship", "url": "https://docs.sqlalchemy.org/en/stable/session/query.html", "author": "Official Docs", "date_published": "07-09-2019", "votes_or_stars": 3020, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:8b924b76-7f31-4452-9df7-e4fbbdb750f3": {"content": "requests.exceptions.SSLError: certificate verify failed\nNever use `verify=False` in production — it disables SSL verification entirely. Solutions: (1) Update certifi: `pip install --upgrade certifi`. (2) Provide CA bundle: `requests.get(url, verify='/path/to/ca-bundle.crt')`. (3) For self-signed certs in dev: `verify=False` only, and never commit this. (4) Check system time — expired system clock causes SSL failures.\nCode signature: Your SSL certificate chain is incomplete or self-signed. Fix the cert, don't disable verification.", "file_path": "https://stackoverflow.com/questions/7907400", "function_name": "Your SSL certificate chain is incomplete or self-signed. Fix the cert, don't disable verification.", "type": "stack_overflow", "metadata": {"record_id": "8b924b76-7f31-4452-9df7-e4fbbdb750f3", "source_type": "stack_overflow", "domain": "Requests/HTTP/APIs", "library": "websocket", "title": "requests.exceptions.SSLError: certificate verify failed", "content": "Never use `verify=False` in production — it disables SSL verification entirely. Solutions: (1) Update certifi: `pip install --upgrade certifi`. (2) Provide CA bundle: `requests.get(url, verify='/path/to/ca-bundle.crt')`. (3) For self-signed certs in dev: `verify=False` only, and never commit this. (4) Check system time — expired system clock causes SSL failures.", "code_signature": "Your SSL certificate chain is incomplete or self-signed. Fix the cert, don't disable verification.", "tags": "rest-api, json, headers, timeout", "url": "https://stackoverflow.com/questions/7907400", "author": "user_851204", "date_published": "20-02-2019", "votes_or_stars": 1355, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:7ed4ecde-1e44-4e8c-928c-33670a8e3e69": {"content": "How to limit concurrency with asyncio?\n```python\nsem = asyncio.Semaphore(10)\nasync def limited(url):\n    async with sem:\n        return await fetch(url)\nresults = await asyncio.gather(*[limited(u) for u in urls])\n```\nThis prevents overwhelming APIs or databases with too many simultaneous connections.\nCode signature: Use asyncio.Semaphore to cap the number of concurrent coroutines.", "file_path": "https://stackoverflow.com/questions/2375453", "function_name": "Use asyncio.Semaphore to cap the number of concurrent coroutines.", "type": "stack_overflow", "metadata": {"record_id": "7ed4ecde-1e44-4e8c-928c-33670a8e3e69", "source_type": "stack_overflow", "domain": "Asyncio/Threading", "library": "asyncio", "title": "How to limit concurrency with asyncio?", "content": "```python\nsem = asyncio.Semaphore(10)\nasync def limited(url):\n    async with sem:\n        return await fetch(url)\nresults = await asyncio.gather(*[limited(u) for u in urls])\n```\nThis prevents overwhelming APIs or databases with too many simultaneous connections.", "code_signature": "Use asyncio.Semaphore to cap the number of concurrent coroutines.", "tags": "lock, executor, await", "url": "https://stackoverflow.com/questions/2375453", "author": "user_449589", "date_published": "01-07-2019", "votes_or_stars": 2455, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:7ba69af9-baf0-413b-9161-ca339f401265": {"content": "How to stream large HTTP responses with requests?\n```python\nwith requests.get(url, stream=True) as r:\n    r.raise_for_status()\n    with open('file', 'wb') as f:\n        for chunk in r.iter_content(chunk_size=8192):\n            f.write(chunk)\n```\nAlways use as context manager with stream=True to ensure connection is released.\nCode signature: Use stream=True and iterate over response.iter_content() or iter_lines().", "file_path": "https://stackoverflow.com/questions/3592974", "function_name": "Use stream=True and iterate over response.iter_content() or iter_lines().", "type": "stack_overflow", "metadata": {"record_id": "7ba69af9-baf0-413b-9161-ca339f401265", "source_type": "stack_overflow", "domain": "Requests/HTTP/APIs", "library": "json", "title": "How to stream large HTTP responses with requests?", "content": "```python\nwith requests.get(url, stream=True) as r:\n    r.raise_for_status()\n    with open('file', 'wb') as f:\n        for chunk in r.iter_content(chunk_size=8192):\n            f.write(chunk)\n```\nAlways use as context manager with stream=True to ensure connection is released.", "code_signature": "Use stream=True and iterate over response.iter_content() or iter_lines().", "tags": "retry, rest-api, httpx, requests", "url": "https://stackoverflow.com/questions/3592974", "author": "user_980733", "date_published": "22-07-2021", "votes_or_stars": 1666, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:6c78cabb-77a0-42e4-a67f-83bb37abe274": {"content": "How to limit concurrency with asyncio?\n```python\nsem = asyncio.Semaphore(10)\nasync def limited(url):\n    async with sem:\n        return await fetch(url)\nresults = await asyncio.gather(*[limited(u) for u in urls])\n```\nThis prevents overwhelming APIs or databases with too many simultaneous connections.\nCode signature: Use asyncio.Semaphore to cap the number of concurrent coroutines.", "file_path": "https://stackoverflow.com/questions/2375453", "function_name": "Use asyncio.Semaphore to cap the number of concurrent coroutines.", "type": "stack_overflow", "metadata": {"record_id": "6c78cabb-77a0-42e4-a67f-83bb37abe274", "source_type": "stack_overflow", "domain": "Asyncio/Threading", "library": "asyncio", "title": "How to limit concurrency with asyncio?", "content": "```python\nsem = asyncio.Semaphore(10)\nasync def limited(url):\n    async with sem:\n        return await fetch(url)\nresults = await asyncio.gather(*[limited(u) for u in urls])\n```\nThis prevents overwhelming APIs or databases with too many simultaneous connections.", "code_signature": "Use asyncio.Semaphore to cap the number of concurrent coroutines.", "tags": "semaphore, asyncio, threading, executor, async", "url": "https://stackoverflow.com/questions/2375453", "author": "user_449589", "date_published": "19-09-2023", "votes_or_stars": 2449, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:b4eca5bd-3474-4d7e-b22b-461308606d62": {"content": "How to implement database connection pooling with SQLAlchemy?\n`create_engine(url, pool_size=10, max_overflow=20, pool_timeout=30, pool_recycle=1800)`. Use `pool_pre_ping=True` to detect stale connections. For async: use `AsyncEngine` with `create_async_engine()`. Monitor with `engine.pool.status()`. NullPool disables pooling for scripts/serverless environments.\nCode signature: create_engine() has built-in pooling. Configure pool_size and max_overflow.", "file_path": "https://stackoverflow.com/questions/7358113", "function_name": "create_engine() has built-in pooling. Configure pool_size and max_overflow.", "type": "stack_overflow", "metadata": {"record_id": "b4eca5bd-3474-4d7e-b22b-461308606d62", "source_type": "stack_overflow", "domain": "SQLAlchemy/Databases", "library": "session", "title": "How to implement database connection pooling with SQLAlchemy?", "content": "`create_engine(url, pool_size=10, max_overflow=20, pool_timeout=30, pool_recycle=1800)`. Use `pool_pre_ping=True` to detect stale connections. For async: use `AsyncEngine` with `create_async_engine()`. Monitor with `engine.pool.status()`. NullPool disables pooling for scripts/serverless environments.", "code_signature": "create_engine() has built-in pooling. Configure pool_size and max_overflow.", "tags": "relationship, postgresql, migration, sqlite", "url": "https://stackoverflow.com/questions/7358113", "author": "user_12260", "date_published": "06-09-2021", "votes_or_stars": 259, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:c5767e47-76e8-4fca-8daa-645c20028b49": {"content": "BUG: SQLAlchemy connection pool exhaustion under load\nProblem: Application hangs under concurrent load — all connections are checked out and pool is exhausted.\n\nFix/Discussion: Increase `pool_size` and `max_overflow`. Ensure sessions are always closed: use `contextmanager` or `with Session() as session:`. Set `pool_timeout` to fail fast. Enable `pool_pre_ping=True`. Monitor with `engine.pool.checkedout()`. In async: use `AsyncSession` with `async_scoped_session`.\nCode signature: status:closed", "file_path": "https://github.com/sqlalchemy/sqlalchemy/issues/106", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "c5767e47-76e8-4fca-8daa-645c20028b49", "source_type": "github_issue", "domain": "SQLAlchemy/Databases", "library": "sqlalchemy", "title": "BUG: SQLAlchemy connection pool exhaustion under load", "content": "Problem: Application hangs under concurrent load — all connections are checked out and pool is exhausted.\n\nFix/Discussion: Increase `pool_size` and `max_overflow`. Ensure sessions are always closed: use `contextmanager` or `with Session() as session:`. Set `pool_timeout` to fail fast. Enable `pool_pre_ping=True`. Monitor with `engine.pool.checkedout()`. In async: use `AsyncSession` with `async_scoped_session`.", "code_signature": "status:closed", "tags": "alembic, relationship, postgresql, migration", "url": "https://github.com/sqlalchemy/sqlalchemy/issues/106", "author": "contributor_5078", "date_published": "11-01-2022", "votes_or_stars": 262, "relevance_label": "low", "difficulty_level": "intermediate"}}, "kb:baf6a521-3d2b-49fd-8c32-2f37d35d69fd": {"content": "NumPy — Linear Algebra\nThe `np.einsum` function in NumPy evaluates Einstein summation convention on operands. It accepts `subscripts, *operands` as a parameter and returns ndarray. Common use cases include matrix multiplication, dot products, tensor contractions. Note that prefer np.einsum over loops for large arrays.\nCode signature: np.einsum(subscripts, *operands) -> ndarray", "file_path": "https://docs.numpy.org/en/stable/np/einsum.html", "function_name": "np.einsum(subscripts, *operands) -> ndarray", "type": "documentation", "metadata": {"record_id": "baf6a521-3d2b-49fd-8c32-2f37d35d69fd", "source_type": "documentation", "domain": "NumPy/Pandas", "library": "NumPy", "title": "NumPy — Linear Algebra", "content": "The `np.einsum` function in NumPy evaluates Einstein summation convention on operands. It accepts `subscripts, *operands` as a parameter and returns ndarray. Common use cases include matrix multiplication, dot products, tensor contractions. Note that prefer np.einsum over loops for large arrays.", "code_signature": "np.einsum(subscripts, *operands) -> ndarray", "tags": "vectorization, broadcasting, iloc", "url": "https://docs.numpy.org/en/stable/np/einsum.html", "author": "Official Docs", "date_published": "04-04-2020", "votes_or_stars": 1892, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:05d38304-7d0c-400b-9b13-8d4cab200bbd": {"content": "How to handle rate limiting with the requests library?\n```python\nfrom tenacity import retry, wait_exponential, stop_after_attempt\n@retry(wait=wait_exponential(min=1, max=60), stop=stop_after_attempt(5))\ndef call_api(url):\n    r = requests.get(url)\n    r.raise_for_status()\n    return r.json()\n```\nCheck `Retry-After` header in 429 responses. For async: use `aiohttp` with tenacity.\nCode signature: Implement exponential backoff with the tenacity or backoff library.", "file_path": "https://stackoverflow.com/questions/1604451", "function_name": "Implement exponential backoff with the tenacity or backoff library.", "type": "stack_overflow", "metadata": {"record_id": "05d38304-7d0c-400b-9b13-8d4cab200bbd", "source_type": "stack_overflow", "domain": "Requests/HTTP/APIs", "library": "retry", "title": "How to handle rate limiting with the requests library?", "content": "```python\nfrom tenacity import retry, wait_exponential, stop_after_attempt\n@retry(wait=wait_exponential(min=1, max=60), stop=stop_after_attempt(5))\ndef call_api(url):\n    r = requests.get(url)\n    r.raise_for_status()\n    return r.json()\n```\nCheck `Retry-After` header in 429 responses. For async: use `aiohttp` with tenacity.", "code_signature": "Implement exponential backoff with the tenacity or backoff library.", "tags": "rate-limiting, json, requests", "url": "https://stackoverflow.com/questions/1604451", "author": "user_885136", "date_published": "08-01-2022", "votes_or_stars": 607, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:9e652b1a-240d-40a2-b7e8-957bdf78eb03": {"content": "How to add retry logic to requests?\n```python\nfrom requests.adapters import HTTPAdapter\nfrom urllib3.util.retry import Retry\nretry = Retry(total=3, backoff_factor=1, status_forcelist=[500,502,503,504])\nadapter = HTTPAdapter(max_retries=retry)\nsession.mount('https://', adapter)\n```\nCode signature: Mount an HTTPAdapter with urllib3 Retry on the Session.", "file_path": "https://stackoverflow.com/questions/1669324", "function_name": "Mount an HTTPAdapter with urllib3 Retry on the Session.", "type": "stack_overflow", "metadata": {"record_id": "9e652b1a-240d-40a2-b7e8-957bdf78eb03", "source_type": "stack_overflow", "domain": "Requests/HTTP/APIs", "library": "websocket", "title": "How to add retry logic to requests?", "content": "```python\nfrom requests.adapters import HTTPAdapter\nfrom urllib3.util.retry import Retry\nretry = Retry(total=3, backoff_factor=1, status_forcelist=[500,502,503,504])\nadapter = HTTPAdapter(max_retries=retry)\nsession.mount('https://', adapter)\n```", "code_signature": "Mount an HTTPAdapter with urllib3 Retry on the Session.", "tags": "json, authentication, websocket, rest-api, headers, http", "url": "https://stackoverflow.com/questions/1669324", "author": "user_952590", "date_published": "25-07-2024", "votes_or_stars": 1489, "relevance_label": "low", "difficulty_level": "intermediate"}}, "kb:300eed94-19bb-46bd-9538-6fe24bfc695e": {"content": "asyncio — Concurrent Coroutines\nThe `asyncio.gather` function in asyncio runs multiple coroutines concurrently and collects results. It accepts `*coros, return_exceptions=False` as a parameter and returns list of results. Common use cases include parallel I/O operations such as multiple API calls. Note that set return_exceptions=True to prevent one failure from cancelling all tasks.\nCode signature: asyncio.gather(*coros, return_exceptions=False) -> list of results", "file_path": "https://docs.asyncio.org/en/stable/asyncio/gather.html", "function_name": "asyncio.gather(*coros, return_exceptions=False) -> list of results", "type": "documentation", "metadata": {"record_id": "300eed94-19bb-46bd-9538-6fe24bfc695e", "source_type": "documentation", "domain": "Asyncio/Threading", "library": "asyncio", "title": "asyncio — Concurrent Coroutines", "content": "The `asyncio.gather` function in asyncio runs multiple coroutines concurrently and collects results. It accepts `*coros, return_exceptions=False` as a parameter and returns list of results. Common use cases include parallel I/O operations such as multiple API calls. Note that set return_exceptions=True to prevent one failure from cancelling all tasks.", "code_signature": "asyncio.gather(*coros, return_exceptions=False) -> list of results", "tags": "semaphore, async, executor, race-condition, lock, coroutine", "url": "https://docs.asyncio.org/en/stable/asyncio/gather.html", "author": "Official Docs", "date_published": "23-06-2024", "votes_or_stars": 518, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:9482c227-0776-46bf-bb9f-187c6004b22f": {"content": "threading Thread Synchronization — official reference\nOfficial documentation for threading Thread Synchronization. The method signature is `threading.Lock()`. creates a mutual-exclusion lock for thread-safe access. Raises `RuntimeError` when lock is released without being acquired. See also: threading.RLock, threading.Semaphore, threading.Event.\nCode signature: threading.Lock()", "file_path": "https://docs.threading.org/en/stable/threading/Lock.html", "function_name": "threading.Lock()", "type": "documentation", "metadata": {"record_id": "9482c227-0776-46bf-bb9f-187c6004b22f", "source_type": "documentation", "domain": "Asyncio/Threading", "library": "threading", "title": "threading Thread Synchronization — official reference", "content": "Official documentation for threading Thread Synchronization. The method signature is `threading.Lock()`. creates a mutual-exclusion lock for thread-safe access. Raises `RuntimeError` when lock is released without being acquired. See also: threading.RLock, threading.Semaphore, threading.Event.", "code_signature": "threading.Lock()", "tags": "await, race-condition, threading, task, deadlock, semaphore", "url": "https://docs.threading.org/en/stable/threading/Lock.html", "author": "Official Docs", "date_published": "06-10-2021", "votes_or_stars": 4812, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:2d330121-29be-4af7-aa74-c4b2ef8817a3": {"content": "PERF: groupby().apply() extremely slow on large DataFrames\nProblem: groupby().apply() with a custom function is 100x slower than expected on 10M row DataFrames.\n\nFix/Discussion: groupby().apply() serializes each group and has high overhead. Fix: vectorize with numpy, use groupby().transform() for same-shape output, or groupby().agg() with named aggregations. For complex logic, consider Polars or Dask. Numba's @jit can accelerate custom group functions.\nCode signature: status:closed", "file_path": "https://github.com/pandas/pandas/issues/8446", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "2d330121-29be-4af7-aa74-c4b2ef8817a3", "source_type": "github_issue", "domain": "NumPy/Pandas", "library": "pandas", "title": "PERF: groupby().apply() extremely slow on large DataFrames", "content": "Problem: groupby().apply() with a custom function is 100x slower than expected on 10M row DataFrames.\n\nFix/Discussion: groupby().apply() serializes each group and has high overhead. Fix: vectorize with numpy, use groupby().transform() for same-shape output, or groupby().agg() with named aggregations. For complex logic, consider Polars or Dask. Numba's @jit can accelerate custom group functions.", "code_signature": "status:closed", "tags": "nan, groupby, vectorization", "url": "https://github.com/pandas/pandas/issues/8446", "author": "contributor_6648", "date_published": "23-02-2023", "votes_or_stars": 344, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:2220ff68-392f-4923-97c5-6464d2c6f8cd": {"content": "How to run asyncio code from synchronous Python?\n`asyncio.run(main())` creates an event loop, runs the coroutine, closes the loop. Never call asyncio.run() from inside a running event loop (use await instead). In Jupyter notebooks (which have a running loop), use `await coroutine()` directly or `nest_asyncio.apply()`. For mixing sync/async, use `loop.run_until_complete()`.\nCode signature: Use asyncio.run() in Python 3.7+ to execute a coroutine from sync code.", "file_path": "https://stackoverflow.com/questions/2140194", "function_name": "Use asyncio.run() in Python 3.7+ to execute a coroutine from sync code.", "type": "stack_overflow", "metadata": {"record_id": "2220ff68-392f-4923-97c5-6464d2c6f8cd", "source_type": "stack_overflow", "domain": "Asyncio/Threading", "library": "gather", "title": "How to run asyncio code from synchronous Python?", "content": "`asyncio.run(main())` creates an event loop, runs the coroutine, closes the loop. Never call asyncio.run() from inside a running event loop (use await instead). In Jupyter notebooks (which have a running loop), use `await coroutine()` directly or `nest_asyncio.apply()`. For mixing sync/async, use `loop.run_until_complete()`.", "code_signature": "Use asyncio.run() in Python 3.7+ to execute a coroutine from sync code.", "tags": "deadlock, async, threading, await", "url": "https://stackoverflow.com/questions/2140194", "author": "user_718011", "date_published": "04-07-2019", "votes_or_stars": 314, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:e59cfb01-341a-49c3-9892-dbfd7c62af7d": {"content": "How to use @app.route in Flask\n`Flask.@app.route` maps a URL rule to a view function. Example usage:\n```python\n@app.route('/users/<int:uid>')\ndef get_user(uid):\n    return jsonify(user)\n```\nThis is useful when defining HTTP endpoints in a Flask application. Performance tip: use Blueprints to organize large applications.\nCode signature: @app.route", "file_path": "https://docs.flask.org/en/stable/@app/route.html", "function_name": "@app.route", "type": "documentation", "metadata": {"record_id": "e59cfb01-341a-49c3-9892-dbfd7c62af7d", "source_type": "documentation", "domain": "FastAPI/Flask/Django", "library": "Flask", "title": "How to use @app.route in Flask", "content": "`Flask.@app.route` maps a URL rule to a view function. Example usage:\n```python\n@app.route('/users/<int:uid>')\ndef get_user(uid):\n    return jsonify(user)\n```\nThis is useful when defining HTTP endpoints in a Flask application. Performance tip: use Blueprints to organize large applications.", "code_signature": "@app.route", "tags": "rest, template, request", "url": "https://docs.flask.org/en/stable/@app/route.html", "author": "Official Docs", "date_published": "16-03-2022", "votes_or_stars": 2996, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:1f4c34ed-fe19-40fc-a148-f7f64d7096ef": {"content": "How do I efficiently fill NaN values in a Pandas DataFrame?\nFor large DataFrames, avoid looping. Use `df.fillna({'col': val})` to fill specific columns. `df.ffill()` propagates last valid observation forward. For time-series data, `df.interpolate()` provides smoother results. Always check `df.isna().sum()` before and after to verify.\nCode signature: Use df.fillna(value) or df.ffill()/df.bfill() for forward/backward fill.", "file_path": "https://stackoverflow.com/questions/6438436", "function_name": "Use df.fillna(value) or df.ffill()/df.bfill() for forward/backward fill.", "type": "stack_overflow", "metadata": {"record_id": "1f4c34ed-fe19-40fc-a148-f7f64d7096ef", "source_type": "stack_overflow", "domain": "NumPy/Pandas", "library": "loc", "title": "How do I efficiently fill NaN values in a Pandas DataFrame?", "content": "For large DataFrames, avoid looping. Use `df.fillna({'col': val})` to fill specific columns. `df.ffill()` propagates last valid observation forward. For time-series data, `df.interpolate()` provides smoother results. Always check `df.isna().sum()` before and after to verify.", "code_signature": "Use df.fillna(value) or df.ffill()/df.bfill() for forward/backward fill.", "tags": "loc, nan, dtype, numpy, iloc, array", "url": "https://stackoverflow.com/questions/6438436", "author": "user_522340", "date_published": "23-12-2023", "votes_or_stars": 33, "relevance_label": "low", "difficulty_level": "advanced"}}, "kb:1780de86-7d17-4722-8dc1-4ebe79c4c52f": {"content": "FEAT: pathlib: add support for reading/writing with encoding shorthand\nProblem: pathlib.Path.read_text() does not default to UTF-8, causing issues across platforms.\n\nFix/Discussion: Always pass encoding explicitly: `Path('file').read_text(encoding='utf-8')`. Python 3.10+ added `encoding='locale'` default change proposal. Best practice: always specify encoding in all file I/O operations for portability.\nCode signature: status:closed", "file_path": "https://github.com/cpython/cpython/issues/2707", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "1780de86-7d17-4722-8dc1-4ebe79c4c52f", "source_type": "github_issue", "domain": "OS/Subprocess/File I/O", "library": "cpython", "title": "FEAT: pathlib: add support for reading/writing with encoding shorthand", "content": "Problem: pathlib.Path.read_text() does not default to UTF-8, causing issues across platforms.\n\nFix/Discussion: Always pass encoding explicitly: `Path('file').read_text(encoding='utf-8')`. Python 3.10+ added `encoding='locale'` default change proposal. Best practice: always specify encoding in all file I/O operations for portability.", "code_signature": "status:closed", "tags": "stdout, shutil, os, stdin", "url": "https://github.com/cpython/cpython/issues/2707", "author": "contributor_7877", "date_published": "29-05-2018", "votes_or_stars": 487, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:94585900-20d6-4af3-ba35-de071695e35d": {"content": "How to watch a directory for file changes in Python?\n```python\nfrom watchdog.observers import Observer\nfrom watchdog.events import FileSystemEventHandler\nhandler = FileSystemEventHandler()\nobserver = Observer()\nobserver.schedule(handler, path='.', recursive=True)\nobserver.start()\n```\nFor simple polling: `os.stat(file).st_mtime`. Linux-native: `inotify`. macOS-native: `FSEvents`.\nCode signature: Use the watchdog library for cross-platform directory monitoring.", "file_path": "https://stackoverflow.com/questions/5213115", "function_name": "Use the watchdog library for cross-platform directory monitoring.", "type": "stack_overflow", "metadata": {"record_id": "94585900-20d6-4af3-ba35-de071695e35d", "source_type": "stack_overflow", "domain": "OS/Subprocess/File I/O", "library": "env-variable", "title": "How to watch a directory for file changes in Python?", "content": "```python\nfrom watchdog.observers import Observer\nfrom watchdog.events import FileSystemEventHandler\nhandler = FileSystemEventHandler()\nobserver = Observer()\nobserver.schedule(handler, path='.', recursive=True)\nobserver.start()\n```\nFor simple polling: `os.stat(file).st_mtime`. Linux-native: `inotify`. macOS-native: `FSEvents`.", "code_signature": "Use the watchdog library for cross-platform directory monitoring.", "tags": "pathlib, tempfile, stdin, process, pipe, glob", "url": "https://stackoverflow.com/questions/5213115", "author": "user_959314", "date_published": "10-02-2019", "votes_or_stars": 2107, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:1a6aba4c-7581-4467-985b-8fcf88d0d395": {"content": "Django N+1 query problem: how to fix?\nN+1 occurs when accessing related objects in a loop. Diagnose with `django-debug-toolbar` or `connection.queries`. Fix ForeignKey: `User.objects.select_related('profile').all()`. Fix M2M: `Post.objects.prefetch_related('tags').all()`. Use `only()` to fetch specific fields and reduce data transfer.\nCode signature: Use select_related() for ForeignKey and prefetch_related() for ManyToMany relationships.", "file_path": "https://stackoverflow.com/questions/8106470", "function_name": "Use select_related() for ForeignKey and prefetch_related() for ManyToMany relationships.", "type": "stack_overflow", "metadata": {"record_id": "1a6aba4c-7581-4467-985b-8fcf88d0d395", "source_type": "stack_overflow", "domain": "FastAPI/Flask/Django", "library": "request", "title": "Django N+1 query problem: how to fix?", "content": "N+1 occurs when accessing related objects in a loop. Diagnose with `django-debug-toolbar` or `connection.queries`. Fix ForeignKey: `User.objects.select_related('profile').all()`. Fix M2M: `Post.objects.prefetch_related('tags').all()`. Use `only()` to fetch specific fields and reduce data transfer.", "code_signature": "Use select_related() for ForeignKey and prefetch_related() for ManyToMany relationships.", "tags": "response, fastapi, flask", "url": "https://stackoverflow.com/questions/8106470", "author": "user_441071", "date_published": "29-06-2023", "votes_or_stars": 1764, "relevance_label": "low", "difficulty_level": "advanced"}}, "kb:aff533d3-368f-4e33-a857-9eaee89fdf17": {"content": "subprocess.run vs Popen: which should I use?\n`subprocess.run()` is a high-level wrapper that waits for completion — use for most cases. `subprocess.Popen()` gives full control: stream stdout/stderr in real-time, write to stdin, run processes concurrently. For capturing output: `subprocess.run(..., capture_output=True, text=True)`. Always use `shell=False` and pass args as a list to prevent shell injection.\nCode signature: Use subprocess.run() for simple cases; Popen for streaming output or interactive processes.", "file_path": "https://stackoverflow.com/questions/5727085", "function_name": "Use subprocess.run() for simple cases; Popen for streaming output or interactive processes.", "type": "stack_overflow", "metadata": {"record_id": "aff533d3-368f-4e33-a857-9eaee89fdf17", "source_type": "stack_overflow", "domain": "OS/Subprocess/File I/O", "library": "shutil", "title": "subprocess.run vs Popen: which should I use?", "content": "`subprocess.run()` is a high-level wrapper that waits for completion — use for most cases. `subprocess.Popen()` gives full control: stream stdout/stderr in real-time, write to stdin, run processes concurrently. For capturing output: `subprocess.run(..., capture_output=True, text=True)`. Always use `shell=False` and pass args as a list to prevent shell injection.", "code_signature": "Use subprocess.run() for simple cases; Popen for streaming output or interactive processes.", "tags": "subprocess, stdout, process", "url": "https://stackoverflow.com/questions/5727085", "author": "user_644210", "date_published": "12-05-2020", "votes_or_stars": 646, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:37b125f0-860a-4c82-9d39-5cd9900580ba": {"content": "threading Thread Synchronization — official reference\nOfficial documentation for threading Thread Synchronization. The method signature is `threading.Lock()`. creates a mutual-exclusion lock for thread-safe access. Raises `RuntimeError` when lock is released without being acquired. See also: threading.RLock, threading.Semaphore, threading.Event.\nCode signature: threading.Lock()", "file_path": "https://docs.threading.org/en/stable/threading/Lock.html", "function_name": "threading.Lock()", "type": "documentation", "metadata": {"record_id": "37b125f0-860a-4c82-9d39-5cd9900580ba", "source_type": "documentation", "domain": "Asyncio/Threading", "library": "threading", "title": "threading Thread Synchronization — official reference", "content": "Official documentation for threading Thread Synchronization. The method signature is `threading.Lock()`. creates a mutual-exclusion lock for thread-safe access. Raises `RuntimeError` when lock is released without being acquired. See also: threading.RLock, threading.Semaphore, threading.Event.", "code_signature": "threading.Lock()", "tags": "async, task, await", "url": "https://docs.threading.org/en/stable/threading/Lock.html", "author": "Official Docs", "date_published": "13-06-2019", "votes_or_stars": 4835, "relevance_label": "low", "difficulty_level": "advanced"}}, "kb:51f93e99-00ce-4fbb-a134-a8162759f8c0": {"content": "BUG: asyncio task leaks when exception is not handled\nProblem: Unhandled exceptions in fire-and-forget asyncio tasks are silently ignored, causing task leaks.\n\nFix/Discussion: Always await tasks or add exception handlers. Use `task.add_done_callback()` to log exceptions. In Python 3.11+, use TaskGroup for structured concurrency — exceptions propagate automatically. Set `asyncio.get_event_loop().set_exception_handler()` for global unhandled task exception logging.\nCode signature: status:closed", "file_path": "https://github.com/cpython/cpython/issues/5591", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "51f93e99-00ce-4fbb-a134-a8162759f8c0", "source_type": "github_issue", "domain": "Asyncio/Threading", "library": "cpython", "title": "BUG: asyncio task leaks when exception is not handled", "content": "Problem: Unhandled exceptions in fire-and-forget asyncio tasks are silently ignored, causing task leaks.\n\nFix/Discussion: Always await tasks or add exception handlers. Use `task.add_done_callback()` to log exceptions. In Python 3.11+, use TaskGroup for structured concurrency — exceptions propagate automatically. Set `asyncio.get_event_loop().set_exception_handler()` for global unhandled task exception logging.", "code_signature": "status:closed", "tags": "asyncio, task, lock, threading", "url": "https://github.com/cpython/cpython/issues/5591", "author": "contributor_1630", "date_published": "20-06-2023", "votes_or_stars": 360, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:5d9070ab-89d6-4a09-9651-03e2867e2f5e": {"content": "BUG: pd.read_csv() silently truncates large integers\nProblem: Integer columns with values > 2^53 are silently corrupted when read from CSV.\n\nFix/Discussion: Floating point cannot represent integers > 2^53 exactly. Fix: `pd.read_csv(f, dtype={'col': str})` then convert: `df['col'] = df['col'].astype('Int64')`. Use `dtype_backend='numpy_nullable'` in Pandas 2.0+ for better integer handling.\nCode signature: status:closed", "file_path": "https://github.com/pandas/pandas/issues/4449", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "5d9070ab-89d6-4a09-9651-03e2867e2f5e", "source_type": "github_issue", "domain": "NumPy/Pandas", "library": "pandas", "title": "BUG: pd.read_csv() silently truncates large integers", "content": "Problem: Integer columns with values > 2^53 are silently corrupted when read from CSV.\n\nFix/Discussion: Floating point cannot represent integers > 2^53 exactly. Fix: `pd.read_csv(f, dtype={'col': str})` then convert: `df['col'] = df['col'].astype('Int64')`. Use `dtype_backend='numpy_nullable'` in Pandas 2.0+ for better integer handling.", "code_signature": "status:closed", "tags": "vectorization, broadcasting, array, pandas, nan", "url": "https://github.com/pandas/pandas/issues/4449", "author": "contributor_726", "date_published": "04-03-2024", "votes_or_stars": 488, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:f64a3471-e379-4276-b09f-2d38bdc7fd49": {"content": "How to use df.groupby in Pandas\n`Pandas.df.groupby` groups DataFrame rows by column values. Example usage:\n```python\ndf.groupby('category')['value'].mean()\n```\nThis is useful when aggregation, transformation, filtering by group. Performance tip: avoid apply() on large DataFrames; prefer agg().\nCode signature: df.groupby", "file_path": "https://docs.pandas.org/en/stable/df/groupby.html", "function_name": "df.groupby", "type": "documentation", "metadata": {"record_id": "f64a3471-e379-4276-b09f-2d38bdc7fd49", "source_type": "documentation", "domain": "NumPy/Pandas", "library": "Pandas", "title": "How to use df.groupby in Pandas", "content": "`Pandas.df.groupby` groups DataFrame rows by column values. Example usage:\n```python\ndf.groupby('category')['value'].mean()\n```\nThis is useful when aggregation, transformation, filtering by group. Performance tip: avoid apply() on large DataFrames; prefer agg().", "code_signature": "df.groupby", "tags": "pivot, nan, dataframe, groupby, reshape", "url": "https://docs.pandas.org/en/stable/df/groupby.html", "author": "Official Docs", "date_published": "29-11-2020", "votes_or_stars": 797, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:15994367-e764-48e1-9ab1-e705064fb4db": {"content": "BUG: subprocess.Popen hangs when stdout and stderr buffers fill\nProblem: subprocess.Popen hangs indefinitely when child process produces large output.\n\nFix/Discussion: This is a classic deadlock: parent waits for child, child waits for parent to read buffer. Fix: use `subprocess.run(..., capture_output=True)` which handles this correctly. Or: `stdout, stderr = proc.communicate()`. Never use `proc.stdout.read()` and `proc.wait()` separately.\nCode signature: status:closed", "file_path": "https://github.com/cpython/cpython/issues/8586", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "15994367-e764-48e1-9ab1-e705064fb4db", "source_type": "github_issue", "domain": "OS/Subprocess/File I/O", "library": "cpython", "title": "BUG: subprocess.Popen hangs when stdout and stderr buffers fill", "content": "Problem: subprocess.Popen hangs indefinitely when child process produces large output.\n\nFix/Discussion: This is a classic deadlock: parent waits for child, child waits for parent to read buffer. Fix: use `subprocess.run(..., capture_output=True)` which handles this correctly. Or: `stdout, stderr = proc.communicate()`. Never use `proc.stdout.read()` and `proc.wait()` separately.", "code_signature": "status:closed", "tags": "os, env-variable, pathlib, stderr, process", "url": "https://github.com/cpython/cpython/issues/8586", "author": "contributor_7711", "date_published": "23-02-2023", "votes_or_stars": 446, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:acf54ac1-a885-44c2-9654-9be9b9cc3f6a": {"content": "BUG: Alembic autogenerate misses index changes\nProblem: Alembic --autogenerate does not detect changes to indexes or constraints on existing tables.\n\nFix/Discussion: Alembic autogenerate compares metadata to database state but has limitations. Workarounds: (1) Manually add `op.create_index()` to migration. (2) Use `include_schemas=True` in env.py. (3) Run `alembic check` to see detected changes. (4) Always review autogenerated migrations before applying.\nCode signature: status:open", "file_path": "https://github.com/alembic/alembic/issues/7344", "function_name": "status:open", "type": "github_issue", "metadata": {"record_id": "acf54ac1-a885-44c2-9654-9be9b9cc3f6a", "source_type": "github_issue", "domain": "SQLAlchemy/Databases", "library": "alembic", "title": "BUG: Alembic autogenerate misses index changes", "content": "Problem: Alembic --autogenerate does not detect changes to indexes or constraints on existing tables.\n\nFix/Discussion: Alembic autogenerate compares metadata to database state but has limitations. Workarounds: (1) Manually add `op.create_index()` to migration. (2) Use `include_schemas=True` in env.py. (3) Run `alembic check` to see detected changes. (4) Always review autogenerated migrations before applying.", "code_signature": "status:open", "tags": "relationship, postgresql, migration, connection-pool", "url": "https://github.com/alembic/alembic/issues/7344", "author": "contributor_3601", "date_published": "24-09-2024", "votes_or_stars": 231, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:0c04f5b2-70d7-449b-9765-369567507f8b": {"content": "SQLAlchemy bulk insert: what's the fastest way?\n`session.bulk_insert_mappings(Model, list_of_dicts)` skips ORM overhead. Even faster: `session.execute(insert(Model), data)`. Avoid `session.add()` in loops. For PostgreSQL, use `insert().on_conflict_do_nothing()` for upserts. Batch in chunks of 1000-5000 rows for optimal performance.\nCode signature: Use session.bulk_insert_mappings() or execute(insert(), list_of_dicts) for maximum speed.", "file_path": "https://stackoverflow.com/questions/5977931", "function_name": "Use session.bulk_insert_mappings() or execute(insert(), list_of_dicts) for maximum speed.", "type": "stack_overflow", "metadata": {"record_id": "0c04f5b2-70d7-449b-9765-369567507f8b", "source_type": "stack_overflow", "domain": "SQLAlchemy/Databases", "library": "query", "title": "SQLAlchemy bulk insert: what's the fastest way?", "content": "`session.bulk_insert_mappings(Model, list_of_dicts)` skips ORM overhead. Even faster: `session.execute(insert(Model), data)`. Avoid `session.add()` in loops. For PostgreSQL, use `insert().on_conflict_do_nothing()` for upserts. Batch in chunks of 1000-5000 rows for optimal performance.", "code_signature": "Use session.bulk_insert_mappings() or execute(insert(), list_of_dicts) for maximum speed.", "tags": "alembic, sqlite, relationship, postgresql, orm", "url": "https://stackoverflow.com/questions/5977931", "author": "user_238275", "date_published": "17-08-2019", "votes_or_stars": 669, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:634a1bd6-ade2-421a-bb64-afd7bc6c8295": {"content": "How to handle rate limiting with the requests library?\n```python\nfrom tenacity import retry, wait_exponential, stop_after_attempt\n@retry(wait=wait_exponential(min=1, max=60), stop=stop_after_attempt(5))\ndef call_api(url):\n    r = requests.get(url)\n    r.raise_for_status()\n    return r.json()\n```\nCheck `Retry-After` header in 429 responses. For async: use `aiohttp` with tenacity.\nCode signature: Implement exponential backoff with the tenacity or backoff library.", "file_path": "https://stackoverflow.com/questions/1604451", "function_name": "Implement exponential backoff with the tenacity or backoff library.", "type": "stack_overflow", "metadata": {"record_id": "634a1bd6-ade2-421a-bb64-afd7bc6c8295", "source_type": "stack_overflow", "domain": "Requests/HTTP/APIs", "library": "retry", "title": "How to handle rate limiting with the requests library?", "content": "```python\nfrom tenacity import retry, wait_exponential, stop_after_attempt\n@retry(wait=wait_exponential(min=1, max=60), stop=stop_after_attempt(5))\ndef call_api(url):\n    r = requests.get(url)\n    r.raise_for_status()\n    return r.json()\n```\nCheck `Retry-After` header in 429 responses. For async: use `aiohttp` with tenacity.", "code_signature": "Implement exponential backoff with the tenacity or backoff library.", "tags": "http, requests, json, retry, oauth", "url": "https://stackoverflow.com/questions/1604451", "author": "user_885136", "date_published": "18-07-2023", "votes_or_stars": 618, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:16c0e347-e666-4a3d-8b3d-2830eb4dc337": {"content": "Django ORM Queries — official reference\nOfficial documentation for Django ORM Queries. The method signature is `QuerySet.filter(**kwargs)`. filters QuerySet rows matching given field lookups. Raises `FieldError` when an unknown field lookup is provided. See also: QuerySet.exclude, QuerySet.annotate, Q objects.\nCode signature: QuerySet.filter(**kwargs)", "file_path": "https://docs.django.org/en/stable/QuerySet/filter.html", "function_name": "QuerySet.filter(**kwargs)", "type": "documentation", "metadata": {"record_id": "16c0e347-e666-4a3d-8b3d-2830eb4dc337", "source_type": "documentation", "domain": "FastAPI/Flask/Django", "library": "Django", "title": "Django ORM Queries — official reference", "content": "Official documentation for Django ORM Queries. The method signature is `QuerySet.filter(**kwargs)`. filters QuerySet rows matching given field lookups. Raises `FieldError` when an unknown field lookup is provided. See also: QuerySet.exclude, QuerySet.annotate, Q objects.", "code_signature": "QuerySet.filter(**kwargs)", "tags": "flask, django, pydantic, rest, response", "url": "https://docs.django.org/en/stable/QuerySet/filter.html", "author": "Official Docs", "date_published": "10-01-2019", "votes_or_stars": 2390, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:1ec00635-1f97-49ec-a87c-a3fca3e48823": {"content": "asyncio — Concurrent Coroutines\nThe `asyncio.gather` function in asyncio runs multiple coroutines concurrently and collects results. It accepts `*coros, return_exceptions=False` as a parameter and returns list of results. Common use cases include parallel I/O operations such as multiple API calls. Note that set return_exceptions=True to prevent one failure from cancelling all tasks.\nCode signature: asyncio.gather(*coros, return_exceptions=False) -> list of results", "file_path": "https://docs.asyncio.org/en/stable/asyncio/gather.html", "function_name": "asyncio.gather(*coros, return_exceptions=False) -> list of results", "type": "documentation", "metadata": {"record_id": "1ec00635-1f97-49ec-a87c-a3fca3e48823", "source_type": "documentation", "domain": "Asyncio/Threading", "library": "asyncio", "title": "asyncio — Concurrent Coroutines", "content": "The `asyncio.gather` function in asyncio runs multiple coroutines concurrently and collects results. It accepts `*coros, return_exceptions=False` as a parameter and returns list of results. Common use cases include parallel I/O operations such as multiple API calls. Note that set return_exceptions=True to prevent one failure from cancelling all tasks.", "code_signature": "asyncio.gather(*coros, return_exceptions=False) -> list of results", "tags": "gather, threading, coroutine", "url": "https://docs.asyncio.org/en/stable/asyncio/gather.html", "author": "Official Docs", "date_published": "29-09-2024", "votes_or_stars": 496, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:3ec29d23-034c-4373-b23a-8c77ae8fb0bf": {"content": "SQLAlchemy bulk insert: what's the fastest way?\n`session.bulk_insert_mappings(Model, list_of_dicts)` skips ORM overhead. Even faster: `session.execute(insert(Model), data)`. Avoid `session.add()` in loops. For PostgreSQL, use `insert().on_conflict_do_nothing()` for upserts. Batch in chunks of 1000-5000 rows for optimal performance.\nCode signature: Use session.bulk_insert_mappings() or execute(insert(), list_of_dicts) for maximum speed.", "file_path": "https://stackoverflow.com/questions/5977931", "function_name": "Use session.bulk_insert_mappings() or execute(insert(), list_of_dicts) for maximum speed.", "type": "stack_overflow", "metadata": {"record_id": "3ec29d23-034c-4373-b23a-8c77ae8fb0bf", "source_type": "stack_overflow", "domain": "SQLAlchemy/Databases", "library": "query", "title": "SQLAlchemy bulk insert: what's the fastest way?", "content": "`session.bulk_insert_mappings(Model, list_of_dicts)` skips ORM overhead. Even faster: `session.execute(insert(Model), data)`. Avoid `session.add()` in loops. For PostgreSQL, use `insert().on_conflict_do_nothing()` for upserts. Batch in chunks of 1000-5000 rows for optimal performance.", "code_signature": "Use session.bulk_insert_mappings() or execute(insert(), list_of_dicts) for maximum speed.", "tags": "transaction, alembic, foreign-key, connection-pool", "url": "https://stackoverflow.com/questions/5977931", "author": "user_238275", "date_published": "28-08-2018", "votes_or_stars": 644, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:40d9bc83-2a6c-48a6-ac07-f4eca89676f8": {"content": "BUG: Flask session not persisting between requests in tests\nProblem: Session data set in one request is not available in the next request during testing.\n\nFix/Discussion: Use Flask test client's session_transaction() context manager: `with client.session_transaction() as sess:\n    sess['user_id'] = 1`. Ensure SECRET_KEY is set. For testing: `app.config['TESTING'] = True; app.config['SECRET_KEY'] = 'test'`.\nCode signature: status:closed", "file_path": "https://github.com/flask/flask/issues/2141", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "40d9bc83-2a6c-48a6-ac07-f4eca89676f8", "source_type": "github_issue", "domain": "FastAPI/Flask/Django", "library": "flask", "title": "BUG: Flask session not persisting between requests in tests", "content": "Problem: Session data set in one request is not available in the next request during testing.\n\nFix/Discussion: Use Flask test client's session_transaction() context manager: `with client.session_transaction() as sess:\n    sess['user_id'] = 1`. Ensure SECRET_KEY is set. For testing: `app.config['TESTING'] = True; app.config['SECRET_KEY'] = 'test'`.", "code_signature": "status:closed", "tags": "blueprint, pydantic, fastapi, response", "url": "https://github.com/flask/flask/issues/2141", "author": "contributor_5020", "date_published": "26-02-2023", "votes_or_stars": 136, "relevance_label": "low", "difficulty_level": "beginner"}}, "kb:88a93267-cde5-4c42-92f2-56389b5c0f9e": {"content": "Django N+1 query problem: how to fix?\nN+1 occurs when accessing related objects in a loop. Diagnose with `django-debug-toolbar` or `connection.queries`. Fix ForeignKey: `User.objects.select_related('profile').all()`. Fix M2M: `Post.objects.prefetch_related('tags').all()`. Use `only()` to fetch specific fields and reduce data transfer.\nCode signature: Use select_related() for ForeignKey and prefetch_related() for ManyToMany relationships.", "file_path": "https://stackoverflow.com/questions/8106470", "function_name": "Use select_related() for ForeignKey and prefetch_related() for ManyToMany relationships.", "type": "stack_overflow", "metadata": {"record_id": "88a93267-cde5-4c42-92f2-56389b5c0f9e", "source_type": "stack_overflow", "domain": "FastAPI/Flask/Django", "library": "request", "title": "Django N+1 query problem: how to fix?", "content": "N+1 occurs when accessing related objects in a loop. Diagnose with `django-debug-toolbar` or `connection.queries`. Fix ForeignKey: `User.objects.select_related('profile').all()`. Fix M2M: `Post.objects.prefetch_related('tags').all()`. Use `only()` to fetch specific fields and reduce data transfer.", "code_signature": "Use select_related() for ForeignKey and prefetch_related() for ManyToMany relationships.", "tags": "response, blueprint, fastapi, django", "url": "https://stackoverflow.com/questions/8106470", "author": "user_441071", "date_published": "13-06-2021", "votes_or_stars": 1751, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:4497af6c-a02c-4dbd-9e4d-fe9d5d0e06ce": {"content": "Pandas Merging DataFrames — official reference\nOfficial documentation for Pandas Merging DataFrames. The method signature is `pd.merge(left, right, on, how='inner')`. merges two DataFrames using SQL-style joins. Raises `MergeError` when merge keys produce duplicate columns. See also: df.join, df.concat.\nCode signature: pd.merge(left, right, on, how='inner')", "file_path": "https://docs.pandas.org/en/stable/pd/merge.html", "function_name": "pd.merge(left, right, on, how='inner')", "type": "documentation", "metadata": {"record_id": "4497af6c-a02c-4dbd-9e4d-fe9d5d0e06ce", "source_type": "documentation", "domain": "NumPy/Pandas", "library": "Pandas", "title": "Pandas Merging DataFrames — official reference", "content": "Official documentation for Pandas Merging DataFrames. The method signature is `pd.merge(left, right, on, how='inner')`. merges two DataFrames using SQL-style joins. Raises `MergeError` when merge keys produce duplicate columns. See also: df.join, df.concat.", "code_signature": "pd.merge(left, right, on, how='inner')", "tags": "array, nan, dtype", "url": "https://docs.pandas.org/en/stable/pd/merge.html", "author": "Official Docs", "date_published": "03-08-2019", "votes_or_stars": 4515, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:f576d553-989b-4097-a3cd-7c2278fb2669": {"content": "How to use create_engine in SQLAlchemy\n`SQLAlchemy.create_engine` creates a database engine representing a connection pool. Example usage:\n```python\nengine = create_engine('postgresql://user:pass@host/db', echo=True)\n```\nThis is useful when establishing the central database connection for an application. Performance tip: set pool_size and max_overflow based on expected concurrency.\nCode signature: create_engine", "file_path": "https://docs.sqlalchemy.org/en/stable/create_engine.html", "function_name": "create_engine", "type": "documentation", "metadata": {"record_id": "f576d553-989b-4097-a3cd-7c2278fb2669", "source_type": "documentation", "domain": "SQLAlchemy/Databases", "library": "SQLAlchemy", "title": "How to use create_engine in SQLAlchemy", "content": "`SQLAlchemy.create_engine` creates a database engine representing a connection pool. Example usage:\n```python\nengine = create_engine('postgresql://user:pass@host/db', echo=True)\n```\nThis is useful when establishing the central database connection for an application. Performance tip: set pool_size and max_overflow based on expected concurrency.", "code_signature": "create_engine", "tags": "connection-pool, migration, foreign-key, orm, postgresql, transaction", "url": "https://docs.sqlalchemy.org/en/stable/create_engine.html", "author": "Official Docs", "date_published": "21-09-2022", "votes_or_stars": 2058, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:96c37d08-22ff-41ed-8825-fd0eefaf6360": {"content": "Django N+1 query problem: how to fix?\nN+1 occurs when accessing related objects in a loop. Diagnose with `django-debug-toolbar` or `connection.queries`. Fix ForeignKey: `User.objects.select_related('profile').all()`. Fix M2M: `Post.objects.prefetch_related('tags').all()`. Use `only()` to fetch specific fields and reduce data transfer.\nCode signature: Use select_related() for ForeignKey and prefetch_related() for ManyToMany relationships.", "file_path": "https://stackoverflow.com/questions/8106470", "function_name": "Use select_related() for ForeignKey and prefetch_related() for ManyToMany relationships.", "type": "stack_overflow", "metadata": {"record_id": "96c37d08-22ff-41ed-8825-fd0eefaf6360", "source_type": "stack_overflow", "domain": "FastAPI/Flask/Django", "library": "request", "title": "Django N+1 query problem: how to fix?", "content": "N+1 occurs when accessing related objects in a loop. Diagnose with `django-debug-toolbar` or `connection.queries`. Fix ForeignKey: `User.objects.select_related('profile').all()`. Fix M2M: `Post.objects.prefetch_related('tags').all()`. Use `only()` to fetch specific fields and reduce data transfer.", "code_signature": "Use select_related() for ForeignKey and prefetch_related() for ManyToMany relationships.", "tags": "middleware, fastapi, pydantic, flask", "url": "https://stackoverflow.com/questions/8106470", "author": "user_441071", "date_published": "05-06-2022", "votes_or_stars": 1797, "relevance_label": "low", "difficulty_level": "intermediate"}}, "kb:02f50f29-89f9-4957-a7df-8b6f4dccc50b": {"content": "os Environment Variables — official reference\nOfficial documentation for os Environment Variables. The method signature is `os.environ(key)`. provides a mapping of the current environment variables. Raises `KeyError` when key is accessed with [] notation and is not set. See also: dotenv, os.getenv, os.putenv.\nCode signature: os.environ(key)", "file_path": "https://docs.os.org/en/stable/os/environ.html", "function_name": "os.environ(key)", "type": "documentation", "metadata": {"record_id": "02f50f29-89f9-4957-a7df-8b6f4dccc50b", "source_type": "documentation", "domain": "OS/Subprocess/File I/O", "library": "os", "title": "os Environment Variables — official reference", "content": "Official documentation for os Environment Variables. The method signature is `os.environ(key)`. provides a mapping of the current environment variables. Raises `KeyError` when key is accessed with [] notation and is not set. See also: dotenv, os.getenv, os.putenv.", "code_signature": "os.environ(key)", "tags": "stdin, stderr, glob, process", "url": "https://docs.os.org/en/stable/os/environ.html", "author": "Official Docs", "date_published": "21-08-2022", "votes_or_stars": 2224, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:b2400fc3-76f6-4704-adcf-5fba52c063c0": {"content": "subprocess.run vs Popen: which should I use?\n`subprocess.run()` is a high-level wrapper that waits for completion — use for most cases. `subprocess.Popen()` gives full control: stream stdout/stderr in real-time, write to stdin, run processes concurrently. For capturing output: `subprocess.run(..., capture_output=True, text=True)`. Always use `shell=False` and pass args as a list to prevent shell injection.\nCode signature: Use subprocess.run() for simple cases; Popen for streaming output or interactive processes.", "file_path": "https://stackoverflow.com/questions/5727085", "function_name": "Use subprocess.run() for simple cases; Popen for streaming output or interactive processes.", "type": "stack_overflow", "metadata": {"record_id": "b2400fc3-76f6-4704-adcf-5fba52c063c0", "source_type": "stack_overflow", "domain": "OS/Subprocess/File I/O", "library": "shutil", "title": "subprocess.run vs Popen: which should I use?", "content": "`subprocess.run()` is a high-level wrapper that waits for completion — use for most cases. `subprocess.Popen()` gives full control: stream stdout/stderr in real-time, write to stdin, run processes concurrently. For capturing output: `subprocess.run(..., capture_output=True, text=True)`. Always use `shell=False` and pass args as a list to prevent shell injection.", "code_signature": "Use subprocess.run() for simple cases; Popen for streaming output or interactive processes.", "tags": "glob, file-io, tempfile, subprocess, shutil, process", "url": "https://stackoverflow.com/questions/5727085", "author": "user_644210", "date_published": "11-10-2021", "votes_or_stars": 633, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:bad8970c-0c2b-4d2c-9829-dbe0ce0438ad": {"content": "subprocess — Process Execution\nThe `subprocess.run` function in subprocess runs a subprocess and waits for it to complete. It accepts `args, capture_output=False, timeout=None` as a parameter and returns CompletedProcess. Common use cases include executing shell commands, scripts, or external programs from Python. Note that use shell=False (default) to avoid shell injection vulnerabilities.\nCode signature: subprocess.run(args, capture_output=False, timeout=None) -> CompletedProcess", "file_path": "https://docs.subprocess.org/en/stable/subprocess/run.html", "function_name": "subprocess.run(args, capture_output=False, timeout=None) -> CompletedProcess", "type": "documentation", "metadata": {"record_id": "bad8970c-0c2b-4d2c-9829-dbe0ce0438ad", "source_type": "documentation", "domain": "OS/Subprocess/File I/O", "library": "subprocess", "title": "subprocess — Process Execution", "content": "The `subprocess.run` function in subprocess runs a subprocess and waits for it to complete. It accepts `args, capture_output=False, timeout=None` as a parameter and returns CompletedProcess. Common use cases include executing shell commands, scripts, or external programs from Python. Note that use shell=False (default) to avoid shell injection vulnerabilities.", "code_signature": "subprocess.run(args, capture_output=False, timeout=None) -> CompletedProcess", "tags": "pipe, file-io, process, tempfile, env-variable, stdin", "url": "https://docs.subprocess.org/en/stable/subprocess/run.html", "author": "Official Docs", "date_published": "31-01-2018", "votes_or_stars": 414, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:a5a7f873-62f1-474f-b938-463fbf945ef4": {"content": "asyncio RuntimeError: This event loop is already running\nThis error is common in Jupyter and FastAPI. Solutions: (1) In async context: just use `await coroutine()`. (2) In Jupyter: `import nest_asyncio; nest_asyncio.apply()`. (3) Run in thread: `asyncio.get_event_loop().run_in_executor(None, sync_func)`. (4) Use `asyncio.ensure_future()` to schedule from within async code.\nCode signature: You're calling asyncio.run() inside an already running event loop. Use await instead.", "file_path": "https://stackoverflow.com/questions/2229110", "function_name": "You're calling asyncio.run() inside an already running event loop. Use await instead.", "type": "stack_overflow", "metadata": {"record_id": "a5a7f873-62f1-474f-b938-463fbf945ef4", "source_type": "stack_overflow", "domain": "Asyncio/Threading", "library": "async", "title": "asyncio RuntimeError: This event loop is already running", "content": "This error is common in Jupyter and FastAPI. Solutions: (1) In async context: just use `await coroutine()`. (2) In Jupyter: `import nest_asyncio; nest_asyncio.apply()`. (3) Run in thread: `asyncio.get_event_loop().run_in_executor(None, sync_func)`. (4) Use `asyncio.ensure_future()` to schedule from within async code.", "code_signature": "You're calling asyncio.run() inside an already running event loop. Use await instead.", "tags": "coroutine, semaphore, gather, lock, await", "url": "https://stackoverflow.com/questions/2229110", "author": "user_573750", "date_published": "18-04-2021", "votes_or_stars": 2306, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:38799911-32c3-4bd7-9baf-d138bd538322": {"content": "Python: how to safely write to a file atomically?\n```python\nimport tempfile, os\nwith tempfile.NamedTemporaryFile('w', delete=False, dir='.') as tmp:\n    tmp.write(data)\n    tmp.flush()\n    os.fsync(tmp.fileno())\nos.replace(tmp.name, target_path)\n```\n`os.replace()` is atomic on POSIX. This prevents partial writes from corrupting files.\nCode signature: Write to a temp file then rename — rename is atomic on POSIX systems.", "file_path": "https://stackoverflow.com/questions/5394880", "function_name": "Write to a temp file then rename — rename is atomic on POSIX systems.", "type": "stack_overflow", "metadata": {"record_id": "38799911-32c3-4bd7-9baf-d138bd538322", "source_type": "stack_overflow", "domain": "OS/Subprocess/File I/O", "library": "stdin", "title": "Python: how to safely write to a file atomically?", "content": "```python\nimport tempfile, os\nwith tempfile.NamedTemporaryFile('w', delete=False, dir='.') as tmp:\n    tmp.write(data)\n    tmp.flush()\n    os.fsync(tmp.fileno())\nos.replace(tmp.name, target_path)\n```\n`os.replace()` is atomic on POSIX. This prevents partial writes from corrupting files.", "code_signature": "Write to a temp file then rename — rename is atomic on POSIX systems.", "tags": "pathlib, stdin, subprocess", "url": "https://stackoverflow.com/questions/5394880", "author": "user_179430", "date_published": "22-02-2019", "votes_or_stars": 567, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:9b7820a6-3923-4081-ad29-f537ea12ac74": {"content": "PERF: requests connection not reused between calls — new TCP connection each time\nProblem: Each requests.get() call opens a new TCP connection, causing high latency.\n\nFix/Discussion: requests.get/post etc. create a new Session per call. Fix: use a persistent Session: `session = requests.Session()` and reuse it. Sessions pool connections via urllib3. For thread-safety: create one Session per thread. For async: use httpx.AsyncClient or aiohttp.ClientSession for async connection pooling.\nCode signature: status:closed", "file_path": "https://github.com/requests/requests/issues/3605", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "9b7820a6-3923-4081-ad29-f537ea12ac74", "source_type": "github_issue", "domain": "Requests/HTTP/APIs", "library": "requests", "title": "PERF: requests connection not reused between calls — new TCP connection each time", "content": "Problem: Each requests.get() call opens a new TCP connection, causing high latency.\n\nFix/Discussion: requests.get/post etc. create a new Session per call. Fix: use a persistent Session: `session = requests.Session()` and reuse it. Sessions pool connections via urllib3. For thread-safety: create one Session per thread. For async: use httpx.AsyncClient or aiohttp.ClientSession for async connection pooling.", "code_signature": "status:closed", "tags": "rate-limiting, headers, timeout, rest-api, aiohttp, websocket", "url": "https://github.com/requests/requests/issues/3605", "author": "contributor_1152", "date_published": "31-12-2022", "votes_or_stars": 391, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:cc1e4b66-c778-4bab-9c65-1700db66e4b5": {"content": "Flask: how to return JSON responses properly?\n`return jsonify({'key': 'value'})` sets Content-Type to application/json. Flask 1.0+ automatically converts returned dicts to JSON responses. For custom status codes: `return jsonify(data), 201`. For errors: use `abort(404)` or return `({'error': 'Not found'}, 404)`.\nCode signature: Use jsonify() or return a dict directly (Flask 1.0+).", "file_path": "https://stackoverflow.com/questions/8930103", "function_name": "Use jsonify() or return a dict directly (Flask 1.0+).", "type": "stack_overflow", "metadata": {"record_id": "cc1e4b66-c778-4bab-9c65-1700db66e4b5", "source_type": "stack_overflow", "domain": "FastAPI/Flask/Django", "library": "django", "title": "Flask: how to return JSON responses properly?", "content": "`return jsonify({'key': 'value'})` sets Content-Type to application/json. Flask 1.0+ automatically converts returned dicts to JSON responses. For custom status codes: `return jsonify(data), 201`. For errors: use `abort(404)` or return `({'error': 'Not found'}, 404)`.", "code_signature": "Use jsonify() or return a dict directly (Flask 1.0+).", "tags": "template, fastapi, routing, orm", "url": "https://stackoverflow.com/questions/8930103", "author": "user_264801", "date_published": "05-08-2023", "votes_or_stars": 2206, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:5050fa50-175a-427a-ac81-db89b158ad0b": {"content": "Why is my Pandas merge creating duplicate rows?\nDuplicate rows after merge usually mean the join key is not unique. Diagnose with `df.duplicated(subset=['key']).sum()`. Use `how='left'` carefully when right table has multiple matches. Set `validate='one_to_many'` or `'many_to_one'` to enforce cardinality. Consider deduplicating with `df.drop_duplicates(subset=['key'])` before merging.\nCode signature: Your merge key has duplicates in one or both DataFrames. Use validate='one_to_one' to catch this early.", "file_path": "https://stackoverflow.com/questions/2321324", "function_name": "Your merge key has duplicates in one or both DataFrames. Use validate='one_to_one' to catch this early.", "type": "stack_overflow", "metadata": {"record_id": "5050fa50-175a-427a-ac81-db89b158ad0b", "source_type": "stack_overflow", "domain": "NumPy/Pandas", "library": "array", "title": "Why is my Pandas merge creating duplicate rows?", "content": "Duplicate rows after merge usually mean the join key is not unique. Diagnose with `df.duplicated(subset=['key']).sum()`. Use `how='left'` carefully when right table has multiple matches. Set `validate='one_to_many'` or `'many_to_one'` to enforce cardinality. Consider deduplicating with `df.drop_duplicates(subset=['key'])` before merging.", "code_signature": "Your merge key has duplicates in one or both DataFrames. Use validate='one_to_one' to catch this early.", "tags": "vectorization, iloc, merge, reshape, pivot", "url": "https://stackoverflow.com/questions/2321324", "author": "user_99814", "date_published": "05-04-2020", "votes_or_stars": 219, "relevance_label": "low", "difficulty_level": "beginner"}}, "kb:b6e7b06c-8709-4878-b7ba-bbcfbd9dc994": {"content": "How to use @app.route in Flask\n`Flask.@app.route` maps a URL rule to a view function. Example usage:\n```python\n@app.route('/users/<int:uid>')\ndef get_user(uid):\n    return jsonify(user)\n```\nThis is useful when defining HTTP endpoints in a Flask application. Performance tip: use Blueprints to organize large applications.\nCode signature: @app.route", "file_path": "https://docs.flask.org/en/stable/@app/route.html", "function_name": "@app.route", "type": "documentation", "metadata": {"record_id": "b6e7b06c-8709-4878-b7ba-bbcfbd9dc994", "source_type": "documentation", "domain": "FastAPI/Flask/Django", "library": "Flask", "title": "How to use @app.route in Flask", "content": "`Flask.@app.route` maps a URL rule to a view function. Example usage:\n```python\n@app.route('/users/<int:uid>')\ndef get_user(uid):\n    return jsonify(user)\n```\nThis is useful when defining HTTP endpoints in a Flask application. Performance tip: use Blueprints to organize large applications.", "code_signature": "@app.route", "tags": "routing, dependency-injection, response, middleware, fastapi", "url": "https://docs.flask.org/en/stable/@app/route.html", "author": "Official Docs", "date_published": "11-05-2023", "votes_or_stars": 3001, "relevance_label": "low", "difficulty_level": "beginner"}}, "kb:e6220e3e-9c10-4655-addd-d90e1fc87570": {"content": "BUG: SettingWithCopyWarning when modifying sliced DataFrame\nProblem: Modifying a slice of a DataFrame raises SettingWithCopyWarning and may not update original.\n\nFix/Discussion: Always use .loc or .copy() when slicing. Use `df.loc[mask, 'col'] = val` for assignment. Create explicit copies: `subset = df[mask].copy()`. In Pandas 2.0+, Copy-on-Write makes this behavior consistent — enable with `pd.options.mode.copy_on_write = True`.\nCode signature: status:closed", "file_path": "https://github.com/pandas/pandas/issues/3264", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "e6220e3e-9c10-4655-addd-d90e1fc87570", "source_type": "github_issue", "domain": "NumPy/Pandas", "library": "pandas", "title": "BUG: SettingWithCopyWarning when modifying sliced DataFrame", "content": "Problem: Modifying a slice of a DataFrame raises SettingWithCopyWarning and may not update original.\n\nFix/Discussion: Always use .loc or .copy() when slicing. Use `df.loc[mask, 'col'] = val` for assignment. Create explicit copies: `subset = df[mask].copy()`. In Pandas 2.0+, Copy-on-Write makes this behavior consistent — enable with `pd.options.mode.copy_on_write = True`.", "code_signature": "status:closed", "tags": "dtype, nan, iloc, vectorization, merge, numpy", "url": "https://github.com/pandas/pandas/issues/3264", "author": "contributor_6628", "date_published": "18-04-2019", "votes_or_stars": 35, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:8e092a24-0501-4720-8714-fe2799756f1f": {"content": "How to use asyncio.create_task in asyncio\n`asyncio.asyncio.create_task` schedules a coroutine as a Task in the event loop. Example usage:\n```python\ntask = asyncio.create_task(send_email(user))\nawait task\n```\nThis is useful when fire-and-forget background tasks, concurrent task management. Performance tip: use TaskGroup (Python 3.11+) for structured concurrency.\nCode signature: asyncio.create_task", "file_path": "https://docs.asyncio.org/en/stable/asyncio/create_task.html", "function_name": "asyncio.create_task", "type": "documentation", "metadata": {"record_id": "8e092a24-0501-4720-8714-fe2799756f1f", "source_type": "documentation", "domain": "Asyncio/Threading", "library": "asyncio", "title": "How to use asyncio.create_task in asyncio", "content": "`asyncio.asyncio.create_task` schedules a coroutine as a Task in the event loop. Example usage:\n```python\ntask = asyncio.create_task(send_email(user))\nawait task\n```\nThis is useful when fire-and-forget background tasks, concurrent task management. Performance tip: use TaskGroup (Python 3.11+) for structured concurrency.", "code_signature": "asyncio.create_task", "tags": "semaphore, race-condition, task, gather, executor", "url": "https://docs.asyncio.org/en/stable/asyncio/create_task.html", "author": "Official Docs", "date_published": "25-09-2018", "votes_or_stars": 4161, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:20c43b14-ce57-4bd3-a92a-f54eeae50530": {"content": "FEAT: pathlib: add support for reading/writing with encoding shorthand\nProblem: pathlib.Path.read_text() does not default to UTF-8, causing issues across platforms.\n\nFix/Discussion: Always pass encoding explicitly: `Path('file').read_text(encoding='utf-8')`. Python 3.10+ added `encoding='locale'` default change proposal. Best practice: always specify encoding in all file I/O operations for portability.\nCode signature: status:closed", "file_path": "https://github.com/cpython/cpython/issues/2707", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "20c43b14-ce57-4bd3-a92a-f54eeae50530", "source_type": "github_issue", "domain": "OS/Subprocess/File I/O", "library": "cpython", "title": "FEAT: pathlib: add support for reading/writing with encoding shorthand", "content": "Problem: pathlib.Path.read_text() does not default to UTF-8, causing issues across platforms.\n\nFix/Discussion: Always pass encoding explicitly: `Path('file').read_text(encoding='utf-8')`. Python 3.10+ added `encoding='locale'` default change proposal. Best practice: always specify encoding in all file I/O operations for portability.", "code_signature": "status:closed", "tags": "os, subprocess, process, tempfile", "url": "https://github.com/cpython/cpython/issues/2707", "author": "contributor_7877", "date_published": "19-08-2018", "votes_or_stars": 440, "relevance_label": "low", "difficulty_level": "intermediate"}}, "kb:63400391-8b82-433c-9a8c-3a7c699ab454": {"content": "How to add retry logic to requests?\n```python\nfrom requests.adapters import HTTPAdapter\nfrom urllib3.util.retry import Retry\nretry = Retry(total=3, backoff_factor=1, status_forcelist=[500,502,503,504])\nadapter = HTTPAdapter(max_retries=retry)\nsession.mount('https://', adapter)\n```\nCode signature: Mount an HTTPAdapter with urllib3 Retry on the Session.", "file_path": "https://stackoverflow.com/questions/1669324", "function_name": "Mount an HTTPAdapter with urllib3 Retry on the Session.", "type": "stack_overflow", "metadata": {"record_id": "63400391-8b82-433c-9a8c-3a7c699ab454", "source_type": "stack_overflow", "domain": "Requests/HTTP/APIs", "library": "websocket", "title": "How to add retry logic to requests?", "content": "```python\nfrom requests.adapters import HTTPAdapter\nfrom urllib3.util.retry import Retry\nretry = Retry(total=3, backoff_factor=1, status_forcelist=[500,502,503,504])\nadapter = HTTPAdapter(max_retries=retry)\nsession.mount('https://', adapter)\n```", "code_signature": "Mount an HTTPAdapter with urllib3 Retry on the Session.", "tags": "rest-api, timeout, httpx, requests, aiohttp, rate-limiting", "url": "https://stackoverflow.com/questions/1669324", "author": "user_952590", "date_published": "22-05-2021", "votes_or_stars": 1503, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:2c6a55c4-451a-4638-acbd-83c4326ded0f": {"content": "How to apply a function to each row in Pandas without using iterrows?\n`iterrows()` is slow because it creates a Series per row. Prefer `df.apply(func, axis=1)` for row-wise operations, or better yet, vectorize using NumPy: `np.where(condition, a, b)`. For complex logic, `df.assign()` with vectorized expressions is cleanest. Benchmark: vectorized > apply > itertuples > iterrows.\nCode signature: Use df.apply(func, axis=1) or vectorized NumPy operations for best performance.", "file_path": "https://stackoverflow.com/questions/5446912", "function_name": "Use df.apply(func, axis=1) or vectorized NumPy operations for best performance.", "type": "stack_overflow", "metadata": {"record_id": "2c6a55c4-451a-4638-acbd-83c4326ded0f", "source_type": "stack_overflow", "domain": "NumPy/Pandas", "library": "dataframe", "title": "How to apply a function to each row in Pandas without using iterrows?", "content": "`iterrows()` is slow because it creates a Series per row. Prefer `df.apply(func, axis=1)` for row-wise operations, or better yet, vectorize using NumPy: `np.where(condition, a, b)`. For complex logic, `df.assign()` with vectorized expressions is cleanest. Benchmark: vectorized > apply > itertuples > iterrows.", "code_signature": "Use df.apply(func, axis=1) or vectorized NumPy operations for best performance.", "tags": "groupby, iloc, vectorization, pandas", "url": "https://stackoverflow.com/questions/5446912", "author": "user_563306", "date_published": "06-11-2019", "votes_or_stars": 2241, "relevance_label": "low", "difficulty_level": "intermediate"}}, "kb:a92277aa-9ee9-4fdd-937e-9da8f7b73208": {"content": "asyncio RuntimeError: This event loop is already running\nThis error is common in Jupyter and FastAPI. Solutions: (1) In async context: just use `await coroutine()`. (2) In Jupyter: `import nest_asyncio; nest_asyncio.apply()`. (3) Run in thread: `asyncio.get_event_loop().run_in_executor(None, sync_func)`. (4) Use `asyncio.ensure_future()` to schedule from within async code.\nCode signature: You're calling asyncio.run() inside an already running event loop. Use await instead.", "file_path": "https://stackoverflow.com/questions/2229110", "function_name": "You're calling asyncio.run() inside an already running event loop. Use await instead.", "type": "stack_overflow", "metadata": {"record_id": "a92277aa-9ee9-4fdd-937e-9da8f7b73208", "source_type": "stack_overflow", "domain": "Asyncio/Threading", "library": "async", "title": "asyncio RuntimeError: This event loop is already running", "content": "This error is common in Jupyter and FastAPI. Solutions: (1) In async context: just use `await coroutine()`. (2) In Jupyter: `import nest_asyncio; nest_asyncio.apply()`. (3) Run in thread: `asyncio.get_event_loop().run_in_executor(None, sync_func)`. (4) Use `asyncio.ensure_future()` to schedule from within async code.", "code_signature": "You're calling asyncio.run() inside an already running event loop. Use await instead.", "tags": "gather, executor, await, task", "url": "https://stackoverflow.com/questions/2229110", "author": "user_573750", "date_published": "28-04-2022", "votes_or_stars": 2298, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:b37d3a2a-e6b8-40f9-9c72-2409eecf91d7": {"content": "BUG: SQLAlchemy connection pool exhaustion under load\nProblem: Application hangs under concurrent load — all connections are checked out and pool is exhausted.\n\nFix/Discussion: Increase `pool_size` and `max_overflow`. Ensure sessions are always closed: use `contextmanager` or `with Session() as session:`. Set `pool_timeout` to fail fast. Enable `pool_pre_ping=True`. Monitor with `engine.pool.checkedout()`. In async: use `AsyncSession` with `async_scoped_session`.\nCode signature: status:closed", "file_path": "https://github.com/sqlalchemy/sqlalchemy/issues/106", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "b37d3a2a-e6b8-40f9-9c72-2409eecf91d7", "source_type": "github_issue", "domain": "SQLAlchemy/Databases", "library": "sqlalchemy", "title": "BUG: SQLAlchemy connection pool exhaustion under load", "content": "Problem: Application hangs under concurrent load — all connections are checked out and pool is exhausted.\n\nFix/Discussion: Increase `pool_size` and `max_overflow`. Ensure sessions are always closed: use `contextmanager` or `with Session() as session:`. Set `pool_timeout` to fail fast. Enable `pool_pre_ping=True`. Monitor with `engine.pool.checkedout()`. In async: use `AsyncSession` with `async_scoped_session`.", "code_signature": "status:closed", "tags": "sqlalchemy, sqlite, relationship, orm, connection-pool, query", "url": "https://github.com/sqlalchemy/sqlalchemy/issues/106", "author": "contributor_5078", "date_published": "20-02-2020", "votes_or_stars": 300, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:3b0b9cba-3b53-44d1-ae1c-e6d79efad2b5": {"content": "How to run asyncio code from synchronous Python?\n`asyncio.run(main())` creates an event loop, runs the coroutine, closes the loop. Never call asyncio.run() from inside a running event loop (use await instead). In Jupyter notebooks (which have a running loop), use `await coroutine()` directly or `nest_asyncio.apply()`. For mixing sync/async, use `loop.run_until_complete()`.\nCode signature: Use asyncio.run() in Python 3.7+ to execute a coroutine from sync code.", "file_path": "https://stackoverflow.com/questions/2140194", "function_name": "Use asyncio.run() in Python 3.7+ to execute a coroutine from sync code.", "type": "stack_overflow", "metadata": {"record_id": "3b0b9cba-3b53-44d1-ae1c-e6d79efad2b5", "source_type": "stack_overflow", "domain": "Asyncio/Threading", "library": "gather", "title": "How to run asyncio code from synchronous Python?", "content": "`asyncio.run(main())` creates an event loop, runs the coroutine, closes the loop. Never call asyncio.run() from inside a running event loop (use await instead). In Jupyter notebooks (which have a running loop), use `await coroutine()` directly or `nest_asyncio.apply()`. For mixing sync/async, use `loop.run_until_complete()`.", "code_signature": "Use asyncio.run() in Python 3.7+ to execute a coroutine from sync code.", "tags": "event-loop, deadlock, race-condition, executor, threading", "url": "https://stackoverflow.com/questions/2140194", "author": "user_718011", "date_published": "28-03-2020", "votes_or_stars": 275, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:5cad7d1e-18f7-4c02-b505-90726720c9e3": {"content": "SQLAlchemy: how to avoid DetachedInstanceError?\nDetachedInstanceError occurs when accessing a relationship after the session is closed. Solutions: (1) Use `joinedload()` or `subqueryload()` to eagerly load relationships. (2) Keep session open while accessing objects. (3) Use `expire_on_commit=False` on sessionmaker. (4) Convert to dict/DTO before closing session.\nCode signature: Access lazy-loaded relationships within the session scope, or use eager loading.", "file_path": "https://stackoverflow.com/questions/1247595", "function_name": "Access lazy-loaded relationships within the session scope, or use eager loading.", "type": "stack_overflow", "metadata": {"record_id": "5cad7d1e-18f7-4c02-b505-90726720c9e3", "source_type": "stack_overflow", "domain": "SQLAlchemy/Databases", "library": "sqlalchemy", "title": "SQLAlchemy: how to avoid DetachedInstanceError?", "content": "DetachedInstanceError occurs when accessing a relationship after the session is closed. Solutions: (1) Use `joinedload()` or `subqueryload()` to eagerly load relationships. (2) Keep session open while accessing objects. (3) Use `expire_on_commit=False` on sessionmaker. (4) Convert to dict/DTO before closing session.", "code_signature": "Access lazy-loaded relationships within the session scope, or use eager loading.", "tags": "alembic, sqlalchemy, sqlite", "url": "https://stackoverflow.com/questions/1247595", "author": "user_107793", "date_published": "01-07-2024", "votes_or_stars": 395, "relevance_label": "low", "difficulty_level": "advanced"}}, "kb:cca621c4-c5d7-4df2-a068-a41513162c4c": {"content": "How do I efficiently fill NaN values in a Pandas DataFrame?\nFor large DataFrames, avoid looping. Use `df.fillna({'col': val})` to fill specific columns. `df.ffill()` propagates last valid observation forward. For time-series data, `df.interpolate()` provides smoother results. Always check `df.isna().sum()` before and after to verify.\nCode signature: Use df.fillna(value) or df.ffill()/df.bfill() for forward/backward fill.", "file_path": "https://stackoverflow.com/questions/6438436", "function_name": "Use df.fillna(value) or df.ffill()/df.bfill() for forward/backward fill.", "type": "stack_overflow", "metadata": {"record_id": "cca621c4-c5d7-4df2-a068-a41513162c4c", "source_type": "stack_overflow", "domain": "NumPy/Pandas", "library": "loc", "title": "How do I efficiently fill NaN values in a Pandas DataFrame?", "content": "For large DataFrames, avoid looping. Use `df.fillna({'col': val})` to fill specific columns. `df.ffill()` propagates last valid observation forward. For time-series data, `df.interpolate()` provides smoother results. Always check `df.isna().sum()` before and after to verify.", "code_signature": "Use df.fillna(value) or df.ffill()/df.bfill() for forward/backward fill.", "tags": "pivot, dataframe, reshape, dtype, numpy, loc", "url": "https://stackoverflow.com/questions/6438436", "author": "user_522340", "date_published": "06-08-2018", "votes_or_stars": 1, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:7b7327c5-f2dd-4db2-9167-a553a562351d": {"content": "NumPy broadcasting error: operands could not be broadcast with shapes\nNumPy broadcasts shapes right-to-left. Arrays (3,4) and (4,) are compatible. Arrays (3,4) and (3,) are not — reshape to (3,1). Use `arr.reshape(-1,1)` to add a dimension. Always print `a.shape, b.shape` before operations to debug. `np.expand_dims(arr, axis=1)` is cleaner than reshape for adding dimensions.\nCode signature: Shapes must be compatible: dimensions must match or one of them must be 1.", "file_path": "https://stackoverflow.com/questions/4860684", "function_name": "Shapes must be compatible: dimensions must match or one of them must be 1.", "type": "stack_overflow", "metadata": {"record_id": "7b7327c5-f2dd-4db2-9167-a553a562351d", "source_type": "stack_overflow", "domain": "NumPy/Pandas", "library": "numpy", "title": "NumPy broadcasting error: operands could not be broadcast with shapes", "content": "NumPy broadcasts shapes right-to-left. Arrays (3,4) and (4,) are compatible. Arrays (3,4) and (3,) are not — reshape to (3,1). Use `arr.reshape(-1,1)` to add a dimension. Always print `a.shape, b.shape` before operations to debug. `np.expand_dims(arr, axis=1)` is cleaner than reshape for adding dimensions.", "code_signature": "Shapes must be compatible: dimensions must match or one of them must be 1.", "tags": "merge, reshape, loc, numpy, nan, broadcasting", "url": "https://stackoverflow.com/questions/4860684", "author": "user_627024", "date_published": "01-02-2024", "votes_or_stars": 1393, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:9b4c1039-94bb-4919-bf34-d07b2a54439b": {"content": "BUG: threading.Timer not cancellable after it fires\nProblem: Calling cancel() on a Timer that has already executed does not raise an error but has no effect.\n\nFix/Discussion: threading.Timer.cancel() only works before the timer fires. Store Timer reference and check `timer.finished.is_set()` to know if it's already run. For repeating timers, use a loop with `threading.Event` for clean cancellation: `stop_event = threading.Event(); stop_event.wait(interval)`.\nCode signature: status:closed", "file_path": "https://github.com/cpython/cpython/issues/4111", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "9b4c1039-94bb-4919-bf34-d07b2a54439b", "source_type": "github_issue", "domain": "Asyncio/Threading", "library": "cpython", "title": "BUG: threading.Timer not cancellable after it fires", "content": "Problem: Calling cancel() on a Timer that has already executed does not raise an error but has no effect.\n\nFix/Discussion: threading.Timer.cancel() only works before the timer fires. Store Timer reference and check `timer.finished.is_set()` to know if it's already run. For repeating timers, use a loop with `threading.Event` for clean cancellation: `stop_event = threading.Event(); stop_event.wait(interval)`.", "code_signature": "status:closed", "tags": "threading, task, gather", "url": "https://github.com/cpython/cpython/issues/4111", "author": "contributor_7884", "date_published": "22-11-2020", "votes_or_stars": 38, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:fa5d7323-c258-4c91-a861-96104f03d838": {"content": "Django N+1 query problem: how to fix?\nN+1 occurs when accessing related objects in a loop. Diagnose with `django-debug-toolbar` or `connection.queries`. Fix ForeignKey: `User.objects.select_related('profile').all()`. Fix M2M: `Post.objects.prefetch_related('tags').all()`. Use `only()` to fetch specific fields and reduce data transfer.\nCode signature: Use select_related() for ForeignKey and prefetch_related() for ManyToMany relationships.", "file_path": "https://stackoverflow.com/questions/8106470", "function_name": "Use select_related() for ForeignKey and prefetch_related() for ManyToMany relationships.", "type": "stack_overflow", "metadata": {"record_id": "fa5d7323-c258-4c91-a861-96104f03d838", "source_type": "stack_overflow", "domain": "FastAPI/Flask/Django", "library": "request", "title": "Django N+1 query problem: how to fix?", "content": "N+1 occurs when accessing related objects in a loop. Diagnose with `django-debug-toolbar` or `connection.queries`. Fix ForeignKey: `User.objects.select_related('profile').all()`. Fix M2M: `Post.objects.prefetch_related('tags').all()`. Use `only()` to fetch specific fields and reduce data transfer.", "code_signature": "Use select_related() for ForeignKey and prefetch_related() for ManyToMany relationships.", "tags": "flask, rest, template, routing, request, blueprint", "url": "https://stackoverflow.com/questions/8106470", "author": "user_441071", "date_published": "25-05-2024", "votes_or_stars": 1769, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:66e556fa-a0a1-464f-a6f4-20b4e129c9ec": {"content": "Python threading: how to share data between threads safely?\nFor simple counters: use `threading.Lock()` as context manager. For complex shared state: prefer `queue.Queue` (thread-safe by design). Avoid global variables without locks. `threading.local()` gives each thread its own data copy. For read-heavy workloads, `threading.RLock` allows re-entrant acquisition.\nCode signature: Use threading.Lock for mutual exclusion or queue.Queue for producer-consumer patterns.", "file_path": "https://stackoverflow.com/questions/3195773", "function_name": "Use threading.Lock for mutual exclusion or queue.Queue for producer-consumer patterns.", "type": "stack_overflow", "metadata": {"record_id": "66e556fa-a0a1-464f-a6f4-20b4e129c9ec", "source_type": "stack_overflow", "domain": "Asyncio/Threading", "library": "lock", "title": "Python threading: how to share data between threads safely?", "content": "For simple counters: use `threading.Lock()` as context manager. For complex shared state: prefer `queue.Queue` (thread-safe by design). Avoid global variables without locks. `threading.local()` gives each thread its own data copy. For read-heavy workloads, `threading.RLock` allows re-entrant acquisition.", "code_signature": "Use threading.Lock for mutual exclusion or queue.Queue for producer-consumer patterns.", "tags": "task, lock, threading, await, semaphore, executor", "url": "https://stackoverflow.com/questions/3195773", "author": "user_714318", "date_published": "29-04-2020", "votes_or_stars": 1102, "relevance_label": "low", "difficulty_level": "intermediate"}}, "kb:cb1f3f90-f0b3-48c8-8c4f-d5492e1055dc": {"content": "subprocess.run vs Popen: which should I use?\n`subprocess.run()` is a high-level wrapper that waits for completion — use for most cases. `subprocess.Popen()` gives full control: stream stdout/stderr in real-time, write to stdin, run processes concurrently. For capturing output: `subprocess.run(..., capture_output=True, text=True)`. Always use `shell=False` and pass args as a list to prevent shell injection.\nCode signature: Use subprocess.run() for simple cases; Popen for streaming output or interactive processes.", "file_path": "https://stackoverflow.com/questions/5727085", "function_name": "Use subprocess.run() for simple cases; Popen for streaming output or interactive processes.", "type": "stack_overflow", "metadata": {"record_id": "cb1f3f90-f0b3-48c8-8c4f-d5492e1055dc", "source_type": "stack_overflow", "domain": "OS/Subprocess/File I/O", "library": "shutil", "title": "subprocess.run vs Popen: which should I use?", "content": "`subprocess.run()` is a high-level wrapper that waits for completion — use for most cases. `subprocess.Popen()` gives full control: stream stdout/stderr in real-time, write to stdin, run processes concurrently. For capturing output: `subprocess.run(..., capture_output=True, text=True)`. Always use `shell=False` and pass args as a list to prevent shell injection.", "code_signature": "Use subprocess.run() for simple cases; Popen for streaming output or interactive processes.", "tags": "stderr, pipe, os, shutil", "url": "https://stackoverflow.com/questions/5727085", "author": "user_644210", "date_published": "03-06-2020", "votes_or_stars": 668, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:d7faab7f-3280-4954-8e0b-21e2f24e1d62": {"content": "How to implement database connection pooling with SQLAlchemy?\n`create_engine(url, pool_size=10, max_overflow=20, pool_timeout=30, pool_recycle=1800)`. Use `pool_pre_ping=True` to detect stale connections. For async: use `AsyncEngine` with `create_async_engine()`. Monitor with `engine.pool.status()`. NullPool disables pooling for scripts/serverless environments.\nCode signature: create_engine() has built-in pooling. Configure pool_size and max_overflow.", "file_path": "https://stackoverflow.com/questions/7358113", "function_name": "create_engine() has built-in pooling. Configure pool_size and max_overflow.", "type": "stack_overflow", "metadata": {"record_id": "d7faab7f-3280-4954-8e0b-21e2f24e1d62", "source_type": "stack_overflow", "domain": "SQLAlchemy/Databases", "library": "session", "title": "How to implement database connection pooling with SQLAlchemy?", "content": "`create_engine(url, pool_size=10, max_overflow=20, pool_timeout=30, pool_recycle=1800)`. Use `pool_pre_ping=True` to detect stale connections. For async: use `AsyncEngine` with `create_async_engine()`. Monitor with `engine.pool.status()`. NullPool disables pooling for scripts/serverless environments.", "code_signature": "create_engine() has built-in pooling. Configure pool_size and max_overflow.", "tags": "transaction, sqlite, migration", "url": "https://stackoverflow.com/questions/7358113", "author": "user_12260", "date_published": "29-12-2020", "votes_or_stars": 241, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:8130c8d2-8b73-4e7c-bc25-1128faee5996": {"content": "subprocess.run vs Popen: which should I use?\n`subprocess.run()` is a high-level wrapper that waits for completion — use for most cases. `subprocess.Popen()` gives full control: stream stdout/stderr in real-time, write to stdin, run processes concurrently. For capturing output: `subprocess.run(..., capture_output=True, text=True)`. Always use `shell=False` and pass args as a list to prevent shell injection.\nCode signature: Use subprocess.run() for simple cases; Popen for streaming output or interactive processes.", "file_path": "https://stackoverflow.com/questions/5727085", "function_name": "Use subprocess.run() for simple cases; Popen for streaming output or interactive processes.", "type": "stack_overflow", "metadata": {"record_id": "8130c8d2-8b73-4e7c-bc25-1128faee5996", "source_type": "stack_overflow", "domain": "OS/Subprocess/File I/O", "library": "shutil", "title": "subprocess.run vs Popen: which should I use?", "content": "`subprocess.run()` is a high-level wrapper that waits for completion — use for most cases. `subprocess.Popen()` gives full control: stream stdout/stderr in real-time, write to stdin, run processes concurrently. For capturing output: `subprocess.run(..., capture_output=True, text=True)`. Always use `shell=False` and pass args as a list to prevent shell injection.", "code_signature": "Use subprocess.run() for simple cases; Popen for streaming output or interactive processes.", "tags": "pipe, tempfile, shutil, stdout, os", "url": "https://stackoverflow.com/questions/5727085", "author": "user_644210", "date_published": "26-01-2024", "votes_or_stars": 665, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:c622b562-7ddf-4c84-8539-dc014e9da1e4": {"content": "NumPy — Vectorization\nThe `np.vectorize` function in NumPy generalizes a scalar function to operate on arrays. It accepts `pyfunc` as a parameter and returns vectorized function. Common use cases include applying Python functions element-wise without explicit loops. Note that np.vectorize is a convenience wrapper; it does not improve performance like true ufuncs.\nCode signature: np.vectorize(pyfunc) -> vectorized function", "file_path": "https://docs.numpy.org/en/stable/np/vectorize.html", "function_name": "np.vectorize(pyfunc) -> vectorized function", "type": "documentation", "metadata": {"record_id": "c622b562-7ddf-4c84-8539-dc014e9da1e4", "source_type": "documentation", "domain": "NumPy/Pandas", "library": "NumPy", "title": "NumPy — Vectorization", "content": "The `np.vectorize` function in NumPy generalizes a scalar function to operate on arrays. It accepts `pyfunc` as a parameter and returns vectorized function. Common use cases include applying Python functions element-wise without explicit loops. Note that np.vectorize is a convenience wrapper; it does not improve performance like true ufuncs.", "code_signature": "np.vectorize(pyfunc) -> vectorized function", "tags": "numpy, vectorization, reshape, iloc, pandas", "url": "https://docs.numpy.org/en/stable/np/vectorize.html", "author": "Official Docs", "date_published": "21-01-2021", "votes_or_stars": 1316, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:6b4b17e8-6386-4055-9bb6-a1e72a6934b3": {"content": "How to implement database connection pooling with SQLAlchemy?\n`create_engine(url, pool_size=10, max_overflow=20, pool_timeout=30, pool_recycle=1800)`. Use `pool_pre_ping=True` to detect stale connections. For async: use `AsyncEngine` with `create_async_engine()`. Monitor with `engine.pool.status()`. NullPool disables pooling for scripts/serverless environments.\nCode signature: create_engine() has built-in pooling. Configure pool_size and max_overflow.", "file_path": "https://stackoverflow.com/questions/7358113", "function_name": "create_engine() has built-in pooling. Configure pool_size and max_overflow.", "type": "stack_overflow", "metadata": {"record_id": "6b4b17e8-6386-4055-9bb6-a1e72a6934b3", "source_type": "stack_overflow", "domain": "SQLAlchemy/Databases", "library": "session", "title": "How to implement database connection pooling with SQLAlchemy?", "content": "`create_engine(url, pool_size=10, max_overflow=20, pool_timeout=30, pool_recycle=1800)`. Use `pool_pre_ping=True` to detect stale connections. For async: use `AsyncEngine` with `create_async_engine()`. Monitor with `engine.pool.status()`. NullPool disables pooling for scripts/serverless environments.", "code_signature": "create_engine() has built-in pooling. Configure pool_size and max_overflow.", "tags": "query, connection-pool, transaction, sqlite, relationship", "url": "https://stackoverflow.com/questions/7358113", "author": "user_12260", "date_published": "24-03-2023", "votes_or_stars": 240, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:ce3726a9-66bb-4ac7-8923-73f8f7ccb96e": {"content": "How to implement a thread pool in Python?\n```python\nfrom concurrent.futures import ThreadPoolExecutor\nwith ThreadPoolExecutor(max_workers=8) as ex:\n    futures = [ex.submit(task, arg) for arg in args]\n    results = [f.result() for f in futures]\n```\nFor CPU-bound: use ProcessPoolExecutor instead. For async integration: `await loop.run_in_executor(executor, func, *args)`.\nCode signature: Use concurrent.futures.ThreadPoolExecutor for I/O-bound tasks.", "file_path": "https://stackoverflow.com/questions/3646552", "function_name": "Use concurrent.futures.ThreadPoolExecutor for I/O-bound tasks.", "type": "stack_overflow", "metadata": {"record_id": "ce3726a9-66bb-4ac7-8923-73f8f7ccb96e", "source_type": "stack_overflow", "domain": "Asyncio/Threading", "library": "coroutine", "title": "How to implement a thread pool in Python?", "content": "```python\nfrom concurrent.futures import ThreadPoolExecutor\nwith ThreadPoolExecutor(max_workers=8) as ex:\n    futures = [ex.submit(task, arg) for arg in args]\n    results = [f.result() for f in futures]\n```\nFor CPU-bound: use ProcessPoolExecutor instead. For async integration: `await loop.run_in_executor(executor, func, *args)`.", "code_signature": "Use concurrent.futures.ThreadPoolExecutor for I/O-bound tasks.", "tags": "coroutine, asyncio, event-loop, deadlock, gather, semaphore", "url": "https://stackoverflow.com/questions/3646552", "author": "user_469469", "date_published": "04-11-2018", "votes_or_stars": 1503, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:7c79f4f3-13a0-402e-986c-fbd6d7619809": {"content": "How to stream large HTTP responses with requests?\n```python\nwith requests.get(url, stream=True) as r:\n    r.raise_for_status()\n    with open('file', 'wb') as f:\n        for chunk in r.iter_content(chunk_size=8192):\n            f.write(chunk)\n```\nAlways use as context manager with stream=True to ensure connection is released.\nCode signature: Use stream=True and iterate over response.iter_content() or iter_lines().", "file_path": "https://stackoverflow.com/questions/3592974", "function_name": "Use stream=True and iterate over response.iter_content() or iter_lines().", "type": "stack_overflow", "metadata": {"record_id": "7c79f4f3-13a0-402e-986c-fbd6d7619809", "source_type": "stack_overflow", "domain": "Requests/HTTP/APIs", "library": "json", "title": "How to stream large HTTP responses with requests?", "content": "```python\nwith requests.get(url, stream=True) as r:\n    r.raise_for_status()\n    with open('file', 'wb') as f:\n        for chunk in r.iter_content(chunk_size=8192):\n            f.write(chunk)\n```\nAlways use as context manager with stream=True to ensure connection is released.", "code_signature": "Use stream=True and iterate over response.iter_content() or iter_lines().", "tags": "rest-api, rate-limiting, httpx, json, aiohttp, headers", "url": "https://stackoverflow.com/questions/3592974", "author": "user_980733", "date_published": "19-09-2023", "votes_or_stars": 1658, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:a0def889-3002-42c3-85b5-191496039f7b": {"content": "How to implement JWT authentication in FastAPI?\n```python\nfrom fastapi.security import OAuth2PasswordBearer\nfrom jose import jwt\noauth2_scheme = OAuth2PasswordBearer(tokenUrl='token')\n```\nCreate access tokens with `jwt.encode(payload, SECRET_KEY, algorithm='HS256')`. Verify with `jwt.decode(token, SECRET_KEY, algorithms=['HS256'])`. Store SECRET_KEY in environment variables, never in code.\nCode signature: Use python-jose for JWT and passlib for password hashing with OAuth2PasswordBearer.", "file_path": "https://stackoverflow.com/questions/7754864", "function_name": "Use python-jose for JWT and passlib for password hashing with OAuth2PasswordBearer.", "type": "stack_overflow", "metadata": {"record_id": "a0def889-3002-42c3-85b5-191496039f7b", "source_type": "stack_overflow", "domain": "FastAPI/Flask/Django", "library": "fastapi", "title": "How to implement JWT authentication in FastAPI?", "content": "```python\nfrom fastapi.security import OAuth2PasswordBearer\nfrom jose import jwt\noauth2_scheme = OAuth2PasswordBearer(tokenUrl='token')\n```\nCreate access tokens with `jwt.encode(payload, SECRET_KEY, algorithm='HS256')`. Verify with `jwt.decode(token, SECRET_KEY, algorithms=['HS256'])`. Store SECRET_KEY in environment variables, never in code.", "code_signature": "Use python-jose for JWT and passlib for password hashing with OAuth2PasswordBearer.", "tags": "template, dependency-injection, django, orm, routing, flask", "url": "https://stackoverflow.com/questions/7754864", "author": "user_773587", "date_published": "30-04-2019", "votes_or_stars": 396, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:9d62678e-54a7-4427-8bda-6ba66d1a94bd": {"content": "PERF: requests connection not reused between calls — new TCP connection each time\nProblem: Each requests.get() call opens a new TCP connection, causing high latency.\n\nFix/Discussion: requests.get/post etc. create a new Session per call. Fix: use a persistent Session: `session = requests.Session()` and reuse it. Sessions pool connections via urllib3. For thread-safety: create one Session per thread. For async: use httpx.AsyncClient or aiohttp.ClientSession for async connection pooling.\nCode signature: status:closed", "file_path": "https://github.com/requests/requests/issues/3605", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "9d62678e-54a7-4427-8bda-6ba66d1a94bd", "source_type": "github_issue", "domain": "Requests/HTTP/APIs", "library": "requests", "title": "PERF: requests connection not reused between calls — new TCP connection each time", "content": "Problem: Each requests.get() call opens a new TCP connection, causing high latency.\n\nFix/Discussion: requests.get/post etc. create a new Session per call. Fix: use a persistent Session: `session = requests.Session()` and reuse it. Sessions pool connections via urllib3. For thread-safety: create one Session per thread. For async: use httpx.AsyncClient or aiohttp.ClientSession for async connection pooling.", "code_signature": "status:closed", "tags": "authentication, rate-limiting, json, aiohttp, timeout, httpx", "url": "https://github.com/requests/requests/issues/3605", "author": "contributor_1152", "date_published": "09-02-2019", "votes_or_stars": 395, "relevance_label": "low", "difficulty_level": "beginner"}}, "kb:098d9d47-d2a7-4b5c-b1c3-2f055d13b7b2": {"content": "How do I efficiently fill NaN values in a Pandas DataFrame?\nFor large DataFrames, avoid looping. Use `df.fillna({'col': val})` to fill specific columns. `df.ffill()` propagates last valid observation forward. For time-series data, `df.interpolate()` provides smoother results. Always check `df.isna().sum()` before and after to verify.\nCode signature: Use df.fillna(value) or df.ffill()/df.bfill() for forward/backward fill.", "file_path": "https://stackoverflow.com/questions/6438436", "function_name": "Use df.fillna(value) or df.ffill()/df.bfill() for forward/backward fill.", "type": "stack_overflow", "metadata": {"record_id": "098d9d47-d2a7-4b5c-b1c3-2f055d13b7b2", "source_type": "stack_overflow", "domain": "NumPy/Pandas", "library": "loc", "title": "How do I efficiently fill NaN values in a Pandas DataFrame?", "content": "For large DataFrames, avoid looping. Use `df.fillna({'col': val})` to fill specific columns. `df.ffill()` propagates last valid observation forward. For time-series data, `df.interpolate()` provides smoother results. Always check `df.isna().sum()` before and after to verify.", "code_signature": "Use df.fillna(value) or df.ffill()/df.bfill() for forward/backward fill.", "tags": "pandas, dataframe, numpy, pivot", "url": "https://stackoverflow.com/questions/6438436", "author": "user_522340", "date_published": "28-05-2019", "votes_or_stars": 27, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:4b519eb6-677c-4193-bc40-c1be1b86a910": {"content": "How to handle database migrations with Alembic?\nWorkflow: (1) Change SQLAlchemy models. (2) `alembic revision --autogenerate -m 'description'`. (3) Review generated migration script. (4) `alembic upgrade head` to apply. Always review autogenerated migrations — they miss some changes (indexes, constraints). Use `alembic downgrade -1` to rollback. Store migrations in version control.\nCode signature: Use alembic revision --autogenerate to create migrations from model changes.", "file_path": "https://stackoverflow.com/questions/9436429", "function_name": "Use alembic revision --autogenerate to create migrations from model changes.", "type": "stack_overflow", "metadata": {"record_id": "4b519eb6-677c-4193-bc40-c1be1b86a910", "source_type": "stack_overflow", "domain": "SQLAlchemy/Databases", "library": "connection-pool", "title": "How to handle database migrations with Alembic?", "content": "Workflow: (1) Change SQLAlchemy models. (2) `alembic revision --autogenerate -m 'description'`. (3) Review generated migration script. (4) `alembic upgrade head` to apply. Always review autogenerated migrations — they miss some changes (indexes, constraints). Use `alembic downgrade -1` to rollback. Store migrations in version control.", "code_signature": "Use alembic revision --autogenerate to create migrations from model changes.", "tags": "postgresql, sqlite, connection-pool, query, migration", "url": "https://stackoverflow.com/questions/9436429", "author": "user_974047", "date_published": "10-07-2021", "votes_or_stars": 2383, "relevance_label": "low", "difficulty_level": "intermediate"}}, "kb:dc68a09a-aaa3-4ed1-ac86-12e06b73f18f": {"content": "subprocess — Process Execution\nThe `subprocess.run` function in subprocess runs a subprocess and waits for it to complete. It accepts `args, capture_output=False, timeout=None` as a parameter and returns CompletedProcess. Common use cases include executing shell commands, scripts, or external programs from Python. Note that use shell=False (default) to avoid shell injection vulnerabilities.\nCode signature: subprocess.run(args, capture_output=False, timeout=None) -> CompletedProcess", "file_path": "https://docs.subprocess.org/en/stable/subprocess/run.html", "function_name": "subprocess.run(args, capture_output=False, timeout=None) -> CompletedProcess", "type": "documentation", "metadata": {"record_id": "dc68a09a-aaa3-4ed1-ac86-12e06b73f18f", "source_type": "documentation", "domain": "OS/Subprocess/File I/O", "library": "subprocess", "title": "subprocess — Process Execution", "content": "The `subprocess.run` function in subprocess runs a subprocess and waits for it to complete. It accepts `args, capture_output=False, timeout=None` as a parameter and returns CompletedProcess. Common use cases include executing shell commands, scripts, or external programs from Python. Note that use shell=False (default) to avoid shell injection vulnerabilities.", "code_signature": "subprocess.run(args, capture_output=False, timeout=None) -> CompletedProcess", "tags": "pipe, env-variable, shutil, stderr, stdout, file-io", "url": "https://docs.subprocess.org/en/stable/subprocess/run.html", "author": "Official Docs", "date_published": "08-10-2018", "votes_or_stars": 416, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:1c76a9e1-6357-4989-9050-0ac76e20ebdd": {"content": "asyncio RuntimeError: This event loop is already running\nThis error is common in Jupyter and FastAPI. Solutions: (1) In async context: just use `await coroutine()`. (2) In Jupyter: `import nest_asyncio; nest_asyncio.apply()`. (3) Run in thread: `asyncio.get_event_loop().run_in_executor(None, sync_func)`. (4) Use `asyncio.ensure_future()` to schedule from within async code.\nCode signature: You're calling asyncio.run() inside an already running event loop. Use await instead.", "file_path": "https://stackoverflow.com/questions/2229110", "function_name": "You're calling asyncio.run() inside an already running event loop. Use await instead.", "type": "stack_overflow", "metadata": {"record_id": "1c76a9e1-6357-4989-9050-0ac76e20ebdd", "source_type": "stack_overflow", "domain": "Asyncio/Threading", "library": "async", "title": "asyncio RuntimeError: This event loop is already running", "content": "This error is common in Jupyter and FastAPI. Solutions: (1) In async context: just use `await coroutine()`. (2) In Jupyter: `import nest_asyncio; nest_asyncio.apply()`. (3) Run in thread: `asyncio.get_event_loop().run_in_executor(None, sync_func)`. (4) Use `asyncio.ensure_future()` to schedule from within async code.", "code_signature": "You're calling asyncio.run() inside an already running event loop. Use await instead.", "tags": "deadlock, async, asyncio, semaphore", "url": "https://stackoverflow.com/questions/2229110", "author": "user_573750", "date_published": "23-05-2020", "votes_or_stars": 2316, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:18fb5e13-2c9f-4cb2-852c-bd04fe14e877": {"content": "How to read a large file line by line in Python without loading it all into memory?\n```python\nwith open('large.txt', 'r') as f:\n    for line in f:\n        process(line.strip())\n```\nThis reads one line at a time. For binary files: use `f.read(chunk_size)` in a loop. For CSV: use `pd.read_csv(file, chunksize=10000)`. Never use `f.readlines()` on large files.\nCode signature: Iterate over the file object directly — it yields lines lazily.", "file_path": "https://stackoverflow.com/questions/2737893", "function_name": "Iterate over the file object directly — it yields lines lazily.", "type": "stack_overflow", "metadata": {"record_id": "18fb5e13-2c9f-4cb2-852c-bd04fe14e877", "source_type": "stack_overflow", "domain": "OS/Subprocess/File I/O", "library": "shutil", "title": "How to read a large file line by line in Python without loading it all into memory?", "content": "```python\nwith open('large.txt', 'r') as f:\n    for line in f:\n        process(line.strip())\n```\nThis reads one line at a time. For binary files: use `f.read(chunk_size)` in a loop. For CSV: use `pd.read_csv(file, chunksize=10000)`. Never use `f.readlines()` on large files.", "code_signature": "Iterate over the file object directly — it yields lines lazily.", "tags": "stderr, subprocess, env-variable, tempfile", "url": "https://stackoverflow.com/questions/2737893", "author": "user_994539", "date_published": "18-02-2023", "votes_or_stars": 2260, "relevance_label": "low", "difficulty_level": "intermediate"}}, "kb:7563d01c-c6f6-4f81-9aae-69fc929169d6": {"content": "Django N+1 query problem: how to fix?\nN+1 occurs when accessing related objects in a loop. Diagnose with `django-debug-toolbar` or `connection.queries`. Fix ForeignKey: `User.objects.select_related('profile').all()`. Fix M2M: `Post.objects.prefetch_related('tags').all()`. Use `only()` to fetch specific fields and reduce data transfer.\nCode signature: Use select_related() for ForeignKey and prefetch_related() for ManyToMany relationships.", "file_path": "https://stackoverflow.com/questions/8106470", "function_name": "Use select_related() for ForeignKey and prefetch_related() for ManyToMany relationships.", "type": "stack_overflow", "metadata": {"record_id": "7563d01c-c6f6-4f81-9aae-69fc929169d6", "source_type": "stack_overflow", "domain": "FastAPI/Flask/Django", "library": "request", "title": "Django N+1 query problem: how to fix?", "content": "N+1 occurs when accessing related objects in a loop. Diagnose with `django-debug-toolbar` or `connection.queries`. Fix ForeignKey: `User.objects.select_related('profile').all()`. Fix M2M: `Post.objects.prefetch_related('tags').all()`. Use `only()` to fetch specific fields and reduce data transfer.", "code_signature": "Use select_related() for ForeignKey and prefetch_related() for ManyToMany relationships.", "tags": "template, orm, flask", "url": "https://stackoverflow.com/questions/8106470", "author": "user_441071", "date_published": "10-11-2023", "votes_or_stars": 1752, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:461d03c6-f343-4cf4-9959-4662492ed0a4": {"content": "Django N+1 query problem: how to fix?\nN+1 occurs when accessing related objects in a loop. Diagnose with `django-debug-toolbar` or `connection.queries`. Fix ForeignKey: `User.objects.select_related('profile').all()`. Fix M2M: `Post.objects.prefetch_related('tags').all()`. Use `only()` to fetch specific fields and reduce data transfer.\nCode signature: Use select_related() for ForeignKey and prefetch_related() for ManyToMany relationships.", "file_path": "https://stackoverflow.com/questions/8106470", "function_name": "Use select_related() for ForeignKey and prefetch_related() for ManyToMany relationships.", "type": "stack_overflow", "metadata": {"record_id": "461d03c6-f343-4cf4-9959-4662492ed0a4", "source_type": "stack_overflow", "domain": "FastAPI/Flask/Django", "library": "request", "title": "Django N+1 query problem: how to fix?", "content": "N+1 occurs when accessing related objects in a loop. Diagnose with `django-debug-toolbar` or `connection.queries`. Fix ForeignKey: `User.objects.select_related('profile').all()`. Fix M2M: `Post.objects.prefetch_related('tags').all()`. Use `only()` to fetch specific fields and reduce data transfer.", "code_signature": "Use select_related() for ForeignKey and prefetch_related() for ManyToMany relationships.", "tags": "orm, blueprint, routing", "url": "https://stackoverflow.com/questions/8106470", "author": "user_441071", "date_published": "26-12-2021", "votes_or_stars": 1759, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:ad009653-07d6-464d-826b-01eb2aad1e14": {"content": "NumPy — Linear Algebra\nThe `np.einsum` function in NumPy evaluates Einstein summation convention on operands. It accepts `subscripts, *operands` as a parameter and returns ndarray. Common use cases include matrix multiplication, dot products, tensor contractions. Note that prefer np.einsum over loops for large arrays.\nCode signature: np.einsum(subscripts, *operands) -> ndarray", "file_path": "https://docs.numpy.org/en/stable/np/einsum.html", "function_name": "np.einsum(subscripts, *operands) -> ndarray", "type": "documentation", "metadata": {"record_id": "ad009653-07d6-464d-826b-01eb2aad1e14", "source_type": "documentation", "domain": "NumPy/Pandas", "library": "NumPy", "title": "NumPy — Linear Algebra", "content": "The `np.einsum` function in NumPy evaluates Einstein summation convention on operands. It accepts `subscripts, *operands` as a parameter and returns ndarray. Common use cases include matrix multiplication, dot products, tensor contractions. Note that prefer np.einsum over loops for large arrays.", "code_signature": "np.einsum(subscripts, *operands) -> ndarray", "tags": "dtype, loc, groupby, numpy, pandas", "url": "https://docs.numpy.org/en/stable/np/einsum.html", "author": "Official Docs", "date_published": "14-05-2022", "votes_or_stars": 1861, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:61699cf2-99c3-4112-a9a6-9bdeb818ccbb": {"content": "How to implement JWT authentication in FastAPI?\n```python\nfrom fastapi.security import OAuth2PasswordBearer\nfrom jose import jwt\noauth2_scheme = OAuth2PasswordBearer(tokenUrl='token')\n```\nCreate access tokens with `jwt.encode(payload, SECRET_KEY, algorithm='HS256')`. Verify with `jwt.decode(token, SECRET_KEY, algorithms=['HS256'])`. Store SECRET_KEY in environment variables, never in code.\nCode signature: Use python-jose for JWT and passlib for password hashing with OAuth2PasswordBearer.", "file_path": "https://stackoverflow.com/questions/7754864", "function_name": "Use python-jose for JWT and passlib for password hashing with OAuth2PasswordBearer.", "type": "stack_overflow", "metadata": {"record_id": "61699cf2-99c3-4112-a9a6-9bdeb818ccbb", "source_type": "stack_overflow", "domain": "FastAPI/Flask/Django", "library": "fastapi", "title": "How to implement JWT authentication in FastAPI?", "content": "```python\nfrom fastapi.security import OAuth2PasswordBearer\nfrom jose import jwt\noauth2_scheme = OAuth2PasswordBearer(tokenUrl='token')\n```\nCreate access tokens with `jwt.encode(payload, SECRET_KEY, algorithm='HS256')`. Verify with `jwt.decode(token, SECRET_KEY, algorithms=['HS256'])`. Store SECRET_KEY in environment variables, never in code.", "code_signature": "Use python-jose for JWT and passlib for password hashing with OAuth2PasswordBearer.", "tags": "fastapi, rest, dependency-injection", "url": "https://stackoverflow.com/questions/7754864", "author": "user_773587", "date_published": "07-06-2019", "votes_or_stars": 437, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:08cd2ee4-f668-403f-b9e7-3c8133ff801a": {"content": "BUG: FastAPI dependency_overrides not working in pytest\nProblem: Dependency overrides set in conftest are not applied during test execution.\n\nFix/Discussion: Set overrides on the app object before TestClient is created. Use `app.dependency_overrides[dep] = mock_dep` in a pytest fixture with function scope. Clear overrides after test: `app.dependency_overrides = {}`. Ensure TestClient is created after overrides are set.\nCode signature: status:closed", "file_path": "https://github.com/fastapi/fastapi/issues/8479", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "08cd2ee4-f668-403f-b9e7-3c8133ff801a", "source_type": "github_issue", "domain": "FastAPI/Flask/Django", "library": "fastapi", "title": "BUG: FastAPI dependency_overrides not working in pytest", "content": "Problem: Dependency overrides set in conftest are not applied during test execution.\n\nFix/Discussion: Set overrides on the app object before TestClient is created. Use `app.dependency_overrides[dep] = mock_dep` in a pytest fixture with function scope. Clear overrides after test: `app.dependency_overrides = {}`. Ensure TestClient is created after overrides are set.", "code_signature": "status:closed", "tags": "request, rest, pydantic, response", "url": "https://github.com/fastapi/fastapi/issues/8479", "author": "contributor_1994", "date_published": "27-04-2023", "votes_or_stars": 315, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:611979f2-0e88-4f37-a674-cffa45ec9fa9": {"content": "How do I efficiently fill NaN values in a Pandas DataFrame?\nFor large DataFrames, avoid looping. Use `df.fillna({'col': val})` to fill specific columns. `df.ffill()` propagates last valid observation forward. For time-series data, `df.interpolate()` provides smoother results. Always check `df.isna().sum()` before and after to verify.\nCode signature: Use df.fillna(value) or df.ffill()/df.bfill() for forward/backward fill.", "file_path": "https://stackoverflow.com/questions/6438436", "function_name": "Use df.fillna(value) or df.ffill()/df.bfill() for forward/backward fill.", "type": "stack_overflow", "metadata": {"record_id": "611979f2-0e88-4f37-a674-cffa45ec9fa9", "source_type": "stack_overflow", "domain": "NumPy/Pandas", "library": "loc", "title": "How do I efficiently fill NaN values in a Pandas DataFrame?", "content": "For large DataFrames, avoid looping. Use `df.fillna({'col': val})` to fill specific columns. `df.ffill()` propagates last valid observation forward. For time-series data, `df.interpolate()` provides smoother results. Always check `df.isna().sum()` before and after to verify.", "code_signature": "Use df.fillna(value) or df.ffill()/df.bfill() for forward/backward fill.", "tags": "reshape, dtype, merge", "url": "https://stackoverflow.com/questions/6438436", "author": "user_522340", "date_published": "12-02-2020", "votes_or_stars": 41, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:9919ba82-cd91-4f29-a759-451e37ad806e": {"content": "BUG: threading.Timer not cancellable after it fires\nProblem: Calling cancel() on a Timer that has already executed does not raise an error but has no effect.\n\nFix/Discussion: threading.Timer.cancel() only works before the timer fires. Store Timer reference and check `timer.finished.is_set()` to know if it's already run. For repeating timers, use a loop with `threading.Event` for clean cancellation: `stop_event = threading.Event(); stop_event.wait(interval)`.\nCode signature: status:closed", "file_path": "https://github.com/cpython/cpython/issues/4111", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "9919ba82-cd91-4f29-a759-451e37ad806e", "source_type": "github_issue", "domain": "Asyncio/Threading", "library": "cpython", "title": "BUG: threading.Timer not cancellable after it fires", "content": "Problem: Calling cancel() on a Timer that has already executed does not raise an error but has no effect.\n\nFix/Discussion: threading.Timer.cancel() only works before the timer fires. Store Timer reference and check `timer.finished.is_set()` to know if it's already run. For repeating timers, use a loop with `threading.Event` for clean cancellation: `stop_event = threading.Event(); stop_event.wait(interval)`.", "code_signature": "status:closed", "tags": "gather, await, semaphore, task, lock, executor", "url": "https://github.com/cpython/cpython/issues/4111", "author": "contributor_7884", "date_published": "11-02-2023", "votes_or_stars": 55, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:7a3f54c9-8888-4592-9b59-34ffbec73e28": {"content": "How to use df.groupby in Pandas\n`Pandas.df.groupby` groups DataFrame rows by column values. Example usage:\n```python\ndf.groupby('category')['value'].mean()\n```\nThis is useful when aggregation, transformation, filtering by group. Performance tip: avoid apply() on large DataFrames; prefer agg().\nCode signature: df.groupby", "file_path": "https://docs.pandas.org/en/stable/df/groupby.html", "function_name": "df.groupby", "type": "documentation", "metadata": {"record_id": "7a3f54c9-8888-4592-9b59-34ffbec73e28", "source_type": "documentation", "domain": "NumPy/Pandas", "library": "Pandas", "title": "How to use df.groupby in Pandas", "content": "`Pandas.df.groupby` groups DataFrame rows by column values. Example usage:\n```python\ndf.groupby('category')['value'].mean()\n```\nThis is useful when aggregation, transformation, filtering by group. Performance tip: avoid apply() on large DataFrames; prefer agg().", "code_signature": "df.groupby", "tags": "broadcasting, loc, groupby, merge, dataframe, numpy", "url": "https://docs.pandas.org/en/stable/df/groupby.html", "author": "Official Docs", "date_published": "29-12-2022", "votes_or_stars": 830, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:c48262e5-f9f4-42cf-8e05-69d1e9c32bff": {"content": "How to implement database connection pooling with SQLAlchemy?\n`create_engine(url, pool_size=10, max_overflow=20, pool_timeout=30, pool_recycle=1800)`. Use `pool_pre_ping=True` to detect stale connections. For async: use `AsyncEngine` with `create_async_engine()`. Monitor with `engine.pool.status()`. NullPool disables pooling for scripts/serverless environments.\nCode signature: create_engine() has built-in pooling. Configure pool_size and max_overflow.", "file_path": "https://stackoverflow.com/questions/7358113", "function_name": "create_engine() has built-in pooling. Configure pool_size and max_overflow.", "type": "stack_overflow", "metadata": {"record_id": "c48262e5-f9f4-42cf-8e05-69d1e9c32bff", "source_type": "stack_overflow", "domain": "SQLAlchemy/Databases", "library": "session", "title": "How to implement database connection pooling with SQLAlchemy?", "content": "`create_engine(url, pool_size=10, max_overflow=20, pool_timeout=30, pool_recycle=1800)`. Use `pool_pre_ping=True` to detect stale connections. For async: use `AsyncEngine` with `create_async_engine()`. Monitor with `engine.pool.status()`. NullPool disables pooling for scripts/serverless environments.", "code_signature": "create_engine() has built-in pooling. Configure pool_size and max_overflow.", "tags": "sqlalchemy, sqlite, migration, alembic", "url": "https://stackoverflow.com/questions/7358113", "author": "user_12260", "date_published": "11-09-2020", "votes_or_stars": 277, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:7132e6d4-daa1-49a8-a578-b6c3ec35f4f5": {"content": "subprocess.run vs Popen: which should I use?\n`subprocess.run()` is a high-level wrapper that waits for completion — use for most cases. `subprocess.Popen()` gives full control: stream stdout/stderr in real-time, write to stdin, run processes concurrently. For capturing output: `subprocess.run(..., capture_output=True, text=True)`. Always use `shell=False` and pass args as a list to prevent shell injection.\nCode signature: Use subprocess.run() for simple cases; Popen for streaming output or interactive processes.", "file_path": "https://stackoverflow.com/questions/5727085", "function_name": "Use subprocess.run() for simple cases; Popen for streaming output or interactive processes.", "type": "stack_overflow", "metadata": {"record_id": "7132e6d4-daa1-49a8-a578-b6c3ec35f4f5", "source_type": "stack_overflow", "domain": "OS/Subprocess/File I/O", "library": "shutil", "title": "subprocess.run vs Popen: which should I use?", "content": "`subprocess.run()` is a high-level wrapper that waits for completion — use for most cases. `subprocess.Popen()` gives full control: stream stdout/stderr in real-time, write to stdin, run processes concurrently. For capturing output: `subprocess.run(..., capture_output=True, text=True)`. Always use `shell=False` and pass args as a list to prevent shell injection.", "code_signature": "Use subprocess.run() for simple cases; Popen for streaming output or interactive processes.", "tags": "stdout, stdin, pathlib, subprocess, pipe", "url": "https://stackoverflow.com/questions/5727085", "author": "user_644210", "date_published": "07-04-2020", "votes_or_stars": 672, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:0c0f3342-d1ce-4103-aaed-fff670c5a44b": {"content": "BUG: Flask session not persisting between requests in tests\nProblem: Session data set in one request is not available in the next request during testing.\n\nFix/Discussion: Use Flask test client's session_transaction() context manager: `with client.session_transaction() as sess:\n    sess['user_id'] = 1`. Ensure SECRET_KEY is set. For testing: `app.config['TESTING'] = True; app.config['SECRET_KEY'] = 'test'`.\nCode signature: status:closed", "file_path": "https://github.com/flask/flask/issues/2141", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "0c0f3342-d1ce-4103-aaed-fff670c5a44b", "source_type": "github_issue", "domain": "FastAPI/Flask/Django", "library": "flask", "title": "BUG: Flask session not persisting between requests in tests", "content": "Problem: Session data set in one request is not available in the next request during testing.\n\nFix/Discussion: Use Flask test client's session_transaction() context manager: `with client.session_transaction() as sess:\n    sess['user_id'] = 1`. Ensure SECRET_KEY is set. For testing: `app.config['TESTING'] = True; app.config['SECRET_KEY'] = 'test'`.", "code_signature": "status:closed", "tags": "flask, template, rest, middleware, response, django", "url": "https://github.com/flask/flask/issues/2141", "author": "contributor_5020", "date_published": "11-01-2018", "votes_or_stars": 144, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:93294c46-0f28-43c1-bc24-1345643af66d": {"content": "SQLAlchemy — ORM Session Query\nThe `session.query` function in SQLAlchemy constructs a SELECT query against mapped entities. It accepts `*entities` as a parameter and returns Query object. Common use cases include retrieving ORM-mapped objects from the database. Note that prefer session.execute(select(...)) in SQLAlchemy 2.0+.\nCode signature: session.query(*entities) -> Query object", "file_path": "https://docs.sqlalchemy.org/en/stable/session/query.html", "function_name": "session.query(*entities) -> Query object", "type": "documentation", "metadata": {"record_id": "93294c46-0f28-43c1-bc24-1345643af66d", "source_type": "documentation", "domain": "SQLAlchemy/Databases", "library": "SQLAlchemy", "title": "SQLAlchemy — ORM Session Query", "content": "The `session.query` function in SQLAlchemy constructs a SELECT query against mapped entities. It accepts `*entities` as a parameter and returns Query object. Common use cases include retrieving ORM-mapped objects from the database. Note that prefer session.execute(select(...)) in SQLAlchemy 2.0+.", "code_signature": "session.query(*entities) -> Query object", "tags": "alembic, transaction, connection-pool", "url": "https://docs.sqlalchemy.org/en/stable/session/query.html", "author": "Official Docs", "date_published": "25-07-2023", "votes_or_stars": 3038, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:64ed976a-8230-4fe3-8d85-62940fe6a52a": {"content": "How do I efficiently fill NaN values in a Pandas DataFrame?\nFor large DataFrames, avoid looping. Use `df.fillna({'col': val})` to fill specific columns. `df.ffill()` propagates last valid observation forward. For time-series data, `df.interpolate()` provides smoother results. Always check `df.isna().sum()` before and after to verify.\nCode signature: Use df.fillna(value) or df.ffill()/df.bfill() for forward/backward fill.", "file_path": "https://stackoverflow.com/questions/6438436", "function_name": "Use df.fillna(value) or df.ffill()/df.bfill() for forward/backward fill.", "type": "stack_overflow", "metadata": {"record_id": "64ed976a-8230-4fe3-8d85-62940fe6a52a", "source_type": "stack_overflow", "domain": "NumPy/Pandas", "library": "loc", "title": "How do I efficiently fill NaN values in a Pandas DataFrame?", "content": "For large DataFrames, avoid looping. Use `df.fillna({'col': val})` to fill specific columns. `df.ffill()` propagates last valid observation forward. For time-series data, `df.interpolate()` provides smoother results. Always check `df.isna().sum()` before and after to verify.", "code_signature": "Use df.fillna(value) or df.ffill()/df.bfill() for forward/backward fill.", "tags": "dataframe, merge, loc, iloc", "url": "https://stackoverflow.com/questions/6438436", "author": "user_522340", "date_published": "18-06-2022", "votes_or_stars": 3, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:f2db3151-c16b-4474-9148-0397479201a3": {"content": "SQLAlchemy bulk insert: what's the fastest way?\n`session.bulk_insert_mappings(Model, list_of_dicts)` skips ORM overhead. Even faster: `session.execute(insert(Model), data)`. Avoid `session.add()` in loops. For PostgreSQL, use `insert().on_conflict_do_nothing()` for upserts. Batch in chunks of 1000-5000 rows for optimal performance.\nCode signature: Use session.bulk_insert_mappings() or execute(insert(), list_of_dicts) for maximum speed.", "file_path": "https://stackoverflow.com/questions/5977931", "function_name": "Use session.bulk_insert_mappings() or execute(insert(), list_of_dicts) for maximum speed.", "type": "stack_overflow", "metadata": {"record_id": "f2db3151-c16b-4474-9148-0397479201a3", "source_type": "stack_overflow", "domain": "SQLAlchemy/Databases", "library": "query", "title": "SQLAlchemy bulk insert: what's the fastest way?", "content": "`session.bulk_insert_mappings(Model, list_of_dicts)` skips ORM overhead. Even faster: `session.execute(insert(Model), data)`. Avoid `session.add()` in loops. For PostgreSQL, use `insert().on_conflict_do_nothing()` for upserts. Batch in chunks of 1000-5000 rows for optimal performance.", "code_signature": "Use session.bulk_insert_mappings() or execute(insert(), list_of_dicts) for maximum speed.", "tags": "alembic, postgresql, session, orm, relationship", "url": "https://stackoverflow.com/questions/5977931", "author": "user_238275", "date_published": "24-08-2018", "votes_or_stars": 637, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:95d57332-3e25-4350-adf7-3c73505abffe": {"content": "How to read a large file line by line in Python without loading it all into memory?\n```python\nwith open('large.txt', 'r') as f:\n    for line in f:\n        process(line.strip())\n```\nThis reads one line at a time. For binary files: use `f.read(chunk_size)` in a loop. For CSV: use `pd.read_csv(file, chunksize=10000)`. Never use `f.readlines()` on large files.\nCode signature: Iterate over the file object directly — it yields lines lazily.", "file_path": "https://stackoverflow.com/questions/2737893", "function_name": "Iterate over the file object directly — it yields lines lazily.", "type": "stack_overflow", "metadata": {"record_id": "95d57332-3e25-4350-adf7-3c73505abffe", "source_type": "stack_overflow", "domain": "OS/Subprocess/File I/O", "library": "shutil", "title": "How to read a large file line by line in Python without loading it all into memory?", "content": "```python\nwith open('large.txt', 'r') as f:\n    for line in f:\n        process(line.strip())\n```\nThis reads one line at a time. For binary files: use `f.read(chunk_size)` in a loop. For CSV: use `pd.read_csv(file, chunksize=10000)`. Never use `f.readlines()` on large files.", "code_signature": "Iterate over the file object directly — it yields lines lazily.", "tags": "tempfile, stdout, shutil, stdin", "url": "https://stackoverflow.com/questions/2737893", "author": "user_994539", "date_published": "07-12-2021", "votes_or_stars": 2285, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:475f871a-72d4-4a99-b3ff-9663e1e20435": {"content": "FEAT: Django: add support for async ORM queries\nProblem: Django ORM is synchronous; using it in async views requires sync_to_async wrapper.\n\nFix/Discussion: Django 4.1+ added native async ORM support. Use `await Model.objects.filter().afirst()`, `async for obj in qs:`. For older versions: wrap with `sync_to_async`: `get_user = sync_to_async(User.objects.get)\nuser = await get_user(pk=1)`.\nCode signature: status:closed", "file_path": "https://github.com/django/django/issues/8618", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "475f871a-72d4-4a99-b3ff-9663e1e20435", "source_type": "github_issue", "domain": "FastAPI/Flask/Django", "library": "django", "title": "FEAT: Django: add support for async ORM queries", "content": "Problem: Django ORM is synchronous; using it in async views requires sync_to_async wrapper.\n\nFix/Discussion: Django 4.1+ added native async ORM support. Use `await Model.objects.filter().afirst()`, `async for obj in qs:`. For older versions: wrap with `sync_to_async`: `get_user = sync_to_async(User.objects.get)\nuser = await get_user(pk=1)`.", "code_signature": "status:closed", "tags": "orm, dependency-injection, django, response", "url": "https://github.com/django/django/issues/8618", "author": "contributor_8921", "date_published": "13-12-2018", "votes_or_stars": 21, "relevance_label": "low", "difficulty_level": "advanced"}}, "kb:c92835d9-c9bc-4a4f-823c-2517e8e951f0": {"content": "Python threading: how to share data between threads safely?\nFor simple counters: use `threading.Lock()` as context manager. For complex shared state: prefer `queue.Queue` (thread-safe by design). Avoid global variables without locks. `threading.local()` gives each thread its own data copy. For read-heavy workloads, `threading.RLock` allows re-entrant acquisition.\nCode signature: Use threading.Lock for mutual exclusion or queue.Queue for producer-consumer patterns.", "file_path": "https://stackoverflow.com/questions/3195773", "function_name": "Use threading.Lock for mutual exclusion or queue.Queue for producer-consumer patterns.", "type": "stack_overflow", "metadata": {"record_id": "c92835d9-c9bc-4a4f-823c-2517e8e951f0", "source_type": "stack_overflow", "domain": "Asyncio/Threading", "library": "lock", "title": "Python threading: how to share data between threads safely?", "content": "For simple counters: use `threading.Lock()` as context manager. For complex shared state: prefer `queue.Queue` (thread-safe by design). Avoid global variables without locks. `threading.local()` gives each thread its own data copy. For read-heavy workloads, `threading.RLock` allows re-entrant acquisition.", "code_signature": "Use threading.Lock for mutual exclusion or queue.Queue for producer-consumer patterns.", "tags": "threading, await, executor, asyncio, task", "url": "https://stackoverflow.com/questions/3195773", "author": "user_714318", "date_published": "03-05-2020", "votes_or_stars": 1097, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:fdaffda1-58f8-440d-9abd-51b417b245c6": {"content": "How to watch a directory for file changes in Python?\n```python\nfrom watchdog.observers import Observer\nfrom watchdog.events import FileSystemEventHandler\nhandler = FileSystemEventHandler()\nobserver = Observer()\nobserver.schedule(handler, path='.', recursive=True)\nobserver.start()\n```\nFor simple polling: `os.stat(file).st_mtime`. Linux-native: `inotify`. macOS-native: `FSEvents`.\nCode signature: Use the watchdog library for cross-platform directory monitoring.", "file_path": "https://stackoverflow.com/questions/5213115", "function_name": "Use the watchdog library for cross-platform directory monitoring.", "type": "stack_overflow", "metadata": {"record_id": "fdaffda1-58f8-440d-9abd-51b417b245c6", "source_type": "stack_overflow", "domain": "OS/Subprocess/File I/O", "library": "env-variable", "title": "How to watch a directory for file changes in Python?", "content": "```python\nfrom watchdog.observers import Observer\nfrom watchdog.events import FileSystemEventHandler\nhandler = FileSystemEventHandler()\nobserver = Observer()\nobserver.schedule(handler, path='.', recursive=True)\nobserver.start()\n```\nFor simple polling: `os.stat(file).st_mtime`. Linux-native: `inotify`. macOS-native: `FSEvents`.", "code_signature": "Use the watchdog library for cross-platform directory monitoring.", "tags": "file-io, stdin, stdout, shutil", "url": "https://stackoverflow.com/questions/5213115", "author": "user_959314", "date_published": "28-07-2018", "votes_or_stars": 2080, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:e495844e-5b0c-484b-9a93-674b1a02692a": {"content": "FEAT: Django: add support for async ORM queries\nProblem: Django ORM is synchronous; using it in async views requires sync_to_async wrapper.\n\nFix/Discussion: Django 4.1+ added native async ORM support. Use `await Model.objects.filter().afirst()`, `async for obj in qs:`. For older versions: wrap with `sync_to_async`: `get_user = sync_to_async(User.objects.get)\nuser = await get_user(pk=1)`.\nCode signature: status:closed", "file_path": "https://github.com/django/django/issues/8618", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "e495844e-5b0c-484b-9a93-674b1a02692a", "source_type": "github_issue", "domain": "FastAPI/Flask/Django", "library": "django", "title": "FEAT: Django: add support for async ORM queries", "content": "Problem: Django ORM is synchronous; using it in async views requires sync_to_async wrapper.\n\nFix/Discussion: Django 4.1+ added native async ORM support. Use `await Model.objects.filter().afirst()`, `async for obj in qs:`. For older versions: wrap with `sync_to_async`: `get_user = sync_to_async(User.objects.get)\nuser = await get_user(pk=1)`.", "code_signature": "status:closed", "tags": "rest, dependency-injection, orm, flask", "url": "https://github.com/django/django/issues/8618", "author": "contributor_8921", "date_published": "11-07-2018", "votes_or_stars": 24, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:f82d3dc9-3acc-4264-bc08-f09e7d80d76c": {"content": "requests.exceptions.SSLError: certificate verify failed\nNever use `verify=False` in production — it disables SSL verification entirely. Solutions: (1) Update certifi: `pip install --upgrade certifi`. (2) Provide CA bundle: `requests.get(url, verify='/path/to/ca-bundle.crt')`. (3) For self-signed certs in dev: `verify=False` only, and never commit this. (4) Check system time — expired system clock causes SSL failures.\nCode signature: Your SSL certificate chain is incomplete or self-signed. Fix the cert, don't disable verification.", "file_path": "https://stackoverflow.com/questions/7907400", "function_name": "Your SSL certificate chain is incomplete or self-signed. Fix the cert, don't disable verification.", "type": "stack_overflow", "metadata": {"record_id": "f82d3dc9-3acc-4264-bc08-f09e7d80d76c", "source_type": "stack_overflow", "domain": "Requests/HTTP/APIs", "library": "websocket", "title": "requests.exceptions.SSLError: certificate verify failed", "content": "Never use `verify=False` in production — it disables SSL verification entirely. Solutions: (1) Update certifi: `pip install --upgrade certifi`. (2) Provide CA bundle: `requests.get(url, verify='/path/to/ca-bundle.crt')`. (3) For self-signed certs in dev: `verify=False` only, and never commit this. (4) Check system time — expired system clock causes SSL failures.", "code_signature": "Your SSL certificate chain is incomplete or self-signed. Fix the cert, don't disable verification.", "tags": "aiohttp, retry, authentication, requests, headers", "url": "https://stackoverflow.com/questions/7907400", "author": "user_851204", "date_published": "15-11-2023", "votes_or_stars": 1397, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:10a70b2d-e915-4b8d-9b28-45f77f671cd6": {"content": "BUG: SettingWithCopyWarning when modifying sliced DataFrame\nProblem: Modifying a slice of a DataFrame raises SettingWithCopyWarning and may not update original.\n\nFix/Discussion: Always use .loc or .copy() when slicing. Use `df.loc[mask, 'col'] = val` for assignment. Create explicit copies: `subset = df[mask].copy()`. In Pandas 2.0+, Copy-on-Write makes this behavior consistent — enable with `pd.options.mode.copy_on_write = True`.\nCode signature: status:closed", "file_path": "https://github.com/pandas/pandas/issues/3264", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "10a70b2d-e915-4b8d-9b28-45f77f671cd6", "source_type": "github_issue", "domain": "NumPy/Pandas", "library": "pandas", "title": "BUG: SettingWithCopyWarning when modifying sliced DataFrame", "content": "Problem: Modifying a slice of a DataFrame raises SettingWithCopyWarning and may not update original.\n\nFix/Discussion: Always use .loc or .copy() when slicing. Use `df.loc[mask, 'col'] = val` for assignment. Create explicit copies: `subset = df[mask].copy()`. In Pandas 2.0+, Copy-on-Write makes this behavior consistent — enable with `pd.options.mode.copy_on_write = True`.", "code_signature": "status:closed", "tags": "iloc, array, merge", "url": "https://github.com/pandas/pandas/issues/3264", "author": "contributor_6628", "date_published": "24-04-2022", "votes_or_stars": 12, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:2d7e50cb-ce4e-4400-bd85-e8cb3668ffb4": {"content": "asyncio — Concurrent Coroutines\nThe `asyncio.gather` function in asyncio runs multiple coroutines concurrently and collects results. It accepts `*coros, return_exceptions=False` as a parameter and returns list of results. Common use cases include parallel I/O operations such as multiple API calls. Note that set return_exceptions=True to prevent one failure from cancelling all tasks.\nCode signature: asyncio.gather(*coros, return_exceptions=False) -> list of results", "file_path": "https://docs.asyncio.org/en/stable/asyncio/gather.html", "function_name": "asyncio.gather(*coros, return_exceptions=False) -> list of results", "type": "documentation", "metadata": {"record_id": "2d7e50cb-ce4e-4400-bd85-e8cb3668ffb4", "source_type": "documentation", "domain": "Asyncio/Threading", "library": "asyncio", "title": "asyncio — Concurrent Coroutines", "content": "The `asyncio.gather` function in asyncio runs multiple coroutines concurrently and collects results. It accepts `*coros, return_exceptions=False` as a parameter and returns list of results. Common use cases include parallel I/O operations such as multiple API calls. Note that set return_exceptions=True to prevent one failure from cancelling all tasks.", "code_signature": "asyncio.gather(*coros, return_exceptions=False) -> list of results", "tags": "deadlock, task, threading, async, lock, asyncio", "url": "https://docs.asyncio.org/en/stable/asyncio/gather.html", "author": "Official Docs", "date_published": "13-07-2020", "votes_or_stars": 533, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:eb306a79-4405-4450-946c-70d60054e426": {"content": "How to upload a file with requests?\n```python\nwith open('file.pdf', 'rb') as f:\n    r = requests.post(url, files={'file': ('file.pdf', f, 'application/pdf')})\n```\nFor multiple files: pass a list of tuples. For multipart form data with fields: combine `files` and `data` parameters. Don't set Content-Type manually — requests sets it with boundary.\nCode signature: Use the files parameter with a file-like object or tuple.", "file_path": "https://stackoverflow.com/questions/8896868", "function_name": "Use the files parameter with a file-like object or tuple.", "type": "stack_overflow", "metadata": {"record_id": "eb306a79-4405-4450-946c-70d60054e426", "source_type": "stack_overflow", "domain": "Requests/HTTP/APIs", "library": "requests", "title": "How to upload a file with requests?", "content": "```python\nwith open('file.pdf', 'rb') as f:\n    r = requests.post(url, files={'file': ('file.pdf', f, 'application/pdf')})\n```\nFor multiple files: pass a list of tuples. For multipart form data with fields: combine `files` and `data` parameters. Don't set Content-Type manually — requests sets it with boundary.", "code_signature": "Use the files parameter with a file-like object or tuple.", "tags": "timeout, aiohttp, websocket", "url": "https://stackoverflow.com/questions/8896868", "author": "user_243238", "date_published": "05-07-2023", "votes_or_stars": 1558, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:c3696374-a7e8-4813-8abd-4e3e92a06328": {"content": "BUG: SettingWithCopyWarning when modifying sliced DataFrame\nProblem: Modifying a slice of a DataFrame raises SettingWithCopyWarning and may not update original.\n\nFix/Discussion: Always use .loc or .copy() when slicing. Use `df.loc[mask, 'col'] = val` for assignment. Create explicit copies: `subset = df[mask].copy()`. In Pandas 2.0+, Copy-on-Write makes this behavior consistent — enable with `pd.options.mode.copy_on_write = True`.\nCode signature: status:closed", "file_path": "https://github.com/pandas/pandas/issues/3264", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "c3696374-a7e8-4813-8abd-4e3e92a06328", "source_type": "github_issue", "domain": "NumPy/Pandas", "library": "pandas", "title": "BUG: SettingWithCopyWarning when modifying sliced DataFrame", "content": "Problem: Modifying a slice of a DataFrame raises SettingWithCopyWarning and may not update original.\n\nFix/Discussion: Always use .loc or .copy() when slicing. Use `df.loc[mask, 'col'] = val` for assignment. Create explicit copies: `subset = df[mask].copy()`. In Pandas 2.0+, Copy-on-Write makes this behavior consistent — enable with `pd.options.mode.copy_on_write = True`.", "code_signature": "status:closed", "tags": "array, nan, pivot, groupby, broadcasting, numpy", "url": "https://github.com/pandas/pandas/issues/3264", "author": "contributor_6628", "date_published": "06-08-2022", "votes_or_stars": 5, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:9a9867e3-7841-4f74-921c-69f152c660d8": {"content": "Pandas Merging DataFrames — official reference\nOfficial documentation for Pandas Merging DataFrames. The method signature is `pd.merge(left, right, on, how='inner')`. merges two DataFrames using SQL-style joins. Raises `MergeError` when merge keys produce duplicate columns. See also: df.join, df.concat.\nCode signature: pd.merge(left, right, on, how='inner')", "file_path": "https://docs.pandas.org/en/stable/pd/merge.html", "function_name": "pd.merge(left, right, on, how='inner')", "type": "documentation", "metadata": {"record_id": "9a9867e3-7841-4f74-921c-69f152c660d8", "source_type": "documentation", "domain": "NumPy/Pandas", "library": "Pandas", "title": "Pandas Merging DataFrames — official reference", "content": "Official documentation for Pandas Merging DataFrames. The method signature is `pd.merge(left, right, on, how='inner')`. merges two DataFrames using SQL-style joins. Raises `MergeError` when merge keys produce duplicate columns. See also: df.join, df.concat.", "code_signature": "pd.merge(left, right, on, how='inner')", "tags": "iloc, broadcasting, array, dtype, pivot", "url": "https://docs.pandas.org/en/stable/pd/merge.html", "author": "Official Docs", "date_published": "01-04-2024", "votes_or_stars": 4519, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:66fe7fc2-a6c6-4a8d-8015-a38c8356d8b3": {"content": "Django N+1 query problem: how to fix?\nN+1 occurs when accessing related objects in a loop. Diagnose with `django-debug-toolbar` or `connection.queries`. Fix ForeignKey: `User.objects.select_related('profile').all()`. Fix M2M: `Post.objects.prefetch_related('tags').all()`. Use `only()` to fetch specific fields and reduce data transfer.\nCode signature: Use select_related() for ForeignKey and prefetch_related() for ManyToMany relationships.", "file_path": "https://stackoverflow.com/questions/8106470", "function_name": "Use select_related() for ForeignKey and prefetch_related() for ManyToMany relationships.", "type": "stack_overflow", "metadata": {"record_id": "66fe7fc2-a6c6-4a8d-8015-a38c8356d8b3", "source_type": "stack_overflow", "domain": "FastAPI/Flask/Django", "library": "request", "title": "Django N+1 query problem: how to fix?", "content": "N+1 occurs when accessing related objects in a loop. Diagnose with `django-debug-toolbar` or `connection.queries`. Fix ForeignKey: `User.objects.select_related('profile').all()`. Fix M2M: `Post.objects.prefetch_related('tags').all()`. Use `only()` to fetch specific fields and reduce data transfer.", "code_signature": "Use select_related() for ForeignKey and prefetch_related() for ManyToMany relationships.", "tags": "middleware, response, flask, fastapi, routing", "url": "https://stackoverflow.com/questions/8106470", "author": "user_441071", "date_published": "09-02-2019", "votes_or_stars": 1765, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:b3d02ee4-32c4-45ba-8147-4cd66c2e6779": {"content": "How do I reshape a wide DataFrame to long format?\n`pd.melt(df, id_vars=['id'], value_vars=['col1','col2'])` unpivots columns to rows. `df.stack()` moves column index to row index. For the reverse, use `df.pivot()` or `df.unstack()`. Always reset_index() after stack/unstack to get a clean DataFrame.\nCode signature: Use pd.melt() or df.stack() depending on your structure.", "file_path": "https://stackoverflow.com/questions/6229731", "function_name": "Use pd.melt() or df.stack() depending on your structure.", "type": "stack_overflow", "metadata": {"record_id": "b3d02ee4-32c4-45ba-8147-4cd66c2e6779", "source_type": "stack_overflow", "domain": "NumPy/Pandas", "library": "nan", "title": "How do I reshape a wide DataFrame to long format?", "content": "`pd.melt(df, id_vars=['id'], value_vars=['col1','col2'])` unpivots columns to rows. `df.stack()` moves column index to row index. For the reverse, use `df.pivot()` or `df.unstack()`. Always reset_index() after stack/unstack to get a clean DataFrame.", "code_signature": "Use pd.melt() or df.stack() depending on your structure.", "tags": "iloc, reshape, nan", "url": "https://stackoverflow.com/questions/6229731", "author": "user_428373", "date_published": "10-03-2022", "votes_or_stars": 833, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:0f5b7309-553b-49f5-9649-5ac0a00e7389": {"content": "How to watch a directory for file changes in Python?\n```python\nfrom watchdog.observers import Observer\nfrom watchdog.events import FileSystemEventHandler\nhandler = FileSystemEventHandler()\nobserver = Observer()\nobserver.schedule(handler, path='.', recursive=True)\nobserver.start()\n```\nFor simple polling: `os.stat(file).st_mtime`. Linux-native: `inotify`. macOS-native: `FSEvents`.\nCode signature: Use the watchdog library for cross-platform directory monitoring.", "file_path": "https://stackoverflow.com/questions/5213115", "function_name": "Use the watchdog library for cross-platform directory monitoring.", "type": "stack_overflow", "metadata": {"record_id": "0f5b7309-553b-49f5-9649-5ac0a00e7389", "source_type": "stack_overflow", "domain": "OS/Subprocess/File I/O", "library": "env-variable", "title": "How to watch a directory for file changes in Python?", "content": "```python\nfrom watchdog.observers import Observer\nfrom watchdog.events import FileSystemEventHandler\nhandler = FileSystemEventHandler()\nobserver = Observer()\nobserver.schedule(handler, path='.', recursive=True)\nobserver.start()\n```\nFor simple polling: `os.stat(file).st_mtime`. Linux-native: `inotify`. macOS-native: `FSEvents`.", "code_signature": "Use the watchdog library for cross-platform directory monitoring.", "tags": "pathlib, glob, pipe, shutil, tempfile", "url": "https://stackoverflow.com/questions/5213115", "author": "user_959314", "date_published": "13-10-2023", "votes_or_stars": 2058, "relevance_label": "low", "difficulty_level": "intermediate"}}, "kb:8729e38d-dbd3-4ed6-afee-5ec16dbdd981": {"content": "How to apply a function to each row in Pandas without using iterrows?\n`iterrows()` is slow because it creates a Series per row. Prefer `df.apply(func, axis=1)` for row-wise operations, or better yet, vectorize using NumPy: `np.where(condition, a, b)`. For complex logic, `df.assign()` with vectorized expressions is cleanest. Benchmark: vectorized > apply > itertuples > iterrows.\nCode signature: Use df.apply(func, axis=1) or vectorized NumPy operations for best performance.", "file_path": "https://stackoverflow.com/questions/5446912", "function_name": "Use df.apply(func, axis=1) or vectorized NumPy operations for best performance.", "type": "stack_overflow", "metadata": {"record_id": "8729e38d-dbd3-4ed6-afee-5ec16dbdd981", "source_type": "stack_overflow", "domain": "NumPy/Pandas", "library": "dataframe", "title": "How to apply a function to each row in Pandas without using iterrows?", "content": "`iterrows()` is slow because it creates a Series per row. Prefer `df.apply(func, axis=1)` for row-wise operations, or better yet, vectorize using NumPy: `np.where(condition, a, b)`. For complex logic, `df.assign()` with vectorized expressions is cleanest. Benchmark: vectorized > apply > itertuples > iterrows.", "code_signature": "Use df.apply(func, axis=1) or vectorized NumPy operations for best performance.", "tags": "pivot, pandas, array", "url": "https://stackoverflow.com/questions/5446912", "author": "user_563306", "date_published": "13-05-2018", "votes_or_stars": 2259, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:46ab343a-daff-4056-bf11-e34b0e3f061f": {"content": "BUG: FastAPI dependency_overrides not working in pytest\nProblem: Dependency overrides set in conftest are not applied during test execution.\n\nFix/Discussion: Set overrides on the app object before TestClient is created. Use `app.dependency_overrides[dep] = mock_dep` in a pytest fixture with function scope. Clear overrides after test: `app.dependency_overrides = {}`. Ensure TestClient is created after overrides are set.\nCode signature: status:closed", "file_path": "https://github.com/fastapi/fastapi/issues/8479", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "46ab343a-daff-4056-bf11-e34b0e3f061f", "source_type": "github_issue", "domain": "FastAPI/Flask/Django", "library": "fastapi", "title": "BUG: FastAPI dependency_overrides not working in pytest", "content": "Problem: Dependency overrides set in conftest are not applied during test execution.\n\nFix/Discussion: Set overrides on the app object before TestClient is created. Use `app.dependency_overrides[dep] = mock_dep` in a pytest fixture with function scope. Clear overrides after test: `app.dependency_overrides = {}`. Ensure TestClient is created after overrides are set.", "code_signature": "status:closed", "tags": "blueprint, flask, middleware", "url": "https://github.com/fastapi/fastapi/issues/8479", "author": "contributor_1994", "date_published": "02-11-2022", "votes_or_stars": 287, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:4e5c2fbe-0ce6-41d5-955c-d0e76ee0c007": {"content": "How to use create_engine in SQLAlchemy\n`SQLAlchemy.create_engine` creates a database engine representing a connection pool. Example usage:\n```python\nengine = create_engine('postgresql://user:pass@host/db', echo=True)\n```\nThis is useful when establishing the central database connection for an application. Performance tip: set pool_size and max_overflow based on expected concurrency.\nCode signature: create_engine", "file_path": "https://docs.sqlalchemy.org/en/stable/create_engine.html", "function_name": "create_engine", "type": "documentation", "metadata": {"record_id": "4e5c2fbe-0ce6-41d5-955c-d0e76ee0c007", "source_type": "documentation", "domain": "SQLAlchemy/Databases", "library": "SQLAlchemy", "title": "How to use create_engine in SQLAlchemy", "content": "`SQLAlchemy.create_engine` creates a database engine representing a connection pool. Example usage:\n```python\nengine = create_engine('postgresql://user:pass@host/db', echo=True)\n```\nThis is useful when establishing the central database connection for an application. Performance tip: set pool_size and max_overflow based on expected concurrency.", "code_signature": "create_engine", "tags": "relationship, sqlite, connection-pool, session, alembic, postgresql", "url": "https://docs.sqlalchemy.org/en/stable/create_engine.html", "author": "Official Docs", "date_published": "29-08-2018", "votes_or_stars": 2085, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:34bc285c-db76-49ba-940f-dac72b6c6f55": {"content": "BUG: Alembic autogenerate misses index changes\nProblem: Alembic --autogenerate does not detect changes to indexes or constraints on existing tables.\n\nFix/Discussion: Alembic autogenerate compares metadata to database state but has limitations. Workarounds: (1) Manually add `op.create_index()` to migration. (2) Use `include_schemas=True` in env.py. (3) Run `alembic check` to see detected changes. (4) Always review autogenerated migrations before applying.\nCode signature: status:open", "file_path": "https://github.com/alembic/alembic/issues/7344", "function_name": "status:open", "type": "github_issue", "metadata": {"record_id": "34bc285c-db76-49ba-940f-dac72b6c6f55", "source_type": "github_issue", "domain": "SQLAlchemy/Databases", "library": "alembic", "title": "BUG: Alembic autogenerate misses index changes", "content": "Problem: Alembic --autogenerate does not detect changes to indexes or constraints on existing tables.\n\nFix/Discussion: Alembic autogenerate compares metadata to database state but has limitations. Workarounds: (1) Manually add `op.create_index()` to migration. (2) Use `include_schemas=True` in env.py. (3) Run `alembic check` to see detected changes. (4) Always review autogenerated migrations before applying.", "code_signature": "status:open", "tags": "sqlite, connection-pool, session, query", "url": "https://github.com/alembic/alembic/issues/7344", "author": "contributor_3601", "date_published": "25-07-2023", "votes_or_stars": 249, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:63fa51b9-c2a3-467a-9f80-f5a0d9654bba": {"content": "BUG: threading.Timer not cancellable after it fires\nProblem: Calling cancel() on a Timer that has already executed does not raise an error but has no effect.\n\nFix/Discussion: threading.Timer.cancel() only works before the timer fires. Store Timer reference and check `timer.finished.is_set()` to know if it's already run. For repeating timers, use a loop with `threading.Event` for clean cancellation: `stop_event = threading.Event(); stop_event.wait(interval)`.\nCode signature: status:closed", "file_path": "https://github.com/cpython/cpython/issues/4111", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "63fa51b9-c2a3-467a-9f80-f5a0d9654bba", "source_type": "github_issue", "domain": "Asyncio/Threading", "library": "cpython", "title": "BUG: threading.Timer not cancellable after it fires", "content": "Problem: Calling cancel() on a Timer that has already executed does not raise an error but has no effect.\n\nFix/Discussion: threading.Timer.cancel() only works before the timer fires. Store Timer reference and check `timer.finished.is_set()` to know if it's already run. For repeating timers, use a loop with `threading.Event` for clean cancellation: `stop_event = threading.Event(); stop_event.wait(interval)`.", "code_signature": "status:closed", "tags": "event-loop, semaphore, threading, executor, lock", "url": "https://github.com/cpython/cpython/issues/4111", "author": "contributor_7884", "date_published": "07-03-2023", "votes_or_stars": 9, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:6bd3d061-afe5-4e77-a969-85efec648232": {"content": "How to read a large file line by line in Python without loading it all into memory?\n```python\nwith open('large.txt', 'r') as f:\n    for line in f:\n        process(line.strip())\n```\nThis reads one line at a time. For binary files: use `f.read(chunk_size)` in a loop. For CSV: use `pd.read_csv(file, chunksize=10000)`. Never use `f.readlines()` on large files.\nCode signature: Iterate over the file object directly — it yields lines lazily.", "file_path": "https://stackoverflow.com/questions/2737893", "function_name": "Iterate over the file object directly — it yields lines lazily.", "type": "stack_overflow", "metadata": {"record_id": "6bd3d061-afe5-4e77-a969-85efec648232", "source_type": "stack_overflow", "domain": "OS/Subprocess/File I/O", "library": "shutil", "title": "How to read a large file line by line in Python without loading it all into memory?", "content": "```python\nwith open('large.txt', 'r') as f:\n    for line in f:\n        process(line.strip())\n```\nThis reads one line at a time. For binary files: use `f.read(chunk_size)` in a loop. For CSV: use `pd.read_csv(file, chunksize=10000)`. Never use `f.readlines()` on large files.", "code_signature": "Iterate over the file object directly — it yields lines lazily.", "tags": "stderr, shutil, process", "url": "https://stackoverflow.com/questions/2737893", "author": "user_994539", "date_published": "05-08-2024", "votes_or_stars": 2288, "relevance_label": "low", "difficulty_level": "beginner"}}, "kb:21090ed8-e00c-4abe-93a0-aa64bb237c96": {"content": "asyncio — Concurrent Coroutines\nThe `asyncio.gather` function in asyncio runs multiple coroutines concurrently and collects results. It accepts `*coros, return_exceptions=False` as a parameter and returns list of results. Common use cases include parallel I/O operations such as multiple API calls. Note that set return_exceptions=True to prevent one failure from cancelling all tasks.\nCode signature: asyncio.gather(*coros, return_exceptions=False) -> list of results", "file_path": "https://docs.asyncio.org/en/stable/asyncio/gather.html", "function_name": "asyncio.gather(*coros, return_exceptions=False) -> list of results", "type": "documentation", "metadata": {"record_id": "21090ed8-e00c-4abe-93a0-aa64bb237c96", "source_type": "documentation", "domain": "Asyncio/Threading", "library": "asyncio", "title": "asyncio — Concurrent Coroutines", "content": "The `asyncio.gather` function in asyncio runs multiple coroutines concurrently and collects results. It accepts `*coros, return_exceptions=False` as a parameter and returns list of results. Common use cases include parallel I/O operations such as multiple API calls. Note that set return_exceptions=True to prevent one failure from cancelling all tasks.", "code_signature": "asyncio.gather(*coros, return_exceptions=False) -> list of results", "tags": "threading, lock, await, task, coroutine", "url": "https://docs.asyncio.org/en/stable/asyncio/gather.html", "author": "Official Docs", "date_published": "21-10-2020", "votes_or_stars": 492, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:7d5d281a-6a3e-428c-afdc-27659a924795": {"content": "SQLAlchemy bulk insert: what's the fastest way?\n`session.bulk_insert_mappings(Model, list_of_dicts)` skips ORM overhead. Even faster: `session.execute(insert(Model), data)`. Avoid `session.add()` in loops. For PostgreSQL, use `insert().on_conflict_do_nothing()` for upserts. Batch in chunks of 1000-5000 rows for optimal performance.\nCode signature: Use session.bulk_insert_mappings() or execute(insert(), list_of_dicts) for maximum speed.", "file_path": "https://stackoverflow.com/questions/5977931", "function_name": "Use session.bulk_insert_mappings() or execute(insert(), list_of_dicts) for maximum speed.", "type": "stack_overflow", "metadata": {"record_id": "7d5d281a-6a3e-428c-afdc-27659a924795", "source_type": "stack_overflow", "domain": "SQLAlchemy/Databases", "library": "query", "title": "SQLAlchemy bulk insert: what's the fastest way?", "content": "`session.bulk_insert_mappings(Model, list_of_dicts)` skips ORM overhead. Even faster: `session.execute(insert(Model), data)`. Avoid `session.add()` in loops. For PostgreSQL, use `insert().on_conflict_do_nothing()` for upserts. Batch in chunks of 1000-5000 rows for optimal performance.", "code_signature": "Use session.bulk_insert_mappings() or execute(insert(), list_of_dicts) for maximum speed.", "tags": "session, query, relationship, postgresql, transaction, sqlalchemy", "url": "https://stackoverflow.com/questions/5977931", "author": "user_238275", "date_published": "28-02-2021", "votes_or_stars": 663, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:f413e23b-fdda-4934-a22d-53b2d605837a": {"content": "How to add retry logic to requests?\n```python\nfrom requests.adapters import HTTPAdapter\nfrom urllib3.util.retry import Retry\nretry = Retry(total=3, backoff_factor=1, status_forcelist=[500,502,503,504])\nadapter = HTTPAdapter(max_retries=retry)\nsession.mount('https://', adapter)\n```\nCode signature: Mount an HTTPAdapter with urllib3 Retry on the Session.", "file_path": "https://stackoverflow.com/questions/1669324", "function_name": "Mount an HTTPAdapter with urllib3 Retry on the Session.", "type": "stack_overflow", "metadata": {"record_id": "f413e23b-fdda-4934-a22d-53b2d605837a", "source_type": "stack_overflow", "domain": "Requests/HTTP/APIs", "library": "websocket", "title": "How to add retry logic to requests?", "content": "```python\nfrom requests.adapters import HTTPAdapter\nfrom urllib3.util.retry import Retry\nretry = Retry(total=3, backoff_factor=1, status_forcelist=[500,502,503,504])\nadapter = HTTPAdapter(max_retries=retry)\nsession.mount('https://', adapter)\n```", "code_signature": "Mount an HTTPAdapter with urllib3 Retry on the Session.", "tags": "rest-api, httpx, oauth, retry, timeout, json", "url": "https://stackoverflow.com/questions/1669324", "author": "user_952590", "date_published": "01-08-2018", "votes_or_stars": 1527, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:7d6f396d-a683-4bb7-bb29-c75b42258c45": {"content": "FEAT: Django: add support for async ORM queries\nProblem: Django ORM is synchronous; using it in async views requires sync_to_async wrapper.\n\nFix/Discussion: Django 4.1+ added native async ORM support. Use `await Model.objects.filter().afirst()`, `async for obj in qs:`. For older versions: wrap with `sync_to_async`: `get_user = sync_to_async(User.objects.get)\nuser = await get_user(pk=1)`.\nCode signature: status:closed", "file_path": "https://github.com/django/django/issues/8618", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "7d6f396d-a683-4bb7-bb29-c75b42258c45", "source_type": "github_issue", "domain": "FastAPI/Flask/Django", "library": "django", "title": "FEAT: Django: add support for async ORM queries", "content": "Problem: Django ORM is synchronous; using it in async views requires sync_to_async wrapper.\n\nFix/Discussion: Django 4.1+ added native async ORM support. Use `await Model.objects.filter().afirst()`, `async for obj in qs:`. For older versions: wrap with `sync_to_async`: `get_user = sync_to_async(User.objects.get)\nuser = await get_user(pk=1)`.", "code_signature": "status:closed", "tags": "middleware, pydantic, request, dependency-injection, blueprint", "url": "https://github.com/django/django/issues/8618", "author": "contributor_8921", "date_published": "05-03-2021", "votes_or_stars": 1, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:0af717e5-b0df-4461-8fea-97161c9af3f1": {"content": "NumPy — Linear Algebra\nThe `np.einsum` function in NumPy evaluates Einstein summation convention on operands. It accepts `subscripts, *operands` as a parameter and returns ndarray. Common use cases include matrix multiplication, dot products, tensor contractions. Note that prefer np.einsum over loops for large arrays.\nCode signature: np.einsum(subscripts, *operands) -> ndarray", "file_path": "https://docs.numpy.org/en/stable/np/einsum.html", "function_name": "np.einsum(subscripts, *operands) -> ndarray", "type": "documentation", "metadata": {"record_id": "0af717e5-b0df-4461-8fea-97161c9af3f1", "source_type": "documentation", "domain": "NumPy/Pandas", "library": "NumPy", "title": "NumPy — Linear Algebra", "content": "The `np.einsum` function in NumPy evaluates Einstein summation convention on operands. It accepts `subscripts, *operands` as a parameter and returns ndarray. Common use cases include matrix multiplication, dot products, tensor contractions. Note that prefer np.einsum over loops for large arrays.", "code_signature": "np.einsum(subscripts, *operands) -> ndarray", "tags": "nan, dataframe, pivot, pandas, array, merge", "url": "https://docs.numpy.org/en/stable/np/einsum.html", "author": "Official Docs", "date_published": "08-11-2021", "votes_or_stars": 1900, "relevance_label": "low", "difficulty_level": "beginner"}}, "kb:98328fad-c1a2-4ecd-be0b-e60f3ba9a619": {"content": "Django N+1 query problem: how to fix?\nN+1 occurs when accessing related objects in a loop. Diagnose with `django-debug-toolbar` or `connection.queries`. Fix ForeignKey: `User.objects.select_related('profile').all()`. Fix M2M: `Post.objects.prefetch_related('tags').all()`. Use `only()` to fetch specific fields and reduce data transfer.\nCode signature: Use select_related() for ForeignKey and prefetch_related() for ManyToMany relationships.", "file_path": "https://stackoverflow.com/questions/8106470", "function_name": "Use select_related() for ForeignKey and prefetch_related() for ManyToMany relationships.", "type": "stack_overflow", "metadata": {"record_id": "98328fad-c1a2-4ecd-be0b-e60f3ba9a619", "source_type": "stack_overflow", "domain": "FastAPI/Flask/Django", "library": "request", "title": "Django N+1 query problem: how to fix?", "content": "N+1 occurs when accessing related objects in a loop. Diagnose with `django-debug-toolbar` or `connection.queries`. Fix ForeignKey: `User.objects.select_related('profile').all()`. Fix M2M: `Post.objects.prefetch_related('tags').all()`. Use `only()` to fetch specific fields and reduce data transfer.", "code_signature": "Use select_related() for ForeignKey and prefetch_related() for ManyToMany relationships.", "tags": "pydantic, rest, request, dependency-injection, blueprint", "url": "https://stackoverflow.com/questions/8106470", "author": "user_441071", "date_published": "05-05-2024", "votes_or_stars": 1755, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:3b6f1cd9-3469-4a29-af56-e991ff2d0e97": {"content": "How to handle database migrations with Alembic?\nWorkflow: (1) Change SQLAlchemy models. (2) `alembic revision --autogenerate -m 'description'`. (3) Review generated migration script. (4) `alembic upgrade head` to apply. Always review autogenerated migrations — they miss some changes (indexes, constraints). Use `alembic downgrade -1` to rollback. Store migrations in version control.\nCode signature: Use alembic revision --autogenerate to create migrations from model changes.", "file_path": "https://stackoverflow.com/questions/9436429", "function_name": "Use alembic revision --autogenerate to create migrations from model changes.", "type": "stack_overflow", "metadata": {"record_id": "3b6f1cd9-3469-4a29-af56-e991ff2d0e97", "source_type": "stack_overflow", "domain": "SQLAlchemy/Databases", "library": "connection-pool", "title": "How to handle database migrations with Alembic?", "content": "Workflow: (1) Change SQLAlchemy models. (2) `alembic revision --autogenerate -m 'description'`. (3) Review generated migration script. (4) `alembic upgrade head` to apply. Always review autogenerated migrations — they miss some changes (indexes, constraints). Use `alembic downgrade -1` to rollback. Store migrations in version control.", "code_signature": "Use alembic revision --autogenerate to create migrations from model changes.", "tags": "alembic, transaction, session, sqlalchemy, migration", "url": "https://stackoverflow.com/questions/9436429", "author": "user_974047", "date_published": "09-02-2024", "votes_or_stars": 2422, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:bdefb5c3-d430-4b02-bd23-d34656620fff": {"content": "Python: how to safely write to a file atomically?\n```python\nimport tempfile, os\nwith tempfile.NamedTemporaryFile('w', delete=False, dir='.') as tmp:\n    tmp.write(data)\n    tmp.flush()\n    os.fsync(tmp.fileno())\nos.replace(tmp.name, target_path)\n```\n`os.replace()` is atomic on POSIX. This prevents partial writes from corrupting files.\nCode signature: Write to a temp file then rename — rename is atomic on POSIX systems.", "file_path": "https://stackoverflow.com/questions/5394880", "function_name": "Write to a temp file then rename — rename is atomic on POSIX systems.", "type": "stack_overflow", "metadata": {"record_id": "bdefb5c3-d430-4b02-bd23-d34656620fff", "source_type": "stack_overflow", "domain": "OS/Subprocess/File I/O", "library": "stdin", "title": "Python: how to safely write to a file atomically?", "content": "```python\nimport tempfile, os\nwith tempfile.NamedTemporaryFile('w', delete=False, dir='.') as tmp:\n    tmp.write(data)\n    tmp.flush()\n    os.fsync(tmp.fileno())\nos.replace(tmp.name, target_path)\n```\n`os.replace()` is atomic on POSIX. This prevents partial writes from corrupting files.", "code_signature": "Write to a temp file then rename — rename is atomic on POSIX systems.", "tags": "subprocess, os, env-variable, pathlib, tempfile, glob", "url": "https://stackoverflow.com/questions/5394880", "author": "user_179430", "date_published": "22-01-2023", "votes_or_stars": 573, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:314706bf-4156-4262-bed2-8936fb9f4c58": {"content": "FEAT: Django: add support for async ORM queries\nProblem: Django ORM is synchronous; using it in async views requires sync_to_async wrapper.\n\nFix/Discussion: Django 4.1+ added native async ORM support. Use `await Model.objects.filter().afirst()`, `async for obj in qs:`. For older versions: wrap with `sync_to_async`: `get_user = sync_to_async(User.objects.get)\nuser = await get_user(pk=1)`.\nCode signature: status:closed", "file_path": "https://github.com/django/django/issues/8618", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "314706bf-4156-4262-bed2-8936fb9f4c58", "source_type": "github_issue", "domain": "FastAPI/Flask/Django", "library": "django", "title": "FEAT: Django: add support for async ORM queries", "content": "Problem: Django ORM is synchronous; using it in async views requires sync_to_async wrapper.\n\nFix/Discussion: Django 4.1+ added native async ORM support. Use `await Model.objects.filter().afirst()`, `async for obj in qs:`. For older versions: wrap with `sync_to_async`: `get_user = sync_to_async(User.objects.get)\nuser = await get_user(pk=1)`.", "code_signature": "status:closed", "tags": "django, pydantic, dependency-injection", "url": "https://github.com/django/django/issues/8618", "author": "contributor_8921", "date_published": "03-03-2020", "votes_or_stars": 1, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:09fffb2a-8355-40db-90db-cb827a48d187": {"content": "How to add retry logic to requests?\n```python\nfrom requests.adapters import HTTPAdapter\nfrom urllib3.util.retry import Retry\nretry = Retry(total=3, backoff_factor=1, status_forcelist=[500,502,503,504])\nadapter = HTTPAdapter(max_retries=retry)\nsession.mount('https://', adapter)\n```\nCode signature: Mount an HTTPAdapter with urllib3 Retry on the Session.", "file_path": "https://stackoverflow.com/questions/1669324", "function_name": "Mount an HTTPAdapter with urllib3 Retry on the Session.", "type": "stack_overflow", "metadata": {"record_id": "09fffb2a-8355-40db-90db-cb827a48d187", "source_type": "stack_overflow", "domain": "Requests/HTTP/APIs", "library": "websocket", "title": "How to add retry logic to requests?", "content": "```python\nfrom requests.adapters import HTTPAdapter\nfrom urllib3.util.retry import Retry\nretry = Retry(total=3, backoff_factor=1, status_forcelist=[500,502,503,504])\nadapter = HTTPAdapter(max_retries=retry)\nsession.mount('https://', adapter)\n```", "code_signature": "Mount an HTTPAdapter with urllib3 Retry on the Session.", "tags": "httpx, aiohttp, requests", "url": "https://stackoverflow.com/questions/1669324", "author": "user_952590", "date_published": "04-08-2020", "votes_or_stars": 1502, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:ace783e4-5631-4521-a55c-0a55ab94b82a": {"content": "PERF: requests connection not reused between calls — new TCP connection each time\nProblem: Each requests.get() call opens a new TCP connection, causing high latency.\n\nFix/Discussion: requests.get/post etc. create a new Session per call. Fix: use a persistent Session: `session = requests.Session()` and reuse it. Sessions pool connections via urllib3. For thread-safety: create one Session per thread. For async: use httpx.AsyncClient or aiohttp.ClientSession for async connection pooling.\nCode signature: status:closed", "file_path": "https://github.com/requests/requests/issues/3605", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "ace783e4-5631-4521-a55c-0a55ab94b82a", "source_type": "github_issue", "domain": "Requests/HTTP/APIs", "library": "requests", "title": "PERF: requests connection not reused between calls — new TCP connection each time", "content": "Problem: Each requests.get() call opens a new TCP connection, causing high latency.\n\nFix/Discussion: requests.get/post etc. create a new Session per call. Fix: use a persistent Session: `session = requests.Session()` and reuse it. Sessions pool connections via urllib3. For thread-safety: create one Session per thread. For async: use httpx.AsyncClient or aiohttp.ClientSession for async connection pooling.", "code_signature": "status:closed", "tags": "rest-api, httpx, oauth, rate-limiting, requests", "url": "https://github.com/requests/requests/issues/3605", "author": "contributor_1152", "date_published": "02-10-2022", "votes_or_stars": 340, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:25a1f393-ad92-44aa-b7c4-e01fce040aad": {"content": "How to handle database migrations with Alembic?\nWorkflow: (1) Change SQLAlchemy models. (2) `alembic revision --autogenerate -m 'description'`. (3) Review generated migration script. (4) `alembic upgrade head` to apply. Always review autogenerated migrations — they miss some changes (indexes, constraints). Use `alembic downgrade -1` to rollback. Store migrations in version control.\nCode signature: Use alembic revision --autogenerate to create migrations from model changes.", "file_path": "https://stackoverflow.com/questions/9436429", "function_name": "Use alembic revision --autogenerate to create migrations from model changes.", "type": "stack_overflow", "metadata": {"record_id": "25a1f393-ad92-44aa-b7c4-e01fce040aad", "source_type": "stack_overflow", "domain": "SQLAlchemy/Databases", "library": "connection-pool", "title": "How to handle database migrations with Alembic?", "content": "Workflow: (1) Change SQLAlchemy models. (2) `alembic revision --autogenerate -m 'description'`. (3) Review generated migration script. (4) `alembic upgrade head` to apply. Always review autogenerated migrations — they miss some changes (indexes, constraints). Use `alembic downgrade -1` to rollback. Store migrations in version control.", "code_signature": "Use alembic revision --autogenerate to create migrations from model changes.", "tags": "relationship, foreign-key, sqlite, alembic, transaction, connection-pool", "url": "https://stackoverflow.com/questions/9436429", "author": "user_974047", "date_published": "29-05-2020", "votes_or_stars": 2400, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:c9d69902-0962-4a09-b3ab-f5f9a964472b": {"content": "FEAT: pathlib: add support for reading/writing with encoding shorthand\nProblem: pathlib.Path.read_text() does not default to UTF-8, causing issues across platforms.\n\nFix/Discussion: Always pass encoding explicitly: `Path('file').read_text(encoding='utf-8')`. Python 3.10+ added `encoding='locale'` default change proposal. Best practice: always specify encoding in all file I/O operations for portability.\nCode signature: status:closed", "file_path": "https://github.com/cpython/cpython/issues/2707", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "c9d69902-0962-4a09-b3ab-f5f9a964472b", "source_type": "github_issue", "domain": "OS/Subprocess/File I/O", "library": "cpython", "title": "FEAT: pathlib: add support for reading/writing with encoding shorthand", "content": "Problem: pathlib.Path.read_text() does not default to UTF-8, causing issues across platforms.\n\nFix/Discussion: Always pass encoding explicitly: `Path('file').read_text(encoding='utf-8')`. Python 3.10+ added `encoding='locale'` default change proposal. Best practice: always specify encoding in all file I/O operations for portability.", "code_signature": "status:closed", "tags": "glob, stdout, file-io, subprocess, stdin, pipe", "url": "https://github.com/cpython/cpython/issues/2707", "author": "contributor_7877", "date_published": "01-07-2021", "votes_or_stars": 470, "relevance_label": "low", "difficulty_level": "advanced"}}, "kb:bc3aabc0-2e74-4fbe-b408-9abc89aad8a0": {"content": "How to use @app.route in Flask\n`Flask.@app.route` maps a URL rule to a view function. Example usage:\n```python\n@app.route('/users/<int:uid>')\ndef get_user(uid):\n    return jsonify(user)\n```\nThis is useful when defining HTTP endpoints in a Flask application. Performance tip: use Blueprints to organize large applications.\nCode signature: @app.route", "file_path": "https://docs.flask.org/en/stable/@app/route.html", "function_name": "@app.route", "type": "documentation", "metadata": {"record_id": "bc3aabc0-2e74-4fbe-b408-9abc89aad8a0", "source_type": "documentation", "domain": "FastAPI/Flask/Django", "library": "Flask", "title": "How to use @app.route in Flask", "content": "`Flask.@app.route` maps a URL rule to a view function. Example usage:\n```python\n@app.route('/users/<int:uid>')\ndef get_user(uid):\n    return jsonify(user)\n```\nThis is useful when defining HTTP endpoints in a Flask application. Performance tip: use Blueprints to organize large applications.", "code_signature": "@app.route", "tags": "django, response, pydantic, template, flask", "url": "https://docs.flask.org/en/stable/@app/route.html", "author": "Official Docs", "date_published": "24-02-2019", "votes_or_stars": 2988, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:60b38591-4152-4b38-a0ca-bffc53400bbf": {"content": "Python threading: how to share data between threads safely?\nFor simple counters: use `threading.Lock()` as context manager. For complex shared state: prefer `queue.Queue` (thread-safe by design). Avoid global variables without locks. `threading.local()` gives each thread its own data copy. For read-heavy workloads, `threading.RLock` allows re-entrant acquisition.\nCode signature: Use threading.Lock for mutual exclusion or queue.Queue for producer-consumer patterns.", "file_path": "https://stackoverflow.com/questions/3195773", "function_name": "Use threading.Lock for mutual exclusion or queue.Queue for producer-consumer patterns.", "type": "stack_overflow", "metadata": {"record_id": "60b38591-4152-4b38-a0ca-bffc53400bbf", "source_type": "stack_overflow", "domain": "Asyncio/Threading", "library": "lock", "title": "Python threading: how to share data between threads safely?", "content": "For simple counters: use `threading.Lock()` as context manager. For complex shared state: prefer `queue.Queue` (thread-safe by design). Avoid global variables without locks. `threading.local()` gives each thread its own data copy. For read-heavy workloads, `threading.RLock` allows re-entrant acquisition.", "code_signature": "Use threading.Lock for mutual exclusion or queue.Queue for producer-consumer patterns.", "tags": "await, race-condition, coroutine, async, asyncio", "url": "https://stackoverflow.com/questions/3195773", "author": "user_714318", "date_published": "20-10-2023", "votes_or_stars": 1117, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:43cdb84c-cb08-4378-bc79-7c5f6a0545a2": {"content": "How to implement database connection pooling with SQLAlchemy?\n`create_engine(url, pool_size=10, max_overflow=20, pool_timeout=30, pool_recycle=1800)`. Use `pool_pre_ping=True` to detect stale connections. For async: use `AsyncEngine` with `create_async_engine()`. Monitor with `engine.pool.status()`. NullPool disables pooling for scripts/serverless environments.\nCode signature: create_engine() has built-in pooling. Configure pool_size and max_overflow.", "file_path": "https://stackoverflow.com/questions/7358113", "function_name": "create_engine() has built-in pooling. Configure pool_size and max_overflow.", "type": "stack_overflow", "metadata": {"record_id": "43cdb84c-cb08-4378-bc79-7c5f6a0545a2", "source_type": "stack_overflow", "domain": "SQLAlchemy/Databases", "library": "session", "title": "How to implement database connection pooling with SQLAlchemy?", "content": "`create_engine(url, pool_size=10, max_overflow=20, pool_timeout=30, pool_recycle=1800)`. Use `pool_pre_ping=True` to detect stale connections. For async: use `AsyncEngine` with `create_async_engine()`. Monitor with `engine.pool.status()`. NullPool disables pooling for scripts/serverless environments.", "code_signature": "create_engine() has built-in pooling. Configure pool_size and max_overflow.", "tags": "sqlalchemy, session, foreign-key", "url": "https://stackoverflow.com/questions/7358113", "author": "user_12260", "date_published": "12-07-2021", "votes_or_stars": 274, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:d87f0023-0bcd-4239-b6be-457a2d1dc7f1": {"content": "BUG: asyncio task leaks when exception is not handled\nProblem: Unhandled exceptions in fire-and-forget asyncio tasks are silently ignored, causing task leaks.\n\nFix/Discussion: Always await tasks or add exception handlers. Use `task.add_done_callback()` to log exceptions. In Python 3.11+, use TaskGroup for structured concurrency — exceptions propagate automatically. Set `asyncio.get_event_loop().set_exception_handler()` for global unhandled task exception logging.\nCode signature: status:closed", "file_path": "https://github.com/cpython/cpython/issues/5591", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "d87f0023-0bcd-4239-b6be-457a2d1dc7f1", "source_type": "github_issue", "domain": "Asyncio/Threading", "library": "cpython", "title": "BUG: asyncio task leaks when exception is not handled", "content": "Problem: Unhandled exceptions in fire-and-forget asyncio tasks are silently ignored, causing task leaks.\n\nFix/Discussion: Always await tasks or add exception handlers. Use `task.add_done_callback()` to log exceptions. In Python 3.11+, use TaskGroup for structured concurrency — exceptions propagate automatically. Set `asyncio.get_event_loop().set_exception_handler()` for global unhandled task exception logging.", "code_signature": "status:closed", "tags": "task, asyncio, gather, async, event-loop, coroutine", "url": "https://github.com/cpython/cpython/issues/5591", "author": "contributor_1630", "date_published": "26-12-2023", "votes_or_stars": 338, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:65546b72-44ef-4b6c-9569-63db4c865849": {"content": "BUG: SettingWithCopyWarning when modifying sliced DataFrame\nProblem: Modifying a slice of a DataFrame raises SettingWithCopyWarning and may not update original.\n\nFix/Discussion: Always use .loc or .copy() when slicing. Use `df.loc[mask, 'col'] = val` for assignment. Create explicit copies: `subset = df[mask].copy()`. In Pandas 2.0+, Copy-on-Write makes this behavior consistent — enable with `pd.options.mode.copy_on_write = True`.\nCode signature: status:closed", "file_path": "https://github.com/pandas/pandas/issues/3264", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "65546b72-44ef-4b6c-9569-63db4c865849", "source_type": "github_issue", "domain": "NumPy/Pandas", "library": "pandas", "title": "BUG: SettingWithCopyWarning when modifying sliced DataFrame", "content": "Problem: Modifying a slice of a DataFrame raises SettingWithCopyWarning and may not update original.\n\nFix/Discussion: Always use .loc or .copy() when slicing. Use `df.loc[mask, 'col'] = val` for assignment. Create explicit copies: `subset = df[mask].copy()`. In Pandas 2.0+, Copy-on-Write makes this behavior consistent — enable with `pd.options.mode.copy_on_write = True`.", "code_signature": "status:closed", "tags": "dataframe, pivot, pandas", "url": "https://github.com/pandas/pandas/issues/3264", "author": "contributor_6628", "date_published": "10-08-2019", "votes_or_stars": 42, "relevance_label": "low", "difficulty_level": "advanced"}}, "kb:bbe9225e-81be-402b-a3b1-7bd0e8033872": {"content": "SQLAlchemy — ORM Session Query\nThe `session.query` function in SQLAlchemy constructs a SELECT query against mapped entities. It accepts `*entities` as a parameter and returns Query object. Common use cases include retrieving ORM-mapped objects from the database. Note that prefer session.execute(select(...)) in SQLAlchemy 2.0+.\nCode signature: session.query(*entities) -> Query object", "file_path": "https://docs.sqlalchemy.org/en/stable/session/query.html", "function_name": "session.query(*entities) -> Query object", "type": "documentation", "metadata": {"record_id": "bbe9225e-81be-402b-a3b1-7bd0e8033872", "source_type": "documentation", "domain": "SQLAlchemy/Databases", "library": "SQLAlchemy", "title": "SQLAlchemy — ORM Session Query", "content": "The `session.query` function in SQLAlchemy constructs a SELECT query against mapped entities. It accepts `*entities` as a parameter and returns Query object. Common use cases include retrieving ORM-mapped objects from the database. Note that prefer session.execute(select(...)) in SQLAlchemy 2.0+.", "code_signature": "session.query(*entities) -> Query object", "tags": "migration, sqlite, orm, transaction, relationship, sqlalchemy", "url": "https://docs.sqlalchemy.org/en/stable/session/query.html", "author": "Official Docs", "date_published": "18-02-2018", "votes_or_stars": 3011, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:04aaf556-4fb2-4ddb-9da3-7751f5246b8e": {"content": "threading Thread Synchronization — official reference\nOfficial documentation for threading Thread Synchronization. The method signature is `threading.Lock()`. creates a mutual-exclusion lock for thread-safe access. Raises `RuntimeError` when lock is released without being acquired. See also: threading.RLock, threading.Semaphore, threading.Event.\nCode signature: threading.Lock()", "file_path": "https://docs.threading.org/en/stable/threading/Lock.html", "function_name": "threading.Lock()", "type": "documentation", "metadata": {"record_id": "04aaf556-4fb2-4ddb-9da3-7751f5246b8e", "source_type": "documentation", "domain": "Asyncio/Threading", "library": "threading", "title": "threading Thread Synchronization — official reference", "content": "Official documentation for threading Thread Synchronization. The method signature is `threading.Lock()`. creates a mutual-exclusion lock for thread-safe access. Raises `RuntimeError` when lock is released without being acquired. See also: threading.RLock, threading.Semaphore, threading.Event.", "code_signature": "threading.Lock()", "tags": "task, event-loop, await, race-condition, asyncio", "url": "https://docs.threading.org/en/stable/threading/Lock.html", "author": "Official Docs", "date_published": "15-06-2020", "votes_or_stars": 4864, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:7357df0c-433c-43af-85c3-23b3272f2f47": {"content": "Django N+1 query problem: how to fix?\nN+1 occurs when accessing related objects in a loop. Diagnose with `django-debug-toolbar` or `connection.queries`. Fix ForeignKey: `User.objects.select_related('profile').all()`. Fix M2M: `Post.objects.prefetch_related('tags').all()`. Use `only()` to fetch specific fields and reduce data transfer.\nCode signature: Use select_related() for ForeignKey and prefetch_related() for ManyToMany relationships.", "file_path": "https://stackoverflow.com/questions/8106470", "function_name": "Use select_related() for ForeignKey and prefetch_related() for ManyToMany relationships.", "type": "stack_overflow", "metadata": {"record_id": "7357df0c-433c-43af-85c3-23b3272f2f47", "source_type": "stack_overflow", "domain": "FastAPI/Flask/Django", "library": "request", "title": "Django N+1 query problem: how to fix?", "content": "N+1 occurs when accessing related objects in a loop. Diagnose with `django-debug-toolbar` or `connection.queries`. Fix ForeignKey: `User.objects.select_related('profile').all()`. Fix M2M: `Post.objects.prefetch_related('tags').all()`. Use `only()` to fetch specific fields and reduce data transfer.", "code_signature": "Use select_related() for ForeignKey and prefetch_related() for ManyToMany relationships.", "tags": "orm, response, flask, template, middleware, routing", "url": "https://stackoverflow.com/questions/8106470", "author": "user_441071", "date_published": "22-01-2018", "votes_or_stars": 1796, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:45bfdfa7-8b89-49ae-81a5-1d83bf0d2098": {"content": "Python: how to safely write to a file atomically?\n```python\nimport tempfile, os\nwith tempfile.NamedTemporaryFile('w', delete=False, dir='.') as tmp:\n    tmp.write(data)\n    tmp.flush()\n    os.fsync(tmp.fileno())\nos.replace(tmp.name, target_path)\n```\n`os.replace()` is atomic on POSIX. This prevents partial writes from corrupting files.\nCode signature: Write to a temp file then rename — rename is atomic on POSIX systems.", "file_path": "https://stackoverflow.com/questions/5394880", "function_name": "Write to a temp file then rename — rename is atomic on POSIX systems.", "type": "stack_overflow", "metadata": {"record_id": "45bfdfa7-8b89-49ae-81a5-1d83bf0d2098", "source_type": "stack_overflow", "domain": "OS/Subprocess/File I/O", "library": "stdin", "title": "Python: how to safely write to a file atomically?", "content": "```python\nimport tempfile, os\nwith tempfile.NamedTemporaryFile('w', delete=False, dir='.') as tmp:\n    tmp.write(data)\n    tmp.flush()\n    os.fsync(tmp.fileno())\nos.replace(tmp.name, target_path)\n```\n`os.replace()` is atomic on POSIX. This prevents partial writes from corrupting files.", "code_signature": "Write to a temp file then rename — rename is atomic on POSIX systems.", "tags": "stderr, shutil, subprocess, pathlib, pipe, os", "url": "https://stackoverflow.com/questions/5394880", "author": "user_179430", "date_published": "01-12-2024", "votes_or_stars": 538, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:78a40e0a-6f78-4e9d-89d9-79c49ffac98a": {"content": "threading Thread Synchronization — official reference\nOfficial documentation for threading Thread Synchronization. The method signature is `threading.Lock()`. creates a mutual-exclusion lock for thread-safe access. Raises `RuntimeError` when lock is released without being acquired. See also: threading.RLock, threading.Semaphore, threading.Event.\nCode signature: threading.Lock()", "file_path": "https://docs.threading.org/en/stable/threading/Lock.html", "function_name": "threading.Lock()", "type": "documentation", "metadata": {"record_id": "78a40e0a-6f78-4e9d-89d9-79c49ffac98a", "source_type": "documentation", "domain": "Asyncio/Threading", "library": "threading", "title": "threading Thread Synchronization — official reference", "content": "Official documentation for threading Thread Synchronization. The method signature is `threading.Lock()`. creates a mutual-exclusion lock for thread-safe access. Raises `RuntimeError` when lock is released without being acquired. See also: threading.RLock, threading.Semaphore, threading.Event.", "code_signature": "threading.Lock()", "tags": "semaphore, task, threading", "url": "https://docs.threading.org/en/stable/threading/Lock.html", "author": "Official Docs", "date_published": "26-05-2020", "votes_or_stars": 4825, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:f1573f39-4762-4e6d-a638-900cdd9874be": {"content": "NumPy broadcasting error: operands could not be broadcast with shapes\nNumPy broadcasts shapes right-to-left. Arrays (3,4) and (4,) are compatible. Arrays (3,4) and (3,) are not — reshape to (3,1). Use `arr.reshape(-1,1)` to add a dimension. Always print `a.shape, b.shape` before operations to debug. `np.expand_dims(arr, axis=1)` is cleaner than reshape for adding dimensions.\nCode signature: Shapes must be compatible: dimensions must match or one of them must be 1.", "file_path": "https://stackoverflow.com/questions/4860684", "function_name": "Shapes must be compatible: dimensions must match or one of them must be 1.", "type": "stack_overflow", "metadata": {"record_id": "f1573f39-4762-4e6d-a638-900cdd9874be", "source_type": "stack_overflow", "domain": "NumPy/Pandas", "library": "numpy", "title": "NumPy broadcasting error: operands could not be broadcast with shapes", "content": "NumPy broadcasts shapes right-to-left. Arrays (3,4) and (4,) are compatible. Arrays (3,4) and (3,) are not — reshape to (3,1). Use `arr.reshape(-1,1)` to add a dimension. Always print `a.shape, b.shape` before operations to debug. `np.expand_dims(arr, axis=1)` is cleaner than reshape for adding dimensions.", "code_signature": "Shapes must be compatible: dimensions must match or one of them must be 1.", "tags": "pivot, broadcasting, iloc", "url": "https://stackoverflow.com/questions/4860684", "author": "user_627024", "date_published": "25-10-2022", "votes_or_stars": 1381, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:7db09454-1d13-4ead-8acf-050a6a0204e4": {"content": "BUG: FastAPI dependency_overrides not working in pytest\nProblem: Dependency overrides set in conftest are not applied during test execution.\n\nFix/Discussion: Set overrides on the app object before TestClient is created. Use `app.dependency_overrides[dep] = mock_dep` in a pytest fixture with function scope. Clear overrides after test: `app.dependency_overrides = {}`. Ensure TestClient is created after overrides are set.\nCode signature: status:closed", "file_path": "https://github.com/fastapi/fastapi/issues/8479", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "7db09454-1d13-4ead-8acf-050a6a0204e4", "source_type": "github_issue", "domain": "FastAPI/Flask/Django", "library": "fastapi", "title": "BUG: FastAPI dependency_overrides not working in pytest", "content": "Problem: Dependency overrides set in conftest are not applied during test execution.\n\nFix/Discussion: Set overrides on the app object before TestClient is created. Use `app.dependency_overrides[dep] = mock_dep` in a pytest fixture with function scope. Clear overrides after test: `app.dependency_overrides = {}`. Ensure TestClient is created after overrides are set.", "code_signature": "status:closed", "tags": "fastapi, blueprint, request, django, rest", "url": "https://github.com/fastapi/fastapi/issues/8479", "author": "contributor_1994", "date_published": "19-03-2023", "votes_or_stars": 321, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:ffed51be-2c1c-46a3-b26e-ddec6c52f3f6": {"content": "How do I reshape a wide DataFrame to long format?\n`pd.melt(df, id_vars=['id'], value_vars=['col1','col2'])` unpivots columns to rows. `df.stack()` moves column index to row index. For the reverse, use `df.pivot()` or `df.unstack()`. Always reset_index() after stack/unstack to get a clean DataFrame.\nCode signature: Use pd.melt() or df.stack() depending on your structure.", "file_path": "https://stackoverflow.com/questions/6229731", "function_name": "Use pd.melt() or df.stack() depending on your structure.", "type": "stack_overflow", "metadata": {"record_id": "ffed51be-2c1c-46a3-b26e-ddec6c52f3f6", "source_type": "stack_overflow", "domain": "NumPy/Pandas", "library": "nan", "title": "How do I reshape a wide DataFrame to long format?", "content": "`pd.melt(df, id_vars=['id'], value_vars=['col1','col2'])` unpivots columns to rows. `df.stack()` moves column index to row index. For the reverse, use `df.pivot()` or `df.unstack()`. Always reset_index() after stack/unstack to get a clean DataFrame.", "code_signature": "Use pd.melt() or df.stack() depending on your structure.", "tags": "array, broadcasting, nan, pivot", "url": "https://stackoverflow.com/questions/6229731", "author": "user_428373", "date_published": "05-05-2019", "votes_or_stars": 850, "relevance_label": "low", "difficulty_level": "advanced"}}, "kb:8511bb58-973f-4396-ba27-b7c8321d364c": {"content": "Python threading: how to share data between threads safely?\nFor simple counters: use `threading.Lock()` as context manager. For complex shared state: prefer `queue.Queue` (thread-safe by design). Avoid global variables without locks. `threading.local()` gives each thread its own data copy. For read-heavy workloads, `threading.RLock` allows re-entrant acquisition.\nCode signature: Use threading.Lock for mutual exclusion or queue.Queue for producer-consumer patterns.", "file_path": "https://stackoverflow.com/questions/3195773", "function_name": "Use threading.Lock for mutual exclusion or queue.Queue for producer-consumer patterns.", "type": "stack_overflow", "metadata": {"record_id": "8511bb58-973f-4396-ba27-b7c8321d364c", "source_type": "stack_overflow", "domain": "Asyncio/Threading", "library": "lock", "title": "Python threading: how to share data between threads safely?", "content": "For simple counters: use `threading.Lock()` as context manager. For complex shared state: prefer `queue.Queue` (thread-safe by design). Avoid global variables without locks. `threading.local()` gives each thread its own data copy. For read-heavy workloads, `threading.RLock` allows re-entrant acquisition.", "code_signature": "Use threading.Lock for mutual exclusion or queue.Queue for producer-consumer patterns.", "tags": "deadlock, semaphore, executor, async", "url": "https://stackoverflow.com/questions/3195773", "author": "user_714318", "date_published": "26-05-2024", "votes_or_stars": 1078, "relevance_label": "low", "difficulty_level": "intermediate"}}, "kb:3f79a80c-ceaf-4dbb-bde7-ecf3560863c9": {"content": "How to apply a function to each row in Pandas without using iterrows?\n`iterrows()` is slow because it creates a Series per row. Prefer `df.apply(func, axis=1)` for row-wise operations, or better yet, vectorize using NumPy: `np.where(condition, a, b)`. For complex logic, `df.assign()` with vectorized expressions is cleanest. Benchmark: vectorized > apply > itertuples > iterrows.\nCode signature: Use df.apply(func, axis=1) or vectorized NumPy operations for best performance.", "file_path": "https://stackoverflow.com/questions/5446912", "function_name": "Use df.apply(func, axis=1) or vectorized NumPy operations for best performance.", "type": "stack_overflow", "metadata": {"record_id": "3f79a80c-ceaf-4dbb-bde7-ecf3560863c9", "source_type": "stack_overflow", "domain": "NumPy/Pandas", "library": "dataframe", "title": "How to apply a function to each row in Pandas without using iterrows?", "content": "`iterrows()` is slow because it creates a Series per row. Prefer `df.apply(func, axis=1)` for row-wise operations, or better yet, vectorize using NumPy: `np.where(condition, a, b)`. For complex logic, `df.assign()` with vectorized expressions is cleanest. Benchmark: vectorized > apply > itertuples > iterrows.", "code_signature": "Use df.apply(func, axis=1) or vectorized NumPy operations for best performance.", "tags": "reshape, dataframe, broadcasting, dtype, pivot", "url": "https://stackoverflow.com/questions/5446912", "author": "user_563306", "date_published": "20-10-2024", "votes_or_stars": 2261, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:6791cb23-f811-487d-81f7-cf44f6ef746a": {"content": "How to run background tasks in FastAPI?\nFastAPI's `BackgroundTasks`: add `background_tasks: BackgroundTasks` as a parameter, then `background_tasks.add_task(func, *args)`. Runs after response is sent. For heavy workloads (email, ML inference), use Celery with Redis/RabbitMQ broker. For async background work, `asyncio.create_task()` works within async endpoints.\nCode signature: Use BackgroundTasks for lightweight tasks or Celery for heavy/distributed work.", "file_path": "https://stackoverflow.com/questions/8077999", "function_name": "Use BackgroundTasks for lightweight tasks or Celery for heavy/distributed work.", "type": "stack_overflow", "metadata": {"record_id": "6791cb23-f811-487d-81f7-cf44f6ef746a", "source_type": "stack_overflow", "domain": "FastAPI/Flask/Django", "library": "django", "title": "How to run background tasks in FastAPI?", "content": "FastAPI's `BackgroundTasks`: add `background_tasks: BackgroundTasks` as a parameter, then `background_tasks.add_task(func, *args)`. Runs after response is sent. For heavy workloads (email, ML inference), use Celery with Redis/RabbitMQ broker. For async background work, `asyncio.create_task()` works within async endpoints.", "code_signature": "Use BackgroundTasks for lightweight tasks or Celery for heavy/distributed work.", "tags": "pydantic, response, blueprint, orm, django", "url": "https://stackoverflow.com/questions/8077999", "author": "user_202401", "date_published": "25-05-2023", "votes_or_stars": 1853, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:446e1ef3-c6ea-43cb-a460-2220d90b0d76": {"content": "How to upload a file with requests?\n```python\nwith open('file.pdf', 'rb') as f:\n    r = requests.post(url, files={'file': ('file.pdf', f, 'application/pdf')})\n```\nFor multiple files: pass a list of tuples. For multipart form data with fields: combine `files` and `data` parameters. Don't set Content-Type manually — requests sets it with boundary.\nCode signature: Use the files parameter with a file-like object or tuple.", "file_path": "https://stackoverflow.com/questions/8896868", "function_name": "Use the files parameter with a file-like object or tuple.", "type": "stack_overflow", "metadata": {"record_id": "446e1ef3-c6ea-43cb-a460-2220d90b0d76", "source_type": "stack_overflow", "domain": "Requests/HTTP/APIs", "library": "requests", "title": "How to upload a file with requests?", "content": "```python\nwith open('file.pdf', 'rb') as f:\n    r = requests.post(url, files={'file': ('file.pdf', f, 'application/pdf')})\n```\nFor multiple files: pass a list of tuples. For multipart form data with fields: combine `files` and `data` parameters. Don't set Content-Type manually — requests sets it with boundary.", "code_signature": "Use the files parameter with a file-like object or tuple.", "tags": "rest-api, aiohttp, authentication, timeout", "url": "https://stackoverflow.com/questions/8896868", "author": "user_243238", "date_published": "08-04-2019", "votes_or_stars": 1566, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:28ba3412-f27c-41fd-b788-56e21d223142": {"content": "Why is my Pandas merge creating duplicate rows?\nDuplicate rows after merge usually mean the join key is not unique. Diagnose with `df.duplicated(subset=['key']).sum()`. Use `how='left'` carefully when right table has multiple matches. Set `validate='one_to_many'` or `'many_to_one'` to enforce cardinality. Consider deduplicating with `df.drop_duplicates(subset=['key'])` before merging.\nCode signature: Your merge key has duplicates in one or both DataFrames. Use validate='one_to_one' to catch this early.", "file_path": "https://stackoverflow.com/questions/2321324", "function_name": "Your merge key has duplicates in one or both DataFrames. Use validate='one_to_one' to catch this early.", "type": "stack_overflow", "metadata": {"record_id": "28ba3412-f27c-41fd-b788-56e21d223142", "source_type": "stack_overflow", "domain": "NumPy/Pandas", "library": "array", "title": "Why is my Pandas merge creating duplicate rows?", "content": "Duplicate rows after merge usually mean the join key is not unique. Diagnose with `df.duplicated(subset=['key']).sum()`. Use `how='left'` carefully when right table has multiple matches. Set `validate='one_to_many'` or `'many_to_one'` to enforce cardinality. Consider deduplicating with `df.drop_duplicates(subset=['key'])` before merging.", "code_signature": "Your merge key has duplicates in one or both DataFrames. Use validate='one_to_one' to catch this early.", "tags": "reshape, array, loc, pandas, nan, dtype", "url": "https://stackoverflow.com/questions/2321324", "author": "user_99814", "date_published": "22-12-2020", "votes_or_stars": 220, "relevance_label": "low", "difficulty_level": "intermediate"}}, "kb:1c390b34-3ce0-4697-96ee-3f6f7d1984db": {"content": "How to upload a file with requests?\n```python\nwith open('file.pdf', 'rb') as f:\n    r = requests.post(url, files={'file': ('file.pdf', f, 'application/pdf')})\n```\nFor multiple files: pass a list of tuples. For multipart form data with fields: combine `files` and `data` parameters. Don't set Content-Type manually — requests sets it with boundary.\nCode signature: Use the files parameter with a file-like object or tuple.", "file_path": "https://stackoverflow.com/questions/8896868", "function_name": "Use the files parameter with a file-like object or tuple.", "type": "stack_overflow", "metadata": {"record_id": "1c390b34-3ce0-4697-96ee-3f6f7d1984db", "source_type": "stack_overflow", "domain": "Requests/HTTP/APIs", "library": "requests", "title": "How to upload a file with requests?", "content": "```python\nwith open('file.pdf', 'rb') as f:\n    r = requests.post(url, files={'file': ('file.pdf', f, 'application/pdf')})\n```\nFor multiple files: pass a list of tuples. For multipart form data with fields: combine `files` and `data` parameters. Don't set Content-Type manually — requests sets it with boundary.", "code_signature": "Use the files parameter with a file-like object or tuple.", "tags": "http, aiohttp, timeout, rate-limiting", "url": "https://stackoverflow.com/questions/8896868", "author": "user_243238", "date_published": "26-04-2019", "votes_or_stars": 1586, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:dda118ad-7023-4673-a522-48b314224093": {"content": "How to limit concurrency with asyncio?\n```python\nsem = asyncio.Semaphore(10)\nasync def limited(url):\n    async with sem:\n        return await fetch(url)\nresults = await asyncio.gather(*[limited(u) for u in urls])\n```\nThis prevents overwhelming APIs or databases with too many simultaneous connections.\nCode signature: Use asyncio.Semaphore to cap the number of concurrent coroutines.", "file_path": "https://stackoverflow.com/questions/2375453", "function_name": "Use asyncio.Semaphore to cap the number of concurrent coroutines.", "type": "stack_overflow", "metadata": {"record_id": "dda118ad-7023-4673-a522-48b314224093", "source_type": "stack_overflow", "domain": "Asyncio/Threading", "library": "asyncio", "title": "How to limit concurrency with asyncio?", "content": "```python\nsem = asyncio.Semaphore(10)\nasync def limited(url):\n    async with sem:\n        return await fetch(url)\nresults = await asyncio.gather(*[limited(u) for u in urls])\n```\nThis prevents overwhelming APIs or databases with too many simultaneous connections.", "code_signature": "Use asyncio.Semaphore to cap the number of concurrent coroutines.", "tags": "semaphore, coroutine, lock, asyncio", "url": "https://stackoverflow.com/questions/2375453", "author": "user_449589", "date_published": "21-05-2019", "votes_or_stars": 2432, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:e7a46b3c-ac82-421d-80de-dff515ae64de": {"content": "requests.exceptions.SSLError: certificate verify failed\nNever use `verify=False` in production — it disables SSL verification entirely. Solutions: (1) Update certifi: `pip install --upgrade certifi`. (2) Provide CA bundle: `requests.get(url, verify='/path/to/ca-bundle.crt')`. (3) For self-signed certs in dev: `verify=False` only, and never commit this. (4) Check system time — expired system clock causes SSL failures.\nCode signature: Your SSL certificate chain is incomplete or self-signed. Fix the cert, don't disable verification.", "file_path": "https://stackoverflow.com/questions/7907400", "function_name": "Your SSL certificate chain is incomplete or self-signed. Fix the cert, don't disable verification.", "type": "stack_overflow", "metadata": {"record_id": "e7a46b3c-ac82-421d-80de-dff515ae64de", "source_type": "stack_overflow", "domain": "Requests/HTTP/APIs", "library": "websocket", "title": "requests.exceptions.SSLError: certificate verify failed", "content": "Never use `verify=False` in production — it disables SSL verification entirely. Solutions: (1) Update certifi: `pip install --upgrade certifi`. (2) Provide CA bundle: `requests.get(url, verify='/path/to/ca-bundle.crt')`. (3) For self-signed certs in dev: `verify=False` only, and never commit this. (4) Check system time — expired system clock causes SSL failures.", "code_signature": "Your SSL certificate chain is incomplete or self-signed. Fix the cert, don't disable verification.", "tags": "requests, rate-limiting, websocket, rest-api", "url": "https://stackoverflow.com/questions/7907400", "author": "user_851204", "date_published": "25-11-2019", "votes_or_stars": 1343, "relevance_label": "low", "difficulty_level": "advanced"}}, "kb:5dd859da-9b08-4a8e-8cbb-ae082e81ab30": {"content": "FEAT: Django: add support for async ORM queries\nProblem: Django ORM is synchronous; using it in async views requires sync_to_async wrapper.\n\nFix/Discussion: Django 4.1+ added native async ORM support. Use `await Model.objects.filter().afirst()`, `async for obj in qs:`. For older versions: wrap with `sync_to_async`: `get_user = sync_to_async(User.objects.get)\nuser = await get_user(pk=1)`.\nCode signature: status:closed", "file_path": "https://github.com/django/django/issues/8618", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "5dd859da-9b08-4a8e-8cbb-ae082e81ab30", "source_type": "github_issue", "domain": "FastAPI/Flask/Django", "library": "django", "title": "FEAT: Django: add support for async ORM queries", "content": "Problem: Django ORM is synchronous; using it in async views requires sync_to_async wrapper.\n\nFix/Discussion: Django 4.1+ added native async ORM support. Use `await Model.objects.filter().afirst()`, `async for obj in qs:`. For older versions: wrap with `sync_to_async`: `get_user = sync_to_async(User.objects.get)\nuser = await get_user(pk=1)`.", "code_signature": "status:closed", "tags": "orm, template, rest", "url": "https://github.com/django/django/issues/8618", "author": "contributor_8921", "date_published": "13-10-2020", "votes_or_stars": 17, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:fd4693a1-6bf0-4739-b549-fcf006cd8829": {"content": "BUG: requests hangs indefinitely on slow server responses\nProblem: requests.get() hangs forever when server accepts connection but never sends response.\n\nFix/Discussion: Always set both connect and read timeouts: `requests.get(url, timeout=(3.05, 27))`. Tuple: (connect_timeout, read_timeout). Or single value for both. Mount a Session with default timeouts using a custom HTTPAdapter subclass. Never use `timeout=None` in production code.\nCode signature: status:closed", "file_path": "https://github.com/requests/requests/issues/4019", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "fd4693a1-6bf0-4739-b549-fcf006cd8829", "source_type": "github_issue", "domain": "Requests/HTTP/APIs", "library": "requests", "title": "BUG: requests hangs indefinitely on slow server responses", "content": "Problem: requests.get() hangs forever when server accepts connection but never sends response.\n\nFix/Discussion: Always set both connect and read timeouts: `requests.get(url, timeout=(3.05, 27))`. Tuple: (connect_timeout, read_timeout). Or single value for both. Mount a Session with default timeouts using a custom HTTPAdapter subclass. Never use `timeout=None` in production code.", "code_signature": "status:closed", "tags": "requests, authentication, httpx, rest-api, timeout", "url": "https://github.com/requests/requests/issues/4019", "author": "contributor_4599", "date_published": "18-12-2024", "votes_or_stars": 282, "relevance_label": "low", "difficulty_level": "beginner"}}, "kb:cf8b5954-a8bd-410c-bceb-ede7926cb4ff": {"content": "SQLAlchemy bulk insert: what's the fastest way?\n`session.bulk_insert_mappings(Model, list_of_dicts)` skips ORM overhead. Even faster: `session.execute(insert(Model), data)`. Avoid `session.add()` in loops. For PostgreSQL, use `insert().on_conflict_do_nothing()` for upserts. Batch in chunks of 1000-5000 rows for optimal performance.\nCode signature: Use session.bulk_insert_mappings() or execute(insert(), list_of_dicts) for maximum speed.", "file_path": "https://stackoverflow.com/questions/5977931", "function_name": "Use session.bulk_insert_mappings() or execute(insert(), list_of_dicts) for maximum speed.", "type": "stack_overflow", "metadata": {"record_id": "cf8b5954-a8bd-410c-bceb-ede7926cb4ff", "source_type": "stack_overflow", "domain": "SQLAlchemy/Databases", "library": "query", "title": "SQLAlchemy bulk insert: what's the fastest way?", "content": "`session.bulk_insert_mappings(Model, list_of_dicts)` skips ORM overhead. Even faster: `session.execute(insert(Model), data)`. Avoid `session.add()` in loops. For PostgreSQL, use `insert().on_conflict_do_nothing()` for upserts. Batch in chunks of 1000-5000 rows for optimal performance.", "code_signature": "Use session.bulk_insert_mappings() or execute(insert(), list_of_dicts) for maximum speed.", "tags": "postgresql, alembic, query, sqlite, session", "url": "https://stackoverflow.com/questions/5977931", "author": "user_238275", "date_published": "30-12-2024", "votes_or_stars": 627, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:29246401-885b-40c1-8fa0-aeb8e6f23df7": {"content": "How to read a large file line by line in Python without loading it all into memory?\n```python\nwith open('large.txt', 'r') as f:\n    for line in f:\n        process(line.strip())\n```\nThis reads one line at a time. For binary files: use `f.read(chunk_size)` in a loop. For CSV: use `pd.read_csv(file, chunksize=10000)`. Never use `f.readlines()` on large files.\nCode signature: Iterate over the file object directly — it yields lines lazily.", "file_path": "https://stackoverflow.com/questions/2737893", "function_name": "Iterate over the file object directly — it yields lines lazily.", "type": "stack_overflow", "metadata": {"record_id": "29246401-885b-40c1-8fa0-aeb8e6f23df7", "source_type": "stack_overflow", "domain": "OS/Subprocess/File I/O", "library": "shutil", "title": "How to read a large file line by line in Python without loading it all into memory?", "content": "```python\nwith open('large.txt', 'r') as f:\n    for line in f:\n        process(line.strip())\n```\nThis reads one line at a time. For binary files: use `f.read(chunk_size)` in a loop. For CSV: use `pd.read_csv(file, chunksize=10000)`. Never use `f.readlines()` on large files.", "code_signature": "Iterate over the file object directly — it yields lines lazily.", "tags": "stdin, tempfile, process", "url": "https://stackoverflow.com/questions/2737893", "author": "user_994539", "date_published": "10-09-2018", "votes_or_stars": 2260, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:383da64b-ca49-40f1-947e-5b2e33862467": {"content": "requests.exceptions.SSLError: certificate verify failed\nNever use `verify=False` in production — it disables SSL verification entirely. Solutions: (1) Update certifi: `pip install --upgrade certifi`. (2) Provide CA bundle: `requests.get(url, verify='/path/to/ca-bundle.crt')`. (3) For self-signed certs in dev: `verify=False` only, and never commit this. (4) Check system time — expired system clock causes SSL failures.\nCode signature: Your SSL certificate chain is incomplete or self-signed. Fix the cert, don't disable verification.", "file_path": "https://stackoverflow.com/questions/7907400", "function_name": "Your SSL certificate chain is incomplete or self-signed. Fix the cert, don't disable verification.", "type": "stack_overflow", "metadata": {"record_id": "383da64b-ca49-40f1-947e-5b2e33862467", "source_type": "stack_overflow", "domain": "Requests/HTTP/APIs", "library": "websocket", "title": "requests.exceptions.SSLError: certificate verify failed", "content": "Never use `verify=False` in production — it disables SSL verification entirely. Solutions: (1) Update certifi: `pip install --upgrade certifi`. (2) Provide CA bundle: `requests.get(url, verify='/path/to/ca-bundle.crt')`. (3) For self-signed certs in dev: `verify=False` only, and never commit this. (4) Check system time — expired system clock causes SSL failures.", "code_signature": "Your SSL certificate chain is incomplete or self-signed. Fix the cert, don't disable verification.", "tags": "websocket, oauth, requests", "url": "https://stackoverflow.com/questions/7907400", "author": "user_851204", "date_published": "26-04-2020", "votes_or_stars": 1357, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:a76253f7-f18c-413d-9687-c553b8590970": {"content": "How to stream large HTTP responses with requests?\n```python\nwith requests.get(url, stream=True) as r:\n    r.raise_for_status()\n    with open('file', 'wb') as f:\n        for chunk in r.iter_content(chunk_size=8192):\n            f.write(chunk)\n```\nAlways use as context manager with stream=True to ensure connection is released.\nCode signature: Use stream=True and iterate over response.iter_content() or iter_lines().", "file_path": "https://stackoverflow.com/questions/3592974", "function_name": "Use stream=True and iterate over response.iter_content() or iter_lines().", "type": "stack_overflow", "metadata": {"record_id": "a76253f7-f18c-413d-9687-c553b8590970", "source_type": "stack_overflow", "domain": "Requests/HTTP/APIs", "library": "json", "title": "How to stream large HTTP responses with requests?", "content": "```python\nwith requests.get(url, stream=True) as r:\n    r.raise_for_status()\n    with open('file', 'wb') as f:\n        for chunk in r.iter_content(chunk_size=8192):\n            f.write(chunk)\n```\nAlways use as context manager with stream=True to ensure connection is released.", "code_signature": "Use stream=True and iterate over response.iter_content() or iter_lines().", "tags": "retry, rate-limiting, requests", "url": "https://stackoverflow.com/questions/3592974", "author": "user_980733", "date_published": "10-08-2024", "votes_or_stars": 1676, "relevance_label": "low", "difficulty_level": "beginner"}}, "kb:2dd047d6-4bcc-455f-9262-5d2406b02de9": {"content": "PERF: Django template rendering bottleneck in production\nProblem: Template rendering takes 500ms+ for complex pages with many database queries.\n\nFix/Discussion: Primary cause is N+1 queries in template. Solutions: (1) Use select_related/prefetch_related. (2) Cache rendered templates with `cache` template tag. (3) Use `django-cachalot` for query caching. (4) Move heavy computation to views, pass pre-computed context. (5) Consider server-side caching with Redis.\nCode signature: status:closed", "file_path": "https://github.com/django/django/issues/3243", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "2dd047d6-4bcc-455f-9262-5d2406b02de9", "source_type": "github_issue", "domain": "FastAPI/Flask/Django", "library": "django", "title": "PERF: Django template rendering bottleneck in production", "content": "Problem: Template rendering takes 500ms+ for complex pages with many database queries.\n\nFix/Discussion: Primary cause is N+1 queries in template. Solutions: (1) Use select_related/prefetch_related. (2) Cache rendered templates with `cache` template tag. (3) Use `django-cachalot` for query caching. (4) Move heavy computation to views, pass pre-computed context. (5) Consider server-side caching with Redis.", "code_signature": "status:closed", "tags": "response, request, pydantic, middleware, dependency-injection", "url": "https://github.com/django/django/issues/3243", "author": "contributor_6988", "date_published": "02-04-2023", "votes_or_stars": 70, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:fc419228-9073-4927-ad57-4f6932b3b5d4": {"content": "BUG: SettingWithCopyWarning when modifying sliced DataFrame\nProblem: Modifying a slice of a DataFrame raises SettingWithCopyWarning and may not update original.\n\nFix/Discussion: Always use .loc or .copy() when slicing. Use `df.loc[mask, 'col'] = val` for assignment. Create explicit copies: `subset = df[mask].copy()`. In Pandas 2.0+, Copy-on-Write makes this behavior consistent — enable with `pd.options.mode.copy_on_write = True`.\nCode signature: status:closed", "file_path": "https://github.com/pandas/pandas/issues/3264", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "fc419228-9073-4927-ad57-4f6932b3b5d4", "source_type": "github_issue", "domain": "NumPy/Pandas", "library": "pandas", "title": "BUG: SettingWithCopyWarning when modifying sliced DataFrame", "content": "Problem: Modifying a slice of a DataFrame raises SettingWithCopyWarning and may not update original.\n\nFix/Discussion: Always use .loc or .copy() when slicing. Use `df.loc[mask, 'col'] = val` for assignment. Create explicit copies: `subset = df[mask].copy()`. In Pandas 2.0+, Copy-on-Write makes this behavior consistent — enable with `pd.options.mode.copy_on_write = True`.", "code_signature": "status:closed", "tags": "array, iloc, broadcasting, numpy, dataframe, reshape", "url": "https://github.com/pandas/pandas/issues/3264", "author": "contributor_6628", "date_published": "19-07-2018", "votes_or_stars": 2, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:d0724283-d79a-4b24-b369-9bc2eaca1f52": {"content": "How to handle database migrations with Alembic?\nWorkflow: (1) Change SQLAlchemy models. (2) `alembic revision --autogenerate -m 'description'`. (3) Review generated migration script. (4) `alembic upgrade head` to apply. Always review autogenerated migrations — they miss some changes (indexes, constraints). Use `alembic downgrade -1` to rollback. Store migrations in version control.\nCode signature: Use alembic revision --autogenerate to create migrations from model changes.", "file_path": "https://stackoverflow.com/questions/9436429", "function_name": "Use alembic revision --autogenerate to create migrations from model changes.", "type": "stack_overflow", "metadata": {"record_id": "d0724283-d79a-4b24-b369-9bc2eaca1f52", "source_type": "stack_overflow", "domain": "SQLAlchemy/Databases", "library": "connection-pool", "title": "How to handle database migrations with Alembic?", "content": "Workflow: (1) Change SQLAlchemy models. (2) `alembic revision --autogenerate -m 'description'`. (3) Review generated migration script. (4) `alembic upgrade head` to apply. Always review autogenerated migrations — they miss some changes (indexes, constraints). Use `alembic downgrade -1` to rollback. Store migrations in version control.", "code_signature": "Use alembic revision --autogenerate to create migrations from model changes.", "tags": "migration, session, postgresql, sqlalchemy", "url": "https://stackoverflow.com/questions/9436429", "author": "user_974047", "date_published": "03-01-2018", "votes_or_stars": 2393, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:18ba3f12-9f8e-40d1-b196-7092581f88b3": {"content": "Django ORM Queries — official reference\nOfficial documentation for Django ORM Queries. The method signature is `QuerySet.filter(**kwargs)`. filters QuerySet rows matching given field lookups. Raises `FieldError` when an unknown field lookup is provided. See also: QuerySet.exclude, QuerySet.annotate, Q objects.\nCode signature: QuerySet.filter(**kwargs)", "file_path": "https://docs.django.org/en/stable/QuerySet/filter.html", "function_name": "QuerySet.filter(**kwargs)", "type": "documentation", "metadata": {"record_id": "18ba3f12-9f8e-40d1-b196-7092581f88b3", "source_type": "documentation", "domain": "FastAPI/Flask/Django", "library": "Django", "title": "Django ORM Queries — official reference", "content": "Official documentation for Django ORM Queries. The method signature is `QuerySet.filter(**kwargs)`. filters QuerySet rows matching given field lookups. Raises `FieldError` when an unknown field lookup is provided. See also: QuerySet.exclude, QuerySet.annotate, Q objects.", "code_signature": "QuerySet.filter(**kwargs)", "tags": "response, flask, orm, pydantic, routing, fastapi", "url": "https://docs.django.org/en/stable/QuerySet/filter.html", "author": "Official Docs", "date_published": "17-08-2018", "votes_or_stars": 2425, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:e79e0353-ef07-46ad-b07e-d2a37fff27fc": {"content": "Python threading: how to share data between threads safely?\nFor simple counters: use `threading.Lock()` as context manager. For complex shared state: prefer `queue.Queue` (thread-safe by design). Avoid global variables without locks. `threading.local()` gives each thread its own data copy. For read-heavy workloads, `threading.RLock` allows re-entrant acquisition.\nCode signature: Use threading.Lock for mutual exclusion or queue.Queue for producer-consumer patterns.", "file_path": "https://stackoverflow.com/questions/3195773", "function_name": "Use threading.Lock for mutual exclusion or queue.Queue for producer-consumer patterns.", "type": "stack_overflow", "metadata": {"record_id": "e79e0353-ef07-46ad-b07e-d2a37fff27fc", "source_type": "stack_overflow", "domain": "Asyncio/Threading", "library": "lock", "title": "Python threading: how to share data between threads safely?", "content": "For simple counters: use `threading.Lock()` as context manager. For complex shared state: prefer `queue.Queue` (thread-safe by design). Avoid global variables without locks. `threading.local()` gives each thread its own data copy. For read-heavy workloads, `threading.RLock` allows re-entrant acquisition.", "code_signature": "Use threading.Lock for mutual exclusion or queue.Queue for producer-consumer patterns.", "tags": "deadlock, race-condition, gather, coroutine", "url": "https://stackoverflow.com/questions/3195773", "author": "user_714318", "date_published": "28-06-2018", "votes_or_stars": 1083, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:d84bdecb-ef67-4a96-9ec2-c761820433e1": {"content": "Django N+1 query problem: how to fix?\nN+1 occurs when accessing related objects in a loop. Diagnose with `django-debug-toolbar` or `connection.queries`. Fix ForeignKey: `User.objects.select_related('profile').all()`. Fix M2M: `Post.objects.prefetch_related('tags').all()`. Use `only()` to fetch specific fields and reduce data transfer.\nCode signature: Use select_related() for ForeignKey and prefetch_related() for ManyToMany relationships.", "file_path": "https://stackoverflow.com/questions/8106470", "function_name": "Use select_related() for ForeignKey and prefetch_related() for ManyToMany relationships.", "type": "stack_overflow", "metadata": {"record_id": "d84bdecb-ef67-4a96-9ec2-c761820433e1", "source_type": "stack_overflow", "domain": "FastAPI/Flask/Django", "library": "request", "title": "Django N+1 query problem: how to fix?", "content": "N+1 occurs when accessing related objects in a loop. Diagnose with `django-debug-toolbar` or `connection.queries`. Fix ForeignKey: `User.objects.select_related('profile').all()`. Fix M2M: `Post.objects.prefetch_related('tags').all()`. Use `only()` to fetch specific fields and reduce data transfer.", "code_signature": "Use select_related() for ForeignKey and prefetch_related() for ManyToMany relationships.", "tags": "template, blueprint, middleware", "url": "https://stackoverflow.com/questions/8106470", "author": "user_441071", "date_published": "11-08-2020", "votes_or_stars": 1802, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:52cf6626-153f-4034-9933-a1a3039b963f": {"content": "FEAT: Django: add support for async ORM queries\nProblem: Django ORM is synchronous; using it in async views requires sync_to_async wrapper.\n\nFix/Discussion: Django 4.1+ added native async ORM support. Use `await Model.objects.filter().afirst()`, `async for obj in qs:`. For older versions: wrap with `sync_to_async`: `get_user = sync_to_async(User.objects.get)\nuser = await get_user(pk=1)`.\nCode signature: status:closed", "file_path": "https://github.com/django/django/issues/8618", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "52cf6626-153f-4034-9933-a1a3039b963f", "source_type": "github_issue", "domain": "FastAPI/Flask/Django", "library": "django", "title": "FEAT: Django: add support for async ORM queries", "content": "Problem: Django ORM is synchronous; using it in async views requires sync_to_async wrapper.\n\nFix/Discussion: Django 4.1+ added native async ORM support. Use `await Model.objects.filter().afirst()`, `async for obj in qs:`. For older versions: wrap with `sync_to_async`: `get_user = sync_to_async(User.objects.get)\nuser = await get_user(pk=1)`.", "code_signature": "status:closed", "tags": "middleware, fastapi, response", "url": "https://github.com/django/django/issues/8618", "author": "contributor_8921", "date_published": "18-03-2021", "votes_or_stars": 5, "relevance_label": "low", "difficulty_level": "intermediate"}}, "kb:54b2033b-7f8b-4a29-89dd-62fb617ea541": {"content": "How to handle database migrations with Alembic?\nWorkflow: (1) Change SQLAlchemy models. (2) `alembic revision --autogenerate -m 'description'`. (3) Review generated migration script. (4) `alembic upgrade head` to apply. Always review autogenerated migrations — they miss some changes (indexes, constraints). Use `alembic downgrade -1` to rollback. Store migrations in version control.\nCode signature: Use alembic revision --autogenerate to create migrations from model changes.", "file_path": "https://stackoverflow.com/questions/9436429", "function_name": "Use alembic revision --autogenerate to create migrations from model changes.", "type": "stack_overflow", "metadata": {"record_id": "54b2033b-7f8b-4a29-89dd-62fb617ea541", "source_type": "stack_overflow", "domain": "SQLAlchemy/Databases", "library": "connection-pool", "title": "How to handle database migrations with Alembic?", "content": "Workflow: (1) Change SQLAlchemy models. (2) `alembic revision --autogenerate -m 'description'`. (3) Review generated migration script. (4) `alembic upgrade head` to apply. Always review autogenerated migrations — they miss some changes (indexes, constraints). Use `alembic downgrade -1` to rollback. Store migrations in version control.", "code_signature": "Use alembic revision --autogenerate to create migrations from model changes.", "tags": "alembic, transaction, session, sqlite, sqlalchemy", "url": "https://stackoverflow.com/questions/9436429", "author": "user_974047", "date_published": "20-01-2024", "votes_or_stars": 2386, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:a5b9937e-3118-4866-a934-3fb134df1538": {"content": "BUG: SettingWithCopyWarning when modifying sliced DataFrame\nProblem: Modifying a slice of a DataFrame raises SettingWithCopyWarning and may not update original.\n\nFix/Discussion: Always use .loc or .copy() when slicing. Use `df.loc[mask, 'col'] = val` for assignment. Create explicit copies: `subset = df[mask].copy()`. In Pandas 2.0+, Copy-on-Write makes this behavior consistent — enable with `pd.options.mode.copy_on_write = True`.\nCode signature: status:closed", "file_path": "https://github.com/pandas/pandas/issues/3264", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "a5b9937e-3118-4866-a934-3fb134df1538", "source_type": "github_issue", "domain": "NumPy/Pandas", "library": "pandas", "title": "BUG: SettingWithCopyWarning when modifying sliced DataFrame", "content": "Problem: Modifying a slice of a DataFrame raises SettingWithCopyWarning and may not update original.\n\nFix/Discussion: Always use .loc or .copy() when slicing. Use `df.loc[mask, 'col'] = val` for assignment. Create explicit copies: `subset = df[mask].copy()`. In Pandas 2.0+, Copy-on-Write makes this behavior consistent — enable with `pd.options.mode.copy_on_write = True`.", "code_signature": "status:closed", "tags": "nan, groupby, pivot, pandas, loc, dtype", "url": "https://github.com/pandas/pandas/issues/3264", "author": "contributor_6628", "date_published": "20-02-2021", "votes_or_stars": 1, "relevance_label": "low", "difficulty_level": "beginner"}}, "kb:23f3ec6b-60a2-4991-a77c-14f2e24cd069": {"content": "How to use df.groupby in Pandas\n`Pandas.df.groupby` groups DataFrame rows by column values. Example usage:\n```python\ndf.groupby('category')['value'].mean()\n```\nThis is useful when aggregation, transformation, filtering by group. Performance tip: avoid apply() on large DataFrames; prefer agg().\nCode signature: df.groupby", "file_path": "https://docs.pandas.org/en/stable/df/groupby.html", "function_name": "df.groupby", "type": "documentation", "metadata": {"record_id": "23f3ec6b-60a2-4991-a77c-14f2e24cd069", "source_type": "documentation", "domain": "NumPy/Pandas", "library": "Pandas", "title": "How to use df.groupby in Pandas", "content": "`Pandas.df.groupby` groups DataFrame rows by column values. Example usage:\n```python\ndf.groupby('category')['value'].mean()\n```\nThis is useful when aggregation, transformation, filtering by group. Performance tip: avoid apply() on large DataFrames; prefer agg().", "code_signature": "df.groupby", "tags": "dtype, merge, dataframe", "url": "https://docs.pandas.org/en/stable/df/groupby.html", "author": "Official Docs", "date_published": "02-06-2022", "votes_or_stars": 824, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:8326d809-38a7-4f8f-b26e-f0f4da889871": {"content": "BUG: Flask session not persisting between requests in tests\nProblem: Session data set in one request is not available in the next request during testing.\n\nFix/Discussion: Use Flask test client's session_transaction() context manager: `with client.session_transaction() as sess:\n    sess['user_id'] = 1`. Ensure SECRET_KEY is set. For testing: `app.config['TESTING'] = True; app.config['SECRET_KEY'] = 'test'`.\nCode signature: status:closed", "file_path": "https://github.com/flask/flask/issues/2141", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "8326d809-38a7-4f8f-b26e-f0f4da889871", "source_type": "github_issue", "domain": "FastAPI/Flask/Django", "library": "flask", "title": "BUG: Flask session not persisting between requests in tests", "content": "Problem: Session data set in one request is not available in the next request during testing.\n\nFix/Discussion: Use Flask test client's session_transaction() context manager: `with client.session_transaction() as sess:\n    sess['user_id'] = 1`. Ensure SECRET_KEY is set. For testing: `app.config['TESTING'] = True; app.config['SECRET_KEY'] = 'test'`.", "code_signature": "status:closed", "tags": "flask, request, routing, template, rest, dependency-injection", "url": "https://github.com/flask/flask/issues/2141", "author": "contributor_5020", "date_published": "01-09-2023", "votes_or_stars": 145, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:f94fe265-bb6d-4237-968c-19bb891b37af": {"content": "BUG: Flask session not persisting between requests in tests\nProblem: Session data set in one request is not available in the next request during testing.\n\nFix/Discussion: Use Flask test client's session_transaction() context manager: `with client.session_transaction() as sess:\n    sess['user_id'] = 1`. Ensure SECRET_KEY is set. For testing: `app.config['TESTING'] = True; app.config['SECRET_KEY'] = 'test'`.\nCode signature: status:closed", "file_path": "https://github.com/flask/flask/issues/2141", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "f94fe265-bb6d-4237-968c-19bb891b37af", "source_type": "github_issue", "domain": "FastAPI/Flask/Django", "library": "flask", "title": "BUG: Flask session not persisting between requests in tests", "content": "Problem: Session data set in one request is not available in the next request during testing.\n\nFix/Discussion: Use Flask test client's session_transaction() context manager: `with client.session_transaction() as sess:\n    sess['user_id'] = 1`. Ensure SECRET_KEY is set. For testing: `app.config['TESTING'] = True; app.config['SECRET_KEY'] = 'test'`.", "code_signature": "status:closed", "tags": "django, rest, template, request, blueprint", "url": "https://github.com/flask/flask/issues/2141", "author": "contributor_5020", "date_published": "14-07-2024", "votes_or_stars": 187, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:2064dc8f-c507-4e59-93ab-c932ac1bd517": {"content": "How to limit concurrency with asyncio?\n```python\nsem = asyncio.Semaphore(10)\nasync def limited(url):\n    async with sem:\n        return await fetch(url)\nresults = await asyncio.gather(*[limited(u) for u in urls])\n```\nThis prevents overwhelming APIs or databases with too many simultaneous connections.\nCode signature: Use asyncio.Semaphore to cap the number of concurrent coroutines.", "file_path": "https://stackoverflow.com/questions/2375453", "function_name": "Use asyncio.Semaphore to cap the number of concurrent coroutines.", "type": "stack_overflow", "metadata": {"record_id": "2064dc8f-c507-4e59-93ab-c932ac1bd517", "source_type": "stack_overflow", "domain": "Asyncio/Threading", "library": "asyncio", "title": "How to limit concurrency with asyncio?", "content": "```python\nsem = asyncio.Semaphore(10)\nasync def limited(url):\n    async with sem:\n        return await fetch(url)\nresults = await asyncio.gather(*[limited(u) for u in urls])\n```\nThis prevents overwhelming APIs or databases with too many simultaneous connections.", "code_signature": "Use asyncio.Semaphore to cap the number of concurrent coroutines.", "tags": "deadlock, semaphore, asyncio, threading", "url": "https://stackoverflow.com/questions/2375453", "author": "user_449589", "date_published": "07-07-2020", "votes_or_stars": 2422, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:bcb00ac7-d64b-44cf-a0f6-4b4e42f70b1d": {"content": "How to run asyncio code from synchronous Python?\n`asyncio.run(main())` creates an event loop, runs the coroutine, closes the loop. Never call asyncio.run() from inside a running event loop (use await instead). In Jupyter notebooks (which have a running loop), use `await coroutine()` directly or `nest_asyncio.apply()`. For mixing sync/async, use `loop.run_until_complete()`.\nCode signature: Use asyncio.run() in Python 3.7+ to execute a coroutine from sync code.", "file_path": "https://stackoverflow.com/questions/2140194", "function_name": "Use asyncio.run() in Python 3.7+ to execute a coroutine from sync code.", "type": "stack_overflow", "metadata": {"record_id": "bcb00ac7-d64b-44cf-a0f6-4b4e42f70b1d", "source_type": "stack_overflow", "domain": "Asyncio/Threading", "library": "gather", "title": "How to run asyncio code from synchronous Python?", "content": "`asyncio.run(main())` creates an event loop, runs the coroutine, closes the loop. Never call asyncio.run() from inside a running event loop (use await instead). In Jupyter notebooks (which have a running loop), use `await coroutine()` directly or `nest_asyncio.apply()`. For mixing sync/async, use `loop.run_until_complete()`.", "code_signature": "Use asyncio.run() in Python 3.7+ to execute a coroutine from sync code.", "tags": "task, event-loop, await, deadlock, async", "url": "https://stackoverflow.com/questions/2140194", "author": "user_718011", "date_published": "03-06-2024", "votes_or_stars": 295, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:9187e694-1a89-49de-80d6-62e37db4f0a3": {"content": "How to implement database connection pooling with SQLAlchemy?\n`create_engine(url, pool_size=10, max_overflow=20, pool_timeout=30, pool_recycle=1800)`. Use `pool_pre_ping=True` to detect stale connections. For async: use `AsyncEngine` with `create_async_engine()`. Monitor with `engine.pool.status()`. NullPool disables pooling for scripts/serverless environments.\nCode signature: create_engine() has built-in pooling. Configure pool_size and max_overflow.", "file_path": "https://stackoverflow.com/questions/7358113", "function_name": "create_engine() has built-in pooling. Configure pool_size and max_overflow.", "type": "stack_overflow", "metadata": {"record_id": "9187e694-1a89-49de-80d6-62e37db4f0a3", "source_type": "stack_overflow", "domain": "SQLAlchemy/Databases", "library": "session", "title": "How to implement database connection pooling with SQLAlchemy?", "content": "`create_engine(url, pool_size=10, max_overflow=20, pool_timeout=30, pool_recycle=1800)`. Use `pool_pre_ping=True` to detect stale connections. For async: use `AsyncEngine` with `create_async_engine()`. Monitor with `engine.pool.status()`. NullPool disables pooling for scripts/serverless environments.", "code_signature": "create_engine() has built-in pooling. Configure pool_size and max_overflow.", "tags": "migration, foreign-key, orm, relationship", "url": "https://stackoverflow.com/questions/7358113", "author": "user_12260", "date_published": "17-06-2023", "votes_or_stars": 222, "relevance_label": "low", "difficulty_level": "intermediate"}}, "kb:21eead2b-1b23-4a88-bc72-9c09d2863337": {"content": "BUG: os.path.join ignores prefix when component starts with /\nProblem: os.path.join('base', '/absolute') returns '/absolute', ignoring 'base'.\n\nFix/Discussion: This is documented behavior: os.path.join discards previous components when it encounters an absolute path. Fix: use `pathlib.Path(base) / component.lstrip('/')`. Validate user-provided paths with `Path(path).resolve().is_relative_to(base_dir)` to prevent path traversal.\nCode signature: status:closed", "file_path": "https://github.com/cpython/cpython/issues/7712", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "21eead2b-1b23-4a88-bc72-9c09d2863337", "source_type": "github_issue", "domain": "OS/Subprocess/File I/O", "library": "cpython", "title": "BUG: os.path.join ignores prefix when component starts with /", "content": "Problem: os.path.join('base', '/absolute') returns '/absolute', ignoring 'base'.\n\nFix/Discussion: This is documented behavior: os.path.join discards previous components when it encounters an absolute path. Fix: use `pathlib.Path(base) / component.lstrip('/')`. Validate user-provided paths with `Path(path).resolve().is_relative_to(base_dir)` to prevent path traversal.", "code_signature": "status:closed", "tags": "env-variable, glob, stdout, stdin, tempfile", "url": "https://github.com/cpython/cpython/issues/7712", "author": "contributor_8802", "date_published": "16-02-2020", "votes_or_stars": 385, "relevance_label": "low", "difficulty_level": "beginner"}}, "kb:9d32051f-b4c2-4363-91d1-2b923e3b9d34": {"content": "Python: how to safely write to a file atomically?\n```python\nimport tempfile, os\nwith tempfile.NamedTemporaryFile('w', delete=False, dir='.') as tmp:\n    tmp.write(data)\n    tmp.flush()\n    os.fsync(tmp.fileno())\nos.replace(tmp.name, target_path)\n```\n`os.replace()` is atomic on POSIX. This prevents partial writes from corrupting files.\nCode signature: Write to a temp file then rename — rename is atomic on POSIX systems.", "file_path": "https://stackoverflow.com/questions/5394880", "function_name": "Write to a temp file then rename — rename is atomic on POSIX systems.", "type": "stack_overflow", "metadata": {"record_id": "9d32051f-b4c2-4363-91d1-2b923e3b9d34", "source_type": "stack_overflow", "domain": "OS/Subprocess/File I/O", "library": "stdin", "title": "Python: how to safely write to a file atomically?", "content": "```python\nimport tempfile, os\nwith tempfile.NamedTemporaryFile('w', delete=False, dir='.') as tmp:\n    tmp.write(data)\n    tmp.flush()\n    os.fsync(tmp.fileno())\nos.replace(tmp.name, target_path)\n```\n`os.replace()` is atomic on POSIX. This prevents partial writes from corrupting files.", "code_signature": "Write to a temp file then rename — rename is atomic on POSIX systems.", "tags": "env-variable, pipe, stdin", "url": "https://stackoverflow.com/questions/5394880", "author": "user_179430", "date_published": "06-03-2018", "votes_or_stars": 530, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:8a9401c8-aae5-4895-9f21-7ca4f1a33a8c": {"content": "FastAPI — Path Operations\nThe `@app.get` function in FastAPI registers a GET endpoint at the specified path. It accepts `path, response_model` as a parameter and returns Response. Common use cases include building REST API endpoints that return JSON data. Note that use response_model to enforce output schema validation.\nCode signature: @app.get(path, response_model) -> Response", "file_path": "https://docs.fastapi.org/en/stable/@app/get.html", "function_name": "@app.get(path, response_model) -> Response", "type": "documentation", "metadata": {"record_id": "8a9401c8-aae5-4895-9f21-7ca4f1a33a8c", "source_type": "documentation", "domain": "FastAPI/Flask/Django", "library": "FastAPI", "title": "FastAPI — Path Operations", "content": "The `@app.get` function in FastAPI registers a GET endpoint at the specified path. It accepts `path, response_model` as a parameter and returns Response. Common use cases include building REST API endpoints that return JSON data. Note that use response_model to enforce output schema validation.", "code_signature": "@app.get(path, response_model) -> Response", "tags": "pydantic, dependency-injection, template, orm", "url": "https://docs.fastapi.org/en/stable/@app/get.html", "author": "Official Docs", "date_published": "18-01-2023", "votes_or_stars": 2866, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:31015da2-9239-47d5-8fc2-2e1d5265e162": {"content": "How to implement a thread pool in Python?\n```python\nfrom concurrent.futures import ThreadPoolExecutor\nwith ThreadPoolExecutor(max_workers=8) as ex:\n    futures = [ex.submit(task, arg) for arg in args]\n    results = [f.result() for f in futures]\n```\nFor CPU-bound: use ProcessPoolExecutor instead. For async integration: `await loop.run_in_executor(executor, func, *args)`.\nCode signature: Use concurrent.futures.ThreadPoolExecutor for I/O-bound tasks.", "file_path": "https://stackoverflow.com/questions/3646552", "function_name": "Use concurrent.futures.ThreadPoolExecutor for I/O-bound tasks.", "type": "stack_overflow", "metadata": {"record_id": "31015da2-9239-47d5-8fc2-2e1d5265e162", "source_type": "stack_overflow", "domain": "Asyncio/Threading", "library": "coroutine", "title": "How to implement a thread pool in Python?", "content": "```python\nfrom concurrent.futures import ThreadPoolExecutor\nwith ThreadPoolExecutor(max_workers=8) as ex:\n    futures = [ex.submit(task, arg) for arg in args]\n    results = [f.result() for f in futures]\n```\nFor CPU-bound: use ProcessPoolExecutor instead. For async integration: `await loop.run_in_executor(executor, func, *args)`.", "code_signature": "Use concurrent.futures.ThreadPoolExecutor for I/O-bound tasks.", "tags": "executor, task, await, semaphore, gather, async", "url": "https://stackoverflow.com/questions/3646552", "author": "user_469469", "date_published": "12-07-2023", "votes_or_stars": 1544, "relevance_label": "low", "difficulty_level": "beginner"}}, "kb:cf380039-7796-453d-b939-b017f5c83414": {"content": "BUG: asyncio task leaks when exception is not handled\nProblem: Unhandled exceptions in fire-and-forget asyncio tasks are silently ignored, causing task leaks.\n\nFix/Discussion: Always await tasks or add exception handlers. Use `task.add_done_callback()` to log exceptions. In Python 3.11+, use TaskGroup for structured concurrency — exceptions propagate automatically. Set `asyncio.get_event_loop().set_exception_handler()` for global unhandled task exception logging.\nCode signature: status:closed", "file_path": "https://github.com/cpython/cpython/issues/5591", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "cf380039-7796-453d-b939-b017f5c83414", "source_type": "github_issue", "domain": "Asyncio/Threading", "library": "cpython", "title": "BUG: asyncio task leaks when exception is not handled", "content": "Problem: Unhandled exceptions in fire-and-forget asyncio tasks are silently ignored, causing task leaks.\n\nFix/Discussion: Always await tasks or add exception handlers. Use `task.add_done_callback()` to log exceptions. In Python 3.11+, use TaskGroup for structured concurrency — exceptions propagate automatically. Set `asyncio.get_event_loop().set_exception_handler()` for global unhandled task exception logging.", "code_signature": "status:closed", "tags": "executor, gather, async, semaphore", "url": "https://github.com/cpython/cpython/issues/5591", "author": "contributor_1630", "date_published": "13-05-2024", "votes_or_stars": 316, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:66a2d577-7280-4f81-8ca8-941f2a7f8c56": {"content": "BUG: SettingWithCopyWarning when modifying sliced DataFrame\nProblem: Modifying a slice of a DataFrame raises SettingWithCopyWarning and may not update original.\n\nFix/Discussion: Always use .loc or .copy() when slicing. Use `df.loc[mask, 'col'] = val` for assignment. Create explicit copies: `subset = df[mask].copy()`. In Pandas 2.0+, Copy-on-Write makes this behavior consistent — enable with `pd.options.mode.copy_on_write = True`.\nCode signature: status:closed", "file_path": "https://github.com/pandas/pandas/issues/3264", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "66a2d577-7280-4f81-8ca8-941f2a7f8c56", "source_type": "github_issue", "domain": "NumPy/Pandas", "library": "pandas", "title": "BUG: SettingWithCopyWarning when modifying sliced DataFrame", "content": "Problem: Modifying a slice of a DataFrame raises SettingWithCopyWarning and may not update original.\n\nFix/Discussion: Always use .loc or .copy() when slicing. Use `df.loc[mask, 'col'] = val` for assignment. Create explicit copies: `subset = df[mask].copy()`. In Pandas 2.0+, Copy-on-Write makes this behavior consistent — enable with `pd.options.mode.copy_on_write = True`.", "code_signature": "status:closed", "tags": "dtype, dataframe, pivot, reshape", "url": "https://github.com/pandas/pandas/issues/3264", "author": "contributor_6628", "date_published": "17-04-2022", "votes_or_stars": 12, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:ad6e24e5-9519-47fe-bb1a-56d21f8b8e20": {"content": "FastAPI — Path Operations\nThe `@app.get` function in FastAPI registers a GET endpoint at the specified path. It accepts `path, response_model` as a parameter and returns Response. Common use cases include building REST API endpoints that return JSON data. Note that use response_model to enforce output schema validation.\nCode signature: @app.get(path, response_model) -> Response", "file_path": "https://docs.fastapi.org/en/stable/@app/get.html", "function_name": "@app.get(path, response_model) -> Response", "type": "documentation", "metadata": {"record_id": "ad6e24e5-9519-47fe-bb1a-56d21f8b8e20", "source_type": "documentation", "domain": "FastAPI/Flask/Django", "library": "FastAPI", "title": "FastAPI — Path Operations", "content": "The `@app.get` function in FastAPI registers a GET endpoint at the specified path. It accepts `path, response_model` as a parameter and returns Response. Common use cases include building REST API endpoints that return JSON data. Note that use response_model to enforce output schema validation.", "code_signature": "@app.get(path, response_model) -> Response", "tags": "flask, response, pydantic", "url": "https://docs.fastapi.org/en/stable/@app/get.html", "author": "Official Docs", "date_published": "05-01-2022", "votes_or_stars": 2867, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:5b9973dd-9a5b-40a1-b133-16ca1107923b": {"content": "How to implement JWT authentication in FastAPI?\n```python\nfrom fastapi.security import OAuth2PasswordBearer\nfrom jose import jwt\noauth2_scheme = OAuth2PasswordBearer(tokenUrl='token')\n```\nCreate access tokens with `jwt.encode(payload, SECRET_KEY, algorithm='HS256')`. Verify with `jwt.decode(token, SECRET_KEY, algorithms=['HS256'])`. Store SECRET_KEY in environment variables, never in code.\nCode signature: Use python-jose for JWT and passlib for password hashing with OAuth2PasswordBearer.", "file_path": "https://stackoverflow.com/questions/7754864", "function_name": "Use python-jose for JWT and passlib for password hashing with OAuth2PasswordBearer.", "type": "stack_overflow", "metadata": {"record_id": "5b9973dd-9a5b-40a1-b133-16ca1107923b", "source_type": "stack_overflow", "domain": "FastAPI/Flask/Django", "library": "fastapi", "title": "How to implement JWT authentication in FastAPI?", "content": "```python\nfrom fastapi.security import OAuth2PasswordBearer\nfrom jose import jwt\noauth2_scheme = OAuth2PasswordBearer(tokenUrl='token')\n```\nCreate access tokens with `jwt.encode(payload, SECRET_KEY, algorithm='HS256')`. Verify with `jwt.decode(token, SECRET_KEY, algorithms=['HS256'])`. Store SECRET_KEY in environment variables, never in code.", "code_signature": "Use python-jose for JWT and passlib for password hashing with OAuth2PasswordBearer.", "tags": "template, django, request, response, routing, middleware", "url": "https://stackoverflow.com/questions/7754864", "author": "user_773587", "date_published": "13-08-2024", "votes_or_stars": 396, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:429551ed-94fc-42c0-a1fb-a94d63e4a6d2": {"content": "BUG: pd.read_csv() silently truncates large integers\nProblem: Integer columns with values > 2^53 are silently corrupted when read from CSV.\n\nFix/Discussion: Floating point cannot represent integers > 2^53 exactly. Fix: `pd.read_csv(f, dtype={'col': str})` then convert: `df['col'] = df['col'].astype('Int64')`. Use `dtype_backend='numpy_nullable'` in Pandas 2.0+ for better integer handling.\nCode signature: status:closed", "file_path": "https://github.com/pandas/pandas/issues/4449", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "429551ed-94fc-42c0-a1fb-a94d63e4a6d2", "source_type": "github_issue", "domain": "NumPy/Pandas", "library": "pandas", "title": "BUG: pd.read_csv() silently truncates large integers", "content": "Problem: Integer columns with values > 2^53 are silently corrupted when read from CSV.\n\nFix/Discussion: Floating point cannot represent integers > 2^53 exactly. Fix: `pd.read_csv(f, dtype={'col': str})` then convert: `df['col'] = df['col'].astype('Int64')`. Use `dtype_backend='numpy_nullable'` in Pandas 2.0+ for better integer handling.", "code_signature": "status:closed", "tags": "vectorization, dataframe, loc", "url": "https://github.com/pandas/pandas/issues/4449", "author": "contributor_726", "date_published": "20-03-2020", "votes_or_stars": 497, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:ed2880c3-63c6-402b-80f4-47a7db2d4a31": {"content": "FEAT: pathlib: add support for reading/writing with encoding shorthand\nProblem: pathlib.Path.read_text() does not default to UTF-8, causing issues across platforms.\n\nFix/Discussion: Always pass encoding explicitly: `Path('file').read_text(encoding='utf-8')`. Python 3.10+ added `encoding='locale'` default change proposal. Best practice: always specify encoding in all file I/O operations for portability.\nCode signature: status:closed", "file_path": "https://github.com/cpython/cpython/issues/2707", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "ed2880c3-63c6-402b-80f4-47a7db2d4a31", "source_type": "github_issue", "domain": "OS/Subprocess/File I/O", "library": "cpython", "title": "FEAT: pathlib: add support for reading/writing with encoding shorthand", "content": "Problem: pathlib.Path.read_text() does not default to UTF-8, causing issues across platforms.\n\nFix/Discussion: Always pass encoding explicitly: `Path('file').read_text(encoding='utf-8')`. Python 3.10+ added `encoding='locale'` default change proposal. Best practice: always specify encoding in all file I/O operations for portability.", "code_signature": "status:closed", "tags": "tempfile, stdin, file-io, pipe, process, shutil", "url": "https://github.com/cpython/cpython/issues/2707", "author": "contributor_7877", "date_published": "30-05-2023", "votes_or_stars": 451, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:0faa2a30-4a58-491e-805e-6ed2e50ae711": {"content": "NumPy — Vectorization\nThe `np.vectorize` function in NumPy generalizes a scalar function to operate on arrays. It accepts `pyfunc` as a parameter and returns vectorized function. Common use cases include applying Python functions element-wise without explicit loops. Note that np.vectorize is a convenience wrapper; it does not improve performance like true ufuncs.\nCode signature: np.vectorize(pyfunc) -> vectorized function", "file_path": "https://docs.numpy.org/en/stable/np/vectorize.html", "function_name": "np.vectorize(pyfunc) -> vectorized function", "type": "documentation", "metadata": {"record_id": "0faa2a30-4a58-491e-805e-6ed2e50ae711", "source_type": "documentation", "domain": "NumPy/Pandas", "library": "NumPy", "title": "NumPy — Vectorization", "content": "The `np.vectorize` function in NumPy generalizes a scalar function to operate on arrays. It accepts `pyfunc` as a parameter and returns vectorized function. Common use cases include applying Python functions element-wise without explicit loops. Note that np.vectorize is a convenience wrapper; it does not improve performance like true ufuncs.", "code_signature": "np.vectorize(pyfunc) -> vectorized function", "tags": "pivot, array, loc", "url": "https://docs.numpy.org/en/stable/np/vectorize.html", "author": "Official Docs", "date_published": "23-11-2020", "votes_or_stars": 1345, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:ec13a5b3-8751-4a9b-aba7-f04087b33257": {"content": "Flask: how to return JSON responses properly?\n`return jsonify({'key': 'value'})` sets Content-Type to application/json. Flask 1.0+ automatically converts returned dicts to JSON responses. For custom status codes: `return jsonify(data), 201`. For errors: use `abort(404)` or return `({'error': 'Not found'}, 404)`.\nCode signature: Use jsonify() or return a dict directly (Flask 1.0+).", "file_path": "https://stackoverflow.com/questions/8930103", "function_name": "Use jsonify() or return a dict directly (Flask 1.0+).", "type": "stack_overflow", "metadata": {"record_id": "ec13a5b3-8751-4a9b-aba7-f04087b33257", "source_type": "stack_overflow", "domain": "FastAPI/Flask/Django", "library": "django", "title": "Flask: how to return JSON responses properly?", "content": "`return jsonify({'key': 'value'})` sets Content-Type to application/json. Flask 1.0+ automatically converts returned dicts to JSON responses. For custom status codes: `return jsonify(data), 201`. For errors: use `abort(404)` or return `({'error': 'Not found'}, 404)`.", "code_signature": "Use jsonify() or return a dict directly (Flask 1.0+).", "tags": "routing, middleware, template, blueprint, pydantic", "url": "https://stackoverflow.com/questions/8930103", "author": "user_264801", "date_published": "22-04-2023", "votes_or_stars": 2218, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:1b083587-ec99-495c-9aa1-3763bc51454a": {"content": "subprocess — Process Execution\nThe `subprocess.run` function in subprocess runs a subprocess and waits for it to complete. It accepts `args, capture_output=False, timeout=None` as a parameter and returns CompletedProcess. Common use cases include executing shell commands, scripts, or external programs from Python. Note that use shell=False (default) to avoid shell injection vulnerabilities.\nCode signature: subprocess.run(args, capture_output=False, timeout=None) -> CompletedProcess", "file_path": "https://docs.subprocess.org/en/stable/subprocess/run.html", "function_name": "subprocess.run(args, capture_output=False, timeout=None) -> CompletedProcess", "type": "documentation", "metadata": {"record_id": "1b083587-ec99-495c-9aa1-3763bc51454a", "source_type": "documentation", "domain": "OS/Subprocess/File I/O", "library": "subprocess", "title": "subprocess — Process Execution", "content": "The `subprocess.run` function in subprocess runs a subprocess and waits for it to complete. It accepts `args, capture_output=False, timeout=None` as a parameter and returns CompletedProcess. Common use cases include executing shell commands, scripts, or external programs from Python. Note that use shell=False (default) to avoid shell injection vulnerabilities.", "code_signature": "subprocess.run(args, capture_output=False, timeout=None) -> CompletedProcess", "tags": "subprocess, stdin, file-io, pipe, env-variable, stderr", "url": "https://docs.subprocess.org/en/stable/subprocess/run.html", "author": "Official Docs", "date_published": "03-06-2024", "votes_or_stars": 439, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:957c3d9c-d2cc-49b3-8ecd-2ce887376b1f": {"content": "How to stream large HTTP responses with requests?\n```python\nwith requests.get(url, stream=True) as r:\n    r.raise_for_status()\n    with open('file', 'wb') as f:\n        for chunk in r.iter_content(chunk_size=8192):\n            f.write(chunk)\n```\nAlways use as context manager with stream=True to ensure connection is released.\nCode signature: Use stream=True and iterate over response.iter_content() or iter_lines().", "file_path": "https://stackoverflow.com/questions/3592974", "function_name": "Use stream=True and iterate over response.iter_content() or iter_lines().", "type": "stack_overflow", "metadata": {"record_id": "957c3d9c-d2cc-49b3-8ecd-2ce887376b1f", "source_type": "stack_overflow", "domain": "Requests/HTTP/APIs", "library": "json", "title": "How to stream large HTTP responses with requests?", "content": "```python\nwith requests.get(url, stream=True) as r:\n    r.raise_for_status()\n    with open('file', 'wb') as f:\n        for chunk in r.iter_content(chunk_size=8192):\n            f.write(chunk)\n```\nAlways use as context manager with stream=True to ensure connection is released.", "code_signature": "Use stream=True and iterate over response.iter_content() or iter_lines().", "tags": "retry, aiohttp, websocket, requests, headers, rest-api", "url": "https://stackoverflow.com/questions/3592974", "author": "user_980733", "date_published": "18-03-2024", "votes_or_stars": 1687, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:1de3b330-f83b-4086-8d81-813e35ae7601": {"content": "How to read a large file line by line in Python without loading it all into memory?\n```python\nwith open('large.txt', 'r') as f:\n    for line in f:\n        process(line.strip())\n```\nThis reads one line at a time. For binary files: use `f.read(chunk_size)` in a loop. For CSV: use `pd.read_csv(file, chunksize=10000)`. Never use `f.readlines()` on large files.\nCode signature: Iterate over the file object directly — it yields lines lazily.", "file_path": "https://stackoverflow.com/questions/2737893", "function_name": "Iterate over the file object directly — it yields lines lazily.", "type": "stack_overflow", "metadata": {"record_id": "1de3b330-f83b-4086-8d81-813e35ae7601", "source_type": "stack_overflow", "domain": "OS/Subprocess/File I/O", "library": "shutil", "title": "How to read a large file line by line in Python without loading it all into memory?", "content": "```python\nwith open('large.txt', 'r') as f:\n    for line in f:\n        process(line.strip())\n```\nThis reads one line at a time. For binary files: use `f.read(chunk_size)` in a loop. For CSV: use `pd.read_csv(file, chunksize=10000)`. Never use `f.readlines()` on large files.", "code_signature": "Iterate over the file object directly — it yields lines lazily.", "tags": "shutil, env-variable, subprocess", "url": "https://stackoverflow.com/questions/2737893", "author": "user_994539", "date_published": "31-07-2018", "votes_or_stars": 2293, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:24d9de5d-9406-49b0-9851-35d70a207e21": {"content": "How do I efficiently fill NaN values in a Pandas DataFrame?\nFor large DataFrames, avoid looping. Use `df.fillna({'col': val})` to fill specific columns. `df.ffill()` propagates last valid observation forward. For time-series data, `df.interpolate()` provides smoother results. Always check `df.isna().sum()` before and after to verify.\nCode signature: Use df.fillna(value) or df.ffill()/df.bfill() for forward/backward fill.", "file_path": "https://stackoverflow.com/questions/6438436", "function_name": "Use df.fillna(value) or df.ffill()/df.bfill() for forward/backward fill.", "type": "stack_overflow", "metadata": {"record_id": "24d9de5d-9406-49b0-9851-35d70a207e21", "source_type": "stack_overflow", "domain": "NumPy/Pandas", "library": "loc", "title": "How do I efficiently fill NaN values in a Pandas DataFrame?", "content": "For large DataFrames, avoid looping. Use `df.fillna({'col': val})` to fill specific columns. `df.ffill()` propagates last valid observation forward. For time-series data, `df.interpolate()` provides smoother results. Always check `df.isna().sum()` before and after to verify.", "code_signature": "Use df.fillna(value) or df.ffill()/df.bfill() for forward/backward fill.", "tags": "numpy, array, dtype, merge", "url": "https://stackoverflow.com/questions/6438436", "author": "user_522340", "date_published": "12-07-2023", "votes_or_stars": 39, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:ef607481-fe65-4985-bbfa-d184a61e4deb": {"content": "How to use @app.route in Flask\n`Flask.@app.route` maps a URL rule to a view function. Example usage:\n```python\n@app.route('/users/<int:uid>')\ndef get_user(uid):\n    return jsonify(user)\n```\nThis is useful when defining HTTP endpoints in a Flask application. Performance tip: use Blueprints to organize large applications.\nCode signature: @app.route", "file_path": "https://docs.flask.org/en/stable/@app/route.html", "function_name": "@app.route", "type": "documentation", "metadata": {"record_id": "ef607481-fe65-4985-bbfa-d184a61e4deb", "source_type": "documentation", "domain": "FastAPI/Flask/Django", "library": "Flask", "title": "How to use @app.route in Flask", "content": "`Flask.@app.route` maps a URL rule to a view function. Example usage:\n```python\n@app.route('/users/<int:uid>')\ndef get_user(uid):\n    return jsonify(user)\n```\nThis is useful when defining HTTP endpoints in a Flask application. Performance tip: use Blueprints to organize large applications.", "code_signature": "@app.route", "tags": "middleware, response, rest", "url": "https://docs.flask.org/en/stable/@app/route.html", "author": "Official Docs", "date_published": "11-09-2024", "votes_or_stars": 3020, "relevance_label": "low", "difficulty_level": "intermediate"}}, "kb:233e4b55-3909-4867-9e15-4d9633dffde7": {"content": "How to use asyncio.create_task in asyncio\n`asyncio.asyncio.create_task` schedules a coroutine as a Task in the event loop. Example usage:\n```python\ntask = asyncio.create_task(send_email(user))\nawait task\n```\nThis is useful when fire-and-forget background tasks, concurrent task management. Performance tip: use TaskGroup (Python 3.11+) for structured concurrency.\nCode signature: asyncio.create_task", "file_path": "https://docs.asyncio.org/en/stable/asyncio/create_task.html", "function_name": "asyncio.create_task", "type": "documentation", "metadata": {"record_id": "233e4b55-3909-4867-9e15-4d9633dffde7", "source_type": "documentation", "domain": "Asyncio/Threading", "library": "asyncio", "title": "How to use asyncio.create_task in asyncio", "content": "`asyncio.asyncio.create_task` schedules a coroutine as a Task in the event loop. Example usage:\n```python\ntask = asyncio.create_task(send_email(user))\nawait task\n```\nThis is useful when fire-and-forget background tasks, concurrent task management. Performance tip: use TaskGroup (Python 3.11+) for structured concurrency.", "code_signature": "asyncio.create_task", "tags": "async, await, lock, coroutine", "url": "https://docs.asyncio.org/en/stable/asyncio/create_task.html", "author": "Official Docs", "date_published": "23-07-2019", "votes_or_stars": 4126, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:3eca92dd-a76b-4584-8b39-4f5554234ff5": {"content": "requests.exceptions.SSLError: certificate verify failed\nNever use `verify=False` in production — it disables SSL verification entirely. Solutions: (1) Update certifi: `pip install --upgrade certifi`. (2) Provide CA bundle: `requests.get(url, verify='/path/to/ca-bundle.crt')`. (3) For self-signed certs in dev: `verify=False` only, and never commit this. (4) Check system time — expired system clock causes SSL failures.\nCode signature: Your SSL certificate chain is incomplete or self-signed. Fix the cert, don't disable verification.", "file_path": "https://stackoverflow.com/questions/7907400", "function_name": "Your SSL certificate chain is incomplete or self-signed. Fix the cert, don't disable verification.", "type": "stack_overflow", "metadata": {"record_id": "3eca92dd-a76b-4584-8b39-4f5554234ff5", "source_type": "stack_overflow", "domain": "Requests/HTTP/APIs", "library": "websocket", "title": "requests.exceptions.SSLError: certificate verify failed", "content": "Never use `verify=False` in production — it disables SSL verification entirely. Solutions: (1) Update certifi: `pip install --upgrade certifi`. (2) Provide CA bundle: `requests.get(url, verify='/path/to/ca-bundle.crt')`. (3) For self-signed certs in dev: `verify=False` only, and never commit this. (4) Check system time — expired system clock causes SSL failures.", "code_signature": "Your SSL certificate chain is incomplete or self-signed. Fix the cert, don't disable verification.", "tags": "headers, requests, aiohttp", "url": "https://stackoverflow.com/questions/7907400", "author": "user_851204", "date_published": "13-10-2020", "votes_or_stars": 1370, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:9c2389a0-8eb1-4b66-8f9f-e5d9706727f3": {"content": "How to use create_engine in SQLAlchemy\n`SQLAlchemy.create_engine` creates a database engine representing a connection pool. Example usage:\n```python\nengine = create_engine('postgresql://user:pass@host/db', echo=True)\n```\nThis is useful when establishing the central database connection for an application. Performance tip: set pool_size and max_overflow based on expected concurrency.\nCode signature: create_engine", "file_path": "https://docs.sqlalchemy.org/en/stable/create_engine.html", "function_name": "create_engine", "type": "documentation", "metadata": {"record_id": "9c2389a0-8eb1-4b66-8f9f-e5d9706727f3", "source_type": "documentation", "domain": "SQLAlchemy/Databases", "library": "SQLAlchemy", "title": "How to use create_engine in SQLAlchemy", "content": "`SQLAlchemy.create_engine` creates a database engine representing a connection pool. Example usage:\n```python\nengine = create_engine('postgresql://user:pass@host/db', echo=True)\n```\nThis is useful when establishing the central database connection for an application. Performance tip: set pool_size and max_overflow based on expected concurrency.", "code_signature": "create_engine", "tags": "alembic, relationship, foreign-key, transaction, connection-pool", "url": "https://docs.sqlalchemy.org/en/stable/create_engine.html", "author": "Official Docs", "date_published": "31-01-2021", "votes_or_stars": 2064, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:8a792c16-1b89-4fd4-bca7-3b04b43404a6": {"content": "SQLAlchemy bulk insert: what's the fastest way?\n`session.bulk_insert_mappings(Model, list_of_dicts)` skips ORM overhead. Even faster: `session.execute(insert(Model), data)`. Avoid `session.add()` in loops. For PostgreSQL, use `insert().on_conflict_do_nothing()` for upserts. Batch in chunks of 1000-5000 rows for optimal performance.\nCode signature: Use session.bulk_insert_mappings() or execute(insert(), list_of_dicts) for maximum speed.", "file_path": "https://stackoverflow.com/questions/5977931", "function_name": "Use session.bulk_insert_mappings() or execute(insert(), list_of_dicts) for maximum speed.", "type": "stack_overflow", "metadata": {"record_id": "8a792c16-1b89-4fd4-bca7-3b04b43404a6", "source_type": "stack_overflow", "domain": "SQLAlchemy/Databases", "library": "query", "title": "SQLAlchemy bulk insert: what's the fastest way?", "content": "`session.bulk_insert_mappings(Model, list_of_dicts)` skips ORM overhead. Even faster: `session.execute(insert(Model), data)`. Avoid `session.add()` in loops. For PostgreSQL, use `insert().on_conflict_do_nothing()` for upserts. Batch in chunks of 1000-5000 rows for optimal performance.", "code_signature": "Use session.bulk_insert_mappings() or execute(insert(), list_of_dicts) for maximum speed.", "tags": "migration, transaction, alembic, session, relationship, orm", "url": "https://stackoverflow.com/questions/5977931", "author": "user_238275", "date_published": "17-06-2024", "votes_or_stars": 616, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:24dd9eec-42b1-43da-b024-5fe8c4046c38": {"content": "BUG: asyncio task leaks when exception is not handled\nProblem: Unhandled exceptions in fire-and-forget asyncio tasks are silently ignored, causing task leaks.\n\nFix/Discussion: Always await tasks or add exception handlers. Use `task.add_done_callback()` to log exceptions. In Python 3.11+, use TaskGroup for structured concurrency — exceptions propagate automatically. Set `asyncio.get_event_loop().set_exception_handler()` for global unhandled task exception logging.\nCode signature: status:closed", "file_path": "https://github.com/cpython/cpython/issues/5591", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "24dd9eec-42b1-43da-b024-5fe8c4046c38", "source_type": "github_issue", "domain": "Asyncio/Threading", "library": "cpython", "title": "BUG: asyncio task leaks when exception is not handled", "content": "Problem: Unhandled exceptions in fire-and-forget asyncio tasks are silently ignored, causing task leaks.\n\nFix/Discussion: Always await tasks or add exception handlers. Use `task.add_done_callback()` to log exceptions. In Python 3.11+, use TaskGroup for structured concurrency — exceptions propagate automatically. Set `asyncio.get_event_loop().set_exception_handler()` for global unhandled task exception logging.", "code_signature": "status:closed", "tags": "coroutine, deadlock, async, event-loop, race-condition, gather", "url": "https://github.com/cpython/cpython/issues/5591", "author": "contributor_1630", "date_published": "18-12-2022", "votes_or_stars": 338, "relevance_label": "low", "difficulty_level": "beginner"}}, "kb:b89a0a0e-d608-4e9c-85f6-e3ad4a7bb77c": {"content": "SQLAlchemy — ORM Session Query\nThe `session.query` function in SQLAlchemy constructs a SELECT query against mapped entities. It accepts `*entities` as a parameter and returns Query object. Common use cases include retrieving ORM-mapped objects from the database. Note that prefer session.execute(select(...)) in SQLAlchemy 2.0+.\nCode signature: session.query(*entities) -> Query object", "file_path": "https://docs.sqlalchemy.org/en/stable/session/query.html", "function_name": "session.query(*entities) -> Query object", "type": "documentation", "metadata": {"record_id": "b89a0a0e-d608-4e9c-85f6-e3ad4a7bb77c", "source_type": "documentation", "domain": "SQLAlchemy/Databases", "library": "SQLAlchemy", "title": "SQLAlchemy — ORM Session Query", "content": "The `session.query` function in SQLAlchemy constructs a SELECT query against mapped entities. It accepts `*entities` as a parameter and returns Query object. Common use cases include retrieving ORM-mapped objects from the database. Note that prefer session.execute(select(...)) in SQLAlchemy 2.0+.", "code_signature": "session.query(*entities) -> Query object", "tags": "session, postgresql, relationship, transaction, sqlalchemy", "url": "https://docs.sqlalchemy.org/en/stable/session/query.html", "author": "Official Docs", "date_published": "26-03-2022", "votes_or_stars": 3010, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:f804f050-03dc-495a-a7e4-67ca4844ff70": {"content": "How to use @app.route in Flask\n`Flask.@app.route` maps a URL rule to a view function. Example usage:\n```python\n@app.route('/users/<int:uid>')\ndef get_user(uid):\n    return jsonify(user)\n```\nThis is useful when defining HTTP endpoints in a Flask application. Performance tip: use Blueprints to organize large applications.\nCode signature: @app.route", "file_path": "https://docs.flask.org/en/stable/@app/route.html", "function_name": "@app.route", "type": "documentation", "metadata": {"record_id": "f804f050-03dc-495a-a7e4-67ca4844ff70", "source_type": "documentation", "domain": "FastAPI/Flask/Django", "library": "Flask", "title": "How to use @app.route in Flask", "content": "`Flask.@app.route` maps a URL rule to a view function. Example usage:\n```python\n@app.route('/users/<int:uid>')\ndef get_user(uid):\n    return jsonify(user)\n```\nThis is useful when defining HTTP endpoints in a Flask application. Performance tip: use Blueprints to organize large applications.", "code_signature": "@app.route", "tags": "rest, orm, blueprint", "url": "https://docs.flask.org/en/stable/@app/route.html", "author": "Official Docs", "date_published": "04-12-2019", "votes_or_stars": 2988, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:7afeb46e-ebd5-4a11-8a8c-b2d053a4af33": {"content": "How to add retry logic to requests?\n```python\nfrom requests.adapters import HTTPAdapter\nfrom urllib3.util.retry import Retry\nretry = Retry(total=3, backoff_factor=1, status_forcelist=[500,502,503,504])\nadapter = HTTPAdapter(max_retries=retry)\nsession.mount('https://', adapter)\n```\nCode signature: Mount an HTTPAdapter with urllib3 Retry on the Session.", "file_path": "https://stackoverflow.com/questions/1669324", "function_name": "Mount an HTTPAdapter with urllib3 Retry on the Session.", "type": "stack_overflow", "metadata": {"record_id": "7afeb46e-ebd5-4a11-8a8c-b2d053a4af33", "source_type": "stack_overflow", "domain": "Requests/HTTP/APIs", "library": "websocket", "title": "How to add retry logic to requests?", "content": "```python\nfrom requests.adapters import HTTPAdapter\nfrom urllib3.util.retry import Retry\nretry = Retry(total=3, backoff_factor=1, status_forcelist=[500,502,503,504])\nadapter = HTTPAdapter(max_retries=retry)\nsession.mount('https://', adapter)\n```", "code_signature": "Mount an HTTPAdapter with urllib3 Retry on the Session.", "tags": "headers, timeout, rate-limiting, http, oauth", "url": "https://stackoverflow.com/questions/1669324", "author": "user_952590", "date_published": "23-09-2019", "votes_or_stars": 1504, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:2667407a-b204-4d5e-926b-29dd99ebedea": {"content": "How to implement database connection pooling with SQLAlchemy?\n`create_engine(url, pool_size=10, max_overflow=20, pool_timeout=30, pool_recycle=1800)`. Use `pool_pre_ping=True` to detect stale connections. For async: use `AsyncEngine` with `create_async_engine()`. Monitor with `engine.pool.status()`. NullPool disables pooling for scripts/serverless environments.\nCode signature: create_engine() has built-in pooling. Configure pool_size and max_overflow.", "file_path": "https://stackoverflow.com/questions/7358113", "function_name": "create_engine() has built-in pooling. Configure pool_size and max_overflow.", "type": "stack_overflow", "metadata": {"record_id": "2667407a-b204-4d5e-926b-29dd99ebedea", "source_type": "stack_overflow", "domain": "SQLAlchemy/Databases", "library": "session", "title": "How to implement database connection pooling with SQLAlchemy?", "content": "`create_engine(url, pool_size=10, max_overflow=20, pool_timeout=30, pool_recycle=1800)`. Use `pool_pre_ping=True` to detect stale connections. For async: use `AsyncEngine` with `create_async_engine()`. Monitor with `engine.pool.status()`. NullPool disables pooling for scripts/serverless environments.", "code_signature": "create_engine() has built-in pooling. Configure pool_size and max_overflow.", "tags": "query, postgresql, migration, sqlite, foreign-key, sqlalchemy", "url": "https://stackoverflow.com/questions/7358113", "author": "user_12260", "date_published": "03-08-2018", "votes_or_stars": 258, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:5219ea63-67ea-4388-a5fc-181c7e535e14": {"content": "BUG: httpx: SSL certificate error on Windows despite valid cert\nProblem: httpx raises SSLCertVerificationError on Windows when system CA store has the cert.\n\nFix/Discussion: httpx uses its own CA bundle (certifi) by default, not the system store. Fix: `httpx.Client(verify=certifi.where())` — already default. For custom certs: `httpx.Client(verify='/path/to/ca-bundle.pem')`. Install `pip install truststore` and use `ssl.create_default_context(ssl.Purpose.SERVER_AUTH)` to use system store.\nCode signature: status:closed", "file_path": "https://github.com/httpx/httpx/issues/8949", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "5219ea63-67ea-4388-a5fc-181c7e535e14", "source_type": "github_issue", "domain": "Requests/HTTP/APIs", "library": "httpx", "title": "BUG: httpx: SSL certificate error on Windows despite valid cert", "content": "Problem: httpx raises SSLCertVerificationError on Windows when system CA store has the cert.\n\nFix/Discussion: httpx uses its own CA bundle (certifi) by default, not the system store. Fix: `httpx.Client(verify=certifi.where())` — already default. For custom certs: `httpx.Client(verify='/path/to/ca-bundle.pem')`. Install `pip install truststore` and use `ssl.create_default_context(ssl.Purpose.SERVER_AUTH)` to use system store.", "code_signature": "status:closed", "tags": "json, requests, aiohttp, timeout, retry", "url": "https://github.com/httpx/httpx/issues/8949", "author": "contributor_1420", "date_published": "14-06-2024", "votes_or_stars": 469, "relevance_label": "low", "difficulty_level": "beginner"}}, "kb:79fab260-2663-46f0-949c-cc49953ef9ec": {"content": "How to use create_engine in SQLAlchemy\n`SQLAlchemy.create_engine` creates a database engine representing a connection pool. Example usage:\n```python\nengine = create_engine('postgresql://user:pass@host/db', echo=True)\n```\nThis is useful when establishing the central database connection for an application. Performance tip: set pool_size and max_overflow based on expected concurrency.\nCode signature: create_engine", "file_path": "https://docs.sqlalchemy.org/en/stable/create_engine.html", "function_name": "create_engine", "type": "documentation", "metadata": {"record_id": "79fab260-2663-46f0-949c-cc49953ef9ec", "source_type": "documentation", "domain": "SQLAlchemy/Databases", "library": "SQLAlchemy", "title": "How to use create_engine in SQLAlchemy", "content": "`SQLAlchemy.create_engine` creates a database engine representing a connection pool. Example usage:\n```python\nengine = create_engine('postgresql://user:pass@host/db', echo=True)\n```\nThis is useful when establishing the central database connection for an application. Performance tip: set pool_size and max_overflow based on expected concurrency.", "code_signature": "create_engine", "tags": "foreign-key, sqlalchemy, query, sqlite, migration", "url": "https://docs.sqlalchemy.org/en/stable/create_engine.html", "author": "Official Docs", "date_published": "26-03-2019", "votes_or_stars": 2031, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:cad72ad6-522a-4b97-bed2-9f26e92fb0d1": {"content": "How do I reshape a wide DataFrame to long format?\n`pd.melt(df, id_vars=['id'], value_vars=['col1','col2'])` unpivots columns to rows. `df.stack()` moves column index to row index. For the reverse, use `df.pivot()` or `df.unstack()`. Always reset_index() after stack/unstack to get a clean DataFrame.\nCode signature: Use pd.melt() or df.stack() depending on your structure.", "file_path": "https://stackoverflow.com/questions/6229731", "function_name": "Use pd.melt() or df.stack() depending on your structure.", "type": "stack_overflow", "metadata": {"record_id": "cad72ad6-522a-4b97-bed2-9f26e92fb0d1", "source_type": "stack_overflow", "domain": "NumPy/Pandas", "library": "nan", "title": "How do I reshape a wide DataFrame to long format?", "content": "`pd.melt(df, id_vars=['id'], value_vars=['col1','col2'])` unpivots columns to rows. `df.stack()` moves column index to row index. For the reverse, use `df.pivot()` or `df.unstack()`. Always reset_index() after stack/unstack to get a clean DataFrame.", "code_signature": "Use pd.melt() or df.stack() depending on your structure.", "tags": "pandas, merge, vectorization, dataframe, array, reshape", "url": "https://stackoverflow.com/questions/6229731", "author": "user_428373", "date_published": "16-08-2018", "votes_or_stars": 850, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:1e4fba8a-c68b-4617-a365-00641f9b89a7": {"content": "Django N+1 query problem: how to fix?\nN+1 occurs when accessing related objects in a loop. Diagnose with `django-debug-toolbar` or `connection.queries`. Fix ForeignKey: `User.objects.select_related('profile').all()`. Fix M2M: `Post.objects.prefetch_related('tags').all()`. Use `only()` to fetch specific fields and reduce data transfer.\nCode signature: Use select_related() for ForeignKey and prefetch_related() for ManyToMany relationships.", "file_path": "https://stackoverflow.com/questions/8106470", "function_name": "Use select_related() for ForeignKey and prefetch_related() for ManyToMany relationships.", "type": "stack_overflow", "metadata": {"record_id": "1e4fba8a-c68b-4617-a365-00641f9b89a7", "source_type": "stack_overflow", "domain": "FastAPI/Flask/Django", "library": "request", "title": "Django N+1 query problem: how to fix?", "content": "N+1 occurs when accessing related objects in a loop. Diagnose with `django-debug-toolbar` or `connection.queries`. Fix ForeignKey: `User.objects.select_related('profile').all()`. Fix M2M: `Post.objects.prefetch_related('tags').all()`. Use `only()` to fetch specific fields and reduce data transfer.", "code_signature": "Use select_related() for ForeignKey and prefetch_related() for ManyToMany relationships.", "tags": "blueprint, response, request, dependency-injection, rest, flask", "url": "https://stackoverflow.com/questions/8106470", "author": "user_441071", "date_published": "30-11-2022", "votes_or_stars": 1791, "relevance_label": "low", "difficulty_level": "intermediate"}}, "kb:e26da5c0-c346-4ddf-aeef-c304e3b9e882": {"content": "How to use @app.route in Flask\n`Flask.@app.route` maps a URL rule to a view function. Example usage:\n```python\n@app.route('/users/<int:uid>')\ndef get_user(uid):\n    return jsonify(user)\n```\nThis is useful when defining HTTP endpoints in a Flask application. Performance tip: use Blueprints to organize large applications.\nCode signature: @app.route", "file_path": "https://docs.flask.org/en/stable/@app/route.html", "function_name": "@app.route", "type": "documentation", "metadata": {"record_id": "e26da5c0-c346-4ddf-aeef-c304e3b9e882", "source_type": "documentation", "domain": "FastAPI/Flask/Django", "library": "Flask", "title": "How to use @app.route in Flask", "content": "`Flask.@app.route` maps a URL rule to a view function. Example usage:\n```python\n@app.route('/users/<int:uid>')\ndef get_user(uid):\n    return jsonify(user)\n```\nThis is useful when defining HTTP endpoints in a Flask application. Performance tip: use Blueprints to organize large applications.", "code_signature": "@app.route", "tags": "dependency-injection, flask, response, rest, pydantic, middleware", "url": "https://docs.flask.org/en/stable/@app/route.html", "author": "Official Docs", "date_published": "25-08-2024", "votes_or_stars": 3012, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:c922a2cb-e78e-422c-bf85-681c622f0a49": {"content": "How do I reshape a wide DataFrame to long format?\n`pd.melt(df, id_vars=['id'], value_vars=['col1','col2'])` unpivots columns to rows. `df.stack()` moves column index to row index. For the reverse, use `df.pivot()` or `df.unstack()`. Always reset_index() after stack/unstack to get a clean DataFrame.\nCode signature: Use pd.melt() or df.stack() depending on your structure.", "file_path": "https://stackoverflow.com/questions/6229731", "function_name": "Use pd.melt() or df.stack() depending on your structure.", "type": "stack_overflow", "metadata": {"record_id": "c922a2cb-e78e-422c-bf85-681c622f0a49", "source_type": "stack_overflow", "domain": "NumPy/Pandas", "library": "nan", "title": "How do I reshape a wide DataFrame to long format?", "content": "`pd.melt(df, id_vars=['id'], value_vars=['col1','col2'])` unpivots columns to rows. `df.stack()` moves column index to row index. For the reverse, use `df.pivot()` or `df.unstack()`. Always reset_index() after stack/unstack to get a clean DataFrame.", "code_signature": "Use pd.melt() or df.stack() depending on your structure.", "tags": "numpy, groupby, dtype", "url": "https://stackoverflow.com/questions/6229731", "author": "user_428373", "date_published": "03-10-2018", "votes_or_stars": 822, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:f4a32c46-d27e-483c-a73b-5fd9f231f78a": {"content": "How do I reshape a wide DataFrame to long format?\n`pd.melt(df, id_vars=['id'], value_vars=['col1','col2'])` unpivots columns to rows. `df.stack()` moves column index to row index. For the reverse, use `df.pivot()` or `df.unstack()`. Always reset_index() after stack/unstack to get a clean DataFrame.\nCode signature: Use pd.melt() or df.stack() depending on your structure.", "file_path": "https://stackoverflow.com/questions/6229731", "function_name": "Use pd.melt() or df.stack() depending on your structure.", "type": "stack_overflow", "metadata": {"record_id": "f4a32c46-d27e-483c-a73b-5fd9f231f78a", "source_type": "stack_overflow", "domain": "NumPy/Pandas", "library": "nan", "title": "How do I reshape a wide DataFrame to long format?", "content": "`pd.melt(df, id_vars=['id'], value_vars=['col1','col2'])` unpivots columns to rows. `df.stack()` moves column index to row index. For the reverse, use `df.pivot()` or `df.unstack()`. Always reset_index() after stack/unstack to get a clean DataFrame.", "code_signature": "Use pd.melt() or df.stack() depending on your structure.", "tags": "iloc, dtype, pivot, broadcasting", "url": "https://stackoverflow.com/questions/6229731", "author": "user_428373", "date_published": "17-09-2018", "votes_or_stars": 845, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:e6dc0db7-7679-40bb-b0f5-d2414a1eae72": {"content": "Python threading: how to share data between threads safely?\nFor simple counters: use `threading.Lock()` as context manager. For complex shared state: prefer `queue.Queue` (thread-safe by design). Avoid global variables without locks. `threading.local()` gives each thread its own data copy. For read-heavy workloads, `threading.RLock` allows re-entrant acquisition.\nCode signature: Use threading.Lock for mutual exclusion or queue.Queue for producer-consumer patterns.", "file_path": "https://stackoverflow.com/questions/3195773", "function_name": "Use threading.Lock for mutual exclusion or queue.Queue for producer-consumer patterns.", "type": "stack_overflow", "metadata": {"record_id": "e6dc0db7-7679-40bb-b0f5-d2414a1eae72", "source_type": "stack_overflow", "domain": "Asyncio/Threading", "library": "lock", "title": "Python threading: how to share data between threads safely?", "content": "For simple counters: use `threading.Lock()` as context manager. For complex shared state: prefer `queue.Queue` (thread-safe by design). Avoid global variables without locks. `threading.local()` gives each thread its own data copy. For read-heavy workloads, `threading.RLock` allows re-entrant acquisition.", "code_signature": "Use threading.Lock for mutual exclusion or queue.Queue for producer-consumer patterns.", "tags": "threading, coroutine, event-loop, deadlock, executor", "url": "https://stackoverflow.com/questions/3195773", "author": "user_714318", "date_published": "13-05-2021", "votes_or_stars": 1097, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:be30b671-e872-4533-9d69-e5be79f903ec": {"content": "BUG: asyncio task leaks when exception is not handled\nProblem: Unhandled exceptions in fire-and-forget asyncio tasks are silently ignored, causing task leaks.\n\nFix/Discussion: Always await tasks or add exception handlers. Use `task.add_done_callback()` to log exceptions. In Python 3.11+, use TaskGroup for structured concurrency — exceptions propagate automatically. Set `asyncio.get_event_loop().set_exception_handler()` for global unhandled task exception logging.\nCode signature: status:closed", "file_path": "https://github.com/cpython/cpython/issues/5591", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "be30b671-e872-4533-9d69-e5be79f903ec", "source_type": "github_issue", "domain": "Asyncio/Threading", "library": "cpython", "title": "BUG: asyncio task leaks when exception is not handled", "content": "Problem: Unhandled exceptions in fire-and-forget asyncio tasks are silently ignored, causing task leaks.\n\nFix/Discussion: Always await tasks or add exception handlers. Use `task.add_done_callback()` to log exceptions. In Python 3.11+, use TaskGroup for structured concurrency — exceptions propagate automatically. Set `asyncio.get_event_loop().set_exception_handler()` for global unhandled task exception logging.", "code_signature": "status:closed", "tags": "await, executor, async, coroutine, task", "url": "https://github.com/cpython/cpython/issues/5591", "author": "contributor_1630", "date_published": "20-08-2021", "votes_or_stars": 344, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:47650045-f5ad-4a75-84d5-c2473380729d": {"content": "How to use df.groupby in Pandas\n`Pandas.df.groupby` groups DataFrame rows by column values. Example usage:\n```python\ndf.groupby('category')['value'].mean()\n```\nThis is useful when aggregation, transformation, filtering by group. Performance tip: avoid apply() on large DataFrames; prefer agg().\nCode signature: df.groupby", "file_path": "https://docs.pandas.org/en/stable/df/groupby.html", "function_name": "df.groupby", "type": "documentation", "metadata": {"record_id": "47650045-f5ad-4a75-84d5-c2473380729d", "source_type": "documentation", "domain": "NumPy/Pandas", "library": "Pandas", "title": "How to use df.groupby in Pandas", "content": "`Pandas.df.groupby` groups DataFrame rows by column values. Example usage:\n```python\ndf.groupby('category')['value'].mean()\n```\nThis is useful when aggregation, transformation, filtering by group. Performance tip: avoid apply() on large DataFrames; prefer agg().", "code_signature": "df.groupby", "tags": "dtype, broadcasting, pandas, array", "url": "https://docs.pandas.org/en/stable/df/groupby.html", "author": "Official Docs", "date_published": "24-09-2022", "votes_or_stars": 788, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:d68cdc0e-e1c2-415c-ae35-79751a57fd16": {"content": "Python: how to safely write to a file atomically?\n```python\nimport tempfile, os\nwith tempfile.NamedTemporaryFile('w', delete=False, dir='.') as tmp:\n    tmp.write(data)\n    tmp.flush()\n    os.fsync(tmp.fileno())\nos.replace(tmp.name, target_path)\n```\n`os.replace()` is atomic on POSIX. This prevents partial writes from corrupting files.\nCode signature: Write to a temp file then rename — rename is atomic on POSIX systems.", "file_path": "https://stackoverflow.com/questions/5394880", "function_name": "Write to a temp file then rename — rename is atomic on POSIX systems.", "type": "stack_overflow", "metadata": {"record_id": "d68cdc0e-e1c2-415c-ae35-79751a57fd16", "source_type": "stack_overflow", "domain": "OS/Subprocess/File I/O", "library": "stdin", "title": "Python: how to safely write to a file atomically?", "content": "```python\nimport tempfile, os\nwith tempfile.NamedTemporaryFile('w', delete=False, dir='.') as tmp:\n    tmp.write(data)\n    tmp.flush()\n    os.fsync(tmp.fileno())\nos.replace(tmp.name, target_path)\n```\n`os.replace()` is atomic on POSIX. This prevents partial writes from corrupting files.", "code_signature": "Write to a temp file then rename — rename is atomic on POSIX systems.", "tags": "env-variable, stdout, glob", "url": "https://stackoverflow.com/questions/5394880", "author": "user_179430", "date_published": "23-08-2018", "votes_or_stars": 537, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:2345961c-833c-4c09-937a-aa898e7454cd": {"content": "Flask: how to return JSON responses properly?\n`return jsonify({'key': 'value'})` sets Content-Type to application/json. Flask 1.0+ automatically converts returned dicts to JSON responses. For custom status codes: `return jsonify(data), 201`. For errors: use `abort(404)` or return `({'error': 'Not found'}, 404)`.\nCode signature: Use jsonify() or return a dict directly (Flask 1.0+).", "file_path": "https://stackoverflow.com/questions/8930103", "function_name": "Use jsonify() or return a dict directly (Flask 1.0+).", "type": "stack_overflow", "metadata": {"record_id": "2345961c-833c-4c09-937a-aa898e7454cd", "source_type": "stack_overflow", "domain": "FastAPI/Flask/Django", "library": "django", "title": "Flask: how to return JSON responses properly?", "content": "`return jsonify({'key': 'value'})` sets Content-Type to application/json. Flask 1.0+ automatically converts returned dicts to JSON responses. For custom status codes: `return jsonify(data), 201`. For errors: use `abort(404)` or return `({'error': 'Not found'}, 404)`.", "code_signature": "Use jsonify() or return a dict directly (Flask 1.0+).", "tags": "request, middleware, pydantic", "url": "https://stackoverflow.com/questions/8930103", "author": "user_264801", "date_published": "30-05-2024", "votes_or_stars": 2206, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:f862b51c-0489-4d22-a21b-1ba830dd5dec": {"content": "SQLAlchemy bulk insert: what's the fastest way?\n`session.bulk_insert_mappings(Model, list_of_dicts)` skips ORM overhead. Even faster: `session.execute(insert(Model), data)`. Avoid `session.add()` in loops. For PostgreSQL, use `insert().on_conflict_do_nothing()` for upserts. Batch in chunks of 1000-5000 rows for optimal performance.\nCode signature: Use session.bulk_insert_mappings() or execute(insert(), list_of_dicts) for maximum speed.", "file_path": "https://stackoverflow.com/questions/5977931", "function_name": "Use session.bulk_insert_mappings() or execute(insert(), list_of_dicts) for maximum speed.", "type": "stack_overflow", "metadata": {"record_id": "f862b51c-0489-4d22-a21b-1ba830dd5dec", "source_type": "stack_overflow", "domain": "SQLAlchemy/Databases", "library": "query", "title": "SQLAlchemy bulk insert: what's the fastest way?", "content": "`session.bulk_insert_mappings(Model, list_of_dicts)` skips ORM overhead. Even faster: `session.execute(insert(Model), data)`. Avoid `session.add()` in loops. For PostgreSQL, use `insert().on_conflict_do_nothing()` for upserts. Batch in chunks of 1000-5000 rows for optimal performance.", "code_signature": "Use session.bulk_insert_mappings() or execute(insert(), list_of_dicts) for maximum speed.", "tags": "foreign-key, sqlite, query, relationship, orm, transaction", "url": "https://stackoverflow.com/questions/5977931", "author": "user_238275", "date_published": "17-10-2024", "votes_or_stars": 637, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:10297186-ec55-48d4-a92d-669c24022ea2": {"content": "How to implement database connection pooling with SQLAlchemy?\n`create_engine(url, pool_size=10, max_overflow=20, pool_timeout=30, pool_recycle=1800)`. Use `pool_pre_ping=True` to detect stale connections. For async: use `AsyncEngine` with `create_async_engine()`. Monitor with `engine.pool.status()`. NullPool disables pooling for scripts/serverless environments.\nCode signature: create_engine() has built-in pooling. Configure pool_size and max_overflow.", "file_path": "https://stackoverflow.com/questions/7358113", "function_name": "create_engine() has built-in pooling. Configure pool_size and max_overflow.", "type": "stack_overflow", "metadata": {"record_id": "10297186-ec55-48d4-a92d-669c24022ea2", "source_type": "stack_overflow", "domain": "SQLAlchemy/Databases", "library": "session", "title": "How to implement database connection pooling with SQLAlchemy?", "content": "`create_engine(url, pool_size=10, max_overflow=20, pool_timeout=30, pool_recycle=1800)`. Use `pool_pre_ping=True` to detect stale connections. For async: use `AsyncEngine` with `create_async_engine()`. Monitor with `engine.pool.status()`. NullPool disables pooling for scripts/serverless environments.", "code_signature": "create_engine() has built-in pooling. Configure pool_size and max_overflow.", "tags": "connection-pool, postgresql, query, alembic", "url": "https://stackoverflow.com/questions/7358113", "author": "user_12260", "date_published": "19-05-2022", "votes_or_stars": 250, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:2f5663d3-fb1a-4101-886d-cb2147df6602": {"content": "Why is my Pandas merge creating duplicate rows?\nDuplicate rows after merge usually mean the join key is not unique. Diagnose with `df.duplicated(subset=['key']).sum()`. Use `how='left'` carefully when right table has multiple matches. Set `validate='one_to_many'` or `'many_to_one'` to enforce cardinality. Consider deduplicating with `df.drop_duplicates(subset=['key'])` before merging.\nCode signature: Your merge key has duplicates in one or both DataFrames. Use validate='one_to_one' to catch this early.", "file_path": "https://stackoverflow.com/questions/2321324", "function_name": "Your merge key has duplicates in one or both DataFrames. Use validate='one_to_one' to catch this early.", "type": "stack_overflow", "metadata": {"record_id": "2f5663d3-fb1a-4101-886d-cb2147df6602", "source_type": "stack_overflow", "domain": "NumPy/Pandas", "library": "array", "title": "Why is my Pandas merge creating duplicate rows?", "content": "Duplicate rows after merge usually mean the join key is not unique. Diagnose with `df.duplicated(subset=['key']).sum()`. Use `how='left'` carefully when right table has multiple matches. Set `validate='one_to_many'` or `'many_to_one'` to enforce cardinality. Consider deduplicating with `df.drop_duplicates(subset=['key'])` before merging.", "code_signature": "Your merge key has duplicates in one or both DataFrames. Use validate='one_to_one' to catch this early.", "tags": "pandas, dtype, iloc, nan, loc, dataframe", "url": "https://stackoverflow.com/questions/2321324", "author": "user_99814", "date_published": "15-08-2023", "votes_or_stars": 266, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:cebe5a9d-0466-41a3-9c05-90298525e0c1": {"content": "How to handle database migrations with Alembic?\nWorkflow: (1) Change SQLAlchemy models. (2) `alembic revision --autogenerate -m 'description'`. (3) Review generated migration script. (4) `alembic upgrade head` to apply. Always review autogenerated migrations — they miss some changes (indexes, constraints). Use `alembic downgrade -1` to rollback. Store migrations in version control.\nCode signature: Use alembic revision --autogenerate to create migrations from model changes.", "file_path": "https://stackoverflow.com/questions/9436429", "function_name": "Use alembic revision --autogenerate to create migrations from model changes.", "type": "stack_overflow", "metadata": {"record_id": "cebe5a9d-0466-41a3-9c05-90298525e0c1", "source_type": "stack_overflow", "domain": "SQLAlchemy/Databases", "library": "connection-pool", "title": "How to handle database migrations with Alembic?", "content": "Workflow: (1) Change SQLAlchemy models. (2) `alembic revision --autogenerate -m 'description'`. (3) Review generated migration script. (4) `alembic upgrade head` to apply. Always review autogenerated migrations — they miss some changes (indexes, constraints). Use `alembic downgrade -1` to rollback. Store migrations in version control.", "code_signature": "Use alembic revision --autogenerate to create migrations from model changes.", "tags": "query, foreign-key, sqlite, transaction", "url": "https://stackoverflow.com/questions/9436429", "author": "user_974047", "date_published": "04-04-2018", "votes_or_stars": 2393, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:0f7cad67-3f2d-4874-af16-f20846a0a189": {"content": "How to upload a file with requests?\n```python\nwith open('file.pdf', 'rb') as f:\n    r = requests.post(url, files={'file': ('file.pdf', f, 'application/pdf')})\n```\nFor multiple files: pass a list of tuples. For multipart form data with fields: combine `files` and `data` parameters. Don't set Content-Type manually — requests sets it with boundary.\nCode signature: Use the files parameter with a file-like object or tuple.", "file_path": "https://stackoverflow.com/questions/8896868", "function_name": "Use the files parameter with a file-like object or tuple.", "type": "stack_overflow", "metadata": {"record_id": "0f7cad67-3f2d-4874-af16-f20846a0a189", "source_type": "stack_overflow", "domain": "Requests/HTTP/APIs", "library": "requests", "title": "How to upload a file with requests?", "content": "```python\nwith open('file.pdf', 'rb') as f:\n    r = requests.post(url, files={'file': ('file.pdf', f, 'application/pdf')})\n```\nFor multiple files: pass a list of tuples. For multipart form data with fields: combine `files` and `data` parameters. Don't set Content-Type manually — requests sets it with boundary.", "code_signature": "Use the files parameter with a file-like object or tuple.", "tags": "http, oauth, requests, aiohttp, httpx", "url": "https://stackoverflow.com/questions/8896868", "author": "user_243238", "date_published": "06-08-2020", "votes_or_stars": 1585, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:2885d037-cd7a-49b0-a15f-8ada6a7c6fbc": {"content": "FastAPI — Path Operations\nThe `@app.get` function in FastAPI registers a GET endpoint at the specified path. It accepts `path, response_model` as a parameter and returns Response. Common use cases include building REST API endpoints that return JSON data. Note that use response_model to enforce output schema validation.\nCode signature: @app.get(path, response_model) -> Response", "file_path": "https://docs.fastapi.org/en/stable/@app/get.html", "function_name": "@app.get(path, response_model) -> Response", "type": "documentation", "metadata": {"record_id": "2885d037-cd7a-49b0-a15f-8ada6a7c6fbc", "source_type": "documentation", "domain": "FastAPI/Flask/Django", "library": "FastAPI", "title": "FastAPI — Path Operations", "content": "The `@app.get` function in FastAPI registers a GET endpoint at the specified path. It accepts `path, response_model` as a parameter and returns Response. Common use cases include building REST API endpoints that return JSON data. Note that use response_model to enforce output schema validation.", "code_signature": "@app.get(path, response_model) -> Response", "tags": "middleware, request, orm, template", "url": "https://docs.fastapi.org/en/stable/@app/get.html", "author": "Official Docs", "date_published": "01-05-2023", "votes_or_stars": 2844, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:2cf88f9b-3be8-4a46-8aba-537c2ee243ce": {"content": "os Environment Variables — official reference\nOfficial documentation for os Environment Variables. The method signature is `os.environ(key)`. provides a mapping of the current environment variables. Raises `KeyError` when key is accessed with [] notation and is not set. See also: dotenv, os.getenv, os.putenv.\nCode signature: os.environ(key)", "file_path": "https://docs.os.org/en/stable/os/environ.html", "function_name": "os.environ(key)", "type": "documentation", "metadata": {"record_id": "2cf88f9b-3be8-4a46-8aba-537c2ee243ce", "source_type": "documentation", "domain": "OS/Subprocess/File I/O", "library": "os", "title": "os Environment Variables — official reference", "content": "Official documentation for os Environment Variables. The method signature is `os.environ(key)`. provides a mapping of the current environment variables. Raises `KeyError` when key is accessed with [] notation and is not set. See also: dotenv, os.getenv, os.putenv.", "code_signature": "os.environ(key)", "tags": "env-variable, pathlib, shutil", "url": "https://docs.os.org/en/stable/os/environ.html", "author": "Official Docs", "date_published": "06-01-2019", "votes_or_stars": 2232, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:4f9abcad-603a-4213-aae5-68979f9e38d0": {"content": "How to implement a thread pool in Python?\n```python\nfrom concurrent.futures import ThreadPoolExecutor\nwith ThreadPoolExecutor(max_workers=8) as ex:\n    futures = [ex.submit(task, arg) for arg in args]\n    results = [f.result() for f in futures]\n```\nFor CPU-bound: use ProcessPoolExecutor instead. For async integration: `await loop.run_in_executor(executor, func, *args)`.\nCode signature: Use concurrent.futures.ThreadPoolExecutor for I/O-bound tasks.", "file_path": "https://stackoverflow.com/questions/3646552", "function_name": "Use concurrent.futures.ThreadPoolExecutor for I/O-bound tasks.", "type": "stack_overflow", "metadata": {"record_id": "4f9abcad-603a-4213-aae5-68979f9e38d0", "source_type": "stack_overflow", "domain": "Asyncio/Threading", "library": "coroutine", "title": "How to implement a thread pool in Python?", "content": "```python\nfrom concurrent.futures import ThreadPoolExecutor\nwith ThreadPoolExecutor(max_workers=8) as ex:\n    futures = [ex.submit(task, arg) for arg in args]\n    results = [f.result() for f in futures]\n```\nFor CPU-bound: use ProcessPoolExecutor instead. For async integration: `await loop.run_in_executor(executor, func, *args)`.", "code_signature": "Use concurrent.futures.ThreadPoolExecutor for I/O-bound tasks.", "tags": "lock, async, semaphore, gather, coroutine, threading", "url": "https://stackoverflow.com/questions/3646552", "author": "user_469469", "date_published": "10-11-2018", "votes_or_stars": 1551, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:91ccffe1-4c3c-45fd-b57b-bc03585a4d15": {"content": "How to use @app.route in Flask\n`Flask.@app.route` maps a URL rule to a view function. Example usage:\n```python\n@app.route('/users/<int:uid>')\ndef get_user(uid):\n    return jsonify(user)\n```\nThis is useful when defining HTTP endpoints in a Flask application. Performance tip: use Blueprints to organize large applications.\nCode signature: @app.route", "file_path": "https://docs.flask.org/en/stable/@app/route.html", "function_name": "@app.route", "type": "documentation", "metadata": {"record_id": "91ccffe1-4c3c-45fd-b57b-bc03585a4d15", "source_type": "documentation", "domain": "FastAPI/Flask/Django", "library": "Flask", "title": "How to use @app.route in Flask", "content": "`Flask.@app.route` maps a URL rule to a view function. Example usage:\n```python\n@app.route('/users/<int:uid>')\ndef get_user(uid):\n    return jsonify(user)\n```\nThis is useful when defining HTTP endpoints in a Flask application. Performance tip: use Blueprints to organize large applications.", "code_signature": "@app.route", "tags": "middleware, django, template, rest, request", "url": "https://docs.flask.org/en/stable/@app/route.html", "author": "Official Docs", "date_published": "02-08-2021", "votes_or_stars": 3024, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:55bf85ed-6515-498d-b094-abdef69858c8": {"content": "How to use @app.route in Flask\n`Flask.@app.route` maps a URL rule to a view function. Example usage:\n```python\n@app.route('/users/<int:uid>')\ndef get_user(uid):\n    return jsonify(user)\n```\nThis is useful when defining HTTP endpoints in a Flask application. Performance tip: use Blueprints to organize large applications.\nCode signature: @app.route", "file_path": "https://docs.flask.org/en/stable/@app/route.html", "function_name": "@app.route", "type": "documentation", "metadata": {"record_id": "55bf85ed-6515-498d-b094-abdef69858c8", "source_type": "documentation", "domain": "FastAPI/Flask/Django", "library": "Flask", "title": "How to use @app.route in Flask", "content": "`Flask.@app.route` maps a URL rule to a view function. Example usage:\n```python\n@app.route('/users/<int:uid>')\ndef get_user(uid):\n    return jsonify(user)\n```\nThis is useful when defining HTTP endpoints in a Flask application. Performance tip: use Blueprints to organize large applications.", "code_signature": "@app.route", "tags": "fastapi, pydantic, dependency-injection, orm", "url": "https://docs.flask.org/en/stable/@app/route.html", "author": "Official Docs", "date_published": "27-09-2023", "votes_or_stars": 3017, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:f49ea47c-1507-4873-a846-f85d29bb8e41": {"content": "Django ORM Queries — official reference\nOfficial documentation for Django ORM Queries. The method signature is `QuerySet.filter(**kwargs)`. filters QuerySet rows matching given field lookups. Raises `FieldError` when an unknown field lookup is provided. See also: QuerySet.exclude, QuerySet.annotate, Q objects.\nCode signature: QuerySet.filter(**kwargs)", "file_path": "https://docs.django.org/en/stable/QuerySet/filter.html", "function_name": "QuerySet.filter(**kwargs)", "type": "documentation", "metadata": {"record_id": "f49ea47c-1507-4873-a846-f85d29bb8e41", "source_type": "documentation", "domain": "FastAPI/Flask/Django", "library": "Django", "title": "Django ORM Queries — official reference", "content": "Official documentation for Django ORM Queries. The method signature is `QuerySet.filter(**kwargs)`. filters QuerySet rows matching given field lookups. Raises `FieldError` when an unknown field lookup is provided. See also: QuerySet.exclude, QuerySet.annotate, Q objects.", "code_signature": "QuerySet.filter(**kwargs)", "tags": "middleware, flask, request", "url": "https://docs.django.org/en/stable/QuerySet/filter.html", "author": "Official Docs", "date_published": "12-11-2019", "votes_or_stars": 2395, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:e054d6f7-7d51-40a9-b4d0-504bd152894c": {"content": "How do I handle CORS in FastAPI?\n```python\nfrom fastapi.middleware.cors import CORSMiddleware\napp.add_middleware(CORSMiddleware, allow_origins=['*'], allow_methods=['*'], allow_headers=['*'])\n```\nIn production, replace `'*'` with your actual frontend domain. For credentials (cookies), set `allow_credentials=True` and specify exact origins.\nCode signature: Add CORSMiddleware from starlette to your FastAPI app.", "file_path": "https://stackoverflow.com/questions/1527021", "function_name": "Add CORSMiddleware from starlette to your FastAPI app.", "type": "stack_overflow", "metadata": {"record_id": "e054d6f7-7d51-40a9-b4d0-504bd152894c", "source_type": "stack_overflow", "domain": "FastAPI/Flask/Django", "library": "flask", "title": "How do I handle CORS in FastAPI?", "content": "```python\nfrom fastapi.middleware.cors import CORSMiddleware\napp.add_middleware(CORSMiddleware, allow_origins=['*'], allow_methods=['*'], allow_headers=['*'])\n```\nIn production, replace `'*'` with your actual frontend domain. For credentials (cookies), set `allow_credentials=True` and specify exact origins.", "code_signature": "Add CORSMiddleware from starlette to your FastAPI app.", "tags": "dependency-injection, orm, flask, template", "url": "https://stackoverflow.com/questions/1527021", "author": "user_911393", "date_published": "20-10-2019", "votes_or_stars": 971, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:3a7deab8-f378-40fa-8781-585a9b73fc3b": {"content": "How to add retry logic to requests?\n```python\nfrom requests.adapters import HTTPAdapter\nfrom urllib3.util.retry import Retry\nretry = Retry(total=3, backoff_factor=1, status_forcelist=[500,502,503,504])\nadapter = HTTPAdapter(max_retries=retry)\nsession.mount('https://', adapter)\n```\nCode signature: Mount an HTTPAdapter with urllib3 Retry on the Session.", "file_path": "https://stackoverflow.com/questions/1669324", "function_name": "Mount an HTTPAdapter with urllib3 Retry on the Session.", "type": "stack_overflow", "metadata": {"record_id": "3a7deab8-f378-40fa-8781-585a9b73fc3b", "source_type": "stack_overflow", "domain": "Requests/HTTP/APIs", "library": "websocket", "title": "How to add retry logic to requests?", "content": "```python\nfrom requests.adapters import HTTPAdapter\nfrom urllib3.util.retry import Retry\nretry = Retry(total=3, backoff_factor=1, status_forcelist=[500,502,503,504])\nadapter = HTTPAdapter(max_retries=retry)\nsession.mount('https://', adapter)\n```", "code_signature": "Mount an HTTPAdapter with urllib3 Retry on the Session.", "tags": "oauth, websocket, headers", "url": "https://stackoverflow.com/questions/1669324", "author": "user_952590", "date_published": "07-09-2021", "votes_or_stars": 1525, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:e85b0df2-889e-443a-a6dc-0b564e0dbb0b": {"content": "Why is my Pandas merge creating duplicate rows?\nDuplicate rows after merge usually mean the join key is not unique. Diagnose with `df.duplicated(subset=['key']).sum()`. Use `how='left'` carefully when right table has multiple matches. Set `validate='one_to_many'` or `'many_to_one'` to enforce cardinality. Consider deduplicating with `df.drop_duplicates(subset=['key'])` before merging.\nCode signature: Your merge key has duplicates in one or both DataFrames. Use validate='one_to_one' to catch this early.", "file_path": "https://stackoverflow.com/questions/2321324", "function_name": "Your merge key has duplicates in one or both DataFrames. Use validate='one_to_one' to catch this early.", "type": "stack_overflow", "metadata": {"record_id": "e85b0df2-889e-443a-a6dc-0b564e0dbb0b", "source_type": "stack_overflow", "domain": "NumPy/Pandas", "library": "array", "title": "Why is my Pandas merge creating duplicate rows?", "content": "Duplicate rows after merge usually mean the join key is not unique. Diagnose with `df.duplicated(subset=['key']).sum()`. Use `how='left'` carefully when right table has multiple matches. Set `validate='one_to_many'` or `'many_to_one'` to enforce cardinality. Consider deduplicating with `df.drop_duplicates(subset=['key'])` before merging.", "code_signature": "Your merge key has duplicates in one or both DataFrames. Use validate='one_to_one' to catch this early.", "tags": "groupby, dataframe, merge", "url": "https://stackoverflow.com/questions/2321324", "author": "user_99814", "date_published": "24-03-2018", "votes_or_stars": 237, "relevance_label": "low", "difficulty_level": "intermediate"}}, "kb:45819d80-554a-456e-857a-8b86b157f743": {"content": "How to handle database migrations with Alembic?\nWorkflow: (1) Change SQLAlchemy models. (2) `alembic revision --autogenerate -m 'description'`. (3) Review generated migration script. (4) `alembic upgrade head` to apply. Always review autogenerated migrations — they miss some changes (indexes, constraints). Use `alembic downgrade -1` to rollback. Store migrations in version control.\nCode signature: Use alembic revision --autogenerate to create migrations from model changes.", "file_path": "https://stackoverflow.com/questions/9436429", "function_name": "Use alembic revision --autogenerate to create migrations from model changes.", "type": "stack_overflow", "metadata": {"record_id": "45819d80-554a-456e-857a-8b86b157f743", "source_type": "stack_overflow", "domain": "SQLAlchemy/Databases", "library": "connection-pool", "title": "How to handle database migrations with Alembic?", "content": "Workflow: (1) Change SQLAlchemy models. (2) `alembic revision --autogenerate -m 'description'`. (3) Review generated migration script. (4) `alembic upgrade head` to apply. Always review autogenerated migrations — they miss some changes (indexes, constraints). Use `alembic downgrade -1` to rollback. Store migrations in version control.", "code_signature": "Use alembic revision --autogenerate to create migrations from model changes.", "tags": "query, transaction, relationship", "url": "https://stackoverflow.com/questions/9436429", "author": "user_974047", "date_published": "26-06-2019", "votes_or_stars": 2413, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:18818645-b503-4f99-9960-108d10ace2e1": {"content": "How to use Path.glob in pathlib\n`pathlib.Path.glob` yields all paths matching the given glob pattern. Example usage:\n```python\npy_files = list(Path('src').glob('**/*.py'))\n```\nThis is useful when finding files recursively, batch file processing. Performance tip: iterate the generator lazily rather than converting to list for large directories.\nCode signature: Path.glob", "file_path": "https://docs.pathlib.org/en/stable/Path/glob.html", "function_name": "Path.glob", "type": "documentation", "metadata": {"record_id": "18818645-b503-4f99-9960-108d10ace2e1", "source_type": "documentation", "domain": "OS/Subprocess/File I/O", "library": "pathlib", "title": "How to use Path.glob in pathlib", "content": "`pathlib.Path.glob` yields all paths matching the given glob pattern. Example usage:\n```python\npy_files = list(Path('src').glob('**/*.py'))\n```\nThis is useful when finding files recursively, batch file processing. Performance tip: iterate the generator lazily rather than converting to list for large directories.", "code_signature": "Path.glob", "tags": "pathlib, pipe, stderr, file-io, stdout, os", "url": "https://docs.pathlib.org/en/stable/Path/glob.html", "author": "Official Docs", "date_published": "05-08-2024", "votes_or_stars": 3216, "relevance_label": "low", "difficulty_level": "intermediate"}}, "kb:a5a298d3-8f06-492c-a5a7-fe079b73e42a": {"content": "asyncio RuntimeError: This event loop is already running\nThis error is common in Jupyter and FastAPI. Solutions: (1) In async context: just use `await coroutine()`. (2) In Jupyter: `import nest_asyncio; nest_asyncio.apply()`. (3) Run in thread: `asyncio.get_event_loop().run_in_executor(None, sync_func)`. (4) Use `asyncio.ensure_future()` to schedule from within async code.\nCode signature: You're calling asyncio.run() inside an already running event loop. Use await instead.", "file_path": "https://stackoverflow.com/questions/2229110", "function_name": "You're calling asyncio.run() inside an already running event loop. Use await instead.", "type": "stack_overflow", "metadata": {"record_id": "a5a298d3-8f06-492c-a5a7-fe079b73e42a", "source_type": "stack_overflow", "domain": "Asyncio/Threading", "library": "async", "title": "asyncio RuntimeError: This event loop is already running", "content": "This error is common in Jupyter and FastAPI. Solutions: (1) In async context: just use `await coroutine()`. (2) In Jupyter: `import nest_asyncio; nest_asyncio.apply()`. (3) Run in thread: `asyncio.get_event_loop().run_in_executor(None, sync_func)`. (4) Use `asyncio.ensure_future()` to schedule from within async code.", "code_signature": "You're calling asyncio.run() inside an already running event loop. Use await instead.", "tags": "async, task, await, semaphore", "url": "https://stackoverflow.com/questions/2229110", "author": "user_573750", "date_published": "25-11-2021", "votes_or_stars": 2306, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:1b8d9360-2a34-4c73-a849-42a144b47105": {"content": "Flask: how to return JSON responses properly?\n`return jsonify({'key': 'value'})` sets Content-Type to application/json. Flask 1.0+ automatically converts returned dicts to JSON responses. For custom status codes: `return jsonify(data), 201`. For errors: use `abort(404)` or return `({'error': 'Not found'}, 404)`.\nCode signature: Use jsonify() or return a dict directly (Flask 1.0+).", "file_path": "https://stackoverflow.com/questions/8930103", "function_name": "Use jsonify() or return a dict directly (Flask 1.0+).", "type": "stack_overflow", "metadata": {"record_id": "1b8d9360-2a34-4c73-a849-42a144b47105", "source_type": "stack_overflow", "domain": "FastAPI/Flask/Django", "library": "django", "title": "Flask: how to return JSON responses properly?", "content": "`return jsonify({'key': 'value'})` sets Content-Type to application/json. Flask 1.0+ automatically converts returned dicts to JSON responses. For custom status codes: `return jsonify(data), 201`. For errors: use `abort(404)` or return `({'error': 'Not found'}, 404)`.", "code_signature": "Use jsonify() or return a dict directly (Flask 1.0+).", "tags": "flask, template, request", "url": "https://stackoverflow.com/questions/8930103", "author": "user_264801", "date_published": "17-04-2021", "votes_or_stars": 2201, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:edc6116c-1a7e-473b-a0df-f201696d874e": {"content": "requests — HTTP Session Management\nThe `requests.Session` function in requests creates a persistent HTTP session with connection pooling. It accepts `` as a parameter and returns Session object. Common use cases include making multiple requests to the same host efficiently. Note that always close or use Session as context manager to release connections.\nCode signature: requests.Session() -> Session object", "file_path": "https://docs.requests.org/en/stable/requests/Session.html", "function_name": "requests.Session() -> Session object", "type": "documentation", "metadata": {"record_id": "edc6116c-1a7e-473b-a0df-f201696d874e", "source_type": "documentation", "domain": "Requests/HTTP/APIs", "library": "requests", "title": "requests — HTTP Session Management", "content": "The `requests.Session` function in requests creates a persistent HTTP session with connection pooling. It accepts `` as a parameter and returns Session object. Common use cases include making multiple requests to the same host efficiently. Note that always close or use Session as context manager to release connections.", "code_signature": "requests.Session() -> Session object", "tags": "oauth, rate-limiting, timeout, websocket, requests, httpx", "url": "https://docs.requests.org/en/stable/requests/Session.html", "author": "Official Docs", "date_published": "23-07-2022", "votes_or_stars": 77, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:49ab22a3-4388-4d2a-957d-4872e823acbc": {"content": "BUG: pd.read_csv() silently truncates large integers\nProblem: Integer columns with values > 2^53 are silently corrupted when read from CSV.\n\nFix/Discussion: Floating point cannot represent integers > 2^53 exactly. Fix: `pd.read_csv(f, dtype={'col': str})` then convert: `df['col'] = df['col'].astype('Int64')`. Use `dtype_backend='numpy_nullable'` in Pandas 2.0+ for better integer handling.\nCode signature: status:closed", "file_path": "https://github.com/pandas/pandas/issues/4449", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "49ab22a3-4388-4d2a-957d-4872e823acbc", "source_type": "github_issue", "domain": "NumPy/Pandas", "library": "pandas", "title": "BUG: pd.read_csv() silently truncates large integers", "content": "Problem: Integer columns with values > 2^53 are silently corrupted when read from CSV.\n\nFix/Discussion: Floating point cannot represent integers > 2^53 exactly. Fix: `pd.read_csv(f, dtype={'col': str})` then convert: `df['col'] = df['col'].astype('Int64')`. Use `dtype_backend='numpy_nullable'` in Pandas 2.0+ for better integer handling.", "code_signature": "status:closed", "tags": "groupby, vectorization, reshape, dtype", "url": "https://github.com/pandas/pandas/issues/4449", "author": "contributor_726", "date_published": "22-01-2018", "votes_or_stars": 496, "relevance_label": "low", "difficulty_level": "intermediate"}}, "kb:cd921ae4-d41a-4548-a81b-9996ed33673e": {"content": "BUG: Flask session not persisting between requests in tests\nProblem: Session data set in one request is not available in the next request during testing.\n\nFix/Discussion: Use Flask test client's session_transaction() context manager: `with client.session_transaction() as sess:\n    sess['user_id'] = 1`. Ensure SECRET_KEY is set. For testing: `app.config['TESTING'] = True; app.config['SECRET_KEY'] = 'test'`.\nCode signature: status:closed", "file_path": "https://github.com/flask/flask/issues/2141", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "cd921ae4-d41a-4548-a81b-9996ed33673e", "source_type": "github_issue", "domain": "FastAPI/Flask/Django", "library": "flask", "title": "BUG: Flask session not persisting between requests in tests", "content": "Problem: Session data set in one request is not available in the next request during testing.\n\nFix/Discussion: Use Flask test client's session_transaction() context manager: `with client.session_transaction() as sess:\n    sess['user_id'] = 1`. Ensure SECRET_KEY is set. For testing: `app.config['TESTING'] = True; app.config['SECRET_KEY'] = 'test'`.", "code_signature": "status:closed", "tags": "response, django, rest", "url": "https://github.com/flask/flask/issues/2141", "author": "contributor_5020", "date_published": "27-12-2024", "votes_or_stars": 172, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:93f9b3fb-ff7b-4bd1-9de9-23d2b4f34f47": {"content": "requests — HTTP Session Management\nThe `requests.Session` function in requests creates a persistent HTTP session with connection pooling. It accepts `` as a parameter and returns Session object. Common use cases include making multiple requests to the same host efficiently. Note that always close or use Session as context manager to release connections.\nCode signature: requests.Session() -> Session object", "file_path": "https://docs.requests.org/en/stable/requests/Session.html", "function_name": "requests.Session() -> Session object", "type": "documentation", "metadata": {"record_id": "93f9b3fb-ff7b-4bd1-9de9-23d2b4f34f47", "source_type": "documentation", "domain": "Requests/HTTP/APIs", "library": "requests", "title": "requests — HTTP Session Management", "content": "The `requests.Session` function in requests creates a persistent HTTP session with connection pooling. It accepts `` as a parameter and returns Session object. Common use cases include making multiple requests to the same host efficiently. Note that always close or use Session as context manager to release connections.", "code_signature": "requests.Session() -> Session object", "tags": "requests, retry, rest-api, http, aiohttp, authentication", "url": "https://docs.requests.org/en/stable/requests/Session.html", "author": "Official Docs", "date_published": "16-08-2023", "votes_or_stars": 63, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:ceea941e-3b2b-4bea-ae5a-a94dd70d8faf": {"content": "NumPy — Linear Algebra\nThe `np.einsum` function in NumPy evaluates Einstein summation convention on operands. It accepts `subscripts, *operands` as a parameter and returns ndarray. Common use cases include matrix multiplication, dot products, tensor contractions. Note that prefer np.einsum over loops for large arrays.\nCode signature: np.einsum(subscripts, *operands) -> ndarray", "file_path": "https://docs.numpy.org/en/stable/np/einsum.html", "function_name": "np.einsum(subscripts, *operands) -> ndarray", "type": "documentation", "metadata": {"record_id": "ceea941e-3b2b-4bea-ae5a-a94dd70d8faf", "source_type": "documentation", "domain": "NumPy/Pandas", "library": "NumPy", "title": "NumPy — Linear Algebra", "content": "The `np.einsum` function in NumPy evaluates Einstein summation convention on operands. It accepts `subscripts, *operands` as a parameter and returns ndarray. Common use cases include matrix multiplication, dot products, tensor contractions. Note that prefer np.einsum over loops for large arrays.", "code_signature": "np.einsum(subscripts, *operands) -> ndarray", "tags": "broadcasting, pivot, iloc, numpy, loc, reshape", "url": "https://docs.numpy.org/en/stable/np/einsum.html", "author": "Official Docs", "date_published": "25-12-2024", "votes_or_stars": 1896, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:fe3f03ee-ef63-4f61-8299-98b579721f2c": {"content": "How to run asyncio code from synchronous Python?\n`asyncio.run(main())` creates an event loop, runs the coroutine, closes the loop. Never call asyncio.run() from inside a running event loop (use await instead). In Jupyter notebooks (which have a running loop), use `await coroutine()` directly or `nest_asyncio.apply()`. For mixing sync/async, use `loop.run_until_complete()`.\nCode signature: Use asyncio.run() in Python 3.7+ to execute a coroutine from sync code.", "file_path": "https://stackoverflow.com/questions/2140194", "function_name": "Use asyncio.run() in Python 3.7+ to execute a coroutine from sync code.", "type": "stack_overflow", "metadata": {"record_id": "fe3f03ee-ef63-4f61-8299-98b579721f2c", "source_type": "stack_overflow", "domain": "Asyncio/Threading", "library": "gather", "title": "How to run asyncio code from synchronous Python?", "content": "`asyncio.run(main())` creates an event loop, runs the coroutine, closes the loop. Never call asyncio.run() from inside a running event loop (use await instead). In Jupyter notebooks (which have a running loop), use `await coroutine()` directly or `nest_asyncio.apply()`. For mixing sync/async, use `loop.run_until_complete()`.", "code_signature": "Use asyncio.run() in Python 3.7+ to execute a coroutine from sync code.", "tags": "race-condition, lock, await, executor, task, gather", "url": "https://stackoverflow.com/questions/2140194", "author": "user_718011", "date_published": "01-10-2022", "votes_or_stars": 288, "relevance_label": "low", "difficulty_level": "advanced"}}, "kb:892d79a0-ae6c-43e6-823d-49e009cc52e8": {"content": "PERF: concurrent.futures.ProcessPoolExecutor slow startup on Windows\nProblem: ProcessPoolExecutor takes 3-5 seconds to start on Windows due to process spawning overhead.\n\nFix/Discussion: Windows uses 'spawn' start method (vs 'fork' on Linux). Mitigations: (1) Reuse executor across calls — don't create/destroy in loops. (2) Use 'forkserver' or 'fork' on Linux/macOS. (3) For short tasks, ThreadPoolExecutor may be faster. (4) Use `initializer` param to pre-load modules.\nCode signature: status:closed", "file_path": "https://github.com/cpython/cpython/issues/6647", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "892d79a0-ae6c-43e6-823d-49e009cc52e8", "source_type": "github_issue", "domain": "Asyncio/Threading", "library": "cpython", "title": "PERF: concurrent.futures.ProcessPoolExecutor slow startup on Windows", "content": "Problem: ProcessPoolExecutor takes 3-5 seconds to start on Windows due to process spawning overhead.\n\nFix/Discussion: Windows uses 'spawn' start method (vs 'fork' on Linux). Mitigations: (1) Reuse executor across calls — don't create/destroy in loops. (2) Use 'forkserver' or 'fork' on Linux/macOS. (3) For short tasks, ThreadPoolExecutor may be faster. (4) Use `initializer` param to pre-load modules.", "code_signature": "status:closed", "tags": "asyncio, lock, deadlock, gather, task", "url": "https://github.com/cpython/cpython/issues/6647", "author": "contributor_4097", "date_published": "19-12-2020", "votes_or_stars": 274, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:2085409b-ca37-4801-8edb-5ea7462bcc98": {"content": "How to implement database connection pooling with SQLAlchemy?\n`create_engine(url, pool_size=10, max_overflow=20, pool_timeout=30, pool_recycle=1800)`. Use `pool_pre_ping=True` to detect stale connections. For async: use `AsyncEngine` with `create_async_engine()`. Monitor with `engine.pool.status()`. NullPool disables pooling for scripts/serverless environments.\nCode signature: create_engine() has built-in pooling. Configure pool_size and max_overflow.", "file_path": "https://stackoverflow.com/questions/7358113", "function_name": "create_engine() has built-in pooling. Configure pool_size and max_overflow.", "type": "stack_overflow", "metadata": {"record_id": "2085409b-ca37-4801-8edb-5ea7462bcc98", "source_type": "stack_overflow", "domain": "SQLAlchemy/Databases", "library": "session", "title": "How to implement database connection pooling with SQLAlchemy?", "content": "`create_engine(url, pool_size=10, max_overflow=20, pool_timeout=30, pool_recycle=1800)`. Use `pool_pre_ping=True` to detect stale connections. For async: use `AsyncEngine` with `create_async_engine()`. Monitor with `engine.pool.status()`. NullPool disables pooling for scripts/serverless environments.", "code_signature": "create_engine() has built-in pooling. Configure pool_size and max_overflow.", "tags": "foreign-key, sqlite, migration, sqlalchemy, alembic", "url": "https://stackoverflow.com/questions/7358113", "author": "user_12260", "date_published": "25-08-2020", "votes_or_stars": 274, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:1ed5e31b-091b-4a44-81f3-1db7f95fe42c": {"content": "PERF: groupby().apply() extremely slow on large DataFrames\nProblem: groupby().apply() with a custom function is 100x slower than expected on 10M row DataFrames.\n\nFix/Discussion: groupby().apply() serializes each group and has high overhead. Fix: vectorize with numpy, use groupby().transform() for same-shape output, or groupby().agg() with named aggregations. For complex logic, consider Polars or Dask. Numba's @jit can accelerate custom group functions.\nCode signature: status:closed", "file_path": "https://github.com/pandas/pandas/issues/8446", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "1ed5e31b-091b-4a44-81f3-1db7f95fe42c", "source_type": "github_issue", "domain": "NumPy/Pandas", "library": "pandas", "title": "PERF: groupby().apply() extremely slow on large DataFrames", "content": "Problem: groupby().apply() with a custom function is 100x slower than expected on 10M row DataFrames.\n\nFix/Discussion: groupby().apply() serializes each group and has high overhead. Fix: vectorize with numpy, use groupby().transform() for same-shape output, or groupby().agg() with named aggregations. For complex logic, consider Polars or Dask. Numba's @jit can accelerate custom group functions.", "code_signature": "status:closed", "tags": "iloc, pandas, reshape, array", "url": "https://github.com/pandas/pandas/issues/8446", "author": "contributor_6648", "date_published": "16-10-2018", "votes_or_stars": 356, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:fa3de3e4-fd89-4ef0-8428-1b9b138f323c": {"content": "FEAT: Django: add support for async ORM queries\nProblem: Django ORM is synchronous; using it in async views requires sync_to_async wrapper.\n\nFix/Discussion: Django 4.1+ added native async ORM support. Use `await Model.objects.filter().afirst()`, `async for obj in qs:`. For older versions: wrap with `sync_to_async`: `get_user = sync_to_async(User.objects.get)\nuser = await get_user(pk=1)`.\nCode signature: status:closed", "file_path": "https://github.com/django/django/issues/8618", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "fa3de3e4-fd89-4ef0-8428-1b9b138f323c", "source_type": "github_issue", "domain": "FastAPI/Flask/Django", "library": "django", "title": "FEAT: Django: add support for async ORM queries", "content": "Problem: Django ORM is synchronous; using it in async views requires sync_to_async wrapper.\n\nFix/Discussion: Django 4.1+ added native async ORM support. Use `await Model.objects.filter().afirst()`, `async for obj in qs:`. For older versions: wrap with `sync_to_async`: `get_user = sync_to_async(User.objects.get)\nuser = await get_user(pk=1)`.", "code_signature": "status:closed", "tags": "template, blueprint, orm, fastapi, rest, flask", "url": "https://github.com/django/django/issues/8618", "author": "contributor_8921", "date_published": "23-09-2020", "votes_or_stars": 23, "relevance_label": "low", "difficulty_level": "intermediate"}}, "kb:4c857a6d-05fe-4f86-a164-3ea60f8c79bd": {"content": "How to watch a directory for file changes in Python?\n```python\nfrom watchdog.observers import Observer\nfrom watchdog.events import FileSystemEventHandler\nhandler = FileSystemEventHandler()\nobserver = Observer()\nobserver.schedule(handler, path='.', recursive=True)\nobserver.start()\n```\nFor simple polling: `os.stat(file).st_mtime`. Linux-native: `inotify`. macOS-native: `FSEvents`.\nCode signature: Use the watchdog library for cross-platform directory monitoring.", "file_path": "https://stackoverflow.com/questions/5213115", "function_name": "Use the watchdog library for cross-platform directory monitoring.", "type": "stack_overflow", "metadata": {"record_id": "4c857a6d-05fe-4f86-a164-3ea60f8c79bd", "source_type": "stack_overflow", "domain": "OS/Subprocess/File I/O", "library": "env-variable", "title": "How to watch a directory for file changes in Python?", "content": "```python\nfrom watchdog.observers import Observer\nfrom watchdog.events import FileSystemEventHandler\nhandler = FileSystemEventHandler()\nobserver = Observer()\nobserver.schedule(handler, path='.', recursive=True)\nobserver.start()\n```\nFor simple polling: `os.stat(file).st_mtime`. Linux-native: `inotify`. macOS-native: `FSEvents`.", "code_signature": "Use the watchdog library for cross-platform directory monitoring.", "tags": "stdout, shutil, glob, stderr, process, env-variable", "url": "https://stackoverflow.com/questions/5213115", "author": "user_959314", "date_published": "20-05-2024", "votes_or_stars": 2108, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:6ac4cd0b-7a27-454f-94da-d76d99c05e64": {"content": "subprocess — Process Execution\nThe `subprocess.run` function in subprocess runs a subprocess and waits for it to complete. It accepts `args, capture_output=False, timeout=None` as a parameter and returns CompletedProcess. Common use cases include executing shell commands, scripts, or external programs from Python. Note that use shell=False (default) to avoid shell injection vulnerabilities.\nCode signature: subprocess.run(args, capture_output=False, timeout=None) -> CompletedProcess", "file_path": "https://docs.subprocess.org/en/stable/subprocess/run.html", "function_name": "subprocess.run(args, capture_output=False, timeout=None) -> CompletedProcess", "type": "documentation", "metadata": {"record_id": "6ac4cd0b-7a27-454f-94da-d76d99c05e64", "source_type": "documentation", "domain": "OS/Subprocess/File I/O", "library": "subprocess", "title": "subprocess — Process Execution", "content": "The `subprocess.run` function in subprocess runs a subprocess and waits for it to complete. It accepts `args, capture_output=False, timeout=None` as a parameter and returns CompletedProcess. Common use cases include executing shell commands, scripts, or external programs from Python. Note that use shell=False (default) to avoid shell injection vulnerabilities.", "code_signature": "subprocess.run(args, capture_output=False, timeout=None) -> CompletedProcess", "tags": "glob, subprocess, stdout, os, stderr, pathlib", "url": "https://docs.subprocess.org/en/stable/subprocess/run.html", "author": "Official Docs", "date_published": "19-01-2022", "votes_or_stars": 434, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:139417e1-9870-4cc9-9f2c-7f811c20ec44": {"content": "asyncio — Concurrent Coroutines\nThe `asyncio.gather` function in asyncio runs multiple coroutines concurrently and collects results. It accepts `*coros, return_exceptions=False` as a parameter and returns list of results. Common use cases include parallel I/O operations such as multiple API calls. Note that set return_exceptions=True to prevent one failure from cancelling all tasks.\nCode signature: asyncio.gather(*coros, return_exceptions=False) -> list of results", "file_path": "https://docs.asyncio.org/en/stable/asyncio/gather.html", "function_name": "asyncio.gather(*coros, return_exceptions=False) -> list of results", "type": "documentation", "metadata": {"record_id": "139417e1-9870-4cc9-9f2c-7f811c20ec44", "source_type": "documentation", "domain": "Asyncio/Threading", "library": "asyncio", "title": "asyncio — Concurrent Coroutines", "content": "The `asyncio.gather` function in asyncio runs multiple coroutines concurrently and collects results. It accepts `*coros, return_exceptions=False` as a parameter and returns list of results. Common use cases include parallel I/O operations such as multiple API calls. Note that set return_exceptions=True to prevent one failure from cancelling all tasks.", "code_signature": "asyncio.gather(*coros, return_exceptions=False) -> list of results", "tags": "deadlock, coroutine, gather, asyncio, threading", "url": "https://docs.asyncio.org/en/stable/asyncio/gather.html", "author": "Official Docs", "date_published": "07-12-2022", "votes_or_stars": 482, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:9ef1296b-c35e-4751-b535-44b5352637c4": {"content": "BUG: asyncio task leaks when exception is not handled\nProblem: Unhandled exceptions in fire-and-forget asyncio tasks are silently ignored, causing task leaks.\n\nFix/Discussion: Always await tasks or add exception handlers. Use `task.add_done_callback()` to log exceptions. In Python 3.11+, use TaskGroup for structured concurrency — exceptions propagate automatically. Set `asyncio.get_event_loop().set_exception_handler()` for global unhandled task exception logging.\nCode signature: status:closed", "file_path": "https://github.com/cpython/cpython/issues/5591", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "9ef1296b-c35e-4751-b535-44b5352637c4", "source_type": "github_issue", "domain": "Asyncio/Threading", "library": "cpython", "title": "BUG: asyncio task leaks when exception is not handled", "content": "Problem: Unhandled exceptions in fire-and-forget asyncio tasks are silently ignored, causing task leaks.\n\nFix/Discussion: Always await tasks or add exception handlers. Use `task.add_done_callback()` to log exceptions. In Python 3.11+, use TaskGroup for structured concurrency — exceptions propagate automatically. Set `asyncio.get_event_loop().set_exception_handler()` for global unhandled task exception logging.", "code_signature": "status:closed", "tags": "asyncio, event-loop, coroutine, semaphore", "url": "https://github.com/cpython/cpython/issues/5591", "author": "contributor_1630", "date_published": "14-05-2024", "votes_or_stars": 317, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:d505e4ce-af5f-4b63-b30a-048688f85a57": {"content": "How to implement a thread pool in Python?\n```python\nfrom concurrent.futures import ThreadPoolExecutor\nwith ThreadPoolExecutor(max_workers=8) as ex:\n    futures = [ex.submit(task, arg) for arg in args]\n    results = [f.result() for f in futures]\n```\nFor CPU-bound: use ProcessPoolExecutor instead. For async integration: `await loop.run_in_executor(executor, func, *args)`.\nCode signature: Use concurrent.futures.ThreadPoolExecutor for I/O-bound tasks.", "file_path": "https://stackoverflow.com/questions/3646552", "function_name": "Use concurrent.futures.ThreadPoolExecutor for I/O-bound tasks.", "type": "stack_overflow", "metadata": {"record_id": "d505e4ce-af5f-4b63-b30a-048688f85a57", "source_type": "stack_overflow", "domain": "Asyncio/Threading", "library": "coroutine", "title": "How to implement a thread pool in Python?", "content": "```python\nfrom concurrent.futures import ThreadPoolExecutor\nwith ThreadPoolExecutor(max_workers=8) as ex:\n    futures = [ex.submit(task, arg) for arg in args]\n    results = [f.result() for f in futures]\n```\nFor CPU-bound: use ProcessPoolExecutor instead. For async integration: `await loop.run_in_executor(executor, func, *args)`.", "code_signature": "Use concurrent.futures.ThreadPoolExecutor for I/O-bound tasks.", "tags": "deadlock, asyncio, race-condition, async, threading, coroutine", "url": "https://stackoverflow.com/questions/3646552", "author": "user_469469", "date_published": "22-12-2019", "votes_or_stars": 1534, "relevance_label": "low", "difficulty_level": "beginner"}}, "kb:103149b2-06b3-4038-92a0-79cf3092832a": {"content": "How to read a large file line by line in Python without loading it all into memory?\n```python\nwith open('large.txt', 'r') as f:\n    for line in f:\n        process(line.strip())\n```\nThis reads one line at a time. For binary files: use `f.read(chunk_size)` in a loop. For CSV: use `pd.read_csv(file, chunksize=10000)`. Never use `f.readlines()` on large files.\nCode signature: Iterate over the file object directly — it yields lines lazily.", "file_path": "https://stackoverflow.com/questions/2737893", "function_name": "Iterate over the file object directly — it yields lines lazily.", "type": "stack_overflow", "metadata": {"record_id": "103149b2-06b3-4038-92a0-79cf3092832a", "source_type": "stack_overflow", "domain": "OS/Subprocess/File I/O", "library": "shutil", "title": "How to read a large file line by line in Python without loading it all into memory?", "content": "```python\nwith open('large.txt', 'r') as f:\n    for line in f:\n        process(line.strip())\n```\nThis reads one line at a time. For binary files: use `f.read(chunk_size)` in a loop. For CSV: use `pd.read_csv(file, chunksize=10000)`. Never use `f.readlines()` on large files.", "code_signature": "Iterate over the file object directly — it yields lines lazily.", "tags": "process, pathlib, env-variable, shutil", "url": "https://stackoverflow.com/questions/2737893", "author": "user_994539", "date_published": "21-04-2022", "votes_or_stars": 2278, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:3a9a8268-9e23-46ff-ab6d-6aef83f30e37": {"content": "How to use Path.glob in pathlib\n`pathlib.Path.glob` yields all paths matching the given glob pattern. Example usage:\n```python\npy_files = list(Path('src').glob('**/*.py'))\n```\nThis is useful when finding files recursively, batch file processing. Performance tip: iterate the generator lazily rather than converting to list for large directories.\nCode signature: Path.glob", "file_path": "https://docs.pathlib.org/en/stable/Path/glob.html", "function_name": "Path.glob", "type": "documentation", "metadata": {"record_id": "3a9a8268-9e23-46ff-ab6d-6aef83f30e37", "source_type": "documentation", "domain": "OS/Subprocess/File I/O", "library": "pathlib", "title": "How to use Path.glob in pathlib", "content": "`pathlib.Path.glob` yields all paths matching the given glob pattern. Example usage:\n```python\npy_files = list(Path('src').glob('**/*.py'))\n```\nThis is useful when finding files recursively, batch file processing. Performance tip: iterate the generator lazily rather than converting to list for large directories.", "code_signature": "Path.glob", "tags": "file-io, glob, pipe, pathlib", "url": "https://docs.pathlib.org/en/stable/Path/glob.html", "author": "Official Docs", "date_published": "14-01-2023", "votes_or_stars": 3210, "relevance_label": "low", "difficulty_level": "intermediate"}}, "kb:ced6d73b-1f4a-428f-bc55-bc8cdc5884df": {"content": "NumPy — Vectorization\nThe `np.vectorize` function in NumPy generalizes a scalar function to operate on arrays. It accepts `pyfunc` as a parameter and returns vectorized function. Common use cases include applying Python functions element-wise without explicit loops. Note that np.vectorize is a convenience wrapper; it does not improve performance like true ufuncs.\nCode signature: np.vectorize(pyfunc) -> vectorized function", "file_path": "https://docs.numpy.org/en/stable/np/vectorize.html", "function_name": "np.vectorize(pyfunc) -> vectorized function", "type": "documentation", "metadata": {"record_id": "ced6d73b-1f4a-428f-bc55-bc8cdc5884df", "source_type": "documentation", "domain": "NumPy/Pandas", "library": "NumPy", "title": "NumPy — Vectorization", "content": "The `np.vectorize` function in NumPy generalizes a scalar function to operate on arrays. It accepts `pyfunc` as a parameter and returns vectorized function. Common use cases include applying Python functions element-wise without explicit loops. Note that np.vectorize is a convenience wrapper; it does not improve performance like true ufuncs.", "code_signature": "np.vectorize(pyfunc) -> vectorized function", "tags": "broadcasting, pandas, array", "url": "https://docs.numpy.org/en/stable/np/vectorize.html", "author": "Official Docs", "date_published": "04-01-2021", "votes_or_stars": 1317, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:6e096eb0-8eaf-4738-948d-749bb5860a7b": {"content": "NumPy broadcasting error: operands could not be broadcast with shapes\nNumPy broadcasts shapes right-to-left. Arrays (3,4) and (4,) are compatible. Arrays (3,4) and (3,) are not — reshape to (3,1). Use `arr.reshape(-1,1)` to add a dimension. Always print `a.shape, b.shape` before operations to debug. `np.expand_dims(arr, axis=1)` is cleaner than reshape for adding dimensions.\nCode signature: Shapes must be compatible: dimensions must match or one of them must be 1.", "file_path": "https://stackoverflow.com/questions/4860684", "function_name": "Shapes must be compatible: dimensions must match or one of them must be 1.", "type": "stack_overflow", "metadata": {"record_id": "6e096eb0-8eaf-4738-948d-749bb5860a7b", "source_type": "stack_overflow", "domain": "NumPy/Pandas", "library": "numpy", "title": "NumPy broadcasting error: operands could not be broadcast with shapes", "content": "NumPy broadcasts shapes right-to-left. Arrays (3,4) and (4,) are compatible. Arrays (3,4) and (3,) are not — reshape to (3,1). Use `arr.reshape(-1,1)` to add a dimension. Always print `a.shape, b.shape` before operations to debug. `np.expand_dims(arr, axis=1)` is cleaner than reshape for adding dimensions.", "code_signature": "Shapes must be compatible: dimensions must match or one of them must be 1.", "tags": "numpy, merge, dtype", "url": "https://stackoverflow.com/questions/4860684", "author": "user_627024", "date_published": "28-09-2018", "votes_or_stars": 1423, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:17366ac4-e609-4112-a483-ea449e841090": {"content": "NumPy broadcasting error: operands could not be broadcast with shapes\nNumPy broadcasts shapes right-to-left. Arrays (3,4) and (4,) are compatible. Arrays (3,4) and (3,) are not — reshape to (3,1). Use `arr.reshape(-1,1)` to add a dimension. Always print `a.shape, b.shape` before operations to debug. `np.expand_dims(arr, axis=1)` is cleaner than reshape for adding dimensions.\nCode signature: Shapes must be compatible: dimensions must match or one of them must be 1.", "file_path": "https://stackoverflow.com/questions/4860684", "function_name": "Shapes must be compatible: dimensions must match or one of them must be 1.", "type": "stack_overflow", "metadata": {"record_id": "17366ac4-e609-4112-a483-ea449e841090", "source_type": "stack_overflow", "domain": "NumPy/Pandas", "library": "numpy", "title": "NumPy broadcasting error: operands could not be broadcast with shapes", "content": "NumPy broadcasts shapes right-to-left. Arrays (3,4) and (4,) are compatible. Arrays (3,4) and (3,) are not — reshape to (3,1). Use `arr.reshape(-1,1)` to add a dimension. Always print `a.shape, b.shape` before operations to debug. `np.expand_dims(arr, axis=1)` is cleaner than reshape for adding dimensions.", "code_signature": "Shapes must be compatible: dimensions must match or one of them must be 1.", "tags": "pivot, groupby, array, dataframe, merge", "url": "https://stackoverflow.com/questions/4860684", "author": "user_627024", "date_published": "12-11-2022", "votes_or_stars": 1420, "relevance_label": "low", "difficulty_level": "advanced"}}, "kb:fe53c406-d099-417d-afd9-8c3059efdee7": {"content": "BUG: os.path.join ignores prefix when component starts with /\nProblem: os.path.join('base', '/absolute') returns '/absolute', ignoring 'base'.\n\nFix/Discussion: This is documented behavior: os.path.join discards previous components when it encounters an absolute path. Fix: use `pathlib.Path(base) / component.lstrip('/')`. Validate user-provided paths with `Path(path).resolve().is_relative_to(base_dir)` to prevent path traversal.\nCode signature: status:closed", "file_path": "https://github.com/cpython/cpython/issues/7712", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "fe53c406-d099-417d-afd9-8c3059efdee7", "source_type": "github_issue", "domain": "OS/Subprocess/File I/O", "library": "cpython", "title": "BUG: os.path.join ignores prefix when component starts with /", "content": "Problem: os.path.join('base', '/absolute') returns '/absolute', ignoring 'base'.\n\nFix/Discussion: This is documented behavior: os.path.join discards previous components when it encounters an absolute path. Fix: use `pathlib.Path(base) / component.lstrip('/')`. Validate user-provided paths with `Path(path).resolve().is_relative_to(base_dir)` to prevent path traversal.", "code_signature": "status:closed", "tags": "shutil, stderr, env-variable, subprocess, file-io", "url": "https://github.com/cpython/cpython/issues/7712", "author": "contributor_8802", "date_published": "05-01-2021", "votes_or_stars": 442, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:bfd8e501-ac8f-46a0-8de3-f9df49e85ff5": {"content": "threading Thread Synchronization — official reference\nOfficial documentation for threading Thread Synchronization. The method signature is `threading.Lock()`. creates a mutual-exclusion lock for thread-safe access. Raises `RuntimeError` when lock is released without being acquired. See also: threading.RLock, threading.Semaphore, threading.Event.\nCode signature: threading.Lock()", "file_path": "https://docs.threading.org/en/stable/threading/Lock.html", "function_name": "threading.Lock()", "type": "documentation", "metadata": {"record_id": "bfd8e501-ac8f-46a0-8de3-f9df49e85ff5", "source_type": "documentation", "domain": "Asyncio/Threading", "library": "threading", "title": "threading Thread Synchronization — official reference", "content": "Official documentation for threading Thread Synchronization. The method signature is `threading.Lock()`. creates a mutual-exclusion lock for thread-safe access. Raises `RuntimeError` when lock is released without being acquired. See also: threading.RLock, threading.Semaphore, threading.Event.", "code_signature": "threading.Lock()", "tags": "await, coroutine, deadlock, event-loop, task, race-condition", "url": "https://docs.threading.org/en/stable/threading/Lock.html", "author": "Official Docs", "date_published": "09-03-2024", "votes_or_stars": 4838, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:bb2d3bde-2e75-4cf2-9910-7b8eab230165": {"content": "requests — HTTP Session Management\nThe `requests.Session` function in requests creates a persistent HTTP session with connection pooling. It accepts `` as a parameter and returns Session object. Common use cases include making multiple requests to the same host efficiently. Note that always close or use Session as context manager to release connections.\nCode signature: requests.Session() -> Session object", "file_path": "https://docs.requests.org/en/stable/requests/Session.html", "function_name": "requests.Session() -> Session object", "type": "documentation", "metadata": {"record_id": "bb2d3bde-2e75-4cf2-9910-7b8eab230165", "source_type": "documentation", "domain": "Requests/HTTP/APIs", "library": "requests", "title": "requests — HTTP Session Management", "content": "The `requests.Session` function in requests creates a persistent HTTP session with connection pooling. It accepts `` as a parameter and returns Session object. Common use cases include making multiple requests to the same host efficiently. Note that always close or use Session as context manager to release connections.", "code_signature": "requests.Session() -> Session object", "tags": "authentication, websocket, rest-api, headers", "url": "https://docs.requests.org/en/stable/requests/Session.html", "author": "Official Docs", "date_published": "29-11-2022", "votes_or_stars": 74, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:131a74e0-8237-4008-8fac-1c3a28835198": {"content": "BUG: Alembic autogenerate misses index changes\nProblem: Alembic --autogenerate does not detect changes to indexes or constraints on existing tables.\n\nFix/Discussion: Alembic autogenerate compares metadata to database state but has limitations. Workarounds: (1) Manually add `op.create_index()` to migration. (2) Use `include_schemas=True` in env.py. (3) Run `alembic check` to see detected changes. (4) Always review autogenerated migrations before applying.\nCode signature: status:open", "file_path": "https://github.com/alembic/alembic/issues/7344", "function_name": "status:open", "type": "github_issue", "metadata": {"record_id": "131a74e0-8237-4008-8fac-1c3a28835198", "source_type": "github_issue", "domain": "SQLAlchemy/Databases", "library": "alembic", "title": "BUG: Alembic autogenerate misses index changes", "content": "Problem: Alembic --autogenerate does not detect changes to indexes or constraints on existing tables.\n\nFix/Discussion: Alembic autogenerate compares metadata to database state but has limitations. Workarounds: (1) Manually add `op.create_index()` to migration. (2) Use `include_schemas=True` in env.py. (3) Run `alembic check` to see detected changes. (4) Always review autogenerated migrations before applying.", "code_signature": "status:open", "tags": "transaction, migration, alembic", "url": "https://github.com/alembic/alembic/issues/7344", "author": "contributor_3601", "date_published": "25-04-2023", "votes_or_stars": 222, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:e746f31d-e0f8-40f2-b79f-c350a228019e": {"content": "BUG: SQLAlchemy connection pool exhaustion under load\nProblem: Application hangs under concurrent load — all connections are checked out and pool is exhausted.\n\nFix/Discussion: Increase `pool_size` and `max_overflow`. Ensure sessions are always closed: use `contextmanager` or `with Session() as session:`. Set `pool_timeout` to fail fast. Enable `pool_pre_ping=True`. Monitor with `engine.pool.checkedout()`. In async: use `AsyncSession` with `async_scoped_session`.\nCode signature: status:closed", "file_path": "https://github.com/sqlalchemy/sqlalchemy/issues/106", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "e746f31d-e0f8-40f2-b79f-c350a228019e", "source_type": "github_issue", "domain": "SQLAlchemy/Databases", "library": "sqlalchemy", "title": "BUG: SQLAlchemy connection pool exhaustion under load", "content": "Problem: Application hangs under concurrent load — all connections are checked out and pool is exhausted.\n\nFix/Discussion: Increase `pool_size` and `max_overflow`. Ensure sessions are always closed: use `contextmanager` or `with Session() as session:`. Set `pool_timeout` to fail fast. Enable `pool_pre_ping=True`. Monitor with `engine.pool.checkedout()`. In async: use `AsyncSession` with `async_scoped_session`.", "code_signature": "status:closed", "tags": "sqlalchemy, alembic, foreign-key, migration, query, connection-pool", "url": "https://github.com/sqlalchemy/sqlalchemy/issues/106", "author": "contributor_5078", "date_published": "22-01-2023", "votes_or_stars": 311, "relevance_label": "low", "difficulty_level": "intermediate"}}, "kb:04ec9286-9911-4652-92f5-fc366ae6ec49": {"content": "How to limit concurrency with asyncio?\n```python\nsem = asyncio.Semaphore(10)\nasync def limited(url):\n    async with sem:\n        return await fetch(url)\nresults = await asyncio.gather(*[limited(u) for u in urls])\n```\nThis prevents overwhelming APIs or databases with too many simultaneous connections.\nCode signature: Use asyncio.Semaphore to cap the number of concurrent coroutines.", "file_path": "https://stackoverflow.com/questions/2375453", "function_name": "Use asyncio.Semaphore to cap the number of concurrent coroutines.", "type": "stack_overflow", "metadata": {"record_id": "04ec9286-9911-4652-92f5-fc366ae6ec49", "source_type": "stack_overflow", "domain": "Asyncio/Threading", "library": "asyncio", "title": "How to limit concurrency with asyncio?", "content": "```python\nsem = asyncio.Semaphore(10)\nasync def limited(url):\n    async with sem:\n        return await fetch(url)\nresults = await asyncio.gather(*[limited(u) for u in urls])\n```\nThis prevents overwhelming APIs or databases with too many simultaneous connections.", "code_signature": "Use asyncio.Semaphore to cap the number of concurrent coroutines.", "tags": "gather, event-loop, deadlock", "url": "https://stackoverflow.com/questions/2375453", "author": "user_449589", "date_published": "18-07-2024", "votes_or_stars": 2445, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:e41748e2-b5d2-4b12-8a19-3a3e3cb33ddb": {"content": "PERF: concurrent.futures.ProcessPoolExecutor slow startup on Windows\nProblem: ProcessPoolExecutor takes 3-5 seconds to start on Windows due to process spawning overhead.\n\nFix/Discussion: Windows uses 'spawn' start method (vs 'fork' on Linux). Mitigations: (1) Reuse executor across calls — don't create/destroy in loops. (2) Use 'forkserver' or 'fork' on Linux/macOS. (3) For short tasks, ThreadPoolExecutor may be faster. (4) Use `initializer` param to pre-load modules.\nCode signature: status:closed", "file_path": "https://github.com/cpython/cpython/issues/6647", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "e41748e2-b5d2-4b12-8a19-3a3e3cb33ddb", "source_type": "github_issue", "domain": "Asyncio/Threading", "library": "cpython", "title": "PERF: concurrent.futures.ProcessPoolExecutor slow startup on Windows", "content": "Problem: ProcessPoolExecutor takes 3-5 seconds to start on Windows due to process spawning overhead.\n\nFix/Discussion: Windows uses 'spawn' start method (vs 'fork' on Linux). Mitigations: (1) Reuse executor across calls — don't create/destroy in loops. (2) Use 'forkserver' or 'fork' on Linux/macOS. (3) For short tasks, ThreadPoolExecutor may be faster. (4) Use `initializer` param to pre-load modules.", "code_signature": "status:closed", "tags": "async, race-condition, task, gather, await, coroutine", "url": "https://github.com/cpython/cpython/issues/6647", "author": "contributor_4097", "date_published": "14-06-2020", "votes_or_stars": 254, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:47cedec7-a19d-4c3c-aeef-91ddb65ac55b": {"content": "How to stream large HTTP responses with requests?\n```python\nwith requests.get(url, stream=True) as r:\n    r.raise_for_status()\n    with open('file', 'wb') as f:\n        for chunk in r.iter_content(chunk_size=8192):\n            f.write(chunk)\n```\nAlways use as context manager with stream=True to ensure connection is released.\nCode signature: Use stream=True and iterate over response.iter_content() or iter_lines().", "file_path": "https://stackoverflow.com/questions/3592974", "function_name": "Use stream=True and iterate over response.iter_content() or iter_lines().", "type": "stack_overflow", "metadata": {"record_id": "47cedec7-a19d-4c3c-aeef-91ddb65ac55b", "source_type": "stack_overflow", "domain": "Requests/HTTP/APIs", "library": "json", "title": "How to stream large HTTP responses with requests?", "content": "```python\nwith requests.get(url, stream=True) as r:\n    r.raise_for_status()\n    with open('file', 'wb') as f:\n        for chunk in r.iter_content(chunk_size=8192):\n            f.write(chunk)\n```\nAlways use as context manager with stream=True to ensure connection is released.", "code_signature": "Use stream=True and iterate over response.iter_content() or iter_lines().", "tags": "aiohttp, rest-api, headers, timeout, httpx, json", "url": "https://stackoverflow.com/questions/3592974", "author": "user_980733", "date_published": "26-05-2021", "votes_or_stars": 1700, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:bd270098-64b4-4aa4-a263-5b127400e32b": {"content": "Python threading: how to share data between threads safely?\nFor simple counters: use `threading.Lock()` as context manager. For complex shared state: prefer `queue.Queue` (thread-safe by design). Avoid global variables without locks. `threading.local()` gives each thread its own data copy. For read-heavy workloads, `threading.RLock` allows re-entrant acquisition.\nCode signature: Use threading.Lock for mutual exclusion or queue.Queue for producer-consumer patterns.", "file_path": "https://stackoverflow.com/questions/3195773", "function_name": "Use threading.Lock for mutual exclusion or queue.Queue for producer-consumer patterns.", "type": "stack_overflow", "metadata": {"record_id": "bd270098-64b4-4aa4-a263-5b127400e32b", "source_type": "stack_overflow", "domain": "Asyncio/Threading", "library": "lock", "title": "Python threading: how to share data between threads safely?", "content": "For simple counters: use `threading.Lock()` as context manager. For complex shared state: prefer `queue.Queue` (thread-safe by design). Avoid global variables without locks. `threading.local()` gives each thread its own data copy. For read-heavy workloads, `threading.RLock` allows re-entrant acquisition.", "code_signature": "Use threading.Lock for mutual exclusion or queue.Queue for producer-consumer patterns.", "tags": "await, async, gather, lock", "url": "https://stackoverflow.com/questions/3195773", "author": "user_714318", "date_published": "15-08-2022", "votes_or_stars": 1103, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:bd16f02c-3d57-4825-8a13-76f46ba0c897": {"content": "BUG: Flask session not persisting between requests in tests\nProblem: Session data set in one request is not available in the next request during testing.\n\nFix/Discussion: Use Flask test client's session_transaction() context manager: `with client.session_transaction() as sess:\n    sess['user_id'] = 1`. Ensure SECRET_KEY is set. For testing: `app.config['TESTING'] = True; app.config['SECRET_KEY'] = 'test'`.\nCode signature: status:closed", "file_path": "https://github.com/flask/flask/issues/2141", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "bd16f02c-3d57-4825-8a13-76f46ba0c897", "source_type": "github_issue", "domain": "FastAPI/Flask/Django", "library": "flask", "title": "BUG: Flask session not persisting between requests in tests", "content": "Problem: Session data set in one request is not available in the next request during testing.\n\nFix/Discussion: Use Flask test client's session_transaction() context manager: `with client.session_transaction() as sess:\n    sess['user_id'] = 1`. Ensure SECRET_KEY is set. For testing: `app.config['TESTING'] = True; app.config['SECRET_KEY'] = 'test'`.", "code_signature": "status:closed", "tags": "routing, orm, pydantic, dependency-injection, django, blueprint", "url": "https://github.com/flask/flask/issues/2141", "author": "contributor_5020", "date_published": "22-05-2021", "votes_or_stars": 169, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:ea2ec08d-8dca-4f62-b309-defc7ec29aa8": {"content": "requests.exceptions.SSLError: certificate verify failed\nNever use `verify=False` in production — it disables SSL verification entirely. Solutions: (1) Update certifi: `pip install --upgrade certifi`. (2) Provide CA bundle: `requests.get(url, verify='/path/to/ca-bundle.crt')`. (3) For self-signed certs in dev: `verify=False` only, and never commit this. (4) Check system time — expired system clock causes SSL failures.\nCode signature: Your SSL certificate chain is incomplete or self-signed. Fix the cert, don't disable verification.", "file_path": "https://stackoverflow.com/questions/7907400", "function_name": "Your SSL certificate chain is incomplete or self-signed. Fix the cert, don't disable verification.", "type": "stack_overflow", "metadata": {"record_id": "ea2ec08d-8dca-4f62-b309-defc7ec29aa8", "source_type": "stack_overflow", "domain": "Requests/HTTP/APIs", "library": "websocket", "title": "requests.exceptions.SSLError: certificate verify failed", "content": "Never use `verify=False` in production — it disables SSL verification entirely. Solutions: (1) Update certifi: `pip install --upgrade certifi`. (2) Provide CA bundle: `requests.get(url, verify='/path/to/ca-bundle.crt')`. (3) For self-signed certs in dev: `verify=False` only, and never commit this. (4) Check system time — expired system clock causes SSL failures.", "code_signature": "Your SSL certificate chain is incomplete or self-signed. Fix the cert, don't disable verification.", "tags": "json, websocket, retry, requests, authentication", "url": "https://stackoverflow.com/questions/7907400", "author": "user_851204", "date_published": "24-12-2018", "votes_or_stars": 1372, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:b7d98b9b-8647-4f40-b856-e9c83e41f813": {"content": "Django ORM Queries — official reference\nOfficial documentation for Django ORM Queries. The method signature is `QuerySet.filter(**kwargs)`. filters QuerySet rows matching given field lookups. Raises `FieldError` when an unknown field lookup is provided. See also: QuerySet.exclude, QuerySet.annotate, Q objects.\nCode signature: QuerySet.filter(**kwargs)", "file_path": "https://docs.django.org/en/stable/QuerySet/filter.html", "function_name": "QuerySet.filter(**kwargs)", "type": "documentation", "metadata": {"record_id": "b7d98b9b-8647-4f40-b856-e9c83e41f813", "source_type": "documentation", "domain": "FastAPI/Flask/Django", "library": "Django", "title": "Django ORM Queries — official reference", "content": "Official documentation for Django ORM Queries. The method signature is `QuerySet.filter(**kwargs)`. filters QuerySet rows matching given field lookups. Raises `FieldError` when an unknown field lookup is provided. See also: QuerySet.exclude, QuerySet.annotate, Q objects.", "code_signature": "QuerySet.filter(**kwargs)", "tags": "routing, flask, blueprint, orm, template, fastapi", "url": "https://docs.django.org/en/stable/QuerySet/filter.html", "author": "Official Docs", "date_published": "29-08-2020", "votes_or_stars": 2437, "relevance_label": "low", "difficulty_level": "intermediate"}}, "kb:3007bcc5-793f-45fd-bc81-8a920a230068": {"content": "How to implement JWT authentication in FastAPI?\n```python\nfrom fastapi.security import OAuth2PasswordBearer\nfrom jose import jwt\noauth2_scheme = OAuth2PasswordBearer(tokenUrl='token')\n```\nCreate access tokens with `jwt.encode(payload, SECRET_KEY, algorithm='HS256')`. Verify with `jwt.decode(token, SECRET_KEY, algorithms=['HS256'])`. Store SECRET_KEY in environment variables, never in code.\nCode signature: Use python-jose for JWT and passlib for password hashing with OAuth2PasswordBearer.", "file_path": "https://stackoverflow.com/questions/7754864", "function_name": "Use python-jose for JWT and passlib for password hashing with OAuth2PasswordBearer.", "type": "stack_overflow", "metadata": {"record_id": "3007bcc5-793f-45fd-bc81-8a920a230068", "source_type": "stack_overflow", "domain": "FastAPI/Flask/Django", "library": "fastapi", "title": "How to implement JWT authentication in FastAPI?", "content": "```python\nfrom fastapi.security import OAuth2PasswordBearer\nfrom jose import jwt\noauth2_scheme = OAuth2PasswordBearer(tokenUrl='token')\n```\nCreate access tokens with `jwt.encode(payload, SECRET_KEY, algorithm='HS256')`. Verify with `jwt.decode(token, SECRET_KEY, algorithms=['HS256'])`. Store SECRET_KEY in environment variables, never in code.", "code_signature": "Use python-jose for JWT and passlib for password hashing with OAuth2PasswordBearer.", "tags": "middleware, rest, response", "url": "https://stackoverflow.com/questions/7754864", "author": "user_773587", "date_published": "30-07-2024", "votes_or_stars": 423, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:8785e171-7c5c-4fc0-8315-265943a60620": {"content": "BUG: SQLAlchemy connection pool exhaustion under load\nProblem: Application hangs under concurrent load — all connections are checked out and pool is exhausted.\n\nFix/Discussion: Increase `pool_size` and `max_overflow`. Ensure sessions are always closed: use `contextmanager` or `with Session() as session:`. Set `pool_timeout` to fail fast. Enable `pool_pre_ping=True`. Monitor with `engine.pool.checkedout()`. In async: use `AsyncSession` with `async_scoped_session`.\nCode signature: status:closed", "file_path": "https://github.com/sqlalchemy/sqlalchemy/issues/106", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "8785e171-7c5c-4fc0-8315-265943a60620", "source_type": "github_issue", "domain": "SQLAlchemy/Databases", "library": "sqlalchemy", "title": "BUG: SQLAlchemy connection pool exhaustion under load", "content": "Problem: Application hangs under concurrent load — all connections are checked out and pool is exhausted.\n\nFix/Discussion: Increase `pool_size` and `max_overflow`. Ensure sessions are always closed: use `contextmanager` or `with Session() as session:`. Set `pool_timeout` to fail fast. Enable `pool_pre_ping=True`. Monitor with `engine.pool.checkedout()`. In async: use `AsyncSession` with `async_scoped_session`.", "code_signature": "status:closed", "tags": "sqlite, migration, foreign-key, transaction, session, alembic", "url": "https://github.com/sqlalchemy/sqlalchemy/issues/106", "author": "contributor_5078", "date_published": "17-02-2020", "votes_or_stars": 276, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:bcf8ff32-3156-4c32-86d4-5cbee7f7dcff": {"content": "How do I reshape a wide DataFrame to long format?\n`pd.melt(df, id_vars=['id'], value_vars=['col1','col2'])` unpivots columns to rows. `df.stack()` moves column index to row index. For the reverse, use `df.pivot()` or `df.unstack()`. Always reset_index() after stack/unstack to get a clean DataFrame.\nCode signature: Use pd.melt() or df.stack() depending on your structure.", "file_path": "https://stackoverflow.com/questions/6229731", "function_name": "Use pd.melt() or df.stack() depending on your structure.", "type": "stack_overflow", "metadata": {"record_id": "bcf8ff32-3156-4c32-86d4-5cbee7f7dcff", "source_type": "stack_overflow", "domain": "NumPy/Pandas", "library": "nan", "title": "How do I reshape a wide DataFrame to long format?", "content": "`pd.melt(df, id_vars=['id'], value_vars=['col1','col2'])` unpivots columns to rows. `df.stack()` moves column index to row index. For the reverse, use `df.pivot()` or `df.unstack()`. Always reset_index() after stack/unstack to get a clean DataFrame.", "code_signature": "Use pd.melt() or df.stack() depending on your structure.", "tags": "dataframe, pandas, vectorization", "url": "https://stackoverflow.com/questions/6229731", "author": "user_428373", "date_published": "21-03-2018", "votes_or_stars": 812, "relevance_label": "low", "difficulty_level": "beginner"}}, "kb:61fed399-750c-416c-8fea-809b3303d23f": {"content": "Django ORM Queries — official reference\nOfficial documentation for Django ORM Queries. The method signature is `QuerySet.filter(**kwargs)`. filters QuerySet rows matching given field lookups. Raises `FieldError` when an unknown field lookup is provided. See also: QuerySet.exclude, QuerySet.annotate, Q objects.\nCode signature: QuerySet.filter(**kwargs)", "file_path": "https://docs.django.org/en/stable/QuerySet/filter.html", "function_name": "QuerySet.filter(**kwargs)", "type": "documentation", "metadata": {"record_id": "61fed399-750c-416c-8fea-809b3303d23f", "source_type": "documentation", "domain": "FastAPI/Flask/Django", "library": "Django", "title": "Django ORM Queries — official reference", "content": "Official documentation for Django ORM Queries. The method signature is `QuerySet.filter(**kwargs)`. filters QuerySet rows matching given field lookups. Raises `FieldError` when an unknown field lookup is provided. See also: QuerySet.exclude, QuerySet.annotate, Q objects.", "code_signature": "QuerySet.filter(**kwargs)", "tags": "dependency-injection, django, request, pydantic", "url": "https://docs.django.org/en/stable/QuerySet/filter.html", "author": "Official Docs", "date_published": "05-11-2019", "votes_or_stars": 2449, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:42c290f2-e157-444d-81bb-96677621b07a": {"content": "Python: how to safely write to a file atomically?\n```python\nimport tempfile, os\nwith tempfile.NamedTemporaryFile('w', delete=False, dir='.') as tmp:\n    tmp.write(data)\n    tmp.flush()\n    os.fsync(tmp.fileno())\nos.replace(tmp.name, target_path)\n```\n`os.replace()` is atomic on POSIX. This prevents partial writes from corrupting files.\nCode signature: Write to a temp file then rename — rename is atomic on POSIX systems.", "file_path": "https://stackoverflow.com/questions/5394880", "function_name": "Write to a temp file then rename — rename is atomic on POSIX systems.", "type": "stack_overflow", "metadata": {"record_id": "42c290f2-e157-444d-81bb-96677621b07a", "source_type": "stack_overflow", "domain": "OS/Subprocess/File I/O", "library": "stdin", "title": "Python: how to safely write to a file atomically?", "content": "```python\nimport tempfile, os\nwith tempfile.NamedTemporaryFile('w', delete=False, dir='.') as tmp:\n    tmp.write(data)\n    tmp.flush()\n    os.fsync(tmp.fileno())\nos.replace(tmp.name, target_path)\n```\n`os.replace()` is atomic on POSIX. This prevents partial writes from corrupting files.", "code_signature": "Write to a temp file then rename — rename is atomic on POSIX systems.", "tags": "pathlib, process, shutil, glob, file-io, stderr", "url": "https://stackoverflow.com/questions/5394880", "author": "user_179430", "date_published": "15-07-2021", "votes_or_stars": 546, "relevance_label": "low", "difficulty_level": "advanced"}}, "kb:060d6e36-4677-4ffa-a7ed-c1fb7288ca2f": {"content": "Django ORM Queries — official reference\nOfficial documentation for Django ORM Queries. The method signature is `QuerySet.filter(**kwargs)`. filters QuerySet rows matching given field lookups. Raises `FieldError` when an unknown field lookup is provided. See also: QuerySet.exclude, QuerySet.annotate, Q objects.\nCode signature: QuerySet.filter(**kwargs)", "file_path": "https://docs.django.org/en/stable/QuerySet/filter.html", "function_name": "QuerySet.filter(**kwargs)", "type": "documentation", "metadata": {"record_id": "060d6e36-4677-4ffa-a7ed-c1fb7288ca2f", "source_type": "documentation", "domain": "FastAPI/Flask/Django", "library": "Django", "title": "Django ORM Queries — official reference", "content": "Official documentation for Django ORM Queries. The method signature is `QuerySet.filter(**kwargs)`. filters QuerySet rows matching given field lookups. Raises `FieldError` when an unknown field lookup is provided. See also: QuerySet.exclude, QuerySet.annotate, Q objects.", "code_signature": "QuerySet.filter(**kwargs)", "tags": "routing, flask, request, dependency-injection, template, rest", "url": "https://docs.django.org/en/stable/QuerySet/filter.html", "author": "Official Docs", "date_published": "25-06-2024", "votes_or_stars": 2422, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:b1df1ac1-fdcc-4f9b-821a-9e196187dde4": {"content": "BUG: os.path.join ignores prefix when component starts with /\nProblem: os.path.join('base', '/absolute') returns '/absolute', ignoring 'base'.\n\nFix/Discussion: This is documented behavior: os.path.join discards previous components when it encounters an absolute path. Fix: use `pathlib.Path(base) / component.lstrip('/')`. Validate user-provided paths with `Path(path).resolve().is_relative_to(base_dir)` to prevent path traversal.\nCode signature: status:closed", "file_path": "https://github.com/cpython/cpython/issues/7712", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "b1df1ac1-fdcc-4f9b-821a-9e196187dde4", "source_type": "github_issue", "domain": "OS/Subprocess/File I/O", "library": "cpython", "title": "BUG: os.path.join ignores prefix when component starts with /", "content": "Problem: os.path.join('base', '/absolute') returns '/absolute', ignoring 'base'.\n\nFix/Discussion: This is documented behavior: os.path.join discards previous components when it encounters an absolute path. Fix: use `pathlib.Path(base) / component.lstrip('/')`. Validate user-provided paths with `Path(path).resolve().is_relative_to(base_dir)` to prevent path traversal.", "code_signature": "status:closed", "tags": "os, process, tempfile, stdout, pipe", "url": "https://github.com/cpython/cpython/issues/7712", "author": "contributor_8802", "date_published": "10-04-2019", "votes_or_stars": 409, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:97672db3-472e-4e79-a265-065f0fd88d9a": {"content": "How to read a large file line by line in Python without loading it all into memory?\n```python\nwith open('large.txt', 'r') as f:\n    for line in f:\n        process(line.strip())\n```\nThis reads one line at a time. For binary files: use `f.read(chunk_size)` in a loop. For CSV: use `pd.read_csv(file, chunksize=10000)`. Never use `f.readlines()` on large files.\nCode signature: Iterate over the file object directly — it yields lines lazily.", "file_path": "https://stackoverflow.com/questions/2737893", "function_name": "Iterate over the file object directly — it yields lines lazily.", "type": "stack_overflow", "metadata": {"record_id": "97672db3-472e-4e79-a265-065f0fd88d9a", "source_type": "stack_overflow", "domain": "OS/Subprocess/File I/O", "library": "shutil", "title": "How to read a large file line by line in Python without loading it all into memory?", "content": "```python\nwith open('large.txt', 'r') as f:\n    for line in f:\n        process(line.strip())\n```\nThis reads one line at a time. For binary files: use `f.read(chunk_size)` in a loop. For CSV: use `pd.read_csv(file, chunksize=10000)`. Never use `f.readlines()` on large files.", "code_signature": "Iterate over the file object directly — it yields lines lazily.", "tags": "process, stdout, stdin, env-variable, pipe, pathlib", "url": "https://stackoverflow.com/questions/2737893", "author": "user_994539", "date_published": "27-12-2024", "votes_or_stars": 2273, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:ab841654-7f00-4774-861e-56ed24c8027d": {"content": "How to implement database connection pooling with SQLAlchemy?\n`create_engine(url, pool_size=10, max_overflow=20, pool_timeout=30, pool_recycle=1800)`. Use `pool_pre_ping=True` to detect stale connections. For async: use `AsyncEngine` with `create_async_engine()`. Monitor with `engine.pool.status()`. NullPool disables pooling for scripts/serverless environments.\nCode signature: create_engine() has built-in pooling. Configure pool_size and max_overflow.", "file_path": "https://stackoverflow.com/questions/7358113", "function_name": "create_engine() has built-in pooling. Configure pool_size and max_overflow.", "type": "stack_overflow", "metadata": {"record_id": "ab841654-7f00-4774-861e-56ed24c8027d", "source_type": "stack_overflow", "domain": "SQLAlchemy/Databases", "library": "session", "title": "How to implement database connection pooling with SQLAlchemy?", "content": "`create_engine(url, pool_size=10, max_overflow=20, pool_timeout=30, pool_recycle=1800)`. Use `pool_pre_ping=True` to detect stale connections. For async: use `AsyncEngine` with `create_async_engine()`. Monitor with `engine.pool.status()`. NullPool disables pooling for scripts/serverless environments.", "code_signature": "create_engine() has built-in pooling. Configure pool_size and max_overflow.", "tags": "connection-pool, sqlalchemy, alembic, relationship", "url": "https://stackoverflow.com/questions/7358113", "author": "user_12260", "date_published": "28-09-2022", "votes_or_stars": 252, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:b7c67a36-01a9-421c-bf30-f234a34b5766": {"content": "Python threading: how to share data between threads safely?\nFor simple counters: use `threading.Lock()` as context manager. For complex shared state: prefer `queue.Queue` (thread-safe by design). Avoid global variables without locks. `threading.local()` gives each thread its own data copy. For read-heavy workloads, `threading.RLock` allows re-entrant acquisition.\nCode signature: Use threading.Lock for mutual exclusion or queue.Queue for producer-consumer patterns.", "file_path": "https://stackoverflow.com/questions/3195773", "function_name": "Use threading.Lock for mutual exclusion or queue.Queue for producer-consumer patterns.", "type": "stack_overflow", "metadata": {"record_id": "b7c67a36-01a9-421c-bf30-f234a34b5766", "source_type": "stack_overflow", "domain": "Asyncio/Threading", "library": "lock", "title": "Python threading: how to share data between threads safely?", "content": "For simple counters: use `threading.Lock()` as context manager. For complex shared state: prefer `queue.Queue` (thread-safe by design). Avoid global variables without locks. `threading.local()` gives each thread its own data copy. For read-heavy workloads, `threading.RLock` allows re-entrant acquisition.", "code_signature": "Use threading.Lock for mutual exclusion or queue.Queue for producer-consumer patterns.", "tags": "await, event-loop, threading, lock", "url": "https://stackoverflow.com/questions/3195773", "author": "user_714318", "date_published": "07-05-2021", "votes_or_stars": 1093, "relevance_label": "low", "difficulty_level": "beginner"}}, "kb:a92a1bb3-0b37-4914-83b4-0d16f5d2a97b": {"content": "requests — HTTP Session Management\nThe `requests.Session` function in requests creates a persistent HTTP session with connection pooling. It accepts `` as a parameter and returns Session object. Common use cases include making multiple requests to the same host efficiently. Note that always close or use Session as context manager to release connections.\nCode signature: requests.Session() -> Session object", "file_path": "https://docs.requests.org/en/stable/requests/Session.html", "function_name": "requests.Session() -> Session object", "type": "documentation", "metadata": {"record_id": "a92a1bb3-0b37-4914-83b4-0d16f5d2a97b", "source_type": "documentation", "domain": "Requests/HTTP/APIs", "library": "requests", "title": "requests — HTTP Session Management", "content": "The `requests.Session` function in requests creates a persistent HTTP session with connection pooling. It accepts `` as a parameter and returns Session object. Common use cases include making multiple requests to the same host efficiently. Note that always close or use Session as context manager to release connections.", "code_signature": "requests.Session() -> Session object", "tags": "httpx, rest-api, requests, authentication, timeout, headers", "url": "https://docs.requests.org/en/stable/requests/Session.html", "author": "Official Docs", "date_published": "11-09-2024", "votes_or_stars": 83, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:6f0793fc-468c-4673-9a81-1fef696157b1": {"content": "Python: how to safely write to a file atomically?\n```python\nimport tempfile, os\nwith tempfile.NamedTemporaryFile('w', delete=False, dir='.') as tmp:\n    tmp.write(data)\n    tmp.flush()\n    os.fsync(tmp.fileno())\nos.replace(tmp.name, target_path)\n```\n`os.replace()` is atomic on POSIX. This prevents partial writes from corrupting files.\nCode signature: Write to a temp file then rename — rename is atomic on POSIX systems.", "file_path": "https://stackoverflow.com/questions/5394880", "function_name": "Write to a temp file then rename — rename is atomic on POSIX systems.", "type": "stack_overflow", "metadata": {"record_id": "6f0793fc-468c-4673-9a81-1fef696157b1", "source_type": "stack_overflow", "domain": "OS/Subprocess/File I/O", "library": "stdin", "title": "Python: how to safely write to a file atomically?", "content": "```python\nimport tempfile, os\nwith tempfile.NamedTemporaryFile('w', delete=False, dir='.') as tmp:\n    tmp.write(data)\n    tmp.flush()\n    os.fsync(tmp.fileno())\nos.replace(tmp.name, target_path)\n```\n`os.replace()` is atomic on POSIX. This prevents partial writes from corrupting files.", "code_signature": "Write to a temp file then rename — rename is atomic on POSIX systems.", "tags": "env-variable, stdin, os", "url": "https://stackoverflow.com/questions/5394880", "author": "user_179430", "date_published": "19-10-2021", "votes_or_stars": 549, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:3aa085ce-8dd6-4f09-a50f-f8178b6c9ea6": {"content": "How do I handle CORS in FastAPI?\n```python\nfrom fastapi.middleware.cors import CORSMiddleware\napp.add_middleware(CORSMiddleware, allow_origins=['*'], allow_methods=['*'], allow_headers=['*'])\n```\nIn production, replace `'*'` with your actual frontend domain. For credentials (cookies), set `allow_credentials=True` and specify exact origins.\nCode signature: Add CORSMiddleware from starlette to your FastAPI app.", "file_path": "https://stackoverflow.com/questions/1527021", "function_name": "Add CORSMiddleware from starlette to your FastAPI app.", "type": "stack_overflow", "metadata": {"record_id": "3aa085ce-8dd6-4f09-a50f-f8178b6c9ea6", "source_type": "stack_overflow", "domain": "FastAPI/Flask/Django", "library": "flask", "title": "How do I handle CORS in FastAPI?", "content": "```python\nfrom fastapi.middleware.cors import CORSMiddleware\napp.add_middleware(CORSMiddleware, allow_origins=['*'], allow_methods=['*'], allow_headers=['*'])\n```\nIn production, replace `'*'` with your actual frontend domain. For credentials (cookies), set `allow_credentials=True` and specify exact origins.", "code_signature": "Add CORSMiddleware from starlette to your FastAPI app.", "tags": "blueprint, fastapi, middleware", "url": "https://stackoverflow.com/questions/1527021", "author": "user_911393", "date_published": "19-09-2018", "votes_or_stars": 951, "relevance_label": "low", "difficulty_level": "beginner"}}, "kb:54961e66-94b1-4149-87ac-4b1aa624dc41": {"content": "Why is my Pandas merge creating duplicate rows?\nDuplicate rows after merge usually mean the join key is not unique. Diagnose with `df.duplicated(subset=['key']).sum()`. Use `how='left'` carefully when right table has multiple matches. Set `validate='one_to_many'` or `'many_to_one'` to enforce cardinality. Consider deduplicating with `df.drop_duplicates(subset=['key'])` before merging.\nCode signature: Your merge key has duplicates in one or both DataFrames. Use validate='one_to_one' to catch this early.", "file_path": "https://stackoverflow.com/questions/2321324", "function_name": "Your merge key has duplicates in one or both DataFrames. Use validate='one_to_one' to catch this early.", "type": "stack_overflow", "metadata": {"record_id": "54961e66-94b1-4149-87ac-4b1aa624dc41", "source_type": "stack_overflow", "domain": "NumPy/Pandas", "library": "array", "title": "Why is my Pandas merge creating duplicate rows?", "content": "Duplicate rows after merge usually mean the join key is not unique. Diagnose with `df.duplicated(subset=['key']).sum()`. Use `how='left'` carefully when right table has multiple matches. Set `validate='one_to_many'` or `'many_to_one'` to enforce cardinality. Consider deduplicating with `df.drop_duplicates(subset=['key'])` before merging.", "code_signature": "Your merge key has duplicates in one or both DataFrames. Use validate='one_to_one' to catch this early.", "tags": "pivot, iloc, reshape, groupby, merge", "url": "https://stackoverflow.com/questions/2321324", "author": "user_99814", "date_published": "26-06-2020", "votes_or_stars": 242, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:0d7c5c67-c9cb-48d9-bc4a-21cef8cfdeaf": {"content": "BUG: FastAPI dependency_overrides not working in pytest\nProblem: Dependency overrides set in conftest are not applied during test execution.\n\nFix/Discussion: Set overrides on the app object before TestClient is created. Use `app.dependency_overrides[dep] = mock_dep` in a pytest fixture with function scope. Clear overrides after test: `app.dependency_overrides = {}`. Ensure TestClient is created after overrides are set.\nCode signature: status:closed", "file_path": "https://github.com/fastapi/fastapi/issues/8479", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "0d7c5c67-c9cb-48d9-bc4a-21cef8cfdeaf", "source_type": "github_issue", "domain": "FastAPI/Flask/Django", "library": "fastapi", "title": "BUG: FastAPI dependency_overrides not working in pytest", "content": "Problem: Dependency overrides set in conftest are not applied during test execution.\n\nFix/Discussion: Set overrides on the app object before TestClient is created. Use `app.dependency_overrides[dep] = mock_dep` in a pytest fixture with function scope. Clear overrides after test: `app.dependency_overrides = {}`. Ensure TestClient is created after overrides are set.", "code_signature": "status:closed", "tags": "orm, template, middleware", "url": "https://github.com/fastapi/fastapi/issues/8479", "author": "contributor_1994", "date_published": "15-02-2021", "votes_or_stars": 321, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:09750392-acb8-4d2c-bd75-b33d21ecda06": {"content": "SQLAlchemy bulk insert: what's the fastest way?\n`session.bulk_insert_mappings(Model, list_of_dicts)` skips ORM overhead. Even faster: `session.execute(insert(Model), data)`. Avoid `session.add()` in loops. For PostgreSQL, use `insert().on_conflict_do_nothing()` for upserts. Batch in chunks of 1000-5000 rows for optimal performance.\nCode signature: Use session.bulk_insert_mappings() or execute(insert(), list_of_dicts) for maximum speed.", "file_path": "https://stackoverflow.com/questions/5977931", "function_name": "Use session.bulk_insert_mappings() or execute(insert(), list_of_dicts) for maximum speed.", "type": "stack_overflow", "metadata": {"record_id": "09750392-acb8-4d2c-bd75-b33d21ecda06", "source_type": "stack_overflow", "domain": "SQLAlchemy/Databases", "library": "query", "title": "SQLAlchemy bulk insert: what's the fastest way?", "content": "`session.bulk_insert_mappings(Model, list_of_dicts)` skips ORM overhead. Even faster: `session.execute(insert(Model), data)`. Avoid `session.add()` in loops. For PostgreSQL, use `insert().on_conflict_do_nothing()` for upserts. Batch in chunks of 1000-5000 rows for optimal performance.", "code_signature": "Use session.bulk_insert_mappings() or execute(insert(), list_of_dicts) for maximum speed.", "tags": "migration, sqlite, sqlalchemy", "url": "https://stackoverflow.com/questions/5977931", "author": "user_238275", "date_published": "22-04-2022", "votes_or_stars": 643, "relevance_label": "low", "difficulty_level": "intermediate"}}, "kb:61b5cba1-ebdf-4291-abc9-346f550b7e03": {"content": "BUG: FastAPI dependency_overrides not working in pytest\nProblem: Dependency overrides set in conftest are not applied during test execution.\n\nFix/Discussion: Set overrides on the app object before TestClient is created. Use `app.dependency_overrides[dep] = mock_dep` in a pytest fixture with function scope. Clear overrides after test: `app.dependency_overrides = {}`. Ensure TestClient is created after overrides are set.\nCode signature: status:closed", "file_path": "https://github.com/fastapi/fastapi/issues/8479", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "61b5cba1-ebdf-4291-abc9-346f550b7e03", "source_type": "github_issue", "domain": "FastAPI/Flask/Django", "library": "fastapi", "title": "BUG: FastAPI dependency_overrides not working in pytest", "content": "Problem: Dependency overrides set in conftest are not applied during test execution.\n\nFix/Discussion: Set overrides on the app object before TestClient is created. Use `app.dependency_overrides[dep] = mock_dep` in a pytest fixture with function scope. Clear overrides after test: `app.dependency_overrides = {}`. Ensure TestClient is created after overrides are set.", "code_signature": "status:closed", "tags": "flask, routing, dependency-injection", "url": "https://github.com/fastapi/fastapi/issues/8479", "author": "contributor_1994", "date_published": "30-01-2018", "votes_or_stars": 327, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:8b8d1eb7-e653-4468-8342-1d980df2fb7c": {"content": "How to handle database migrations with Alembic?\nWorkflow: (1) Change SQLAlchemy models. (2) `alembic revision --autogenerate -m 'description'`. (3) Review generated migration script. (4) `alembic upgrade head` to apply. Always review autogenerated migrations — they miss some changes (indexes, constraints). Use `alembic downgrade -1` to rollback. Store migrations in version control.\nCode signature: Use alembic revision --autogenerate to create migrations from model changes.", "file_path": "https://stackoverflow.com/questions/9436429", "function_name": "Use alembic revision --autogenerate to create migrations from model changes.", "type": "stack_overflow", "metadata": {"record_id": "8b8d1eb7-e653-4468-8342-1d980df2fb7c", "source_type": "stack_overflow", "domain": "SQLAlchemy/Databases", "library": "connection-pool", "title": "How to handle database migrations with Alembic?", "content": "Workflow: (1) Change SQLAlchemy models. (2) `alembic revision --autogenerate -m 'description'`. (3) Review generated migration script. (4) `alembic upgrade head` to apply. Always review autogenerated migrations — they miss some changes (indexes, constraints). Use `alembic downgrade -1` to rollback. Store migrations in version control.", "code_signature": "Use alembic revision --autogenerate to create migrations from model changes.", "tags": "foreign-key, sqlalchemy, migration, sqlite, postgresql", "url": "https://stackoverflow.com/questions/9436429", "author": "user_974047", "date_published": "16-12-2023", "votes_or_stars": 2402, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:1cf0f1cb-a229-4397-b12a-5673ea4a367f": {"content": "How to limit concurrency with asyncio?\n```python\nsem = asyncio.Semaphore(10)\nasync def limited(url):\n    async with sem:\n        return await fetch(url)\nresults = await asyncio.gather(*[limited(u) for u in urls])\n```\nThis prevents overwhelming APIs or databases with too many simultaneous connections.\nCode signature: Use asyncio.Semaphore to cap the number of concurrent coroutines.", "file_path": "https://stackoverflow.com/questions/2375453", "function_name": "Use asyncio.Semaphore to cap the number of concurrent coroutines.", "type": "stack_overflow", "metadata": {"record_id": "1cf0f1cb-a229-4397-b12a-5673ea4a367f", "source_type": "stack_overflow", "domain": "Asyncio/Threading", "library": "asyncio", "title": "How to limit concurrency with asyncio?", "content": "```python\nsem = asyncio.Semaphore(10)\nasync def limited(url):\n    async with sem:\n        return await fetch(url)\nresults = await asyncio.gather(*[limited(u) for u in urls])\n```\nThis prevents overwhelming APIs or databases with too many simultaneous connections.", "code_signature": "Use asyncio.Semaphore to cap the number of concurrent coroutines.", "tags": "threading, coroutine, async", "url": "https://stackoverflow.com/questions/2375453", "author": "user_449589", "date_published": "31-10-2018", "votes_or_stars": 2434, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:8abe2990-666e-402a-9893-2b5e64d73e79": {"content": "Why is my Pandas merge creating duplicate rows?\nDuplicate rows after merge usually mean the join key is not unique. Diagnose with `df.duplicated(subset=['key']).sum()`. Use `how='left'` carefully when right table has multiple matches. Set `validate='one_to_many'` or `'many_to_one'` to enforce cardinality. Consider deduplicating with `df.drop_duplicates(subset=['key'])` before merging.\nCode signature: Your merge key has duplicates in one or both DataFrames. Use validate='one_to_one' to catch this early.", "file_path": "https://stackoverflow.com/questions/2321324", "function_name": "Your merge key has duplicates in one or both DataFrames. Use validate='one_to_one' to catch this early.", "type": "stack_overflow", "metadata": {"record_id": "8abe2990-666e-402a-9893-2b5e64d73e79", "source_type": "stack_overflow", "domain": "NumPy/Pandas", "library": "array", "title": "Why is my Pandas merge creating duplicate rows?", "content": "Duplicate rows after merge usually mean the join key is not unique. Diagnose with `df.duplicated(subset=['key']).sum()`. Use `how='left'` carefully when right table has multiple matches. Set `validate='one_to_many'` or `'many_to_one'` to enforce cardinality. Consider deduplicating with `df.drop_duplicates(subset=['key'])` before merging.", "code_signature": "Your merge key has duplicates in one or both DataFrames. Use validate='one_to_one' to catch this early.", "tags": "merge, broadcasting, dtype, numpy, pivot, dataframe", "url": "https://stackoverflow.com/questions/2321324", "author": "user_99814", "date_published": "19-05-2021", "votes_or_stars": 255, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:69b653f5-88b2-4c72-b9af-6f6ec96d138b": {"content": "BUG: httpx: SSL certificate error on Windows despite valid cert\nProblem: httpx raises SSLCertVerificationError on Windows when system CA store has the cert.\n\nFix/Discussion: httpx uses its own CA bundle (certifi) by default, not the system store. Fix: `httpx.Client(verify=certifi.where())` — already default. For custom certs: `httpx.Client(verify='/path/to/ca-bundle.pem')`. Install `pip install truststore` and use `ssl.create_default_context(ssl.Purpose.SERVER_AUTH)` to use system store.\nCode signature: status:closed", "file_path": "https://github.com/httpx/httpx/issues/8949", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "69b653f5-88b2-4c72-b9af-6f6ec96d138b", "source_type": "github_issue", "domain": "Requests/HTTP/APIs", "library": "httpx", "title": "BUG: httpx: SSL certificate error on Windows despite valid cert", "content": "Problem: httpx raises SSLCertVerificationError on Windows when system CA store has the cert.\n\nFix/Discussion: httpx uses its own CA bundle (certifi) by default, not the system store. Fix: `httpx.Client(verify=certifi.where())` — already default. For custom certs: `httpx.Client(verify='/path/to/ca-bundle.pem')`. Install `pip install truststore` and use `ssl.create_default_context(ssl.Purpose.SERVER_AUTH)` to use system store.", "code_signature": "status:closed", "tags": "aiohttp, rate-limiting, retry, rest-api, oauth, timeout", "url": "https://github.com/httpx/httpx/issues/8949", "author": "contributor_1420", "date_published": "23-01-2022", "votes_or_stars": 472, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:d8703af0-be43-48fa-9741-90973cf8028a": {"content": "How to apply a function to each row in Pandas without using iterrows?\n`iterrows()` is slow because it creates a Series per row. Prefer `df.apply(func, axis=1)` for row-wise operations, or better yet, vectorize using NumPy: `np.where(condition, a, b)`. For complex logic, `df.assign()` with vectorized expressions is cleanest. Benchmark: vectorized > apply > itertuples > iterrows.\nCode signature: Use df.apply(func, axis=1) or vectorized NumPy operations for best performance.", "file_path": "https://stackoverflow.com/questions/5446912", "function_name": "Use df.apply(func, axis=1) or vectorized NumPy operations for best performance.", "type": "stack_overflow", "metadata": {"record_id": "d8703af0-be43-48fa-9741-90973cf8028a", "source_type": "stack_overflow", "domain": "NumPy/Pandas", "library": "dataframe", "title": "How to apply a function to each row in Pandas without using iterrows?", "content": "`iterrows()` is slow because it creates a Series per row. Prefer `df.apply(func, axis=1)` for row-wise operations, or better yet, vectorize using NumPy: `np.where(condition, a, b)`. For complex logic, `df.assign()` with vectorized expressions is cleanest. Benchmark: vectorized > apply > itertuples > iterrows.", "code_signature": "Use df.apply(func, axis=1) or vectorized NumPy operations for best performance.", "tags": "dtype, iloc, pivot, dataframe, loc", "url": "https://stackoverflow.com/questions/5446912", "author": "user_563306", "date_published": "09-07-2018", "votes_or_stars": 2291, "relevance_label": "low", "difficulty_level": "advanced"}}, "kb:b50697ae-7fea-4a77-88bd-3dc7b77ec773": {"content": "BUG: threading.Timer not cancellable after it fires\nProblem: Calling cancel() on a Timer that has already executed does not raise an error but has no effect.\n\nFix/Discussion: threading.Timer.cancel() only works before the timer fires. Store Timer reference and check `timer.finished.is_set()` to know if it's already run. For repeating timers, use a loop with `threading.Event` for clean cancellation: `stop_event = threading.Event(); stop_event.wait(interval)`.\nCode signature: status:closed", "file_path": "https://github.com/cpython/cpython/issues/4111", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "b50697ae-7fea-4a77-88bd-3dc7b77ec773", "source_type": "github_issue", "domain": "Asyncio/Threading", "library": "cpython", "title": "BUG: threading.Timer not cancellable after it fires", "content": "Problem: Calling cancel() on a Timer that has already executed does not raise an error but has no effect.\n\nFix/Discussion: threading.Timer.cancel() only works before the timer fires. Store Timer reference and check `timer.finished.is_set()` to know if it's already run. For repeating timers, use a loop with `threading.Event` for clean cancellation: `stop_event = threading.Event(); stop_event.wait(interval)`.", "code_signature": "status:closed", "tags": "event-loop, deadlock, await, asyncio", "url": "https://github.com/cpython/cpython/issues/4111", "author": "contributor_7884", "date_published": "26-10-2019", "votes_or_stars": 28, "relevance_label": "low", "difficulty_level": "intermediate"}}, "kb:71a24618-ad24-47da-8ec0-b1b84c55e380": {"content": "BUG: SQLAlchemy connection pool exhaustion under load\nProblem: Application hangs under concurrent load — all connections are checked out and pool is exhausted.\n\nFix/Discussion: Increase `pool_size` and `max_overflow`. Ensure sessions are always closed: use `contextmanager` or `with Session() as session:`. Set `pool_timeout` to fail fast. Enable `pool_pre_ping=True`. Monitor with `engine.pool.checkedout()`. In async: use `AsyncSession` with `async_scoped_session`.\nCode signature: status:closed", "file_path": "https://github.com/sqlalchemy/sqlalchemy/issues/106", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "71a24618-ad24-47da-8ec0-b1b84c55e380", "source_type": "github_issue", "domain": "SQLAlchemy/Databases", "library": "sqlalchemy", "title": "BUG: SQLAlchemy connection pool exhaustion under load", "content": "Problem: Application hangs under concurrent load — all connections are checked out and pool is exhausted.\n\nFix/Discussion: Increase `pool_size` and `max_overflow`. Ensure sessions are always closed: use `contextmanager` or `with Session() as session:`. Set `pool_timeout` to fail fast. Enable `pool_pre_ping=True`. Monitor with `engine.pool.checkedout()`. In async: use `AsyncSession` with `async_scoped_session`.", "code_signature": "status:closed", "tags": "foreign-key, alembic, sqlite, orm", "url": "https://github.com/sqlalchemy/sqlalchemy/issues/106", "author": "contributor_5078", "date_published": "01-09-2021", "votes_or_stars": 275, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:29300bb5-0e46-427b-abe9-4407a8cd8d57": {"content": "How to add retry logic to requests?\n```python\nfrom requests.adapters import HTTPAdapter\nfrom urllib3.util.retry import Retry\nretry = Retry(total=3, backoff_factor=1, status_forcelist=[500,502,503,504])\nadapter = HTTPAdapter(max_retries=retry)\nsession.mount('https://', adapter)\n```\nCode signature: Mount an HTTPAdapter with urllib3 Retry on the Session.", "file_path": "https://stackoverflow.com/questions/1669324", "function_name": "Mount an HTTPAdapter with urllib3 Retry on the Session.", "type": "stack_overflow", "metadata": {"record_id": "29300bb5-0e46-427b-abe9-4407a8cd8d57", "source_type": "stack_overflow", "domain": "Requests/HTTP/APIs", "library": "websocket", "title": "How to add retry logic to requests?", "content": "```python\nfrom requests.adapters import HTTPAdapter\nfrom urllib3.util.retry import Retry\nretry = Retry(total=3, backoff_factor=1, status_forcelist=[500,502,503,504])\nadapter = HTTPAdapter(max_retries=retry)\nsession.mount('https://', adapter)\n```", "code_signature": "Mount an HTTPAdapter with urllib3 Retry on the Session.", "tags": "authentication, httpx, requests, rate-limiting, headers", "url": "https://stackoverflow.com/questions/1669324", "author": "user_952590", "date_published": "30-11-2021", "votes_or_stars": 1475, "relevance_label": "low", "difficulty_level": "beginner"}}, "kb:4d0986b1-a2bb-4045-9249-87b5f26a05f7": {"content": "How to run background tasks in FastAPI?\nFastAPI's `BackgroundTasks`: add `background_tasks: BackgroundTasks` as a parameter, then `background_tasks.add_task(func, *args)`. Runs after response is sent. For heavy workloads (email, ML inference), use Celery with Redis/RabbitMQ broker. For async background work, `asyncio.create_task()` works within async endpoints.\nCode signature: Use BackgroundTasks for lightweight tasks or Celery for heavy/distributed work.", "file_path": "https://stackoverflow.com/questions/8077999", "function_name": "Use BackgroundTasks for lightweight tasks or Celery for heavy/distributed work.", "type": "stack_overflow", "metadata": {"record_id": "4d0986b1-a2bb-4045-9249-87b5f26a05f7", "source_type": "stack_overflow", "domain": "FastAPI/Flask/Django", "library": "django", "title": "How to run background tasks in FastAPI?", "content": "FastAPI's `BackgroundTasks`: add `background_tasks: BackgroundTasks` as a parameter, then `background_tasks.add_task(func, *args)`. Runs after response is sent. For heavy workloads (email, ML inference), use Celery with Redis/RabbitMQ broker. For async background work, `asyncio.create_task()` works within async endpoints.", "code_signature": "Use BackgroundTasks for lightweight tasks or Celery for heavy/distributed work.", "tags": "routing, pydantic, dependency-injection", "url": "https://stackoverflow.com/questions/8077999", "author": "user_202401", "date_published": "14-02-2021", "votes_or_stars": 1847, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:f2c6abfe-735a-4896-98a4-7c7457e52474": {"content": "SQLAlchemy — ORM Session Query\nThe `session.query` function in SQLAlchemy constructs a SELECT query against mapped entities. It accepts `*entities` as a parameter and returns Query object. Common use cases include retrieving ORM-mapped objects from the database. Note that prefer session.execute(select(...)) in SQLAlchemy 2.0+.\nCode signature: session.query(*entities) -> Query object", "file_path": "https://docs.sqlalchemy.org/en/stable/session/query.html", "function_name": "session.query(*entities) -> Query object", "type": "documentation", "metadata": {"record_id": "f2c6abfe-735a-4896-98a4-7c7457e52474", "source_type": "documentation", "domain": "SQLAlchemy/Databases", "library": "SQLAlchemy", "title": "SQLAlchemy — ORM Session Query", "content": "The `session.query` function in SQLAlchemy constructs a SELECT query against mapped entities. It accepts `*entities` as a parameter and returns Query object. Common use cases include retrieving ORM-mapped objects from the database. Note that prefer session.execute(select(...)) in SQLAlchemy 2.0+.", "code_signature": "session.query(*entities) -> Query object", "tags": "migration, session, query, foreign-key, sqlalchemy", "url": "https://docs.sqlalchemy.org/en/stable/session/query.html", "author": "Official Docs", "date_published": "18-06-2018", "votes_or_stars": 3028, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:64e76e20-0cbf-4c07-ba35-e1ac6a1fdedb": {"content": "BUG: requests hangs indefinitely on slow server responses\nProblem: requests.get() hangs forever when server accepts connection but never sends response.\n\nFix/Discussion: Always set both connect and read timeouts: `requests.get(url, timeout=(3.05, 27))`. Tuple: (connect_timeout, read_timeout). Or single value for both. Mount a Session with default timeouts using a custom HTTPAdapter subclass. Never use `timeout=None` in production code.\nCode signature: status:closed", "file_path": "https://github.com/requests/requests/issues/4019", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "64e76e20-0cbf-4c07-ba35-e1ac6a1fdedb", "source_type": "github_issue", "domain": "Requests/HTTP/APIs", "library": "requests", "title": "BUG: requests hangs indefinitely on slow server responses", "content": "Problem: requests.get() hangs forever when server accepts connection but never sends response.\n\nFix/Discussion: Always set both connect and read timeouts: `requests.get(url, timeout=(3.05, 27))`. Tuple: (connect_timeout, read_timeout). Or single value for both. Mount a Session with default timeouts using a custom HTTPAdapter subclass. Never use `timeout=None` in production code.", "code_signature": "status:closed", "tags": "oauth, requests, retry, headers, rest-api", "url": "https://github.com/requests/requests/issues/4019", "author": "contributor_4599", "date_published": "23-12-2022", "votes_or_stars": 270, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:c2696633-5bf4-4a7e-9f3a-f3760b8470e0": {"content": "BUG: httpx: SSL certificate error on Windows despite valid cert\nProblem: httpx raises SSLCertVerificationError on Windows when system CA store has the cert.\n\nFix/Discussion: httpx uses its own CA bundle (certifi) by default, not the system store. Fix: `httpx.Client(verify=certifi.where())` — already default. For custom certs: `httpx.Client(verify='/path/to/ca-bundle.pem')`. Install `pip install truststore` and use `ssl.create_default_context(ssl.Purpose.SERVER_AUTH)` to use system store.\nCode signature: status:closed", "file_path": "https://github.com/httpx/httpx/issues/8949", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "c2696633-5bf4-4a7e-9f3a-f3760b8470e0", "source_type": "github_issue", "domain": "Requests/HTTP/APIs", "library": "httpx", "title": "BUG: httpx: SSL certificate error on Windows despite valid cert", "content": "Problem: httpx raises SSLCertVerificationError on Windows when system CA store has the cert.\n\nFix/Discussion: httpx uses its own CA bundle (certifi) by default, not the system store. Fix: `httpx.Client(verify=certifi.where())` — already default. For custom certs: `httpx.Client(verify='/path/to/ca-bundle.pem')`. Install `pip install truststore` and use `ssl.create_default_context(ssl.Purpose.SERVER_AUTH)` to use system store.", "code_signature": "status:closed", "tags": "httpx, json, rate-limiting, timeout", "url": "https://github.com/httpx/httpx/issues/8949", "author": "contributor_1420", "date_published": "27-10-2021", "votes_or_stars": 485, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:7f1b0972-6c6a-4a4c-8bdc-519b59e7ff76": {"content": "BUG: Alembic autogenerate misses index changes\nProblem: Alembic --autogenerate does not detect changes to indexes or constraints on existing tables.\n\nFix/Discussion: Alembic autogenerate compares metadata to database state but has limitations. Workarounds: (1) Manually add `op.create_index()` to migration. (2) Use `include_schemas=True` in env.py. (3) Run `alembic check` to see detected changes. (4) Always review autogenerated migrations before applying.\nCode signature: status:open", "file_path": "https://github.com/alembic/alembic/issues/7344", "function_name": "status:open", "type": "github_issue", "metadata": {"record_id": "7f1b0972-6c6a-4a4c-8bdc-519b59e7ff76", "source_type": "github_issue", "domain": "SQLAlchemy/Databases", "library": "alembic", "title": "BUG: Alembic autogenerate misses index changes", "content": "Problem: Alembic --autogenerate does not detect changes to indexes or constraints on existing tables.\n\nFix/Discussion: Alembic autogenerate compares metadata to database state but has limitations. Workarounds: (1) Manually add `op.create_index()` to migration. (2) Use `include_schemas=True` in env.py. (3) Run `alembic check` to see detected changes. (4) Always review autogenerated migrations before applying.", "code_signature": "status:open", "tags": "query, orm, sqlite, relationship, migration", "url": "https://github.com/alembic/alembic/issues/7344", "author": "contributor_3601", "date_published": "18-08-2023", "votes_or_stars": 233, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:0e336493-b495-4788-8619-59ed1c5ca1e1": {"content": "asyncio — Concurrent Coroutines\nThe `asyncio.gather` function in asyncio runs multiple coroutines concurrently and collects results. It accepts `*coros, return_exceptions=False` as a parameter and returns list of results. Common use cases include parallel I/O operations such as multiple API calls. Note that set return_exceptions=True to prevent one failure from cancelling all tasks.\nCode signature: asyncio.gather(*coros, return_exceptions=False) -> list of results", "file_path": "https://docs.asyncio.org/en/stable/asyncio/gather.html", "function_name": "asyncio.gather(*coros, return_exceptions=False) -> list of results", "type": "documentation", "metadata": {"record_id": "0e336493-b495-4788-8619-59ed1c5ca1e1", "source_type": "documentation", "domain": "Asyncio/Threading", "library": "asyncio", "title": "asyncio — Concurrent Coroutines", "content": "The `asyncio.gather` function in asyncio runs multiple coroutines concurrently and collects results. It accepts `*coros, return_exceptions=False` as a parameter and returns list of results. Common use cases include parallel I/O operations such as multiple API calls. Note that set return_exceptions=True to prevent one failure from cancelling all tasks.", "code_signature": "asyncio.gather(*coros, return_exceptions=False) -> list of results", "tags": "gather, race-condition, asyncio, deadlock", "url": "https://docs.asyncio.org/en/stable/asyncio/gather.html", "author": "Official Docs", "date_published": "14-01-2021", "votes_or_stars": 511, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:76aadb21-cbc8-4136-a78f-0bded6f10679": {"content": "BUG: os.path.join ignores prefix when component starts with /\nProblem: os.path.join('base', '/absolute') returns '/absolute', ignoring 'base'.\n\nFix/Discussion: This is documented behavior: os.path.join discards previous components when it encounters an absolute path. Fix: use `pathlib.Path(base) / component.lstrip('/')`. Validate user-provided paths with `Path(path).resolve().is_relative_to(base_dir)` to prevent path traversal.\nCode signature: status:closed", "file_path": "https://github.com/cpython/cpython/issues/7712", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "76aadb21-cbc8-4136-a78f-0bded6f10679", "source_type": "github_issue", "domain": "OS/Subprocess/File I/O", "library": "cpython", "title": "BUG: os.path.join ignores prefix when component starts with /", "content": "Problem: os.path.join('base', '/absolute') returns '/absolute', ignoring 'base'.\n\nFix/Discussion: This is documented behavior: os.path.join discards previous components when it encounters an absolute path. Fix: use `pathlib.Path(base) / component.lstrip('/')`. Validate user-provided paths with `Path(path).resolve().is_relative_to(base_dir)` to prevent path traversal.", "code_signature": "status:closed", "tags": "stderr, pathlib, shutil, file-io, env-variable", "url": "https://github.com/cpython/cpython/issues/7712", "author": "contributor_8802", "date_published": "20-08-2024", "votes_or_stars": 432, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:6d7bf2e6-e1d1-4f01-ad71-d68fa031e45a": {"content": "How to use @app.route in Flask\n`Flask.@app.route` maps a URL rule to a view function. Example usage:\n```python\n@app.route('/users/<int:uid>')\ndef get_user(uid):\n    return jsonify(user)\n```\nThis is useful when defining HTTP endpoints in a Flask application. Performance tip: use Blueprints to organize large applications.\nCode signature: @app.route", "file_path": "https://docs.flask.org/en/stable/@app/route.html", "function_name": "@app.route", "type": "documentation", "metadata": {"record_id": "6d7bf2e6-e1d1-4f01-ad71-d68fa031e45a", "source_type": "documentation", "domain": "FastAPI/Flask/Django", "library": "Flask", "title": "How to use @app.route in Flask", "content": "`Flask.@app.route` maps a URL rule to a view function. Example usage:\n```python\n@app.route('/users/<int:uid>')\ndef get_user(uid):\n    return jsonify(user)\n```\nThis is useful when defining HTTP endpoints in a Flask application. Performance tip: use Blueprints to organize large applications.", "code_signature": "@app.route", "tags": "middleware, template, rest, django, flask, pydantic", "url": "https://docs.flask.org/en/stable/@app/route.html", "author": "Official Docs", "date_published": "26-03-2018", "votes_or_stars": 3036, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:339ee492-20c7-4f63-96fd-5271b24cc977": {"content": "BUG: subprocess.Popen hangs when stdout and stderr buffers fill\nProblem: subprocess.Popen hangs indefinitely when child process produces large output.\n\nFix/Discussion: This is a classic deadlock: parent waits for child, child waits for parent to read buffer. Fix: use `subprocess.run(..., capture_output=True)` which handles this correctly. Or: `stdout, stderr = proc.communicate()`. Never use `proc.stdout.read()` and `proc.wait()` separately.\nCode signature: status:closed", "file_path": "https://github.com/cpython/cpython/issues/8586", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "339ee492-20c7-4f63-96fd-5271b24cc977", "source_type": "github_issue", "domain": "OS/Subprocess/File I/O", "library": "cpython", "title": "BUG: subprocess.Popen hangs when stdout and stderr buffers fill", "content": "Problem: subprocess.Popen hangs indefinitely when child process produces large output.\n\nFix/Discussion: This is a classic deadlock: parent waits for child, child waits for parent to read buffer. Fix: use `subprocess.run(..., capture_output=True)` which handles this correctly. Or: `stdout, stderr = proc.communicate()`. Never use `proc.stdout.read()` and `proc.wait()` separately.", "code_signature": "status:closed", "tags": "env-variable, pipe, os, process, stderr, shutil", "url": "https://github.com/cpython/cpython/issues/8586", "author": "contributor_7711", "date_published": "22-09-2023", "votes_or_stars": 414, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:981a525d-48ac-4f01-9413-65ee2bdb2f1a": {"content": "How to use @app.route in Flask\n`Flask.@app.route` maps a URL rule to a view function. Example usage:\n```python\n@app.route('/users/<int:uid>')\ndef get_user(uid):\n    return jsonify(user)\n```\nThis is useful when defining HTTP endpoints in a Flask application. Performance tip: use Blueprints to organize large applications.\nCode signature: @app.route", "file_path": "https://docs.flask.org/en/stable/@app/route.html", "function_name": "@app.route", "type": "documentation", "metadata": {"record_id": "981a525d-48ac-4f01-9413-65ee2bdb2f1a", "source_type": "documentation", "domain": "FastAPI/Flask/Django", "library": "Flask", "title": "How to use @app.route in Flask", "content": "`Flask.@app.route` maps a URL rule to a view function. Example usage:\n```python\n@app.route('/users/<int:uid>')\ndef get_user(uid):\n    return jsonify(user)\n```\nThis is useful when defining HTTP endpoints in a Flask application. Performance tip: use Blueprints to organize large applications.", "code_signature": "@app.route", "tags": "pydantic, routing, dependency-injection, response", "url": "https://docs.flask.org/en/stable/@app/route.html", "author": "Official Docs", "date_published": "12-08-2023", "votes_or_stars": 2995, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:e536dbf5-901a-41eb-8418-85776ab76ef0": {"content": "SQLAlchemy: how to avoid DetachedInstanceError?\nDetachedInstanceError occurs when accessing a relationship after the session is closed. Solutions: (1) Use `joinedload()` or `subqueryload()` to eagerly load relationships. (2) Keep session open while accessing objects. (3) Use `expire_on_commit=False` on sessionmaker. (4) Convert to dict/DTO before closing session.\nCode signature: Access lazy-loaded relationships within the session scope, or use eager loading.", "file_path": "https://stackoverflow.com/questions/1247595", "function_name": "Access lazy-loaded relationships within the session scope, or use eager loading.", "type": "stack_overflow", "metadata": {"record_id": "e536dbf5-901a-41eb-8418-85776ab76ef0", "source_type": "stack_overflow", "domain": "SQLAlchemy/Databases", "library": "sqlalchemy", "title": "SQLAlchemy: how to avoid DetachedInstanceError?", "content": "DetachedInstanceError occurs when accessing a relationship after the session is closed. Solutions: (1) Use `joinedload()` or `subqueryload()` to eagerly load relationships. (2) Keep session open while accessing objects. (3) Use `expire_on_commit=False` on sessionmaker. (4) Convert to dict/DTO before closing session.", "code_signature": "Access lazy-loaded relationships within the session scope, or use eager loading.", "tags": "relationship, foreign-key, connection-pool, sqlite, session", "url": "https://stackoverflow.com/questions/1247595", "author": "user_107793", "date_published": "21-01-2024", "votes_or_stars": 401, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:718f4a82-43bd-46d0-9b76-5f8bdc8e61a0": {"content": "How to use Path.glob in pathlib\n`pathlib.Path.glob` yields all paths matching the given glob pattern. Example usage:\n```python\npy_files = list(Path('src').glob('**/*.py'))\n```\nThis is useful when finding files recursively, batch file processing. Performance tip: iterate the generator lazily rather than converting to list for large directories.\nCode signature: Path.glob", "file_path": "https://docs.pathlib.org/en/stable/Path/glob.html", "function_name": "Path.glob", "type": "documentation", "metadata": {"record_id": "718f4a82-43bd-46d0-9b76-5f8bdc8e61a0", "source_type": "documentation", "domain": "OS/Subprocess/File I/O", "library": "pathlib", "title": "How to use Path.glob in pathlib", "content": "`pathlib.Path.glob` yields all paths matching the given glob pattern. Example usage:\n```python\npy_files = list(Path('src').glob('**/*.py'))\n```\nThis is useful when finding files recursively, batch file processing. Performance tip: iterate the generator lazily rather than converting to list for large directories.", "code_signature": "Path.glob", "tags": "shutil, stdout, tempfile, pipe, subprocess", "url": "https://docs.pathlib.org/en/stable/Path/glob.html", "author": "Official Docs", "date_published": "21-03-2022", "votes_or_stars": 3198, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:52058554-411c-4ed6-8d67-4cc0e860e362": {"content": "BUG: threading.Timer not cancellable after it fires\nProblem: Calling cancel() on a Timer that has already executed does not raise an error but has no effect.\n\nFix/Discussion: threading.Timer.cancel() only works before the timer fires. Store Timer reference and check `timer.finished.is_set()` to know if it's already run. For repeating timers, use a loop with `threading.Event` for clean cancellation: `stop_event = threading.Event(); stop_event.wait(interval)`.\nCode signature: status:closed", "file_path": "https://github.com/cpython/cpython/issues/4111", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "52058554-411c-4ed6-8d67-4cc0e860e362", "source_type": "github_issue", "domain": "Asyncio/Threading", "library": "cpython", "title": "BUG: threading.Timer not cancellable after it fires", "content": "Problem: Calling cancel() on a Timer that has already executed does not raise an error but has no effect.\n\nFix/Discussion: threading.Timer.cancel() only works before the timer fires. Store Timer reference and check `timer.finished.is_set()` to know if it's already run. For repeating timers, use a loop with `threading.Event` for clean cancellation: `stop_event = threading.Event(); stop_event.wait(interval)`.", "code_signature": "status:closed", "tags": "task, race-condition, coroutine, asyncio", "url": "https://github.com/cpython/cpython/issues/4111", "author": "contributor_7884", "date_published": "07-07-2020", "votes_or_stars": 45, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:0bc30cf8-6684-461f-b560-530b63919629": {"content": "subprocess — Process Execution\nThe `subprocess.run` function in subprocess runs a subprocess and waits for it to complete. It accepts `args, capture_output=False, timeout=None` as a parameter and returns CompletedProcess. Common use cases include executing shell commands, scripts, or external programs from Python. Note that use shell=False (default) to avoid shell injection vulnerabilities.\nCode signature: subprocess.run(args, capture_output=False, timeout=None) -> CompletedProcess", "file_path": "https://docs.subprocess.org/en/stable/subprocess/run.html", "function_name": "subprocess.run(args, capture_output=False, timeout=None) -> CompletedProcess", "type": "documentation", "metadata": {"record_id": "0bc30cf8-6684-461f-b560-530b63919629", "source_type": "documentation", "domain": "OS/Subprocess/File I/O", "library": "subprocess", "title": "subprocess — Process Execution", "content": "The `subprocess.run` function in subprocess runs a subprocess and waits for it to complete. It accepts `args, capture_output=False, timeout=None` as a parameter and returns CompletedProcess. Common use cases include executing shell commands, scripts, or external programs from Python. Note that use shell=False (default) to avoid shell injection vulnerabilities.", "code_signature": "subprocess.run(args, capture_output=False, timeout=None) -> CompletedProcess", "tags": "shutil, stderr, file-io", "url": "https://docs.subprocess.org/en/stable/subprocess/run.html", "author": "Official Docs", "date_published": "10-09-2022", "votes_or_stars": 416, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:da146b0c-402f-4667-ac31-e4215b423f2d": {"content": "How do I efficiently fill NaN values in a Pandas DataFrame?\nFor large DataFrames, avoid looping. Use `df.fillna({'col': val})` to fill specific columns. `df.ffill()` propagates last valid observation forward. For time-series data, `df.interpolate()` provides smoother results. Always check `df.isna().sum()` before and after to verify.\nCode signature: Use df.fillna(value) or df.ffill()/df.bfill() for forward/backward fill.", "file_path": "https://stackoverflow.com/questions/6438436", "function_name": "Use df.fillna(value) or df.ffill()/df.bfill() for forward/backward fill.", "type": "stack_overflow", "metadata": {"record_id": "da146b0c-402f-4667-ac31-e4215b423f2d", "source_type": "stack_overflow", "domain": "NumPy/Pandas", "library": "loc", "title": "How do I efficiently fill NaN values in a Pandas DataFrame?", "content": "For large DataFrames, avoid looping. Use `df.fillna({'col': val})` to fill specific columns. `df.ffill()` propagates last valid observation forward. For time-series data, `df.interpolate()` provides smoother results. Always check `df.isna().sum()` before and after to verify.", "code_signature": "Use df.fillna(value) or df.ffill()/df.bfill() for forward/backward fill.", "tags": "reshape, groupby, array, vectorization", "url": "https://stackoverflow.com/questions/6438436", "author": "user_522340", "date_published": "14-07-2019", "votes_or_stars": 1, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:9becb431-7255-4777-a5e7-b65e56021a4c": {"content": "BUG: SettingWithCopyWarning when modifying sliced DataFrame\nProblem: Modifying a slice of a DataFrame raises SettingWithCopyWarning and may not update original.\n\nFix/Discussion: Always use .loc or .copy() when slicing. Use `df.loc[mask, 'col'] = val` for assignment. Create explicit copies: `subset = df[mask].copy()`. In Pandas 2.0+, Copy-on-Write makes this behavior consistent — enable with `pd.options.mode.copy_on_write = True`.\nCode signature: status:closed", "file_path": "https://github.com/pandas/pandas/issues/3264", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "9becb431-7255-4777-a5e7-b65e56021a4c", "source_type": "github_issue", "domain": "NumPy/Pandas", "library": "pandas", "title": "BUG: SettingWithCopyWarning when modifying sliced DataFrame", "content": "Problem: Modifying a slice of a DataFrame raises SettingWithCopyWarning and may not update original.\n\nFix/Discussion: Always use .loc or .copy() when slicing. Use `df.loc[mask, 'col'] = val` for assignment. Create explicit copies: `subset = df[mask].copy()`. In Pandas 2.0+, Copy-on-Write makes this behavior consistent — enable with `pd.options.mode.copy_on_write = True`.", "code_signature": "status:closed", "tags": "numpy, broadcasting, loc, pivot, groupby", "url": "https://github.com/pandas/pandas/issues/3264", "author": "contributor_6628", "date_published": "18-08-2023", "votes_or_stars": 4, "relevance_label": "low", "difficulty_level": "intermediate"}}, "kb:73b119bf-0e48-4640-9411-748c031650f3": {"content": "NumPy — Linear Algebra\nThe `np.einsum` function in NumPy evaluates Einstein summation convention on operands. It accepts `subscripts, *operands` as a parameter and returns ndarray. Common use cases include matrix multiplication, dot products, tensor contractions. Note that prefer np.einsum over loops for large arrays.\nCode signature: np.einsum(subscripts, *operands) -> ndarray", "file_path": "https://docs.numpy.org/en/stable/np/einsum.html", "function_name": "np.einsum(subscripts, *operands) -> ndarray", "type": "documentation", "metadata": {"record_id": "73b119bf-0e48-4640-9411-748c031650f3", "source_type": "documentation", "domain": "NumPy/Pandas", "library": "NumPy", "title": "NumPy — Linear Algebra", "content": "The `np.einsum` function in NumPy evaluates Einstein summation convention on operands. It accepts `subscripts, *operands` as a parameter and returns ndarray. Common use cases include matrix multiplication, dot products, tensor contractions. Note that prefer np.einsum over loops for large arrays.", "code_signature": "np.einsum(subscripts, *operands) -> ndarray", "tags": "groupby, numpy, dtype", "url": "https://docs.numpy.org/en/stable/np/einsum.html", "author": "Official Docs", "date_published": "07-08-2022", "votes_or_stars": 1849, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:94ea3126-46ab-4fd4-abbe-9fe975ec2947": {"content": "BUG: httpx: SSL certificate error on Windows despite valid cert\nProblem: httpx raises SSLCertVerificationError on Windows when system CA store has the cert.\n\nFix/Discussion: httpx uses its own CA bundle (certifi) by default, not the system store. Fix: `httpx.Client(verify=certifi.where())` — already default. For custom certs: `httpx.Client(verify='/path/to/ca-bundle.pem')`. Install `pip install truststore` and use `ssl.create_default_context(ssl.Purpose.SERVER_AUTH)` to use system store.\nCode signature: status:closed", "file_path": "https://github.com/httpx/httpx/issues/8949", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "94ea3126-46ab-4fd4-abbe-9fe975ec2947", "source_type": "github_issue", "domain": "Requests/HTTP/APIs", "library": "httpx", "title": "BUG: httpx: SSL certificate error on Windows despite valid cert", "content": "Problem: httpx raises SSLCertVerificationError on Windows when system CA store has the cert.\n\nFix/Discussion: httpx uses its own CA bundle (certifi) by default, not the system store. Fix: `httpx.Client(verify=certifi.where())` — already default. For custom certs: `httpx.Client(verify='/path/to/ca-bundle.pem')`. Install `pip install truststore` and use `ssl.create_default_context(ssl.Purpose.SERVER_AUTH)` to use system store.", "code_signature": "status:closed", "tags": "rest-api, http, websocket, authentication, json", "url": "https://github.com/httpx/httpx/issues/8949", "author": "contributor_1420", "date_published": "20-07-2020", "votes_or_stars": 462, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:4168ffbc-49bd-4c3e-b86d-5509b590f546": {"content": "threading Thread Synchronization — official reference\nOfficial documentation for threading Thread Synchronization. The method signature is `threading.Lock()`. creates a mutual-exclusion lock for thread-safe access. Raises `RuntimeError` when lock is released without being acquired. See also: threading.RLock, threading.Semaphore, threading.Event.\nCode signature: threading.Lock()", "file_path": "https://docs.threading.org/en/stable/threading/Lock.html", "function_name": "threading.Lock()", "type": "documentation", "metadata": {"record_id": "4168ffbc-49bd-4c3e-b86d-5509b590f546", "source_type": "documentation", "domain": "Asyncio/Threading", "library": "threading", "title": "threading Thread Synchronization — official reference", "content": "Official documentation for threading Thread Synchronization. The method signature is `threading.Lock()`. creates a mutual-exclusion lock for thread-safe access. Raises `RuntimeError` when lock is released without being acquired. See also: threading.RLock, threading.Semaphore, threading.Event.", "code_signature": "threading.Lock()", "tags": "await, gather, deadlock", "url": "https://docs.threading.org/en/stable/threading/Lock.html", "author": "Official Docs", "date_published": "01-12-2023", "votes_or_stars": 4863, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:23a5bcd3-c281-4479-8772-dc4cc6b7d302": {"content": "How to run asyncio code from synchronous Python?\n`asyncio.run(main())` creates an event loop, runs the coroutine, closes the loop. Never call asyncio.run() from inside a running event loop (use await instead). In Jupyter notebooks (which have a running loop), use `await coroutine()` directly or `nest_asyncio.apply()`. For mixing sync/async, use `loop.run_until_complete()`.\nCode signature: Use asyncio.run() in Python 3.7+ to execute a coroutine from sync code.", "file_path": "https://stackoverflow.com/questions/2140194", "function_name": "Use asyncio.run() in Python 3.7+ to execute a coroutine from sync code.", "type": "stack_overflow", "metadata": {"record_id": "23a5bcd3-c281-4479-8772-dc4cc6b7d302", "source_type": "stack_overflow", "domain": "Asyncio/Threading", "library": "gather", "title": "How to run asyncio code from synchronous Python?", "content": "`asyncio.run(main())` creates an event loop, runs the coroutine, closes the loop. Never call asyncio.run() from inside a running event loop (use await instead). In Jupyter notebooks (which have a running loop), use `await coroutine()` directly or `nest_asyncio.apply()`. For mixing sync/async, use `loop.run_until_complete()`.", "code_signature": "Use asyncio.run() in Python 3.7+ to execute a coroutine from sync code.", "tags": "threading, executor, async, deadlock", "url": "https://stackoverflow.com/questions/2140194", "author": "user_718011", "date_published": "06-11-2022", "votes_or_stars": 307, "relevance_label": "low", "difficulty_level": "intermediate"}}, "kb:f4cfe2d2-b9ab-4219-9607-5342ab8afc69": {"content": "PERF: concurrent.futures.ProcessPoolExecutor slow startup on Windows\nProblem: ProcessPoolExecutor takes 3-5 seconds to start on Windows due to process spawning overhead.\n\nFix/Discussion: Windows uses 'spawn' start method (vs 'fork' on Linux). Mitigations: (1) Reuse executor across calls — don't create/destroy in loops. (2) Use 'forkserver' or 'fork' on Linux/macOS. (3) For short tasks, ThreadPoolExecutor may be faster. (4) Use `initializer` param to pre-load modules.\nCode signature: status:closed", "file_path": "https://github.com/cpython/cpython/issues/6647", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "f4cfe2d2-b9ab-4219-9607-5342ab8afc69", "source_type": "github_issue", "domain": "Asyncio/Threading", "library": "cpython", "title": "PERF: concurrent.futures.ProcessPoolExecutor slow startup on Windows", "content": "Problem: ProcessPoolExecutor takes 3-5 seconds to start on Windows due to process spawning overhead.\n\nFix/Discussion: Windows uses 'spawn' start method (vs 'fork' on Linux). Mitigations: (1) Reuse executor across calls — don't create/destroy in loops. (2) Use 'forkserver' or 'fork' on Linux/macOS. (3) For short tasks, ThreadPoolExecutor may be faster. (4) Use `initializer` param to pre-load modules.", "code_signature": "status:closed", "tags": "coroutine, task, async, await, executor", "url": "https://github.com/cpython/cpython/issues/6647", "author": "contributor_4097", "date_published": "25-03-2022", "votes_or_stars": 264, "relevance_label": "low", "difficulty_level": "beginner"}}, "kb:c34c0454-109b-4e15-bd7d-0b1a7df40f9b": {"content": "PERF: requests connection not reused between calls — new TCP connection each time\nProblem: Each requests.get() call opens a new TCP connection, causing high latency.\n\nFix/Discussion: requests.get/post etc. create a new Session per call. Fix: use a persistent Session: `session = requests.Session()` and reuse it. Sessions pool connections via urllib3. For thread-safety: create one Session per thread. For async: use httpx.AsyncClient or aiohttp.ClientSession for async connection pooling.\nCode signature: status:closed", "file_path": "https://github.com/requests/requests/issues/3605", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "c34c0454-109b-4e15-bd7d-0b1a7df40f9b", "source_type": "github_issue", "domain": "Requests/HTTP/APIs", "library": "requests", "title": "PERF: requests connection not reused between calls — new TCP connection each time", "content": "Problem: Each requests.get() call opens a new TCP connection, causing high latency.\n\nFix/Discussion: requests.get/post etc. create a new Session per call. Fix: use a persistent Session: `session = requests.Session()` and reuse it. Sessions pool connections via urllib3. For thread-safety: create one Session per thread. For async: use httpx.AsyncClient or aiohttp.ClientSession for async connection pooling.", "code_signature": "status:closed", "tags": "http, timeout, httpx", "url": "https://github.com/requests/requests/issues/3605", "author": "contributor_1152", "date_published": "16-10-2020", "votes_or_stars": 360, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:c299e4e5-9ca5-4d8b-bce1-f51c421c7410": {"content": "Django N+1 query problem: how to fix?\nN+1 occurs when accessing related objects in a loop. Diagnose with `django-debug-toolbar` or `connection.queries`. Fix ForeignKey: `User.objects.select_related('profile').all()`. Fix M2M: `Post.objects.prefetch_related('tags').all()`. Use `only()` to fetch specific fields and reduce data transfer.\nCode signature: Use select_related() for ForeignKey and prefetch_related() for ManyToMany relationships.", "file_path": "https://stackoverflow.com/questions/8106470", "function_name": "Use select_related() for ForeignKey and prefetch_related() for ManyToMany relationships.", "type": "stack_overflow", "metadata": {"record_id": "c299e4e5-9ca5-4d8b-bce1-f51c421c7410", "source_type": "stack_overflow", "domain": "FastAPI/Flask/Django", "library": "request", "title": "Django N+1 query problem: how to fix?", "content": "N+1 occurs when accessing related objects in a loop. Diagnose with `django-debug-toolbar` or `connection.queries`. Fix ForeignKey: `User.objects.select_related('profile').all()`. Fix M2M: `Post.objects.prefetch_related('tags').all()`. Use `only()` to fetch specific fields and reduce data transfer.", "code_signature": "Use select_related() for ForeignKey and prefetch_related() for ManyToMany relationships.", "tags": "dependency-injection, flask, template", "url": "https://stackoverflow.com/questions/8106470", "author": "user_441071", "date_published": "17-04-2018", "votes_or_stars": 1783, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:e6c0fc9f-8351-49f9-a150-c875c756416f": {"content": "How do I efficiently fill NaN values in a Pandas DataFrame?\nFor large DataFrames, avoid looping. Use `df.fillna({'col': val})` to fill specific columns. `df.ffill()` propagates last valid observation forward. For time-series data, `df.interpolate()` provides smoother results. Always check `df.isna().sum()` before and after to verify.\nCode signature: Use df.fillna(value) or df.ffill()/df.bfill() for forward/backward fill.", "file_path": "https://stackoverflow.com/questions/6438436", "function_name": "Use df.fillna(value) or df.ffill()/df.bfill() for forward/backward fill.", "type": "stack_overflow", "metadata": {"record_id": "e6c0fc9f-8351-49f9-a150-c875c756416f", "source_type": "stack_overflow", "domain": "NumPy/Pandas", "library": "loc", "title": "How do I efficiently fill NaN values in a Pandas DataFrame?", "content": "For large DataFrames, avoid looping. Use `df.fillna({'col': val})` to fill specific columns. `df.ffill()` propagates last valid observation forward. For time-series data, `df.interpolate()` provides smoother results. Always check `df.isna().sum()` before and after to verify.", "code_signature": "Use df.fillna(value) or df.ffill()/df.bfill() for forward/backward fill.", "tags": "reshape, dataframe, iloc, nan", "url": "https://stackoverflow.com/questions/6438436", "author": "user_522340", "date_published": "21-03-2018", "votes_or_stars": 12, "relevance_label": "low", "difficulty_level": "beginner"}}, "kb:5dec7d56-e83b-400f-9714-4fb804f33969": {"content": "BUG: os.path.join ignores prefix when component starts with /\nProblem: os.path.join('base', '/absolute') returns '/absolute', ignoring 'base'.\n\nFix/Discussion: This is documented behavior: os.path.join discards previous components when it encounters an absolute path. Fix: use `pathlib.Path(base) / component.lstrip('/')`. Validate user-provided paths with `Path(path).resolve().is_relative_to(base_dir)` to prevent path traversal.\nCode signature: status:closed", "file_path": "https://github.com/cpython/cpython/issues/7712", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "5dec7d56-e83b-400f-9714-4fb804f33969", "source_type": "github_issue", "domain": "OS/Subprocess/File I/O", "library": "cpython", "title": "BUG: os.path.join ignores prefix when component starts with /", "content": "Problem: os.path.join('base', '/absolute') returns '/absolute', ignoring 'base'.\n\nFix/Discussion: This is documented behavior: os.path.join discards previous components when it encounters an absolute path. Fix: use `pathlib.Path(base) / component.lstrip('/')`. Validate user-provided paths with `Path(path).resolve().is_relative_to(base_dir)` to prevent path traversal.", "code_signature": "status:closed", "tags": "os, process, tempfile, stderr", "url": "https://github.com/cpython/cpython/issues/7712", "author": "contributor_8802", "date_published": "26-09-2018", "votes_or_stars": 440, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:6d195762-3a16-48c4-9af8-9c438c94e2d1": {"content": "PERF: groupby().apply() extremely slow on large DataFrames\nProblem: groupby().apply() with a custom function is 100x slower than expected on 10M row DataFrames.\n\nFix/Discussion: groupby().apply() serializes each group and has high overhead. Fix: vectorize with numpy, use groupby().transform() for same-shape output, or groupby().agg() with named aggregations. For complex logic, consider Polars or Dask. Numba's @jit can accelerate custom group functions.\nCode signature: status:closed", "file_path": "https://github.com/pandas/pandas/issues/8446", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "6d195762-3a16-48c4-9af8-9c438c94e2d1", "source_type": "github_issue", "domain": "NumPy/Pandas", "library": "pandas", "title": "PERF: groupby().apply() extremely slow on large DataFrames", "content": "Problem: groupby().apply() with a custom function is 100x slower than expected on 10M row DataFrames.\n\nFix/Discussion: groupby().apply() serializes each group and has high overhead. Fix: vectorize with numpy, use groupby().transform() for same-shape output, or groupby().agg() with named aggregations. For complex logic, consider Polars or Dask. Numba's @jit can accelerate custom group functions.", "code_signature": "status:closed", "tags": "array, vectorization, pivot, iloc", "url": "https://github.com/pandas/pandas/issues/8446", "author": "contributor_6648", "date_published": "24-08-2022", "votes_or_stars": 311, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:e3e42765-9aaa-4e13-a86a-0dcde8aa047b": {"content": "os Environment Variables — official reference\nOfficial documentation for os Environment Variables. The method signature is `os.environ(key)`. provides a mapping of the current environment variables. Raises `KeyError` when key is accessed with [] notation and is not set. See also: dotenv, os.getenv, os.putenv.\nCode signature: os.environ(key)", "file_path": "https://docs.os.org/en/stable/os/environ.html", "function_name": "os.environ(key)", "type": "documentation", "metadata": {"record_id": "e3e42765-9aaa-4e13-a86a-0dcde8aa047b", "source_type": "documentation", "domain": "OS/Subprocess/File I/O", "library": "os", "title": "os Environment Variables — official reference", "content": "Official documentation for os Environment Variables. The method signature is `os.environ(key)`. provides a mapping of the current environment variables. Raises `KeyError` when key is accessed with [] notation and is not set. See also: dotenv, os.getenv, os.putenv.", "code_signature": "os.environ(key)", "tags": "stderr, file-io, pipe, subprocess, os, pathlib", "url": "https://docs.os.org/en/stable/os/environ.html", "author": "Official Docs", "date_published": "28-03-2018", "votes_or_stars": 2244, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:6b85932f-8883-47dd-8c71-81d5ed2d2785": {"content": "How to read a large file line by line in Python without loading it all into memory?\n```python\nwith open('large.txt', 'r') as f:\n    for line in f:\n        process(line.strip())\n```\nThis reads one line at a time. For binary files: use `f.read(chunk_size)` in a loop. For CSV: use `pd.read_csv(file, chunksize=10000)`. Never use `f.readlines()` on large files.\nCode signature: Iterate over the file object directly — it yields lines lazily.", "file_path": "https://stackoverflow.com/questions/2737893", "function_name": "Iterate over the file object directly — it yields lines lazily.", "type": "stack_overflow", "metadata": {"record_id": "6b85932f-8883-47dd-8c71-81d5ed2d2785", "source_type": "stack_overflow", "domain": "OS/Subprocess/File I/O", "library": "shutil", "title": "How to read a large file line by line in Python without loading it all into memory?", "content": "```python\nwith open('large.txt', 'r') as f:\n    for line in f:\n        process(line.strip())\n```\nThis reads one line at a time. For binary files: use `f.read(chunk_size)` in a loop. For CSV: use `pd.read_csv(file, chunksize=10000)`. Never use `f.readlines()` on large files.", "code_signature": "Iterate over the file object directly — it yields lines lazily.", "tags": "glob, process, tempfile, subprocess, stderr, pipe", "url": "https://stackoverflow.com/questions/2737893", "author": "user_994539", "date_published": "02-03-2020", "votes_or_stars": 2262, "relevance_label": "low", "difficulty_level": "advanced"}}, "kb:0833662c-5fe4-43aa-8604-5f41f303eaa8": {"content": "How to apply a function to each row in Pandas without using iterrows?\n`iterrows()` is slow because it creates a Series per row. Prefer `df.apply(func, axis=1)` for row-wise operations, or better yet, vectorize using NumPy: `np.where(condition, a, b)`. For complex logic, `df.assign()` with vectorized expressions is cleanest. Benchmark: vectorized > apply > itertuples > iterrows.\nCode signature: Use df.apply(func, axis=1) or vectorized NumPy operations for best performance.", "file_path": "https://stackoverflow.com/questions/5446912", "function_name": "Use df.apply(func, axis=1) or vectorized NumPy operations for best performance.", "type": "stack_overflow", "metadata": {"record_id": "0833662c-5fe4-43aa-8604-5f41f303eaa8", "source_type": "stack_overflow", "domain": "NumPy/Pandas", "library": "dataframe", "title": "How to apply a function to each row in Pandas without using iterrows?", "content": "`iterrows()` is slow because it creates a Series per row. Prefer `df.apply(func, axis=1)` for row-wise operations, or better yet, vectorize using NumPy: `np.where(condition, a, b)`. For complex logic, `df.assign()` with vectorized expressions is cleanest. Benchmark: vectorized > apply > itertuples > iterrows.", "code_signature": "Use df.apply(func, axis=1) or vectorized NumPy operations for best performance.", "tags": "merge, groupby, numpy, pandas", "url": "https://stackoverflow.com/questions/5446912", "author": "user_563306", "date_published": "20-12-2021", "votes_or_stars": 2237, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:a9f58aa6-7314-475f-8734-17c0add000d8": {"content": "How to use asyncio.create_task in asyncio\n`asyncio.asyncio.create_task` schedules a coroutine as a Task in the event loop. Example usage:\n```python\ntask = asyncio.create_task(send_email(user))\nawait task\n```\nThis is useful when fire-and-forget background tasks, concurrent task management. Performance tip: use TaskGroup (Python 3.11+) for structured concurrency.\nCode signature: asyncio.create_task", "file_path": "https://docs.asyncio.org/en/stable/asyncio/create_task.html", "function_name": "asyncio.create_task", "type": "documentation", "metadata": {"record_id": "a9f58aa6-7314-475f-8734-17c0add000d8", "source_type": "documentation", "domain": "Asyncio/Threading", "library": "asyncio", "title": "How to use asyncio.create_task in asyncio", "content": "`asyncio.asyncio.create_task` schedules a coroutine as a Task in the event loop. Example usage:\n```python\ntask = asyncio.create_task(send_email(user))\nawait task\n```\nThis is useful when fire-and-forget background tasks, concurrent task management. Performance tip: use TaskGroup (Python 3.11+) for structured concurrency.", "code_signature": "asyncio.create_task", "tags": "gather, executor, lock, threading", "url": "https://docs.asyncio.org/en/stable/asyncio/create_task.html", "author": "Official Docs", "date_published": "19-10-2020", "votes_or_stars": 4142, "relevance_label": "low", "difficulty_level": "advanced"}}, "kb:bd04811b-428e-490b-9a82-8e43d4f36db3": {"content": "asyncio RuntimeError: This event loop is already running\nThis error is common in Jupyter and FastAPI. Solutions: (1) In async context: just use `await coroutine()`. (2) In Jupyter: `import nest_asyncio; nest_asyncio.apply()`. (3) Run in thread: `asyncio.get_event_loop().run_in_executor(None, sync_func)`. (4) Use `asyncio.ensure_future()` to schedule from within async code.\nCode signature: You're calling asyncio.run() inside an already running event loop. Use await instead.", "file_path": "https://stackoverflow.com/questions/2229110", "function_name": "You're calling asyncio.run() inside an already running event loop. Use await instead.", "type": "stack_overflow", "metadata": {"record_id": "bd04811b-428e-490b-9a82-8e43d4f36db3", "source_type": "stack_overflow", "domain": "Asyncio/Threading", "library": "async", "title": "asyncio RuntimeError: This event loop is already running", "content": "This error is common in Jupyter and FastAPI. Solutions: (1) In async context: just use `await coroutine()`. (2) In Jupyter: `import nest_asyncio; nest_asyncio.apply()`. (3) Run in thread: `asyncio.get_event_loop().run_in_executor(None, sync_func)`. (4) Use `asyncio.ensure_future()` to schedule from within async code.", "code_signature": "You're calling asyncio.run() inside an already running event loop. Use await instead.", "tags": "async, await, coroutine, asyncio, threading", "url": "https://stackoverflow.com/questions/2229110", "author": "user_573750", "date_published": "18-03-2018", "votes_or_stars": 2299, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:55107fda-4298-4f79-a65e-3d0428e424f0": {"content": "How to add retry logic to requests?\n```python\nfrom requests.adapters import HTTPAdapter\nfrom urllib3.util.retry import Retry\nretry = Retry(total=3, backoff_factor=1, status_forcelist=[500,502,503,504])\nadapter = HTTPAdapter(max_retries=retry)\nsession.mount('https://', adapter)\n```\nCode signature: Mount an HTTPAdapter with urllib3 Retry on the Session.", "file_path": "https://stackoverflow.com/questions/1669324", "function_name": "Mount an HTTPAdapter with urllib3 Retry on the Session.", "type": "stack_overflow", "metadata": {"record_id": "55107fda-4298-4f79-a65e-3d0428e424f0", "source_type": "stack_overflow", "domain": "Requests/HTTP/APIs", "library": "websocket", "title": "How to add retry logic to requests?", "content": "```python\nfrom requests.adapters import HTTPAdapter\nfrom urllib3.util.retry import Retry\nretry = Retry(total=3, backoff_factor=1, status_forcelist=[500,502,503,504])\nadapter = HTTPAdapter(max_retries=retry)\nsession.mount('https://', adapter)\n```", "code_signature": "Mount an HTTPAdapter with urllib3 Retry on the Session.", "tags": "http, requests, httpx, retry, timeout", "url": "https://stackoverflow.com/questions/1669324", "author": "user_952590", "date_published": "10-03-2020", "votes_or_stars": 1510, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:d54d494c-faea-402f-8626-4b996e8929f9": {"content": "How to handle rate limiting with the requests library?\n```python\nfrom tenacity import retry, wait_exponential, stop_after_attempt\n@retry(wait=wait_exponential(min=1, max=60), stop=stop_after_attempt(5))\ndef call_api(url):\n    r = requests.get(url)\n    r.raise_for_status()\n    return r.json()\n```\nCheck `Retry-After` header in 429 responses. For async: use `aiohttp` with tenacity.\nCode signature: Implement exponential backoff with the tenacity or backoff library.", "file_path": "https://stackoverflow.com/questions/1604451", "function_name": "Implement exponential backoff with the tenacity or backoff library.", "type": "stack_overflow", "metadata": {"record_id": "d54d494c-faea-402f-8626-4b996e8929f9", "source_type": "stack_overflow", "domain": "Requests/HTTP/APIs", "library": "retry", "title": "How to handle rate limiting with the requests library?", "content": "```python\nfrom tenacity import retry, wait_exponential, stop_after_attempt\n@retry(wait=wait_exponential(min=1, max=60), stop=stop_after_attempt(5))\ndef call_api(url):\n    r = requests.get(url)\n    r.raise_for_status()\n    return r.json()\n```\nCheck `Retry-After` header in 429 responses. For async: use `aiohttp` with tenacity.", "code_signature": "Implement exponential backoff with the tenacity or backoff library.", "tags": "aiohttp, httpx, headers, retry, rest-api", "url": "https://stackoverflow.com/questions/1604451", "author": "user_885136", "date_published": "16-10-2020", "votes_or_stars": 604, "relevance_label": "low", "difficulty_level": "advanced"}}, "kb:b1b8d36f-2d80-440c-9a33-d428b2cff0d9": {"content": "How to use Path.glob in pathlib\n`pathlib.Path.glob` yields all paths matching the given glob pattern. Example usage:\n```python\npy_files = list(Path('src').glob('**/*.py'))\n```\nThis is useful when finding files recursively, batch file processing. Performance tip: iterate the generator lazily rather than converting to list for large directories.\nCode signature: Path.glob", "file_path": "https://docs.pathlib.org/en/stable/Path/glob.html", "function_name": "Path.glob", "type": "documentation", "metadata": {"record_id": "b1b8d36f-2d80-440c-9a33-d428b2cff0d9", "source_type": "documentation", "domain": "OS/Subprocess/File I/O", "library": "pathlib", "title": "How to use Path.glob in pathlib", "content": "`pathlib.Path.glob` yields all paths matching the given glob pattern. Example usage:\n```python\npy_files = list(Path('src').glob('**/*.py'))\n```\nThis is useful when finding files recursively, batch file processing. Performance tip: iterate the generator lazily rather than converting to list for large directories.", "code_signature": "Path.glob", "tags": "subprocess, os, env-variable, tempfile", "url": "https://docs.pathlib.org/en/stable/Path/glob.html", "author": "Official Docs", "date_published": "13-07-2021", "votes_or_stars": 3173, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:99723715-1dc8-4465-9b94-d5895aa35d73": {"content": "Pandas Merging DataFrames — official reference\nOfficial documentation for Pandas Merging DataFrames. The method signature is `pd.merge(left, right, on, how='inner')`. merges two DataFrames using SQL-style joins. Raises `MergeError` when merge keys produce duplicate columns. See also: df.join, df.concat.\nCode signature: pd.merge(left, right, on, how='inner')", "file_path": "https://docs.pandas.org/en/stable/pd/merge.html", "function_name": "pd.merge(left, right, on, how='inner')", "type": "documentation", "metadata": {"record_id": "99723715-1dc8-4465-9b94-d5895aa35d73", "source_type": "documentation", "domain": "NumPy/Pandas", "library": "Pandas", "title": "Pandas Merging DataFrames — official reference", "content": "Official documentation for Pandas Merging DataFrames. The method signature is `pd.merge(left, right, on, how='inner')`. merges two DataFrames using SQL-style joins. Raises `MergeError` when merge keys produce duplicate columns. See also: df.join, df.concat.", "code_signature": "pd.merge(left, right, on, how='inner')", "tags": "reshape, pandas, loc", "url": "https://docs.pandas.org/en/stable/pd/merge.html", "author": "Official Docs", "date_published": "29-09-2018", "votes_or_stars": 4503, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:ce692790-e421-4d5e-b922-ba5fcf1cefdb": {"content": "How to use httpx.AsyncClient in httpx\n`httpx.httpx.AsyncClient` provides an async HTTP client for non-blocking requests. Example usage:\n```python\nasync with httpx.AsyncClient() as client:\n    r = await client.get(url)\n    data = r.json()\n```\nThis is useful when making concurrent HTTP requests in async applications. Performance tip: set timeout=httpx.Timeout(10.0) to prevent hanging requests.\nCode signature: httpx.AsyncClient", "file_path": "https://docs.httpx.org/en/stable/httpx/AsyncClient.html", "function_name": "httpx.AsyncClient", "type": "documentation", "metadata": {"record_id": "ce692790-e421-4d5e-b922-ba5fcf1cefdb", "source_type": "documentation", "domain": "Requests/HTTP/APIs", "library": "httpx", "title": "How to use httpx.AsyncClient in httpx", "content": "`httpx.httpx.AsyncClient` provides an async HTTP client for non-blocking requests. Example usage:\n```python\nasync with httpx.AsyncClient() as client:\n    r = await client.get(url)\n    data = r.json()\n```\nThis is useful when making concurrent HTTP requests in async applications. Performance tip: set timeout=httpx.Timeout(10.0) to prevent hanging requests.", "code_signature": "httpx.AsyncClient", "tags": "headers, oauth, authentication, retry, requests, httpx", "url": "https://docs.httpx.org/en/stable/httpx/AsyncClient.html", "author": "Official Docs", "date_published": "18-08-2019", "votes_or_stars": 4211, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:57c8596b-3096-468e-b4ae-7f99137405ac": {"content": "How to apply a function to each row in Pandas without using iterrows?\n`iterrows()` is slow because it creates a Series per row. Prefer `df.apply(func, axis=1)` for row-wise operations, or better yet, vectorize using NumPy: `np.where(condition, a, b)`. For complex logic, `df.assign()` with vectorized expressions is cleanest. Benchmark: vectorized > apply > itertuples > iterrows.\nCode signature: Use df.apply(func, axis=1) or vectorized NumPy operations for best performance.", "file_path": "https://stackoverflow.com/questions/5446912", "function_name": "Use df.apply(func, axis=1) or vectorized NumPy operations for best performance.", "type": "stack_overflow", "metadata": {"record_id": "57c8596b-3096-468e-b4ae-7f99137405ac", "source_type": "stack_overflow", "domain": "NumPy/Pandas", "library": "dataframe", "title": "How to apply a function to each row in Pandas without using iterrows?", "content": "`iterrows()` is slow because it creates a Series per row. Prefer `df.apply(func, axis=1)` for row-wise operations, or better yet, vectorize using NumPy: `np.where(condition, a, b)`. For complex logic, `df.assign()` with vectorized expressions is cleanest. Benchmark: vectorized > apply > itertuples > iterrows.", "code_signature": "Use df.apply(func, axis=1) or vectorized NumPy operations for best performance.", "tags": "broadcasting, pandas, dtype, reshape, nan", "url": "https://stackoverflow.com/questions/5446912", "author": "user_563306", "date_published": "23-12-2018", "votes_or_stars": 2291, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:60f2caee-0ef3-4aeb-a0a5-78a46ec9c9f3": {"content": "BUG: Alembic autogenerate misses index changes\nProblem: Alembic --autogenerate does not detect changes to indexes or constraints on existing tables.\n\nFix/Discussion: Alembic autogenerate compares metadata to database state but has limitations. Workarounds: (1) Manually add `op.create_index()` to migration. (2) Use `include_schemas=True` in env.py. (3) Run `alembic check` to see detected changes. (4) Always review autogenerated migrations before applying.\nCode signature: status:open", "file_path": "https://github.com/alembic/alembic/issues/7344", "function_name": "status:open", "type": "github_issue", "metadata": {"record_id": "60f2caee-0ef3-4aeb-a0a5-78a46ec9c9f3", "source_type": "github_issue", "domain": "SQLAlchemy/Databases", "library": "alembic", "title": "BUG: Alembic autogenerate misses index changes", "content": "Problem: Alembic --autogenerate does not detect changes to indexes or constraints on existing tables.\n\nFix/Discussion: Alembic autogenerate compares metadata to database state but has limitations. Workarounds: (1) Manually add `op.create_index()` to migration. (2) Use `include_schemas=True` in env.py. (3) Run `alembic check` to see detected changes. (4) Always review autogenerated migrations before applying.", "code_signature": "status:open", "tags": "migration, foreign-key, sqlite, transaction, postgresql", "url": "https://github.com/alembic/alembic/issues/7344", "author": "contributor_3601", "date_published": "30-01-2019", "votes_or_stars": 229, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:182ebafa-bfa5-4706-963e-6a5a596e13d6": {"content": "PERF: Django template rendering bottleneck in production\nProblem: Template rendering takes 500ms+ for complex pages with many database queries.\n\nFix/Discussion: Primary cause is N+1 queries in template. Solutions: (1) Use select_related/prefetch_related. (2) Cache rendered templates with `cache` template tag. (3) Use `django-cachalot` for query caching. (4) Move heavy computation to views, pass pre-computed context. (5) Consider server-side caching with Redis.\nCode signature: status:closed", "file_path": "https://github.com/django/django/issues/3243", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "182ebafa-bfa5-4706-963e-6a5a596e13d6", "source_type": "github_issue", "domain": "FastAPI/Flask/Django", "library": "django", "title": "PERF: Django template rendering bottleneck in production", "content": "Problem: Template rendering takes 500ms+ for complex pages with many database queries.\n\nFix/Discussion: Primary cause is N+1 queries in template. Solutions: (1) Use select_related/prefetch_related. (2) Cache rendered templates with `cache` template tag. (3) Use `django-cachalot` for query caching. (4) Move heavy computation to views, pass pre-computed context. (5) Consider server-side caching with Redis.", "code_signature": "status:closed", "tags": "blueprint, fastapi, request", "url": "https://github.com/django/django/issues/3243", "author": "contributor_6988", "date_published": "26-12-2023", "votes_or_stars": 78, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:44d1ce57-7910-45f0-a696-53102f278453": {"content": "BUG: os.path.join ignores prefix when component starts with /\nProblem: os.path.join('base', '/absolute') returns '/absolute', ignoring 'base'.\n\nFix/Discussion: This is documented behavior: os.path.join discards previous components when it encounters an absolute path. Fix: use `pathlib.Path(base) / component.lstrip('/')`. Validate user-provided paths with `Path(path).resolve().is_relative_to(base_dir)` to prevent path traversal.\nCode signature: status:closed", "file_path": "https://github.com/cpython/cpython/issues/7712", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "44d1ce57-7910-45f0-a696-53102f278453", "source_type": "github_issue", "domain": "OS/Subprocess/File I/O", "library": "cpython", "title": "BUG: os.path.join ignores prefix when component starts with /", "content": "Problem: os.path.join('base', '/absolute') returns '/absolute', ignoring 'base'.\n\nFix/Discussion: This is documented behavior: os.path.join discards previous components when it encounters an absolute path. Fix: use `pathlib.Path(base) / component.lstrip('/')`. Validate user-provided paths with `Path(path).resolve().is_relative_to(base_dir)` to prevent path traversal.", "code_signature": "status:closed", "tags": "os, stdin, tempfile", "url": "https://github.com/cpython/cpython/issues/7712", "author": "contributor_8802", "date_published": "19-01-2022", "votes_or_stars": 409, "relevance_label": "low", "difficulty_level": "intermediate"}}, "kb:c4c4d1a0-4af2-4dfd-b43a-76c75e99a490": {"content": "subprocess.run vs Popen: which should I use?\n`subprocess.run()` is a high-level wrapper that waits for completion — use for most cases. `subprocess.Popen()` gives full control: stream stdout/stderr in real-time, write to stdin, run processes concurrently. For capturing output: `subprocess.run(..., capture_output=True, text=True)`. Always use `shell=False` and pass args as a list to prevent shell injection.\nCode signature: Use subprocess.run() for simple cases; Popen for streaming output or interactive processes.", "file_path": "https://stackoverflow.com/questions/5727085", "function_name": "Use subprocess.run() for simple cases; Popen for streaming output or interactive processes.", "type": "stack_overflow", "metadata": {"record_id": "c4c4d1a0-4af2-4dfd-b43a-76c75e99a490", "source_type": "stack_overflow", "domain": "OS/Subprocess/File I/O", "library": "shutil", "title": "subprocess.run vs Popen: which should I use?", "content": "`subprocess.run()` is a high-level wrapper that waits for completion — use for most cases. `subprocess.Popen()` gives full control: stream stdout/stderr in real-time, write to stdin, run processes concurrently. For capturing output: `subprocess.run(..., capture_output=True, text=True)`. Always use `shell=False` and pass args as a list to prevent shell injection.", "code_signature": "Use subprocess.run() for simple cases; Popen for streaming output or interactive processes.", "tags": "glob, stdout, env-variable", "url": "https://stackoverflow.com/questions/5727085", "author": "user_644210", "date_published": "27-02-2024", "votes_or_stars": 669, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:4d2530c8-9dcf-4d3c-9317-d5c58affa08e": {"content": "BUG: requests hangs indefinitely on slow server responses\nProblem: requests.get() hangs forever when server accepts connection but never sends response.\n\nFix/Discussion: Always set both connect and read timeouts: `requests.get(url, timeout=(3.05, 27))`. Tuple: (connect_timeout, read_timeout). Or single value for both. Mount a Session with default timeouts using a custom HTTPAdapter subclass. Never use `timeout=None` in production code.\nCode signature: status:closed", "file_path": "https://github.com/requests/requests/issues/4019", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "4d2530c8-9dcf-4d3c-9317-d5c58affa08e", "source_type": "github_issue", "domain": "Requests/HTTP/APIs", "library": "requests", "title": "BUG: requests hangs indefinitely on slow server responses", "content": "Problem: requests.get() hangs forever when server accepts connection but never sends response.\n\nFix/Discussion: Always set both connect and read timeouts: `requests.get(url, timeout=(3.05, 27))`. Tuple: (connect_timeout, read_timeout). Or single value for both. Mount a Session with default timeouts using a custom HTTPAdapter subclass. Never use `timeout=None` in production code.", "code_signature": "status:closed", "tags": "httpx, rate-limiting, authentication, http, websocket, oauth", "url": "https://github.com/requests/requests/issues/4019", "author": "contributor_4599", "date_published": "20-05-2019", "votes_or_stars": 265, "relevance_label": "low", "difficulty_level": "beginner"}}, "kb:cfffc6f1-3e83-4c26-b36b-db55e9637e20": {"content": "How to watch a directory for file changes in Python?\n```python\nfrom watchdog.observers import Observer\nfrom watchdog.events import FileSystemEventHandler\nhandler = FileSystemEventHandler()\nobserver = Observer()\nobserver.schedule(handler, path='.', recursive=True)\nobserver.start()\n```\nFor simple polling: `os.stat(file).st_mtime`. Linux-native: `inotify`. macOS-native: `FSEvents`.\nCode signature: Use the watchdog library for cross-platform directory monitoring.", "file_path": "https://stackoverflow.com/questions/5213115", "function_name": "Use the watchdog library for cross-platform directory monitoring.", "type": "stack_overflow", "metadata": {"record_id": "cfffc6f1-3e83-4c26-b36b-db55e9637e20", "source_type": "stack_overflow", "domain": "OS/Subprocess/File I/O", "library": "env-variable", "title": "How to watch a directory for file changes in Python?", "content": "```python\nfrom watchdog.observers import Observer\nfrom watchdog.events import FileSystemEventHandler\nhandler = FileSystemEventHandler()\nobserver = Observer()\nobserver.schedule(handler, path='.', recursive=True)\nobserver.start()\n```\nFor simple polling: `os.stat(file).st_mtime`. Linux-native: `inotify`. macOS-native: `FSEvents`.", "code_signature": "Use the watchdog library for cross-platform directory monitoring.", "tags": "stdout, subprocess, env-variable, stdin, pipe", "url": "https://stackoverflow.com/questions/5213115", "author": "user_959314", "date_published": "30-04-2019", "votes_or_stars": 2092, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:1742b820-913a-463c-8e07-5f735bd5d73a": {"content": "How to use httpx.AsyncClient in httpx\n`httpx.httpx.AsyncClient` provides an async HTTP client for non-blocking requests. Example usage:\n```python\nasync with httpx.AsyncClient() as client:\n    r = await client.get(url)\n    data = r.json()\n```\nThis is useful when making concurrent HTTP requests in async applications. Performance tip: set timeout=httpx.Timeout(10.0) to prevent hanging requests.\nCode signature: httpx.AsyncClient", "file_path": "https://docs.httpx.org/en/stable/httpx/AsyncClient.html", "function_name": "httpx.AsyncClient", "type": "documentation", "metadata": {"record_id": "1742b820-913a-463c-8e07-5f735bd5d73a", "source_type": "documentation", "domain": "Requests/HTTP/APIs", "library": "httpx", "title": "How to use httpx.AsyncClient in httpx", "content": "`httpx.httpx.AsyncClient` provides an async HTTP client for non-blocking requests. Example usage:\n```python\nasync with httpx.AsyncClient() as client:\n    r = await client.get(url)\n    data = r.json()\n```\nThis is useful when making concurrent HTTP requests in async applications. Performance tip: set timeout=httpx.Timeout(10.0) to prevent hanging requests.", "code_signature": "httpx.AsyncClient", "tags": "rest-api, rate-limiting, authentication, headers, websocket, retry", "url": "https://docs.httpx.org/en/stable/httpx/AsyncClient.html", "author": "Official Docs", "date_published": "24-04-2019", "votes_or_stars": 4198, "relevance_label": "low", "difficulty_level": "beginner"}}, "kb:56b662d8-d215-4858-9974-569b0e5f2e0a": {"content": "threading Thread Synchronization — official reference\nOfficial documentation for threading Thread Synchronization. The method signature is `threading.Lock()`. creates a mutual-exclusion lock for thread-safe access. Raises `RuntimeError` when lock is released without being acquired. See also: threading.RLock, threading.Semaphore, threading.Event.\nCode signature: threading.Lock()", "file_path": "https://docs.threading.org/en/stable/threading/Lock.html", "function_name": "threading.Lock()", "type": "documentation", "metadata": {"record_id": "56b662d8-d215-4858-9974-569b0e5f2e0a", "source_type": "documentation", "domain": "Asyncio/Threading", "library": "threading", "title": "threading Thread Synchronization — official reference", "content": "Official documentation for threading Thread Synchronization. The method signature is `threading.Lock()`. creates a mutual-exclusion lock for thread-safe access. Raises `RuntimeError` when lock is released without being acquired. See also: threading.RLock, threading.Semaphore, threading.Event.", "code_signature": "threading.Lock()", "tags": "lock, executor, async, threading", "url": "https://docs.threading.org/en/stable/threading/Lock.html", "author": "Official Docs", "date_published": "08-12-2020", "votes_or_stars": 4866, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:909c8e12-b998-4997-94e3-c9e9c82b9ae0": {"content": "How to apply a function to each row in Pandas without using iterrows?\n`iterrows()` is slow because it creates a Series per row. Prefer `df.apply(func, axis=1)` for row-wise operations, or better yet, vectorize using NumPy: `np.where(condition, a, b)`. For complex logic, `df.assign()` with vectorized expressions is cleanest. Benchmark: vectorized > apply > itertuples > iterrows.\nCode signature: Use df.apply(func, axis=1) or vectorized NumPy operations for best performance.", "file_path": "https://stackoverflow.com/questions/5446912", "function_name": "Use df.apply(func, axis=1) or vectorized NumPy operations for best performance.", "type": "stack_overflow", "metadata": {"record_id": "909c8e12-b998-4997-94e3-c9e9c82b9ae0", "source_type": "stack_overflow", "domain": "NumPy/Pandas", "library": "dataframe", "title": "How to apply a function to each row in Pandas without using iterrows?", "content": "`iterrows()` is slow because it creates a Series per row. Prefer `df.apply(func, axis=1)` for row-wise operations, or better yet, vectorize using NumPy: `np.where(condition, a, b)`. For complex logic, `df.assign()` with vectorized expressions is cleanest. Benchmark: vectorized > apply > itertuples > iterrows.", "code_signature": "Use df.apply(func, axis=1) or vectorized NumPy operations for best performance.", "tags": "dataframe, numpy, groupby, reshape, merge, pandas", "url": "https://stackoverflow.com/questions/5446912", "author": "user_563306", "date_published": "30-08-2020", "votes_or_stars": 2237, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:65d5f2bc-e2b8-4d00-83ac-de478bcaf74a": {"content": "BUG: os.path.join ignores prefix when component starts with /\nProblem: os.path.join('base', '/absolute') returns '/absolute', ignoring 'base'.\n\nFix/Discussion: This is documented behavior: os.path.join discards previous components when it encounters an absolute path. Fix: use `pathlib.Path(base) / component.lstrip('/')`. Validate user-provided paths with `Path(path).resolve().is_relative_to(base_dir)` to prevent path traversal.\nCode signature: status:closed", "file_path": "https://github.com/cpython/cpython/issues/7712", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "65d5f2bc-e2b8-4d00-83ac-de478bcaf74a", "source_type": "github_issue", "domain": "OS/Subprocess/File I/O", "library": "cpython", "title": "BUG: os.path.join ignores prefix when component starts with /", "content": "Problem: os.path.join('base', '/absolute') returns '/absolute', ignoring 'base'.\n\nFix/Discussion: This is documented behavior: os.path.join discards previous components when it encounters an absolute path. Fix: use `pathlib.Path(base) / component.lstrip('/')`. Validate user-provided paths with `Path(path).resolve().is_relative_to(base_dir)` to prevent path traversal.", "code_signature": "status:closed", "tags": "file-io, pathlib, glob, shutil", "url": "https://github.com/cpython/cpython/issues/7712", "author": "contributor_8802", "date_published": "24-01-2021", "votes_or_stars": 411, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:0a5d64b3-e197-473c-a2c7-87e7af343e28": {"content": "BUG: Alembic autogenerate misses index changes\nProblem: Alembic --autogenerate does not detect changes to indexes or constraints on existing tables.\n\nFix/Discussion: Alembic autogenerate compares metadata to database state but has limitations. Workarounds: (1) Manually add `op.create_index()` to migration. (2) Use `include_schemas=True` in env.py. (3) Run `alembic check` to see detected changes. (4) Always review autogenerated migrations before applying.\nCode signature: status:open", "file_path": "https://github.com/alembic/alembic/issues/7344", "function_name": "status:open", "type": "github_issue", "metadata": {"record_id": "0a5d64b3-e197-473c-a2c7-87e7af343e28", "source_type": "github_issue", "domain": "SQLAlchemy/Databases", "library": "alembic", "title": "BUG: Alembic autogenerate misses index changes", "content": "Problem: Alembic --autogenerate does not detect changes to indexes or constraints on existing tables.\n\nFix/Discussion: Alembic autogenerate compares metadata to database state but has limitations. Workarounds: (1) Manually add `op.create_index()` to migration. (2) Use `include_schemas=True` in env.py. (3) Run `alembic check` to see detected changes. (4) Always review autogenerated migrations before applying.", "code_signature": "status:open", "tags": "session, foreign-key, sqlalchemy, sqlite, migration", "url": "https://github.com/alembic/alembic/issues/7344", "author": "contributor_3601", "date_published": "16-12-2018", "votes_or_stars": 206, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:5fa886c8-f7de-40d6-94ac-a9e0487824c4": {"content": "Django N+1 query problem: how to fix?\nN+1 occurs when accessing related objects in a loop. Diagnose with `django-debug-toolbar` or `connection.queries`. Fix ForeignKey: `User.objects.select_related('profile').all()`. Fix M2M: `Post.objects.prefetch_related('tags').all()`. Use `only()` to fetch specific fields and reduce data transfer.\nCode signature: Use select_related() for ForeignKey and prefetch_related() for ManyToMany relationships.", "file_path": "https://stackoverflow.com/questions/8106470", "function_name": "Use select_related() for ForeignKey and prefetch_related() for ManyToMany relationships.", "type": "stack_overflow", "metadata": {"record_id": "5fa886c8-f7de-40d6-94ac-a9e0487824c4", "source_type": "stack_overflow", "domain": "FastAPI/Flask/Django", "library": "request", "title": "Django N+1 query problem: how to fix?", "content": "N+1 occurs when accessing related objects in a loop. Diagnose with `django-debug-toolbar` or `connection.queries`. Fix ForeignKey: `User.objects.select_related('profile').all()`. Fix M2M: `Post.objects.prefetch_related('tags').all()`. Use `only()` to fetch specific fields and reduce data transfer.", "code_signature": "Use select_related() for ForeignKey and prefetch_related() for ManyToMany relationships.", "tags": "flask, request, django, blueprint, routing, template", "url": "https://stackoverflow.com/questions/8106470", "author": "user_441071", "date_published": "15-03-2023", "votes_or_stars": 1764, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:0b60cad1-1f2b-4bd9-8a26-0f2839fe0f18": {"content": "threading Thread Synchronization — official reference\nOfficial documentation for threading Thread Synchronization. The method signature is `threading.Lock()`. creates a mutual-exclusion lock for thread-safe access. Raises `RuntimeError` when lock is released without being acquired. See also: threading.RLock, threading.Semaphore, threading.Event.\nCode signature: threading.Lock()", "file_path": "https://docs.threading.org/en/stable/threading/Lock.html", "function_name": "threading.Lock()", "type": "documentation", "metadata": {"record_id": "0b60cad1-1f2b-4bd9-8a26-0f2839fe0f18", "source_type": "documentation", "domain": "Asyncio/Threading", "library": "threading", "title": "threading Thread Synchronization — official reference", "content": "Official documentation for threading Thread Synchronization. The method signature is `threading.Lock()`. creates a mutual-exclusion lock for thread-safe access. Raises `RuntimeError` when lock is released without being acquired. See also: threading.RLock, threading.Semaphore, threading.Event.", "code_signature": "threading.Lock()", "tags": "race-condition, gather, async, lock", "url": "https://docs.threading.org/en/stable/threading/Lock.html", "author": "Official Docs", "date_published": "05-12-2024", "votes_or_stars": 4854, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:4a6d5b60-42f0-4193-98b9-9229ef036fe9": {"content": "BUG: requests hangs indefinitely on slow server responses\nProblem: requests.get() hangs forever when server accepts connection but never sends response.\n\nFix/Discussion: Always set both connect and read timeouts: `requests.get(url, timeout=(3.05, 27))`. Tuple: (connect_timeout, read_timeout). Or single value for both. Mount a Session with default timeouts using a custom HTTPAdapter subclass. Never use `timeout=None` in production code.\nCode signature: status:closed", "file_path": "https://github.com/requests/requests/issues/4019", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "4a6d5b60-42f0-4193-98b9-9229ef036fe9", "source_type": "github_issue", "domain": "Requests/HTTP/APIs", "library": "requests", "title": "BUG: requests hangs indefinitely on slow server responses", "content": "Problem: requests.get() hangs forever when server accepts connection but never sends response.\n\nFix/Discussion: Always set both connect and read timeouts: `requests.get(url, timeout=(3.05, 27))`. Tuple: (connect_timeout, read_timeout). Or single value for both. Mount a Session with default timeouts using a custom HTTPAdapter subclass. Never use `timeout=None` in production code.", "code_signature": "status:closed", "tags": "rate-limiting, rest-api, headers, authentication", "url": "https://github.com/requests/requests/issues/4019", "author": "contributor_4599", "date_published": "06-10-2021", "votes_or_stars": 264, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:e7ed50fe-2e33-422a-81b5-e42e3208c8d5": {"content": "BUG: Alembic autogenerate misses index changes\nProblem: Alembic --autogenerate does not detect changes to indexes or constraints on existing tables.\n\nFix/Discussion: Alembic autogenerate compares metadata to database state but has limitations. Workarounds: (1) Manually add `op.create_index()` to migration. (2) Use `include_schemas=True` in env.py. (3) Run `alembic check` to see detected changes. (4) Always review autogenerated migrations before applying.\nCode signature: status:open", "file_path": "https://github.com/alembic/alembic/issues/7344", "function_name": "status:open", "type": "github_issue", "metadata": {"record_id": "e7ed50fe-2e33-422a-81b5-e42e3208c8d5", "source_type": "github_issue", "domain": "SQLAlchemy/Databases", "library": "alembic", "title": "BUG: Alembic autogenerate misses index changes", "content": "Problem: Alembic --autogenerate does not detect changes to indexes or constraints on existing tables.\n\nFix/Discussion: Alembic autogenerate compares metadata to database state but has limitations. Workarounds: (1) Manually add `op.create_index()` to migration. (2) Use `include_schemas=True` in env.py. (3) Run `alembic check` to see detected changes. (4) Always review autogenerated migrations before applying.", "code_signature": "status:open", "tags": "foreign-key, connection-pool, postgresql", "url": "https://github.com/alembic/alembic/issues/7344", "author": "contributor_3601", "date_published": "01-11-2020", "votes_or_stars": 213, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:6169199e-51b6-4da1-8df3-c791534c2902": {"content": "How to use df.groupby in Pandas\n`Pandas.df.groupby` groups DataFrame rows by column values. Example usage:\n```python\ndf.groupby('category')['value'].mean()\n```\nThis is useful when aggregation, transformation, filtering by group. Performance tip: avoid apply() on large DataFrames; prefer agg().\nCode signature: df.groupby", "file_path": "https://docs.pandas.org/en/stable/df/groupby.html", "function_name": "df.groupby", "type": "documentation", "metadata": {"record_id": "6169199e-51b6-4da1-8df3-c791534c2902", "source_type": "documentation", "domain": "NumPy/Pandas", "library": "Pandas", "title": "How to use df.groupby in Pandas", "content": "`Pandas.df.groupby` groups DataFrame rows by column values. Example usage:\n```python\ndf.groupby('category')['value'].mean()\n```\nThis is useful when aggregation, transformation, filtering by group. Performance tip: avoid apply() on large DataFrames; prefer agg().", "code_signature": "df.groupby", "tags": "numpy, broadcasting, vectorization", "url": "https://docs.pandas.org/en/stable/df/groupby.html", "author": "Official Docs", "date_published": "26-07-2024", "votes_or_stars": 802, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:81686058-048a-4813-8fd0-4c8bb2f457df": {"content": "asyncio RuntimeError: This event loop is already running\nThis error is common in Jupyter and FastAPI. Solutions: (1) In async context: just use `await coroutine()`. (2) In Jupyter: `import nest_asyncio; nest_asyncio.apply()`. (3) Run in thread: `asyncio.get_event_loop().run_in_executor(None, sync_func)`. (4) Use `asyncio.ensure_future()` to schedule from within async code.\nCode signature: You're calling asyncio.run() inside an already running event loop. Use await instead.", "file_path": "https://stackoverflow.com/questions/2229110", "function_name": "You're calling asyncio.run() inside an already running event loop. Use await instead.", "type": "stack_overflow", "metadata": {"record_id": "81686058-048a-4813-8fd0-4c8bb2f457df", "source_type": "stack_overflow", "domain": "Asyncio/Threading", "library": "async", "title": "asyncio RuntimeError: This event loop is already running", "content": "This error is common in Jupyter and FastAPI. Solutions: (1) In async context: just use `await coroutine()`. (2) In Jupyter: `import nest_asyncio; nest_asyncio.apply()`. (3) Run in thread: `asyncio.get_event_loop().run_in_executor(None, sync_func)`. (4) Use `asyncio.ensure_future()` to schedule from within async code.", "code_signature": "You're calling asyncio.run() inside an already running event loop. Use await instead.", "tags": "await, coroutine, lock", "url": "https://stackoverflow.com/questions/2229110", "author": "user_573750", "date_published": "20-11-2019", "votes_or_stars": 2304, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:88edb8c2-7a56-4492-9e42-d7e5ad0de109": {"content": "PERF: groupby().apply() extremely slow on large DataFrames\nProblem: groupby().apply() with a custom function is 100x slower than expected on 10M row DataFrames.\n\nFix/Discussion: groupby().apply() serializes each group and has high overhead. Fix: vectorize with numpy, use groupby().transform() for same-shape output, or groupby().agg() with named aggregations. For complex logic, consider Polars or Dask. Numba's @jit can accelerate custom group functions.\nCode signature: status:closed", "file_path": "https://github.com/pandas/pandas/issues/8446", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "88edb8c2-7a56-4492-9e42-d7e5ad0de109", "source_type": "github_issue", "domain": "NumPy/Pandas", "library": "pandas", "title": "PERF: groupby().apply() extremely slow on large DataFrames", "content": "Problem: groupby().apply() with a custom function is 100x slower than expected on 10M row DataFrames.\n\nFix/Discussion: groupby().apply() serializes each group and has high overhead. Fix: vectorize with numpy, use groupby().transform() for same-shape output, or groupby().agg() with named aggregations. For complex logic, consider Polars or Dask. Numba's @jit can accelerate custom group functions.", "code_signature": "status:closed", "tags": "reshape, iloc, merge", "url": "https://github.com/pandas/pandas/issues/8446", "author": "contributor_6648", "date_published": "21-02-2023", "votes_or_stars": 343, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:c1da9964-64a4-4808-a4b2-3d53beffca20": {"content": "Django ORM Queries — official reference\nOfficial documentation for Django ORM Queries. The method signature is `QuerySet.filter(**kwargs)`. filters QuerySet rows matching given field lookups. Raises `FieldError` when an unknown field lookup is provided. See also: QuerySet.exclude, QuerySet.annotate, Q objects.\nCode signature: QuerySet.filter(**kwargs)", "file_path": "https://docs.django.org/en/stable/QuerySet/filter.html", "function_name": "QuerySet.filter(**kwargs)", "type": "documentation", "metadata": {"record_id": "c1da9964-64a4-4808-a4b2-3d53beffca20", "source_type": "documentation", "domain": "FastAPI/Flask/Django", "library": "Django", "title": "Django ORM Queries — official reference", "content": "Official documentation for Django ORM Queries. The method signature is `QuerySet.filter(**kwargs)`. filters QuerySet rows matching given field lookups. Raises `FieldError` when an unknown field lookup is provided. See also: QuerySet.exclude, QuerySet.annotate, Q objects.", "code_signature": "QuerySet.filter(**kwargs)", "tags": "rest, middleware, pydantic, routing", "url": "https://docs.django.org/en/stable/QuerySet/filter.html", "author": "Official Docs", "date_published": "22-11-2023", "votes_or_stars": 2396, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:d2bae488-6c82-4909-b811-dd53cc74fab1": {"content": "How to use @app.route in Flask\n`Flask.@app.route` maps a URL rule to a view function. Example usage:\n```python\n@app.route('/users/<int:uid>')\ndef get_user(uid):\n    return jsonify(user)\n```\nThis is useful when defining HTTP endpoints in a Flask application. Performance tip: use Blueprints to organize large applications.\nCode signature: @app.route", "file_path": "https://docs.flask.org/en/stable/@app/route.html", "function_name": "@app.route", "type": "documentation", "metadata": {"record_id": "d2bae488-6c82-4909-b811-dd53cc74fab1", "source_type": "documentation", "domain": "FastAPI/Flask/Django", "library": "Flask", "title": "How to use @app.route in Flask", "content": "`Flask.@app.route` maps a URL rule to a view function. Example usage:\n```python\n@app.route('/users/<int:uid>')\ndef get_user(uid):\n    return jsonify(user)\n```\nThis is useful when defining HTTP endpoints in a Flask application. Performance tip: use Blueprints to organize large applications.", "code_signature": "@app.route", "tags": "flask, rest, django, middleware", "url": "https://docs.flask.org/en/stable/@app/route.html", "author": "Official Docs", "date_published": "12-10-2024", "votes_or_stars": 2990, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:d7a3f6d3-bef7-4338-882d-56ec7bc6d2d4": {"content": "How to run asyncio code from synchronous Python?\n`asyncio.run(main())` creates an event loop, runs the coroutine, closes the loop. Never call asyncio.run() from inside a running event loop (use await instead). In Jupyter notebooks (which have a running loop), use `await coroutine()` directly or `nest_asyncio.apply()`. For mixing sync/async, use `loop.run_until_complete()`.\nCode signature: Use asyncio.run() in Python 3.7+ to execute a coroutine from sync code.", "file_path": "https://stackoverflow.com/questions/2140194", "function_name": "Use asyncio.run() in Python 3.7+ to execute a coroutine from sync code.", "type": "stack_overflow", "metadata": {"record_id": "d7a3f6d3-bef7-4338-882d-56ec7bc6d2d4", "source_type": "stack_overflow", "domain": "Asyncio/Threading", "library": "gather", "title": "How to run asyncio code from synchronous Python?", "content": "`asyncio.run(main())` creates an event loop, runs the coroutine, closes the loop. Never call asyncio.run() from inside a running event loop (use await instead). In Jupyter notebooks (which have a running loop), use `await coroutine()` directly or `nest_asyncio.apply()`. For mixing sync/async, use `loop.run_until_complete()`.", "code_signature": "Use asyncio.run() in Python 3.7+ to execute a coroutine from sync code.", "tags": "deadlock, race-condition, executor, async, semaphore, lock", "url": "https://stackoverflow.com/questions/2140194", "author": "user_718011", "date_published": "24-05-2021", "votes_or_stars": 304, "relevance_label": "low", "difficulty_level": "advanced"}}, "kb:6ef818d5-00da-43ae-b634-3b19ad168393": {"content": "PERF: concurrent.futures.ProcessPoolExecutor slow startup on Windows\nProblem: ProcessPoolExecutor takes 3-5 seconds to start on Windows due to process spawning overhead.\n\nFix/Discussion: Windows uses 'spawn' start method (vs 'fork' on Linux). Mitigations: (1) Reuse executor across calls — don't create/destroy in loops. (2) Use 'forkserver' or 'fork' on Linux/macOS. (3) For short tasks, ThreadPoolExecutor may be faster. (4) Use `initializer` param to pre-load modules.\nCode signature: status:closed", "file_path": "https://github.com/cpython/cpython/issues/6647", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "6ef818d5-00da-43ae-b634-3b19ad168393", "source_type": "github_issue", "domain": "Asyncio/Threading", "library": "cpython", "title": "PERF: concurrent.futures.ProcessPoolExecutor slow startup on Windows", "content": "Problem: ProcessPoolExecutor takes 3-5 seconds to start on Windows due to process spawning overhead.\n\nFix/Discussion: Windows uses 'spawn' start method (vs 'fork' on Linux). Mitigations: (1) Reuse executor across calls — don't create/destroy in loops. (2) Use 'forkserver' or 'fork' on Linux/macOS. (3) For short tasks, ThreadPoolExecutor may be faster. (4) Use `initializer` param to pre-load modules.", "code_signature": "status:closed", "tags": "deadlock, event-loop, race-condition, task", "url": "https://github.com/cpython/cpython/issues/6647", "author": "contributor_4097", "date_published": "25-09-2018", "votes_or_stars": 235, "relevance_label": "low", "difficulty_level": "beginner"}}, "kb:2090c025-3156-442c-a83f-55f360a32153": {"content": "subprocess — Process Execution\nThe `subprocess.run` function in subprocess runs a subprocess and waits for it to complete. It accepts `args, capture_output=False, timeout=None` as a parameter and returns CompletedProcess. Common use cases include executing shell commands, scripts, or external programs from Python. Note that use shell=False (default) to avoid shell injection vulnerabilities.\nCode signature: subprocess.run(args, capture_output=False, timeout=None) -> CompletedProcess", "file_path": "https://docs.subprocess.org/en/stable/subprocess/run.html", "function_name": "subprocess.run(args, capture_output=False, timeout=None) -> CompletedProcess", "type": "documentation", "metadata": {"record_id": "2090c025-3156-442c-a83f-55f360a32153", "source_type": "documentation", "domain": "OS/Subprocess/File I/O", "library": "subprocess", "title": "subprocess — Process Execution", "content": "The `subprocess.run` function in subprocess runs a subprocess and waits for it to complete. It accepts `args, capture_output=False, timeout=None` as a parameter and returns CompletedProcess. Common use cases include executing shell commands, scripts, or external programs from Python. Note that use shell=False (default) to avoid shell injection vulnerabilities.", "code_signature": "subprocess.run(args, capture_output=False, timeout=None) -> CompletedProcess", "tags": "subprocess, stdin, tempfile, shutil, stderr", "url": "https://docs.subprocess.org/en/stable/subprocess/run.html", "author": "Official Docs", "date_published": "25-06-2019", "votes_or_stars": 461, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:03a57d61-ced9-4f79-937b-eef90a591ef1": {"content": "FastAPI — Path Operations\nThe `@app.get` function in FastAPI registers a GET endpoint at the specified path. It accepts `path, response_model` as a parameter and returns Response. Common use cases include building REST API endpoints that return JSON data. Note that use response_model to enforce output schema validation.\nCode signature: @app.get(path, response_model) -> Response", "file_path": "https://docs.fastapi.org/en/stable/@app/get.html", "function_name": "@app.get(path, response_model) -> Response", "type": "documentation", "metadata": {"record_id": "03a57d61-ced9-4f79-937b-eef90a591ef1", "source_type": "documentation", "domain": "FastAPI/Flask/Django", "library": "FastAPI", "title": "FastAPI — Path Operations", "content": "The `@app.get` function in FastAPI registers a GET endpoint at the specified path. It accepts `path, response_model` as a parameter and returns Response. Common use cases include building REST API endpoints that return JSON data. Note that use response_model to enforce output schema validation.", "code_signature": "@app.get(path, response_model) -> Response", "tags": "response, flask, dependency-injection, routing", "url": "https://docs.fastapi.org/en/stable/@app/get.html", "author": "Official Docs", "date_published": "14-08-2020", "votes_or_stars": 2896, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:d135c0f2-fbef-432a-b144-cc34e81b7fa9": {"content": "PERF: concurrent.futures.ProcessPoolExecutor slow startup on Windows\nProblem: ProcessPoolExecutor takes 3-5 seconds to start on Windows due to process spawning overhead.\n\nFix/Discussion: Windows uses 'spawn' start method (vs 'fork' on Linux). Mitigations: (1) Reuse executor across calls — don't create/destroy in loops. (2) Use 'forkserver' or 'fork' on Linux/macOS. (3) For short tasks, ThreadPoolExecutor may be faster. (4) Use `initializer` param to pre-load modules.\nCode signature: status:closed", "file_path": "https://github.com/cpython/cpython/issues/6647", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "d135c0f2-fbef-432a-b144-cc34e81b7fa9", "source_type": "github_issue", "domain": "Asyncio/Threading", "library": "cpython", "title": "PERF: concurrent.futures.ProcessPoolExecutor slow startup on Windows", "content": "Problem: ProcessPoolExecutor takes 3-5 seconds to start on Windows due to process spawning overhead.\n\nFix/Discussion: Windows uses 'spawn' start method (vs 'fork' on Linux). Mitigations: (1) Reuse executor across calls — don't create/destroy in loops. (2) Use 'forkserver' or 'fork' on Linux/macOS. (3) For short tasks, ThreadPoolExecutor may be faster. (4) Use `initializer` param to pre-load modules.", "code_signature": "status:closed", "tags": "executor, async, coroutine", "url": "https://github.com/cpython/cpython/issues/6647", "author": "contributor_4097", "date_published": "04-08-2018", "votes_or_stars": 237, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:281ce140-ae5c-484b-af37-fe68cad2823d": {"content": "How to implement a thread pool in Python?\n```python\nfrom concurrent.futures import ThreadPoolExecutor\nwith ThreadPoolExecutor(max_workers=8) as ex:\n    futures = [ex.submit(task, arg) for arg in args]\n    results = [f.result() for f in futures]\n```\nFor CPU-bound: use ProcessPoolExecutor instead. For async integration: `await loop.run_in_executor(executor, func, *args)`.\nCode signature: Use concurrent.futures.ThreadPoolExecutor for I/O-bound tasks.", "file_path": "https://stackoverflow.com/questions/3646552", "function_name": "Use concurrent.futures.ThreadPoolExecutor for I/O-bound tasks.", "type": "stack_overflow", "metadata": {"record_id": "281ce140-ae5c-484b-af37-fe68cad2823d", "source_type": "stack_overflow", "domain": "Asyncio/Threading", "library": "coroutine", "title": "How to implement a thread pool in Python?", "content": "```python\nfrom concurrent.futures import ThreadPoolExecutor\nwith ThreadPoolExecutor(max_workers=8) as ex:\n    futures = [ex.submit(task, arg) for arg in args]\n    results = [f.result() for f in futures]\n```\nFor CPU-bound: use ProcessPoolExecutor instead. For async integration: `await loop.run_in_executor(executor, func, *args)`.", "code_signature": "Use concurrent.futures.ThreadPoolExecutor for I/O-bound tasks.", "tags": "async, gather, coroutine, event-loop, deadlock, task", "url": "https://stackoverflow.com/questions/3646552", "author": "user_469469", "date_published": "18-03-2022", "votes_or_stars": 1520, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:c2b8e915-a781-4795-b06c-271f07806622": {"content": "How to limit concurrency with asyncio?\n```python\nsem = asyncio.Semaphore(10)\nasync def limited(url):\n    async with sem:\n        return await fetch(url)\nresults = await asyncio.gather(*[limited(u) for u in urls])\n```\nThis prevents overwhelming APIs or databases with too many simultaneous connections.\nCode signature: Use asyncio.Semaphore to cap the number of concurrent coroutines.", "file_path": "https://stackoverflow.com/questions/2375453", "function_name": "Use asyncio.Semaphore to cap the number of concurrent coroutines.", "type": "stack_overflow", "metadata": {"record_id": "c2b8e915-a781-4795-b06c-271f07806622", "source_type": "stack_overflow", "domain": "Asyncio/Threading", "library": "asyncio", "title": "How to limit concurrency with asyncio?", "content": "```python\nsem = asyncio.Semaphore(10)\nasync def limited(url):\n    async with sem:\n        return await fetch(url)\nresults = await asyncio.gather(*[limited(u) for u in urls])\n```\nThis prevents overwhelming APIs or databases with too many simultaneous connections.", "code_signature": "Use asyncio.Semaphore to cap the number of concurrent coroutines.", "tags": "gather, await, task", "url": "https://stackoverflow.com/questions/2375453", "author": "user_449589", "date_published": "04-03-2020", "votes_or_stars": 2464, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:2bab6571-2f50-423d-ab5a-9ed6ffaf5a22": {"content": "FEAT: pathlib: add support for reading/writing with encoding shorthand\nProblem: pathlib.Path.read_text() does not default to UTF-8, causing issues across platforms.\n\nFix/Discussion: Always pass encoding explicitly: `Path('file').read_text(encoding='utf-8')`. Python 3.10+ added `encoding='locale'` default change proposal. Best practice: always specify encoding in all file I/O operations for portability.\nCode signature: status:closed", "file_path": "https://github.com/cpython/cpython/issues/2707", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "2bab6571-2f50-423d-ab5a-9ed6ffaf5a22", "source_type": "github_issue", "domain": "OS/Subprocess/File I/O", "library": "cpython", "title": "FEAT: pathlib: add support for reading/writing with encoding shorthand", "content": "Problem: pathlib.Path.read_text() does not default to UTF-8, causing issues across platforms.\n\nFix/Discussion: Always pass encoding explicitly: `Path('file').read_text(encoding='utf-8')`. Python 3.10+ added `encoding='locale'` default change proposal. Best practice: always specify encoding in all file I/O operations for portability.", "code_signature": "status:closed", "tags": "tempfile, glob, stdin, stdout, stderr", "url": "https://github.com/cpython/cpython/issues/2707", "author": "contributor_7877", "date_published": "14-12-2021", "votes_or_stars": 491, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:6429143d-2d1e-4d8d-aee8-421803e572be": {"content": "How to implement JWT authentication in FastAPI?\n```python\nfrom fastapi.security import OAuth2PasswordBearer\nfrom jose import jwt\noauth2_scheme = OAuth2PasswordBearer(tokenUrl='token')\n```\nCreate access tokens with `jwt.encode(payload, SECRET_KEY, algorithm='HS256')`. Verify with `jwt.decode(token, SECRET_KEY, algorithms=['HS256'])`. Store SECRET_KEY in environment variables, never in code.\nCode signature: Use python-jose for JWT and passlib for password hashing with OAuth2PasswordBearer.", "file_path": "https://stackoverflow.com/questions/7754864", "function_name": "Use python-jose for JWT and passlib for password hashing with OAuth2PasswordBearer.", "type": "stack_overflow", "metadata": {"record_id": "6429143d-2d1e-4d8d-aee8-421803e572be", "source_type": "stack_overflow", "domain": "FastAPI/Flask/Django", "library": "fastapi", "title": "How to implement JWT authentication in FastAPI?", "content": "```python\nfrom fastapi.security import OAuth2PasswordBearer\nfrom jose import jwt\noauth2_scheme = OAuth2PasswordBearer(tokenUrl='token')\n```\nCreate access tokens with `jwt.encode(payload, SECRET_KEY, algorithm='HS256')`. Verify with `jwt.decode(token, SECRET_KEY, algorithms=['HS256'])`. Store SECRET_KEY in environment variables, never in code.", "code_signature": "Use python-jose for JWT and passlib for password hashing with OAuth2PasswordBearer.", "tags": "response, template, pydantic, blueprint", "url": "https://stackoverflow.com/questions/7754864", "author": "user_773587", "date_published": "05-05-2023", "votes_or_stars": 402, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:d33be8e0-408f-4009-b7de-8daa94a5887e": {"content": "How do I efficiently fill NaN values in a Pandas DataFrame?\nFor large DataFrames, avoid looping. Use `df.fillna({'col': val})` to fill specific columns. `df.ffill()` propagates last valid observation forward. For time-series data, `df.interpolate()` provides smoother results. Always check `df.isna().sum()` before and after to verify.\nCode signature: Use df.fillna(value) or df.ffill()/df.bfill() for forward/backward fill.", "file_path": "https://stackoverflow.com/questions/6438436", "function_name": "Use df.fillna(value) or df.ffill()/df.bfill() for forward/backward fill.", "type": "stack_overflow", "metadata": {"record_id": "d33be8e0-408f-4009-b7de-8daa94a5887e", "source_type": "stack_overflow", "domain": "NumPy/Pandas", "library": "loc", "title": "How do I efficiently fill NaN values in a Pandas DataFrame?", "content": "For large DataFrames, avoid looping. Use `df.fillna({'col': val})` to fill specific columns. `df.ffill()` propagates last valid observation forward. For time-series data, `df.interpolate()` provides smoother results. Always check `df.isna().sum()` before and after to verify.", "code_signature": "Use df.fillna(value) or df.ffill()/df.bfill() for forward/backward fill.", "tags": "groupby, array, dtype", "url": "https://stackoverflow.com/questions/6438436", "author": "user_522340", "date_published": "23-09-2022", "votes_or_stars": 1, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:7aee96b9-0310-41f4-a062-4edd06569754": {"content": "asyncio RuntimeError: This event loop is already running\nThis error is common in Jupyter and FastAPI. Solutions: (1) In async context: just use `await coroutine()`. (2) In Jupyter: `import nest_asyncio; nest_asyncio.apply()`. (3) Run in thread: `asyncio.get_event_loop().run_in_executor(None, sync_func)`. (4) Use `asyncio.ensure_future()` to schedule from within async code.\nCode signature: You're calling asyncio.run() inside an already running event loop. Use await instead.", "file_path": "https://stackoverflow.com/questions/2229110", "function_name": "You're calling asyncio.run() inside an already running event loop. Use await instead.", "type": "stack_overflow", "metadata": {"record_id": "7aee96b9-0310-41f4-a062-4edd06569754", "source_type": "stack_overflow", "domain": "Asyncio/Threading", "library": "async", "title": "asyncio RuntimeError: This event loop is already running", "content": "This error is common in Jupyter and FastAPI. Solutions: (1) In async context: just use `await coroutine()`. (2) In Jupyter: `import nest_asyncio; nest_asyncio.apply()`. (3) Run in thread: `asyncio.get_event_loop().run_in_executor(None, sync_func)`. (4) Use `asyncio.ensure_future()` to schedule from within async code.", "code_signature": "You're calling asyncio.run() inside an already running event loop. Use await instead.", "tags": "executor, await, coroutine, threading, asyncio", "url": "https://stackoverflow.com/questions/2229110", "author": "user_573750", "date_published": "19-06-2020", "votes_or_stars": 2294, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:73132763-ea45-4ee1-a211-8ea6c6a7a26f": {"content": "BUG: SettingWithCopyWarning when modifying sliced DataFrame\nProblem: Modifying a slice of a DataFrame raises SettingWithCopyWarning and may not update original.\n\nFix/Discussion: Always use .loc or .copy() when slicing. Use `df.loc[mask, 'col'] = val` for assignment. Create explicit copies: `subset = df[mask].copy()`. In Pandas 2.0+, Copy-on-Write makes this behavior consistent — enable with `pd.options.mode.copy_on_write = True`.\nCode signature: status:closed", "file_path": "https://github.com/pandas/pandas/issues/3264", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "73132763-ea45-4ee1-a211-8ea6c6a7a26f", "source_type": "github_issue", "domain": "NumPy/Pandas", "library": "pandas", "title": "BUG: SettingWithCopyWarning when modifying sliced DataFrame", "content": "Problem: Modifying a slice of a DataFrame raises SettingWithCopyWarning and may not update original.\n\nFix/Discussion: Always use .loc or .copy() when slicing. Use `df.loc[mask, 'col'] = val` for assignment. Create explicit copies: `subset = df[mask].copy()`. In Pandas 2.0+, Copy-on-Write makes this behavior consistent — enable with `pd.options.mode.copy_on_write = True`.", "code_signature": "status:closed", "tags": "numpy, groupby, reshape, dtype", "url": "https://github.com/pandas/pandas/issues/3264", "author": "contributor_6628", "date_published": "28-01-2023", "votes_or_stars": 1, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:28e3cb60-cd68-44bd-8ece-fe3460ae34e5": {"content": "How to run asyncio code from synchronous Python?\n`asyncio.run(main())` creates an event loop, runs the coroutine, closes the loop. Never call asyncio.run() from inside a running event loop (use await instead). In Jupyter notebooks (which have a running loop), use `await coroutine()` directly or `nest_asyncio.apply()`. For mixing sync/async, use `loop.run_until_complete()`.\nCode signature: Use asyncio.run() in Python 3.7+ to execute a coroutine from sync code.", "file_path": "https://stackoverflow.com/questions/2140194", "function_name": "Use asyncio.run() in Python 3.7+ to execute a coroutine from sync code.", "type": "stack_overflow", "metadata": {"record_id": "28e3cb60-cd68-44bd-8ece-fe3460ae34e5", "source_type": "stack_overflow", "domain": "Asyncio/Threading", "library": "gather", "title": "How to run asyncio code from synchronous Python?", "content": "`asyncio.run(main())` creates an event loop, runs the coroutine, closes the loop. Never call asyncio.run() from inside a running event loop (use await instead). In Jupyter notebooks (which have a running loop), use `await coroutine()` directly or `nest_asyncio.apply()`. For mixing sync/async, use `loop.run_until_complete()`.", "code_signature": "Use asyncio.run() in Python 3.7+ to execute a coroutine from sync code.", "tags": "race-condition, event-loop, semaphore", "url": "https://stackoverflow.com/questions/2140194", "author": "user_718011", "date_published": "18-02-2023", "votes_or_stars": 268, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:d426c73e-7967-485a-9779-70e97b483e50": {"content": "subprocess — Process Execution\nThe `subprocess.run` function in subprocess runs a subprocess and waits for it to complete. It accepts `args, capture_output=False, timeout=None` as a parameter and returns CompletedProcess. Common use cases include executing shell commands, scripts, or external programs from Python. Note that use shell=False (default) to avoid shell injection vulnerabilities.\nCode signature: subprocess.run(args, capture_output=False, timeout=None) -> CompletedProcess", "file_path": "https://docs.subprocess.org/en/stable/subprocess/run.html", "function_name": "subprocess.run(args, capture_output=False, timeout=None) -> CompletedProcess", "type": "documentation", "metadata": {"record_id": "d426c73e-7967-485a-9779-70e97b483e50", "source_type": "documentation", "domain": "OS/Subprocess/File I/O", "library": "subprocess", "title": "subprocess — Process Execution", "content": "The `subprocess.run` function in subprocess runs a subprocess and waits for it to complete. It accepts `args, capture_output=False, timeout=None` as a parameter and returns CompletedProcess. Common use cases include executing shell commands, scripts, or external programs from Python. Note that use shell=False (default) to avoid shell injection vulnerabilities.", "code_signature": "subprocess.run(args, capture_output=False, timeout=None) -> CompletedProcess", "tags": "stdout, env-variable, shutil, pipe, stdin, glob", "url": "https://docs.subprocess.org/en/stable/subprocess/run.html", "author": "Official Docs", "date_published": "02-09-2024", "votes_or_stars": 459, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:0c73a029-6045-4303-8b57-ca95f347b819": {"content": "How to use Path.glob in pathlib\n`pathlib.Path.glob` yields all paths matching the given glob pattern. Example usage:\n```python\npy_files = list(Path('src').glob('**/*.py'))\n```\nThis is useful when finding files recursively, batch file processing. Performance tip: iterate the generator lazily rather than converting to list for large directories.\nCode signature: Path.glob", "file_path": "https://docs.pathlib.org/en/stable/Path/glob.html", "function_name": "Path.glob", "type": "documentation", "metadata": {"record_id": "0c73a029-6045-4303-8b57-ca95f347b819", "source_type": "documentation", "domain": "OS/Subprocess/File I/O", "library": "pathlib", "title": "How to use Path.glob in pathlib", "content": "`pathlib.Path.glob` yields all paths matching the given glob pattern. Example usage:\n```python\npy_files = list(Path('src').glob('**/*.py'))\n```\nThis is useful when finding files recursively, batch file processing. Performance tip: iterate the generator lazily rather than converting to list for large directories.", "code_signature": "Path.glob", "tags": "shutil, tempfile, process, pipe, pathlib", "url": "https://docs.pathlib.org/en/stable/Path/glob.html", "author": "Official Docs", "date_published": "26-10-2022", "votes_or_stars": 3224, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:5a8bee98-24a2-45ac-861a-8e7a0a0ed380": {"content": "BUG: Flask session not persisting between requests in tests\nProblem: Session data set in one request is not available in the next request during testing.\n\nFix/Discussion: Use Flask test client's session_transaction() context manager: `with client.session_transaction() as sess:\n    sess['user_id'] = 1`. Ensure SECRET_KEY is set. For testing: `app.config['TESTING'] = True; app.config['SECRET_KEY'] = 'test'`.\nCode signature: status:closed", "file_path": "https://github.com/flask/flask/issues/2141", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "5a8bee98-24a2-45ac-861a-8e7a0a0ed380", "source_type": "github_issue", "domain": "FastAPI/Flask/Django", "library": "flask", "title": "BUG: Flask session not persisting between requests in tests", "content": "Problem: Session data set in one request is not available in the next request during testing.\n\nFix/Discussion: Use Flask test client's session_transaction() context manager: `with client.session_transaction() as sess:\n    sess['user_id'] = 1`. Ensure SECRET_KEY is set. For testing: `app.config['TESTING'] = True; app.config['SECRET_KEY'] = 'test'`.", "code_signature": "status:closed", "tags": "request, flask, pydantic, routing, response, dependency-injection", "url": "https://github.com/flask/flask/issues/2141", "author": "contributor_5020", "date_published": "13-03-2024", "votes_or_stars": 183, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:ac351aef-4522-4280-ae7d-c219a6542338": {"content": "BUG: pd.read_csv() silently truncates large integers\nProblem: Integer columns with values > 2^53 are silently corrupted when read from CSV.\n\nFix/Discussion: Floating point cannot represent integers > 2^53 exactly. Fix: `pd.read_csv(f, dtype={'col': str})` then convert: `df['col'] = df['col'].astype('Int64')`. Use `dtype_backend='numpy_nullable'` in Pandas 2.0+ for better integer handling.\nCode signature: status:closed", "file_path": "https://github.com/pandas/pandas/issues/4449", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "ac351aef-4522-4280-ae7d-c219a6542338", "source_type": "github_issue", "domain": "NumPy/Pandas", "library": "pandas", "title": "BUG: pd.read_csv() silently truncates large integers", "content": "Problem: Integer columns with values > 2^53 are silently corrupted when read from CSV.\n\nFix/Discussion: Floating point cannot represent integers > 2^53 exactly. Fix: `pd.read_csv(f, dtype={'col': str})` then convert: `df['col'] = df['col'].astype('Int64')`. Use `dtype_backend='numpy_nullable'` in Pandas 2.0+ for better integer handling.", "code_signature": "status:closed", "tags": "iloc, broadcasting, dtype, groupby", "url": "https://github.com/pandas/pandas/issues/4449", "author": "contributor_726", "date_published": "30-10-2022", "votes_or_stars": 478, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:9ddb9e1c-91c5-4fb0-80e8-37a6a580e8ea": {"content": "Pandas Merging DataFrames — official reference\nOfficial documentation for Pandas Merging DataFrames. The method signature is `pd.merge(left, right, on, how='inner')`. merges two DataFrames using SQL-style joins. Raises `MergeError` when merge keys produce duplicate columns. See also: df.join, df.concat.\nCode signature: pd.merge(left, right, on, how='inner')", "file_path": "https://docs.pandas.org/en/stable/pd/merge.html", "function_name": "pd.merge(left, right, on, how='inner')", "type": "documentation", "metadata": {"record_id": "9ddb9e1c-91c5-4fb0-80e8-37a6a580e8ea", "source_type": "documentation", "domain": "NumPy/Pandas", "library": "Pandas", "title": "Pandas Merging DataFrames — official reference", "content": "Official documentation for Pandas Merging DataFrames. The method signature is `pd.merge(left, right, on, how='inner')`. merges two DataFrames using SQL-style joins. Raises `MergeError` when merge keys produce duplicate columns. See also: df.join, df.concat.", "code_signature": "pd.merge(left, right, on, how='inner')", "tags": "iloc, array, nan", "url": "https://docs.pandas.org/en/stable/pd/merge.html", "author": "Official Docs", "date_published": "23-08-2023", "votes_or_stars": 4514, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:7721e542-7b90-4ac4-aa44-a9b82006d7ed": {"content": "BUG: subprocess.Popen hangs when stdout and stderr buffers fill\nProblem: subprocess.Popen hangs indefinitely when child process produces large output.\n\nFix/Discussion: This is a classic deadlock: parent waits for child, child waits for parent to read buffer. Fix: use `subprocess.run(..., capture_output=True)` which handles this correctly. Or: `stdout, stderr = proc.communicate()`. Never use `proc.stdout.read()` and `proc.wait()` separately.\nCode signature: status:closed", "file_path": "https://github.com/cpython/cpython/issues/8586", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "7721e542-7b90-4ac4-aa44-a9b82006d7ed", "source_type": "github_issue", "domain": "OS/Subprocess/File I/O", "library": "cpython", "title": "BUG: subprocess.Popen hangs when stdout and stderr buffers fill", "content": "Problem: subprocess.Popen hangs indefinitely when child process produces large output.\n\nFix/Discussion: This is a classic deadlock: parent waits for child, child waits for parent to read buffer. Fix: use `subprocess.run(..., capture_output=True)` which handles this correctly. Or: `stdout, stderr = proc.communicate()`. Never use `proc.stdout.read()` and `proc.wait()` separately.", "code_signature": "status:closed", "tags": "stderr, shutil, stdout, pathlib, os", "url": "https://github.com/cpython/cpython/issues/8586", "author": "contributor_7711", "date_published": "06-04-2023", "votes_or_stars": 422, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:9ea18b3f-fc50-4496-a9b7-c95154ca15d6": {"content": "How to use asyncio.create_task in asyncio\n`asyncio.asyncio.create_task` schedules a coroutine as a Task in the event loop. Example usage:\n```python\ntask = asyncio.create_task(send_email(user))\nawait task\n```\nThis is useful when fire-and-forget background tasks, concurrent task management. Performance tip: use TaskGroup (Python 3.11+) for structured concurrency.\nCode signature: asyncio.create_task", "file_path": "https://docs.asyncio.org/en/stable/asyncio/create_task.html", "function_name": "asyncio.create_task", "type": "documentation", "metadata": {"record_id": "9ea18b3f-fc50-4496-a9b7-c95154ca15d6", "source_type": "documentation", "domain": "Asyncio/Threading", "library": "asyncio", "title": "How to use asyncio.create_task in asyncio", "content": "`asyncio.asyncio.create_task` schedules a coroutine as a Task in the event loop. Example usage:\n```python\ntask = asyncio.create_task(send_email(user))\nawait task\n```\nThis is useful when fire-and-forget background tasks, concurrent task management. Performance tip: use TaskGroup (Python 3.11+) for structured concurrency.", "code_signature": "asyncio.create_task", "tags": "await, task, executor, asyncio, race-condition, gather", "url": "https://docs.asyncio.org/en/stable/asyncio/create_task.html", "author": "Official Docs", "date_published": "13-12-2021", "votes_or_stars": 4119, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:a1c367d6-1b2c-490f-8551-1d2445e5d80d": {"content": "How do I handle CORS in FastAPI?\n```python\nfrom fastapi.middleware.cors import CORSMiddleware\napp.add_middleware(CORSMiddleware, allow_origins=['*'], allow_methods=['*'], allow_headers=['*'])\n```\nIn production, replace `'*'` with your actual frontend domain. For credentials (cookies), set `allow_credentials=True` and specify exact origins.\nCode signature: Add CORSMiddleware from starlette to your FastAPI app.", "file_path": "https://stackoverflow.com/questions/1527021", "function_name": "Add CORSMiddleware from starlette to your FastAPI app.", "type": "stack_overflow", "metadata": {"record_id": "a1c367d6-1b2c-490f-8551-1d2445e5d80d", "source_type": "stack_overflow", "domain": "FastAPI/Flask/Django", "library": "flask", "title": "How do I handle CORS in FastAPI?", "content": "```python\nfrom fastapi.middleware.cors import CORSMiddleware\napp.add_middleware(CORSMiddleware, allow_origins=['*'], allow_methods=['*'], allow_headers=['*'])\n```\nIn production, replace `'*'` with your actual frontend domain. For credentials (cookies), set `allow_credentials=True` and specify exact origins.", "code_signature": "Add CORSMiddleware from starlette to your FastAPI app.", "tags": "dependency-injection, middleware, django, blueprint, flask, pydantic", "url": "https://stackoverflow.com/questions/1527021", "author": "user_911393", "date_published": "03-05-2023", "votes_or_stars": 959, "relevance_label": "low", "difficulty_level": "advanced"}}, "kb:b4887999-3f83-4943-a9a0-ae02f95f7497": {"content": "How to read a large file line by line in Python without loading it all into memory?\n```python\nwith open('large.txt', 'r') as f:\n    for line in f:\n        process(line.strip())\n```\nThis reads one line at a time. For binary files: use `f.read(chunk_size)` in a loop. For CSV: use `pd.read_csv(file, chunksize=10000)`. Never use `f.readlines()` on large files.\nCode signature: Iterate over the file object directly — it yields lines lazily.", "file_path": "https://stackoverflow.com/questions/2737893", "function_name": "Iterate over the file object directly — it yields lines lazily.", "type": "stack_overflow", "metadata": {"record_id": "b4887999-3f83-4943-a9a0-ae02f95f7497", "source_type": "stack_overflow", "domain": "OS/Subprocess/File I/O", "library": "shutil", "title": "How to read a large file line by line in Python without loading it all into memory?", "content": "```python\nwith open('large.txt', 'r') as f:\n    for line in f:\n        process(line.strip())\n```\nThis reads one line at a time. For binary files: use `f.read(chunk_size)` in a loop. For CSV: use `pd.read_csv(file, chunksize=10000)`. Never use `f.readlines()` on large files.", "code_signature": "Iterate over the file object directly — it yields lines lazily.", "tags": "pipe, pathlib, os", "url": "https://stackoverflow.com/questions/2737893", "author": "user_994539", "date_published": "23-08-2024", "votes_or_stars": 2261, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:33036785-f3bc-4359-a99f-3e5be89db8c6": {"content": "SQLAlchemy bulk insert: what's the fastest way?\n`session.bulk_insert_mappings(Model, list_of_dicts)` skips ORM overhead. Even faster: `session.execute(insert(Model), data)`. Avoid `session.add()` in loops. For PostgreSQL, use `insert().on_conflict_do_nothing()` for upserts. Batch in chunks of 1000-5000 rows for optimal performance.\nCode signature: Use session.bulk_insert_mappings() or execute(insert(), list_of_dicts) for maximum speed.", "file_path": "https://stackoverflow.com/questions/5977931", "function_name": "Use session.bulk_insert_mappings() or execute(insert(), list_of_dicts) for maximum speed.", "type": "stack_overflow", "metadata": {"record_id": "33036785-f3bc-4359-a99f-3e5be89db8c6", "source_type": "stack_overflow", "domain": "SQLAlchemy/Databases", "library": "query", "title": "SQLAlchemy bulk insert: what's the fastest way?", "content": "`session.bulk_insert_mappings(Model, list_of_dicts)` skips ORM overhead. Even faster: `session.execute(insert(Model), data)`. Avoid `session.add()` in loops. For PostgreSQL, use `insert().on_conflict_do_nothing()` for upserts. Batch in chunks of 1000-5000 rows for optimal performance.", "code_signature": "Use session.bulk_insert_mappings() or execute(insert(), list_of_dicts) for maximum speed.", "tags": "sqlite, postgresql, sqlalchemy", "url": "https://stackoverflow.com/questions/5977931", "author": "user_238275", "date_published": "03-05-2024", "votes_or_stars": 629, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:86377054-1cc8-4ec7-b31c-880db432b24e": {"content": "BUG: Flask session not persisting between requests in tests\nProblem: Session data set in one request is not available in the next request during testing.\n\nFix/Discussion: Use Flask test client's session_transaction() context manager: `with client.session_transaction() as sess:\n    sess['user_id'] = 1`. Ensure SECRET_KEY is set. For testing: `app.config['TESTING'] = True; app.config['SECRET_KEY'] = 'test'`.\nCode signature: status:closed", "file_path": "https://github.com/flask/flask/issues/2141", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "86377054-1cc8-4ec7-b31c-880db432b24e", "source_type": "github_issue", "domain": "FastAPI/Flask/Django", "library": "flask", "title": "BUG: Flask session not persisting between requests in tests", "content": "Problem: Session data set in one request is not available in the next request during testing.\n\nFix/Discussion: Use Flask test client's session_transaction() context manager: `with client.session_transaction() as sess:\n    sess['user_id'] = 1`. Ensure SECRET_KEY is set. For testing: `app.config['TESTING'] = True; app.config['SECRET_KEY'] = 'test'`.", "code_signature": "status:closed", "tags": "blueprint, pydantic, request, orm, response", "url": "https://github.com/flask/flask/issues/2141", "author": "contributor_5020", "date_published": "30-01-2018", "votes_or_stars": 159, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:f84f17ec-b275-469c-a2c5-ec62c0922c8c": {"content": "SQLAlchemy bulk insert: what's the fastest way?\n`session.bulk_insert_mappings(Model, list_of_dicts)` skips ORM overhead. Even faster: `session.execute(insert(Model), data)`. Avoid `session.add()` in loops. For PostgreSQL, use `insert().on_conflict_do_nothing()` for upserts. Batch in chunks of 1000-5000 rows for optimal performance.\nCode signature: Use session.bulk_insert_mappings() or execute(insert(), list_of_dicts) for maximum speed.", "file_path": "https://stackoverflow.com/questions/5977931", "function_name": "Use session.bulk_insert_mappings() or execute(insert(), list_of_dicts) for maximum speed.", "type": "stack_overflow", "metadata": {"record_id": "f84f17ec-b275-469c-a2c5-ec62c0922c8c", "source_type": "stack_overflow", "domain": "SQLAlchemy/Databases", "library": "query", "title": "SQLAlchemy bulk insert: what's the fastest way?", "content": "`session.bulk_insert_mappings(Model, list_of_dicts)` skips ORM overhead. Even faster: `session.execute(insert(Model), data)`. Avoid `session.add()` in loops. For PostgreSQL, use `insert().on_conflict_do_nothing()` for upserts. Batch in chunks of 1000-5000 rows for optimal performance.", "code_signature": "Use session.bulk_insert_mappings() or execute(insert(), list_of_dicts) for maximum speed.", "tags": "postgresql, foreign-key, query, session", "url": "https://stackoverflow.com/questions/5977931", "author": "user_238275", "date_published": "17-02-2022", "votes_or_stars": 651, "relevance_label": "low", "difficulty_level": "intermediate"}}, "kb:c3b46f9f-714b-471b-ac4b-298aebe7e7bc": {"content": "PERF: Django template rendering bottleneck in production\nProblem: Template rendering takes 500ms+ for complex pages with many database queries.\n\nFix/Discussion: Primary cause is N+1 queries in template. Solutions: (1) Use select_related/prefetch_related. (2) Cache rendered templates with `cache` template tag. (3) Use `django-cachalot` for query caching. (4) Move heavy computation to views, pass pre-computed context. (5) Consider server-side caching with Redis.\nCode signature: status:closed", "file_path": "https://github.com/django/django/issues/3243", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "c3b46f9f-714b-471b-ac4b-298aebe7e7bc", "source_type": "github_issue", "domain": "FastAPI/Flask/Django", "library": "django", "title": "PERF: Django template rendering bottleneck in production", "content": "Problem: Template rendering takes 500ms+ for complex pages with many database queries.\n\nFix/Discussion: Primary cause is N+1 queries in template. Solutions: (1) Use select_related/prefetch_related. (2) Cache rendered templates with `cache` template tag. (3) Use `django-cachalot` for query caching. (4) Move heavy computation to views, pass pre-computed context. (5) Consider server-side caching with Redis.", "code_signature": "status:closed", "tags": "pydantic, template, routing, orm, rest, request", "url": "https://github.com/django/django/issues/3243", "author": "contributor_6988", "date_published": "07-02-2019", "votes_or_stars": 56, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:a95df8e8-5e0e-42c3-bb28-991435317f56": {"content": "threading Thread Synchronization — official reference\nOfficial documentation for threading Thread Synchronization. The method signature is `threading.Lock()`. creates a mutual-exclusion lock for thread-safe access. Raises `RuntimeError` when lock is released without being acquired. See also: threading.RLock, threading.Semaphore, threading.Event.\nCode signature: threading.Lock()", "file_path": "https://docs.threading.org/en/stable/threading/Lock.html", "function_name": "threading.Lock()", "type": "documentation", "metadata": {"record_id": "a95df8e8-5e0e-42c3-bb28-991435317f56", "source_type": "documentation", "domain": "Asyncio/Threading", "library": "threading", "title": "threading Thread Synchronization — official reference", "content": "Official documentation for threading Thread Synchronization. The method signature is `threading.Lock()`. creates a mutual-exclusion lock for thread-safe access. Raises `RuntimeError` when lock is released without being acquired. See also: threading.RLock, threading.Semaphore, threading.Event.", "code_signature": "threading.Lock()", "tags": "semaphore, race-condition, gather, threading", "url": "https://docs.threading.org/en/stable/threading/Lock.html", "author": "Official Docs", "date_published": "23-10-2019", "votes_or_stars": 4851, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:90b9562f-969c-46e8-b31d-6bf745948352": {"content": "Python threading: how to share data between threads safely?\nFor simple counters: use `threading.Lock()` as context manager. For complex shared state: prefer `queue.Queue` (thread-safe by design). Avoid global variables without locks. `threading.local()` gives each thread its own data copy. For read-heavy workloads, `threading.RLock` allows re-entrant acquisition.\nCode signature: Use threading.Lock for mutual exclusion or queue.Queue for producer-consumer patterns.", "file_path": "https://stackoverflow.com/questions/3195773", "function_name": "Use threading.Lock for mutual exclusion or queue.Queue for producer-consumer patterns.", "type": "stack_overflow", "metadata": {"record_id": "90b9562f-969c-46e8-b31d-6bf745948352", "source_type": "stack_overflow", "domain": "Asyncio/Threading", "library": "lock", "title": "Python threading: how to share data between threads safely?", "content": "For simple counters: use `threading.Lock()` as context manager. For complex shared state: prefer `queue.Queue` (thread-safe by design). Avoid global variables without locks. `threading.local()` gives each thread its own data copy. For read-heavy workloads, `threading.RLock` allows re-entrant acquisition.", "code_signature": "Use threading.Lock for mutual exclusion or queue.Queue for producer-consumer patterns.", "tags": "race-condition, gather, coroutine, task, executor", "url": "https://stackoverflow.com/questions/3195773", "author": "user_714318", "date_published": "09-06-2021", "votes_or_stars": 1104, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:8dabb0b8-a176-48d7-a9c7-ee70dddd1fdc": {"content": "FastAPI — Path Operations\nThe `@app.get` function in FastAPI registers a GET endpoint at the specified path. It accepts `path, response_model` as a parameter and returns Response. Common use cases include building REST API endpoints that return JSON data. Note that use response_model to enforce output schema validation.\nCode signature: @app.get(path, response_model) -> Response", "file_path": "https://docs.fastapi.org/en/stable/@app/get.html", "function_name": "@app.get(path, response_model) -> Response", "type": "documentation", "metadata": {"record_id": "8dabb0b8-a176-48d7-a9c7-ee70dddd1fdc", "source_type": "documentation", "domain": "FastAPI/Flask/Django", "library": "FastAPI", "title": "FastAPI — Path Operations", "content": "The `@app.get` function in FastAPI registers a GET endpoint at the specified path. It accepts `path, response_model` as a parameter and returns Response. Common use cases include building REST API endpoints that return JSON data. Note that use response_model to enforce output schema validation.", "code_signature": "@app.get(path, response_model) -> Response", "tags": "blueprint, flask, request", "url": "https://docs.fastapi.org/en/stable/@app/get.html", "author": "Official Docs", "date_published": "15-06-2022", "votes_or_stars": 2892, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:47400399-4edb-4315-aa43-8f1533167f83": {"content": "How to use asyncio.create_task in asyncio\n`asyncio.asyncio.create_task` schedules a coroutine as a Task in the event loop. Example usage:\n```python\ntask = asyncio.create_task(send_email(user))\nawait task\n```\nThis is useful when fire-and-forget background tasks, concurrent task management. Performance tip: use TaskGroup (Python 3.11+) for structured concurrency.\nCode signature: asyncio.create_task", "file_path": "https://docs.asyncio.org/en/stable/asyncio/create_task.html", "function_name": "asyncio.create_task", "type": "documentation", "metadata": {"record_id": "47400399-4edb-4315-aa43-8f1533167f83", "source_type": "documentation", "domain": "Asyncio/Threading", "library": "asyncio", "title": "How to use asyncio.create_task in asyncio", "content": "`asyncio.asyncio.create_task` schedules a coroutine as a Task in the event loop. Example usage:\n```python\ntask = asyncio.create_task(send_email(user))\nawait task\n```\nThis is useful when fire-and-forget background tasks, concurrent task management. Performance tip: use TaskGroup (Python 3.11+) for structured concurrency.", "code_signature": "asyncio.create_task", "tags": "lock, executor, coroutine", "url": "https://docs.asyncio.org/en/stable/asyncio/create_task.html", "author": "Official Docs", "date_published": "20-05-2019", "votes_or_stars": 4132, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:76ca63c4-e148-46f7-94a7-6df806c1d763": {"content": "How do I reshape a wide DataFrame to long format?\n`pd.melt(df, id_vars=['id'], value_vars=['col1','col2'])` unpivots columns to rows. `df.stack()` moves column index to row index. For the reverse, use `df.pivot()` or `df.unstack()`. Always reset_index() after stack/unstack to get a clean DataFrame.\nCode signature: Use pd.melt() or df.stack() depending on your structure.", "file_path": "https://stackoverflow.com/questions/6229731", "function_name": "Use pd.melt() or df.stack() depending on your structure.", "type": "stack_overflow", "metadata": {"record_id": "76ca63c4-e148-46f7-94a7-6df806c1d763", "source_type": "stack_overflow", "domain": "NumPy/Pandas", "library": "nan", "title": "How do I reshape a wide DataFrame to long format?", "content": "`pd.melt(df, id_vars=['id'], value_vars=['col1','col2'])` unpivots columns to rows. `df.stack()` moves column index to row index. For the reverse, use `df.pivot()` or `df.unstack()`. Always reset_index() after stack/unstack to get a clean DataFrame.", "code_signature": "Use pd.melt() or df.stack() depending on your structure.", "tags": "dtype, pivot, array, iloc, merge", "url": "https://stackoverflow.com/questions/6229731", "author": "user_428373", "date_published": "16-07-2023", "votes_or_stars": 810, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:fd6df30d-59a2-47a3-9863-52e7fd6c71fb": {"content": "Why is my Pandas merge creating duplicate rows?\nDuplicate rows after merge usually mean the join key is not unique. Diagnose with `df.duplicated(subset=['key']).sum()`. Use `how='left'` carefully when right table has multiple matches. Set `validate='one_to_many'` or `'many_to_one'` to enforce cardinality. Consider deduplicating with `df.drop_duplicates(subset=['key'])` before merging.\nCode signature: Your merge key has duplicates in one or both DataFrames. Use validate='one_to_one' to catch this early.", "file_path": "https://stackoverflow.com/questions/2321324", "function_name": "Your merge key has duplicates in one or both DataFrames. Use validate='one_to_one' to catch this early.", "type": "stack_overflow", "metadata": {"record_id": "fd6df30d-59a2-47a3-9863-52e7fd6c71fb", "source_type": "stack_overflow", "domain": "NumPy/Pandas", "library": "array", "title": "Why is my Pandas merge creating duplicate rows?", "content": "Duplicate rows after merge usually mean the join key is not unique. Diagnose with `df.duplicated(subset=['key']).sum()`. Use `how='left'` carefully when right table has multiple matches. Set `validate='one_to_many'` or `'many_to_one'` to enforce cardinality. Consider deduplicating with `df.drop_duplicates(subset=['key'])` before merging.", "code_signature": "Your merge key has duplicates in one or both DataFrames. Use validate='one_to_one' to catch this early.", "tags": "pivot, iloc, loc", "url": "https://stackoverflow.com/questions/2321324", "author": "user_99814", "date_published": "26-12-2022", "votes_or_stars": 250, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:76510d15-f206-40cb-9e50-bdd86ebdaa88": {"content": "subprocess.run vs Popen: which should I use?\n`subprocess.run()` is a high-level wrapper that waits for completion — use for most cases. `subprocess.Popen()` gives full control: stream stdout/stderr in real-time, write to stdin, run processes concurrently. For capturing output: `subprocess.run(..., capture_output=True, text=True)`. Always use `shell=False` and pass args as a list to prevent shell injection.\nCode signature: Use subprocess.run() for simple cases; Popen for streaming output or interactive processes.", "file_path": "https://stackoverflow.com/questions/5727085", "function_name": "Use subprocess.run() for simple cases; Popen for streaming output or interactive processes.", "type": "stack_overflow", "metadata": {"record_id": "76510d15-f206-40cb-9e50-bdd86ebdaa88", "source_type": "stack_overflow", "domain": "OS/Subprocess/File I/O", "library": "shutil", "title": "subprocess.run vs Popen: which should I use?", "content": "`subprocess.run()` is a high-level wrapper that waits for completion — use for most cases. `subprocess.Popen()` gives full control: stream stdout/stderr in real-time, write to stdin, run processes concurrently. For capturing output: `subprocess.run(..., capture_output=True, text=True)`. Always use `shell=False` and pass args as a list to prevent shell injection.", "code_signature": "Use subprocess.run() for simple cases; Popen for streaming output or interactive processes.", "tags": "stderr, glob, process", "url": "https://stackoverflow.com/questions/5727085", "author": "user_644210", "date_published": "04-10-2018", "votes_or_stars": 616, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:0fda8035-e68b-4b7e-9b51-9e47f3f4dec7": {"content": "subprocess.run vs Popen: which should I use?\n`subprocess.run()` is a high-level wrapper that waits for completion — use for most cases. `subprocess.Popen()` gives full control: stream stdout/stderr in real-time, write to stdin, run processes concurrently. For capturing output: `subprocess.run(..., capture_output=True, text=True)`. Always use `shell=False` and pass args as a list to prevent shell injection.\nCode signature: Use subprocess.run() for simple cases; Popen for streaming output or interactive processes.", "file_path": "https://stackoverflow.com/questions/5727085", "function_name": "Use subprocess.run() for simple cases; Popen for streaming output or interactive processes.", "type": "stack_overflow", "metadata": {"record_id": "0fda8035-e68b-4b7e-9b51-9e47f3f4dec7", "source_type": "stack_overflow", "domain": "OS/Subprocess/File I/O", "library": "shutil", "title": "subprocess.run vs Popen: which should I use?", "content": "`subprocess.run()` is a high-level wrapper that waits for completion — use for most cases. `subprocess.Popen()` gives full control: stream stdout/stderr in real-time, write to stdin, run processes concurrently. For capturing output: `subprocess.run(..., capture_output=True, text=True)`. Always use `shell=False` and pass args as a list to prevent shell injection.", "code_signature": "Use subprocess.run() for simple cases; Popen for streaming output or interactive processes.", "tags": "pathlib, process, shutil, file-io, stdin", "url": "https://stackoverflow.com/questions/5727085", "author": "user_644210", "date_published": "17-02-2019", "votes_or_stars": 636, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:47142c73-b11f-46fa-80a7-454dc56c736c": {"content": "asyncio — Concurrent Coroutines\nThe `asyncio.gather` function in asyncio runs multiple coroutines concurrently and collects results. It accepts `*coros, return_exceptions=False` as a parameter and returns list of results. Common use cases include parallel I/O operations such as multiple API calls. Note that set return_exceptions=True to prevent one failure from cancelling all tasks.\nCode signature: asyncio.gather(*coros, return_exceptions=False) -> list of results", "file_path": "https://docs.asyncio.org/en/stable/asyncio/gather.html", "function_name": "asyncio.gather(*coros, return_exceptions=False) -> list of results", "type": "documentation", "metadata": {"record_id": "47142c73-b11f-46fa-80a7-454dc56c736c", "source_type": "documentation", "domain": "Asyncio/Threading", "library": "asyncio", "title": "asyncio — Concurrent Coroutines", "content": "The `asyncio.gather` function in asyncio runs multiple coroutines concurrently and collects results. It accepts `*coros, return_exceptions=False` as a parameter and returns list of results. Common use cases include parallel I/O operations such as multiple API calls. Note that set return_exceptions=True to prevent one failure from cancelling all tasks.", "code_signature": "asyncio.gather(*coros, return_exceptions=False) -> list of results", "tags": "lock, executor, async, task, coroutine", "url": "https://docs.asyncio.org/en/stable/asyncio/gather.html", "author": "Official Docs", "date_published": "21-04-2020", "votes_or_stars": 522, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:6a93a63f-38a2-4b43-a798-7f572dfbfee5": {"content": "How to handle rate limiting with the requests library?\n```python\nfrom tenacity import retry, wait_exponential, stop_after_attempt\n@retry(wait=wait_exponential(min=1, max=60), stop=stop_after_attempt(5))\ndef call_api(url):\n    r = requests.get(url)\n    r.raise_for_status()\n    return r.json()\n```\nCheck `Retry-After` header in 429 responses. For async: use `aiohttp` with tenacity.\nCode signature: Implement exponential backoff with the tenacity or backoff library.", "file_path": "https://stackoverflow.com/questions/1604451", "function_name": "Implement exponential backoff with the tenacity or backoff library.", "type": "stack_overflow", "metadata": {"record_id": "6a93a63f-38a2-4b43-a798-7f572dfbfee5", "source_type": "stack_overflow", "domain": "Requests/HTTP/APIs", "library": "retry", "title": "How to handle rate limiting with the requests library?", "content": "```python\nfrom tenacity import retry, wait_exponential, stop_after_attempt\n@retry(wait=wait_exponential(min=1, max=60), stop=stop_after_attempt(5))\ndef call_api(url):\n    r = requests.get(url)\n    r.raise_for_status()\n    return r.json()\n```\nCheck `Retry-After` header in 429 responses. For async: use `aiohttp` with tenacity.", "code_signature": "Implement exponential backoff with the tenacity or backoff library.", "tags": "headers, requests, json, authentication, http", "url": "https://stackoverflow.com/questions/1604451", "author": "user_885136", "date_published": "12-10-2023", "votes_or_stars": 626, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:4fcc5a97-f6b0-4342-a1a5-2cc29c5051dc": {"content": "How to read a large file line by line in Python without loading it all into memory?\n```python\nwith open('large.txt', 'r') as f:\n    for line in f:\n        process(line.strip())\n```\nThis reads one line at a time. For binary files: use `f.read(chunk_size)` in a loop. For CSV: use `pd.read_csv(file, chunksize=10000)`. Never use `f.readlines()` on large files.\nCode signature: Iterate over the file object directly — it yields lines lazily.", "file_path": "https://stackoverflow.com/questions/2737893", "function_name": "Iterate over the file object directly — it yields lines lazily.", "type": "stack_overflow", "metadata": {"record_id": "4fcc5a97-f6b0-4342-a1a5-2cc29c5051dc", "source_type": "stack_overflow", "domain": "OS/Subprocess/File I/O", "library": "shutil", "title": "How to read a large file line by line in Python without loading it all into memory?", "content": "```python\nwith open('large.txt', 'r') as f:\n    for line in f:\n        process(line.strip())\n```\nThis reads one line at a time. For binary files: use `f.read(chunk_size)` in a loop. For CSV: use `pd.read_csv(file, chunksize=10000)`. Never use `f.readlines()` on large files.", "code_signature": "Iterate over the file object directly — it yields lines lazily.", "tags": "shutil, subprocess, glob, os, tempfile, pathlib", "url": "https://stackoverflow.com/questions/2737893", "author": "user_994539", "date_published": "25-10-2022", "votes_or_stars": 2287, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:4f845d01-e877-46cc-b280-d50905a4cd0e": {"content": "BUG: threading.Timer not cancellable after it fires\nProblem: Calling cancel() on a Timer that has already executed does not raise an error but has no effect.\n\nFix/Discussion: threading.Timer.cancel() only works before the timer fires. Store Timer reference and check `timer.finished.is_set()` to know if it's already run. For repeating timers, use a loop with `threading.Event` for clean cancellation: `stop_event = threading.Event(); stop_event.wait(interval)`.\nCode signature: status:closed", "file_path": "https://github.com/cpython/cpython/issues/4111", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "4f845d01-e877-46cc-b280-d50905a4cd0e", "source_type": "github_issue", "domain": "Asyncio/Threading", "library": "cpython", "title": "BUG: threading.Timer not cancellable after it fires", "content": "Problem: Calling cancel() on a Timer that has already executed does not raise an error but has no effect.\n\nFix/Discussion: threading.Timer.cancel() only works before the timer fires. Store Timer reference and check `timer.finished.is_set()` to know if it's already run. For repeating timers, use a loop with `threading.Event` for clean cancellation: `stop_event = threading.Event(); stop_event.wait(interval)`.", "code_signature": "status:closed", "tags": "gather, executor, deadlock, event-loop, asyncio", "url": "https://github.com/cpython/cpython/issues/4111", "author": "contributor_7884", "date_published": "09-03-2021", "votes_or_stars": 9, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:7662b01e-8d72-4b0b-814e-f996bb4a0548": {"content": "SQLAlchemy — ORM Session Query\nThe `session.query` function in SQLAlchemy constructs a SELECT query against mapped entities. It accepts `*entities` as a parameter and returns Query object. Common use cases include retrieving ORM-mapped objects from the database. Note that prefer session.execute(select(...)) in SQLAlchemy 2.0+.\nCode signature: session.query(*entities) -> Query object", "file_path": "https://docs.sqlalchemy.org/en/stable/session/query.html", "function_name": "session.query(*entities) -> Query object", "type": "documentation", "metadata": {"record_id": "7662b01e-8d72-4b0b-814e-f996bb4a0548", "source_type": "documentation", "domain": "SQLAlchemy/Databases", "library": "SQLAlchemy", "title": "SQLAlchemy — ORM Session Query", "content": "The `session.query` function in SQLAlchemy constructs a SELECT query against mapped entities. It accepts `*entities` as a parameter and returns Query object. Common use cases include retrieving ORM-mapped objects from the database. Note that prefer session.execute(select(...)) in SQLAlchemy 2.0+.", "code_signature": "session.query(*entities) -> Query object", "tags": "transaction, alembic, foreign-key", "url": "https://docs.sqlalchemy.org/en/stable/session/query.html", "author": "Official Docs", "date_published": "30-08-2019", "votes_or_stars": 3039, "relevance_label": "low", "difficulty_level": "beginner"}}, "kb:f8cdfa80-9513-4687-b03f-7bc40f6c2069": {"content": "How to handle database migrations with Alembic?\nWorkflow: (1) Change SQLAlchemy models. (2) `alembic revision --autogenerate -m 'description'`. (3) Review generated migration script. (4) `alembic upgrade head` to apply. Always review autogenerated migrations — they miss some changes (indexes, constraints). Use `alembic downgrade -1` to rollback. Store migrations in version control.\nCode signature: Use alembic revision --autogenerate to create migrations from model changes.", "file_path": "https://stackoverflow.com/questions/9436429", "function_name": "Use alembic revision --autogenerate to create migrations from model changes.", "type": "stack_overflow", "metadata": {"record_id": "f8cdfa80-9513-4687-b03f-7bc40f6c2069", "source_type": "stack_overflow", "domain": "SQLAlchemy/Databases", "library": "connection-pool", "title": "How to handle database migrations with Alembic?", "content": "Workflow: (1) Change SQLAlchemy models. (2) `alembic revision --autogenerate -m 'description'`. (3) Review generated migration script. (4) `alembic upgrade head` to apply. Always review autogenerated migrations — they miss some changes (indexes, constraints). Use `alembic downgrade -1` to rollback. Store migrations in version control.", "code_signature": "Use alembic revision --autogenerate to create migrations from model changes.", "tags": "connection-pool, orm, sqlite, alembic, postgresql", "url": "https://stackoverflow.com/questions/9436429", "author": "user_974047", "date_published": "10-12-2022", "votes_or_stars": 2393, "relevance_label": "low", "difficulty_level": "beginner"}}, "kb:f3c4fb3b-643b-4840-90c1-2bf069fa9ec7": {"content": "BUG: FastAPI dependency_overrides not working in pytest\nProblem: Dependency overrides set in conftest are not applied during test execution.\n\nFix/Discussion: Set overrides on the app object before TestClient is created. Use `app.dependency_overrides[dep] = mock_dep` in a pytest fixture with function scope. Clear overrides after test: `app.dependency_overrides = {}`. Ensure TestClient is created after overrides are set.\nCode signature: status:closed", "file_path": "https://github.com/fastapi/fastapi/issues/8479", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "f3c4fb3b-643b-4840-90c1-2bf069fa9ec7", "source_type": "github_issue", "domain": "FastAPI/Flask/Django", "library": "fastapi", "title": "BUG: FastAPI dependency_overrides not working in pytest", "content": "Problem: Dependency overrides set in conftest are not applied during test execution.\n\nFix/Discussion: Set overrides on the app object before TestClient is created. Use `app.dependency_overrides[dep] = mock_dep` in a pytest fixture with function scope. Clear overrides after test: `app.dependency_overrides = {}`. Ensure TestClient is created after overrides are set.", "code_signature": "status:closed", "tags": "routing, dependency-injection, fastapi, pydantic", "url": "https://github.com/fastapi/fastapi/issues/8479", "author": "contributor_1994", "date_published": "08-04-2018", "votes_or_stars": 299, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:fe03e4e0-30dd-4cae-ae85-2f059e1a27d5": {"content": "How to read a large file line by line in Python without loading it all into memory?\n```python\nwith open('large.txt', 'r') as f:\n    for line in f:\n        process(line.strip())\n```\nThis reads one line at a time. For binary files: use `f.read(chunk_size)` in a loop. For CSV: use `pd.read_csv(file, chunksize=10000)`. Never use `f.readlines()` on large files.\nCode signature: Iterate over the file object directly — it yields lines lazily.", "file_path": "https://stackoverflow.com/questions/2737893", "function_name": "Iterate over the file object directly — it yields lines lazily.", "type": "stack_overflow", "metadata": {"record_id": "fe03e4e0-30dd-4cae-ae85-2f059e1a27d5", "source_type": "stack_overflow", "domain": "OS/Subprocess/File I/O", "library": "shutil", "title": "How to read a large file line by line in Python without loading it all into memory?", "content": "```python\nwith open('large.txt', 'r') as f:\n    for line in f:\n        process(line.strip())\n```\nThis reads one line at a time. For binary files: use `f.read(chunk_size)` in a loop. For CSV: use `pd.read_csv(file, chunksize=10000)`. Never use `f.readlines()` on large files.", "code_signature": "Iterate over the file object directly — it yields lines lazily.", "tags": "env-variable, subprocess, stderr, pipe, tempfile, stdin", "url": "https://stackoverflow.com/questions/2737893", "author": "user_994539", "date_published": "17-07-2021", "votes_or_stars": 2286, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:47931292-669c-4cba-add6-4b0295b6eeaa": {"content": "Django N+1 query problem: how to fix?\nN+1 occurs when accessing related objects in a loop. Diagnose with `django-debug-toolbar` or `connection.queries`. Fix ForeignKey: `User.objects.select_related('profile').all()`. Fix M2M: `Post.objects.prefetch_related('tags').all()`. Use `only()` to fetch specific fields and reduce data transfer.\nCode signature: Use select_related() for ForeignKey and prefetch_related() for ManyToMany relationships.", "file_path": "https://stackoverflow.com/questions/8106470", "function_name": "Use select_related() for ForeignKey and prefetch_related() for ManyToMany relationships.", "type": "stack_overflow", "metadata": {"record_id": "47931292-669c-4cba-add6-4b0295b6eeaa", "source_type": "stack_overflow", "domain": "FastAPI/Flask/Django", "library": "request", "title": "Django N+1 query problem: how to fix?", "content": "N+1 occurs when accessing related objects in a loop. Diagnose with `django-debug-toolbar` or `connection.queries`. Fix ForeignKey: `User.objects.select_related('profile').all()`. Fix M2M: `Post.objects.prefetch_related('tags').all()`. Use `only()` to fetch specific fields and reduce data transfer.", "code_signature": "Use select_related() for ForeignKey and prefetch_related() for ManyToMany relationships.", "tags": "pydantic, blueprint, flask, fastapi", "url": "https://stackoverflow.com/questions/8106470", "author": "user_441071", "date_published": "17-05-2024", "votes_or_stars": 1771, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:d74a0ded-899f-4597-a829-dc9dfce855f0": {"content": "FEAT: Django: add support for async ORM queries\nProblem: Django ORM is synchronous; using it in async views requires sync_to_async wrapper.\n\nFix/Discussion: Django 4.1+ added native async ORM support. Use `await Model.objects.filter().afirst()`, `async for obj in qs:`. For older versions: wrap with `sync_to_async`: `get_user = sync_to_async(User.objects.get)\nuser = await get_user(pk=1)`.\nCode signature: status:closed", "file_path": "https://github.com/django/django/issues/8618", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "d74a0ded-899f-4597-a829-dc9dfce855f0", "source_type": "github_issue", "domain": "FastAPI/Flask/Django", "library": "django", "title": "FEAT: Django: add support for async ORM queries", "content": "Problem: Django ORM is synchronous; using it in async views requires sync_to_async wrapper.\n\nFix/Discussion: Django 4.1+ added native async ORM support. Use `await Model.objects.filter().afirst()`, `async for obj in qs:`. For older versions: wrap with `sync_to_async`: `get_user = sync_to_async(User.objects.get)\nuser = await get_user(pk=1)`.", "code_signature": "status:closed", "tags": "response, request, blueprint, fastapi, django, template", "url": "https://github.com/django/django/issues/8618", "author": "contributor_8921", "date_published": "25-07-2024", "votes_or_stars": 35, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:bd786fa3-1e7c-46b1-bb02-3795e3db27dd": {"content": "How to use @app.route in Flask\n`Flask.@app.route` maps a URL rule to a view function. Example usage:\n```python\n@app.route('/users/<int:uid>')\ndef get_user(uid):\n    return jsonify(user)\n```\nThis is useful when defining HTTP endpoints in a Flask application. Performance tip: use Blueprints to organize large applications.\nCode signature: @app.route", "file_path": "https://docs.flask.org/en/stable/@app/route.html", "function_name": "@app.route", "type": "documentation", "metadata": {"record_id": "bd786fa3-1e7c-46b1-bb02-3795e3db27dd", "source_type": "documentation", "domain": "FastAPI/Flask/Django", "library": "Flask", "title": "How to use @app.route in Flask", "content": "`Flask.@app.route` maps a URL rule to a view function. Example usage:\n```python\n@app.route('/users/<int:uid>')\ndef get_user(uid):\n    return jsonify(user)\n```\nThis is useful when defining HTTP endpoints in a Flask application. Performance tip: use Blueprints to organize large applications.", "code_signature": "@app.route", "tags": "response, template, fastapi, routing", "url": "https://docs.flask.org/en/stable/@app/route.html", "author": "Official Docs", "date_published": "26-05-2019", "votes_or_stars": 3014, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:e03d17a7-4255-48ef-b3ad-6b7ada74bca2": {"content": "BUG: Alembic autogenerate misses index changes\nProblem: Alembic --autogenerate does not detect changes to indexes or constraints on existing tables.\n\nFix/Discussion: Alembic autogenerate compares metadata to database state but has limitations. Workarounds: (1) Manually add `op.create_index()` to migration. (2) Use `include_schemas=True` in env.py. (3) Run `alembic check` to see detected changes. (4) Always review autogenerated migrations before applying.\nCode signature: status:open", "file_path": "https://github.com/alembic/alembic/issues/7344", "function_name": "status:open", "type": "github_issue", "metadata": {"record_id": "e03d17a7-4255-48ef-b3ad-6b7ada74bca2", "source_type": "github_issue", "domain": "SQLAlchemy/Databases", "library": "alembic", "title": "BUG: Alembic autogenerate misses index changes", "content": "Problem: Alembic --autogenerate does not detect changes to indexes or constraints on existing tables.\n\nFix/Discussion: Alembic autogenerate compares metadata to database state but has limitations. Workarounds: (1) Manually add `op.create_index()` to migration. (2) Use `include_schemas=True` in env.py. (3) Run `alembic check` to see detected changes. (4) Always review autogenerated migrations before applying.", "code_signature": "status:open", "tags": "alembic, foreign-key, sqlalchemy, transaction, sqlite", "url": "https://github.com/alembic/alembic/issues/7344", "author": "contributor_3601", "date_published": "04-09-2021", "votes_or_stars": 210, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:4703d317-9645-476b-b0b1-0d1748f96cdb": {"content": "threading Thread Synchronization — official reference\nOfficial documentation for threading Thread Synchronization. The method signature is `threading.Lock()`. creates a mutual-exclusion lock for thread-safe access. Raises `RuntimeError` when lock is released without being acquired. See also: threading.RLock, threading.Semaphore, threading.Event.\nCode signature: threading.Lock()", "file_path": "https://docs.threading.org/en/stable/threading/Lock.html", "function_name": "threading.Lock()", "type": "documentation", "metadata": {"record_id": "4703d317-9645-476b-b0b1-0d1748f96cdb", "source_type": "documentation", "domain": "Asyncio/Threading", "library": "threading", "title": "threading Thread Synchronization — official reference", "content": "Official documentation for threading Thread Synchronization. The method signature is `threading.Lock()`. creates a mutual-exclusion lock for thread-safe access. Raises `RuntimeError` when lock is released without being acquired. See also: threading.RLock, threading.Semaphore, threading.Event.", "code_signature": "threading.Lock()", "tags": "await, race-condition, deadlock, event-loop", "url": "https://docs.threading.org/en/stable/threading/Lock.html", "author": "Official Docs", "date_published": "01-06-2021", "votes_or_stars": 4809, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:ad5c79a5-b21b-4fda-81f9-3ba54418d71a": {"content": "subprocess.run vs Popen: which should I use?\n`subprocess.run()` is a high-level wrapper that waits for completion — use for most cases. `subprocess.Popen()` gives full control: stream stdout/stderr in real-time, write to stdin, run processes concurrently. For capturing output: `subprocess.run(..., capture_output=True, text=True)`. Always use `shell=False` and pass args as a list to prevent shell injection.\nCode signature: Use subprocess.run() for simple cases; Popen for streaming output or interactive processes.", "file_path": "https://stackoverflow.com/questions/5727085", "function_name": "Use subprocess.run() for simple cases; Popen for streaming output or interactive processes.", "type": "stack_overflow", "metadata": {"record_id": "ad5c79a5-b21b-4fda-81f9-3ba54418d71a", "source_type": "stack_overflow", "domain": "OS/Subprocess/File I/O", "library": "shutil", "title": "subprocess.run vs Popen: which should I use?", "content": "`subprocess.run()` is a high-level wrapper that waits for completion — use for most cases. `subprocess.Popen()` gives full control: stream stdout/stderr in real-time, write to stdin, run processes concurrently. For capturing output: `subprocess.run(..., capture_output=True, text=True)`. Always use `shell=False` and pass args as a list to prevent shell injection.", "code_signature": "Use subprocess.run() for simple cases; Popen for streaming output or interactive processes.", "tags": "os, subprocess, file-io, glob", "url": "https://stackoverflow.com/questions/5727085", "author": "user_644210", "date_published": "12-11-2022", "votes_or_stars": 625, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:5e6afa46-5cd9-42f9-b6af-06e019e98c27": {"content": "How to implement JWT authentication in FastAPI?\n```python\nfrom fastapi.security import OAuth2PasswordBearer\nfrom jose import jwt\noauth2_scheme = OAuth2PasswordBearer(tokenUrl='token')\n```\nCreate access tokens with `jwt.encode(payload, SECRET_KEY, algorithm='HS256')`. Verify with `jwt.decode(token, SECRET_KEY, algorithms=['HS256'])`. Store SECRET_KEY in environment variables, never in code.\nCode signature: Use python-jose for JWT and passlib for password hashing with OAuth2PasswordBearer.", "file_path": "https://stackoverflow.com/questions/7754864", "function_name": "Use python-jose for JWT and passlib for password hashing with OAuth2PasswordBearer.", "type": "stack_overflow", "metadata": {"record_id": "5e6afa46-5cd9-42f9-b6af-06e019e98c27", "source_type": "stack_overflow", "domain": "FastAPI/Flask/Django", "library": "fastapi", "title": "How to implement JWT authentication in FastAPI?", "content": "```python\nfrom fastapi.security import OAuth2PasswordBearer\nfrom jose import jwt\noauth2_scheme = OAuth2PasswordBearer(tokenUrl='token')\n```\nCreate access tokens with `jwt.encode(payload, SECRET_KEY, algorithm='HS256')`. Verify with `jwt.decode(token, SECRET_KEY, algorithms=['HS256'])`. Store SECRET_KEY in environment variables, never in code.", "code_signature": "Use python-jose for JWT and passlib for password hashing with OAuth2PasswordBearer.", "tags": "routing, orm, template, flask, blueprint, middleware", "url": "https://stackoverflow.com/questions/7754864", "author": "user_773587", "date_published": "01-01-2020", "votes_or_stars": 424, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:a008678c-d821-4def-ac7f-28063cea7f0c": {"content": "subprocess.run vs Popen: which should I use?\n`subprocess.run()` is a high-level wrapper that waits for completion — use for most cases. `subprocess.Popen()` gives full control: stream stdout/stderr in real-time, write to stdin, run processes concurrently. For capturing output: `subprocess.run(..., capture_output=True, text=True)`. Always use `shell=False` and pass args as a list to prevent shell injection.\nCode signature: Use subprocess.run() for simple cases; Popen for streaming output or interactive processes.", "file_path": "https://stackoverflow.com/questions/5727085", "function_name": "Use subprocess.run() for simple cases; Popen for streaming output or interactive processes.", "type": "stack_overflow", "metadata": {"record_id": "a008678c-d821-4def-ac7f-28063cea7f0c", "source_type": "stack_overflow", "domain": "OS/Subprocess/File I/O", "library": "shutil", "title": "subprocess.run vs Popen: which should I use?", "content": "`subprocess.run()` is a high-level wrapper that waits for completion — use for most cases. `subprocess.Popen()` gives full control: stream stdout/stderr in real-time, write to stdin, run processes concurrently. For capturing output: `subprocess.run(..., capture_output=True, text=True)`. Always use `shell=False` and pass args as a list to prevent shell injection.", "code_signature": "Use subprocess.run() for simple cases; Popen for streaming output or interactive processes.", "tags": "pipe, pathlib, shutil, stdout, stdin", "url": "https://stackoverflow.com/questions/5727085", "author": "user_644210", "date_published": "12-12-2023", "votes_or_stars": 669, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:cef9c19f-8426-4063-8d65-0ae1476cade1": {"content": "requests — HTTP Session Management\nThe `requests.Session` function in requests creates a persistent HTTP session with connection pooling. It accepts `` as a parameter and returns Session object. Common use cases include making multiple requests to the same host efficiently. Note that always close or use Session as context manager to release connections.\nCode signature: requests.Session() -> Session object", "file_path": "https://docs.requests.org/en/stable/requests/Session.html", "function_name": "requests.Session() -> Session object", "type": "documentation", "metadata": {"record_id": "cef9c19f-8426-4063-8d65-0ae1476cade1", "source_type": "documentation", "domain": "Requests/HTTP/APIs", "library": "requests", "title": "requests — HTTP Session Management", "content": "The `requests.Session` function in requests creates a persistent HTTP session with connection pooling. It accepts `` as a parameter and returns Session object. Common use cases include making multiple requests to the same host efficiently. Note that always close or use Session as context manager to release connections.", "code_signature": "requests.Session() -> Session object", "tags": "authentication, oauth, json, timeout", "url": "https://docs.requests.org/en/stable/requests/Session.html", "author": "Official Docs", "date_published": "21-06-2021", "votes_or_stars": 97, "relevance_label": "low", "difficulty_level": "intermediate"}}, "kb:3dbdf12c-d059-4061-bc4e-3f3b03e218a4": {"content": "os Environment Variables — official reference\nOfficial documentation for os Environment Variables. The method signature is `os.environ(key)`. provides a mapping of the current environment variables. Raises `KeyError` when key is accessed with [] notation and is not set. See also: dotenv, os.getenv, os.putenv.\nCode signature: os.environ(key)", "file_path": "https://docs.os.org/en/stable/os/environ.html", "function_name": "os.environ(key)", "type": "documentation", "metadata": {"record_id": "3dbdf12c-d059-4061-bc4e-3f3b03e218a4", "source_type": "documentation", "domain": "OS/Subprocess/File I/O", "library": "os", "title": "os Environment Variables — official reference", "content": "Official documentation for os Environment Variables. The method signature is `os.environ(key)`. provides a mapping of the current environment variables. Raises `KeyError` when key is accessed with [] notation and is not set. See also: dotenv, os.getenv, os.putenv.", "code_signature": "os.environ(key)", "tags": "os, stdin, file-io, process", "url": "https://docs.os.org/en/stable/os/environ.html", "author": "Official Docs", "date_published": "21-06-2023", "votes_or_stars": 2219, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:e5b2b8ee-8538-4e68-b90e-9b2894de1381": {"content": "BUG: subprocess.Popen hangs when stdout and stderr buffers fill\nProblem: subprocess.Popen hangs indefinitely when child process produces large output.\n\nFix/Discussion: This is a classic deadlock: parent waits for child, child waits for parent to read buffer. Fix: use `subprocess.run(..., capture_output=True)` which handles this correctly. Or: `stdout, stderr = proc.communicate()`. Never use `proc.stdout.read()` and `proc.wait()` separately.\nCode signature: status:closed", "file_path": "https://github.com/cpython/cpython/issues/8586", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "e5b2b8ee-8538-4e68-b90e-9b2894de1381", "source_type": "github_issue", "domain": "OS/Subprocess/File I/O", "library": "cpython", "title": "BUG: subprocess.Popen hangs when stdout and stderr buffers fill", "content": "Problem: subprocess.Popen hangs indefinitely when child process produces large output.\n\nFix/Discussion: This is a classic deadlock: parent waits for child, child waits for parent to read buffer. Fix: use `subprocess.run(..., capture_output=True)` which handles this correctly. Or: `stdout, stderr = proc.communicate()`. Never use `proc.stdout.read()` and `proc.wait()` separately.", "code_signature": "status:closed", "tags": "tempfile, pipe, shutil, glob, stdin, env-variable", "url": "https://github.com/cpython/cpython/issues/8586", "author": "contributor_7711", "date_published": "24-03-2021", "votes_or_stars": 429, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:d4054dca-091c-4afc-a0b6-ca0bd8632214": {"content": "SQLAlchemy bulk insert: what's the fastest way?\n`session.bulk_insert_mappings(Model, list_of_dicts)` skips ORM overhead. Even faster: `session.execute(insert(Model), data)`. Avoid `session.add()` in loops. For PostgreSQL, use `insert().on_conflict_do_nothing()` for upserts. Batch in chunks of 1000-5000 rows for optimal performance.\nCode signature: Use session.bulk_insert_mappings() or execute(insert(), list_of_dicts) for maximum speed.", "file_path": "https://stackoverflow.com/questions/5977931", "function_name": "Use session.bulk_insert_mappings() or execute(insert(), list_of_dicts) for maximum speed.", "type": "stack_overflow", "metadata": {"record_id": "d4054dca-091c-4afc-a0b6-ca0bd8632214", "source_type": "stack_overflow", "domain": "SQLAlchemy/Databases", "library": "query", "title": "SQLAlchemy bulk insert: what's the fastest way?", "content": "`session.bulk_insert_mappings(Model, list_of_dicts)` skips ORM overhead. Even faster: `session.execute(insert(Model), data)`. Avoid `session.add()` in loops. For PostgreSQL, use `insert().on_conflict_do_nothing()` for upserts. Batch in chunks of 1000-5000 rows for optimal performance.", "code_signature": "Use session.bulk_insert_mappings() or execute(insert(), list_of_dicts) for maximum speed.", "tags": "session, postgresql, sqlite, relationship, query", "url": "https://stackoverflow.com/questions/5977931", "author": "user_238275", "date_published": "13-11-2019", "votes_or_stars": 653, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:e3903c4a-5076-4848-a6fa-a6346d756e7b": {"content": "How do I handle CORS in FastAPI?\n```python\nfrom fastapi.middleware.cors import CORSMiddleware\napp.add_middleware(CORSMiddleware, allow_origins=['*'], allow_methods=['*'], allow_headers=['*'])\n```\nIn production, replace `'*'` with your actual frontend domain. For credentials (cookies), set `allow_credentials=True` and specify exact origins.\nCode signature: Add CORSMiddleware from starlette to your FastAPI app.", "file_path": "https://stackoverflow.com/questions/1527021", "function_name": "Add CORSMiddleware from starlette to your FastAPI app.", "type": "stack_overflow", "metadata": {"record_id": "e3903c4a-5076-4848-a6fa-a6346d756e7b", "source_type": "stack_overflow", "domain": "FastAPI/Flask/Django", "library": "flask", "title": "How do I handle CORS in FastAPI?", "content": "```python\nfrom fastapi.middleware.cors import CORSMiddleware\napp.add_middleware(CORSMiddleware, allow_origins=['*'], allow_methods=['*'], allow_headers=['*'])\n```\nIn production, replace `'*'` with your actual frontend domain. For credentials (cookies), set `allow_credentials=True` and specify exact origins.", "code_signature": "Add CORSMiddleware from starlette to your FastAPI app.", "tags": "routing, dependency-injection, middleware, rest", "url": "https://stackoverflow.com/questions/1527021", "author": "user_911393", "date_published": "17-12-2022", "votes_or_stars": 958, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:ff61c90e-6b59-4720-9d1a-36745a70ac08": {"content": "Python: how to safely write to a file atomically?\n```python\nimport tempfile, os\nwith tempfile.NamedTemporaryFile('w', delete=False, dir='.') as tmp:\n    tmp.write(data)\n    tmp.flush()\n    os.fsync(tmp.fileno())\nos.replace(tmp.name, target_path)\n```\n`os.replace()` is atomic on POSIX. This prevents partial writes from corrupting files.\nCode signature: Write to a temp file then rename — rename is atomic on POSIX systems.", "file_path": "https://stackoverflow.com/questions/5394880", "function_name": "Write to a temp file then rename — rename is atomic on POSIX systems.", "type": "stack_overflow", "metadata": {"record_id": "ff61c90e-6b59-4720-9d1a-36745a70ac08", "source_type": "stack_overflow", "domain": "OS/Subprocess/File I/O", "library": "stdin", "title": "Python: how to safely write to a file atomically?", "content": "```python\nimport tempfile, os\nwith tempfile.NamedTemporaryFile('w', delete=False, dir='.') as tmp:\n    tmp.write(data)\n    tmp.flush()\n    os.fsync(tmp.fileno())\nos.replace(tmp.name, target_path)\n```\n`os.replace()` is atomic on POSIX. This prevents partial writes from corrupting files.", "code_signature": "Write to a temp file then rename — rename is atomic on POSIX systems.", "tags": "glob, tempfile, pipe", "url": "https://stackoverflow.com/questions/5394880", "author": "user_179430", "date_published": "11-03-2020", "votes_or_stars": 544, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:1646aef2-05ff-4b96-b546-3bacda798a7d": {"content": "BUG: SQLAlchemy connection pool exhaustion under load\nProblem: Application hangs under concurrent load — all connections are checked out and pool is exhausted.\n\nFix/Discussion: Increase `pool_size` and `max_overflow`. Ensure sessions are always closed: use `contextmanager` or `with Session() as session:`. Set `pool_timeout` to fail fast. Enable `pool_pre_ping=True`. Monitor with `engine.pool.checkedout()`. In async: use `AsyncSession` with `async_scoped_session`.\nCode signature: status:closed", "file_path": "https://github.com/sqlalchemy/sqlalchemy/issues/106", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "1646aef2-05ff-4b96-b546-3bacda798a7d", "source_type": "github_issue", "domain": "SQLAlchemy/Databases", "library": "sqlalchemy", "title": "BUG: SQLAlchemy connection pool exhaustion under load", "content": "Problem: Application hangs under concurrent load — all connections are checked out and pool is exhausted.\n\nFix/Discussion: Increase `pool_size` and `max_overflow`. Ensure sessions are always closed: use `contextmanager` or `with Session() as session:`. Set `pool_timeout` to fail fast. Enable `pool_pre_ping=True`. Monitor with `engine.pool.checkedout()`. In async: use `AsyncSession` with `async_scoped_session`.", "code_signature": "status:closed", "tags": "relationship, orm, postgresql", "url": "https://github.com/sqlalchemy/sqlalchemy/issues/106", "author": "contributor_5078", "date_published": "29-12-2022", "votes_or_stars": 266, "relevance_label": "low", "difficulty_level": "intermediate"}}, "kb:dc1182e5-8c9c-4e76-9eb2-376910c15ad8": {"content": "requests — HTTP Session Management\nThe `requests.Session` function in requests creates a persistent HTTP session with connection pooling. It accepts `` as a parameter and returns Session object. Common use cases include making multiple requests to the same host efficiently. Note that always close or use Session as context manager to release connections.\nCode signature: requests.Session() -> Session object", "file_path": "https://docs.requests.org/en/stable/requests/Session.html", "function_name": "requests.Session() -> Session object", "type": "documentation", "metadata": {"record_id": "dc1182e5-8c9c-4e76-9eb2-376910c15ad8", "source_type": "documentation", "domain": "Requests/HTTP/APIs", "library": "requests", "title": "requests — HTTP Session Management", "content": "The `requests.Session` function in requests creates a persistent HTTP session with connection pooling. It accepts `` as a parameter and returns Session object. Common use cases include making multiple requests to the same host efficiently. Note that always close or use Session as context manager to release connections.", "code_signature": "requests.Session() -> Session object", "tags": "rate-limiting, websocket, authentication, oauth, timeout, rest-api", "url": "https://docs.requests.org/en/stable/requests/Session.html", "author": "Official Docs", "date_published": "09-04-2021", "votes_or_stars": 62, "relevance_label": "low", "difficulty_level": "intermediate"}}, "kb:601723e5-890c-4ebc-9c2c-5bdaf0cf6bfa": {"content": "SQLAlchemy — ORM Session Query\nThe `session.query` function in SQLAlchemy constructs a SELECT query against mapped entities. It accepts `*entities` as a parameter and returns Query object. Common use cases include retrieving ORM-mapped objects from the database. Note that prefer session.execute(select(...)) in SQLAlchemy 2.0+.\nCode signature: session.query(*entities) -> Query object", "file_path": "https://docs.sqlalchemy.org/en/stable/session/query.html", "function_name": "session.query(*entities) -> Query object", "type": "documentation", "metadata": {"record_id": "601723e5-890c-4ebc-9c2c-5bdaf0cf6bfa", "source_type": "documentation", "domain": "SQLAlchemy/Databases", "library": "SQLAlchemy", "title": "SQLAlchemy — ORM Session Query", "content": "The `session.query` function in SQLAlchemy constructs a SELECT query against mapped entities. It accepts `*entities` as a parameter and returns Query object. Common use cases include retrieving ORM-mapped objects from the database. Note that prefer session.execute(select(...)) in SQLAlchemy 2.0+.", "code_signature": "session.query(*entities) -> Query object", "tags": "sqlalchemy, query, alembic, postgresql, foreign-key", "url": "https://docs.sqlalchemy.org/en/stable/session/query.html", "author": "Official Docs", "date_published": "06-03-2021", "votes_or_stars": 3051, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:0143ef56-afdf-4cfa-a35e-01f30dbd599e": {"content": "How to handle database migrations with Alembic?\nWorkflow: (1) Change SQLAlchemy models. (2) `alembic revision --autogenerate -m 'description'`. (3) Review generated migration script. (4) `alembic upgrade head` to apply. Always review autogenerated migrations — they miss some changes (indexes, constraints). Use `alembic downgrade -1` to rollback. Store migrations in version control.\nCode signature: Use alembic revision --autogenerate to create migrations from model changes.", "file_path": "https://stackoverflow.com/questions/9436429", "function_name": "Use alembic revision --autogenerate to create migrations from model changes.", "type": "stack_overflow", "metadata": {"record_id": "0143ef56-afdf-4cfa-a35e-01f30dbd599e", "source_type": "stack_overflow", "domain": "SQLAlchemy/Databases", "library": "connection-pool", "title": "How to handle database migrations with Alembic?", "content": "Workflow: (1) Change SQLAlchemy models. (2) `alembic revision --autogenerate -m 'description'`. (3) Review generated migration script. (4) `alembic upgrade head` to apply. Always review autogenerated migrations — they miss some changes (indexes, constraints). Use `alembic downgrade -1` to rollback. Store migrations in version control.", "code_signature": "Use alembic revision --autogenerate to create migrations from model changes.", "tags": "orm, sqlite, foreign-key, alembic, session", "url": "https://stackoverflow.com/questions/9436429", "author": "user_974047", "date_published": "19-04-2019", "votes_or_stars": 2391, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:1370aad6-7513-4461-bc96-d3a9ac599a8c": {"content": "BUG: Alembic autogenerate misses index changes\nProblem: Alembic --autogenerate does not detect changes to indexes or constraints on existing tables.\n\nFix/Discussion: Alembic autogenerate compares metadata to database state but has limitations. Workarounds: (1) Manually add `op.create_index()` to migration. (2) Use `include_schemas=True` in env.py. (3) Run `alembic check` to see detected changes. (4) Always review autogenerated migrations before applying.\nCode signature: status:open", "file_path": "https://github.com/alembic/alembic/issues/7344", "function_name": "status:open", "type": "github_issue", "metadata": {"record_id": "1370aad6-7513-4461-bc96-d3a9ac599a8c", "source_type": "github_issue", "domain": "SQLAlchemy/Databases", "library": "alembic", "title": "BUG: Alembic autogenerate misses index changes", "content": "Problem: Alembic --autogenerate does not detect changes to indexes or constraints on existing tables.\n\nFix/Discussion: Alembic autogenerate compares metadata to database state but has limitations. Workarounds: (1) Manually add `op.create_index()` to migration. (2) Use `include_schemas=True` in env.py. (3) Run `alembic check` to see detected changes. (4) Always review autogenerated migrations before applying.", "code_signature": "status:open", "tags": "alembic, orm, foreign-key, transaction, connection-pool, migration", "url": "https://github.com/alembic/alembic/issues/7344", "author": "contributor_3601", "date_published": "22-03-2018", "votes_or_stars": 204, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:0a1a0577-0a4f-4c05-8e68-3c10a285a109": {"content": "How to upload a file with requests?\n```python\nwith open('file.pdf', 'rb') as f:\n    r = requests.post(url, files={'file': ('file.pdf', f, 'application/pdf')})\n```\nFor multiple files: pass a list of tuples. For multipart form data with fields: combine `files` and `data` parameters. Don't set Content-Type manually — requests sets it with boundary.\nCode signature: Use the files parameter with a file-like object or tuple.", "file_path": "https://stackoverflow.com/questions/8896868", "function_name": "Use the files parameter with a file-like object or tuple.", "type": "stack_overflow", "metadata": {"record_id": "0a1a0577-0a4f-4c05-8e68-3c10a285a109", "source_type": "stack_overflow", "domain": "Requests/HTTP/APIs", "library": "requests", "title": "How to upload a file with requests?", "content": "```python\nwith open('file.pdf', 'rb') as f:\n    r = requests.post(url, files={'file': ('file.pdf', f, 'application/pdf')})\n```\nFor multiple files: pass a list of tuples. For multipart form data with fields: combine `files` and `data` parameters. Don't set Content-Type manually — requests sets it with boundary.", "code_signature": "Use the files parameter with a file-like object or tuple.", "tags": "http, requests, oauth, retry", "url": "https://stackoverflow.com/questions/8896868", "author": "user_243238", "date_published": "17-01-2021", "votes_or_stars": 1554, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:8d43b047-f340-4063-850c-10fb3ce0428d": {"content": "Django N+1 query problem: how to fix?\nN+1 occurs when accessing related objects in a loop. Diagnose with `django-debug-toolbar` or `connection.queries`. Fix ForeignKey: `User.objects.select_related('profile').all()`. Fix M2M: `Post.objects.prefetch_related('tags').all()`. Use `only()` to fetch specific fields and reduce data transfer.\nCode signature: Use select_related() for ForeignKey and prefetch_related() for ManyToMany relationships.", "file_path": "https://stackoverflow.com/questions/8106470", "function_name": "Use select_related() for ForeignKey and prefetch_related() for ManyToMany relationships.", "type": "stack_overflow", "metadata": {"record_id": "8d43b047-f340-4063-850c-10fb3ce0428d", "source_type": "stack_overflow", "domain": "FastAPI/Flask/Django", "library": "request", "title": "Django N+1 query problem: how to fix?", "content": "N+1 occurs when accessing related objects in a loop. Diagnose with `django-debug-toolbar` or `connection.queries`. Fix ForeignKey: `User.objects.select_related('profile').all()`. Fix M2M: `Post.objects.prefetch_related('tags').all()`. Use `only()` to fetch specific fields and reduce data transfer.", "code_signature": "Use select_related() for ForeignKey and prefetch_related() for ManyToMany relationships.", "tags": "flask, pydantic, template", "url": "https://stackoverflow.com/questions/8106470", "author": "user_441071", "date_published": "28-03-2023", "votes_or_stars": 1775, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:2b586a68-0b44-49f7-91cb-8541de4b682d": {"content": "PERF: requests connection not reused between calls — new TCP connection each time\nProblem: Each requests.get() call opens a new TCP connection, causing high latency.\n\nFix/Discussion: requests.get/post etc. create a new Session per call. Fix: use a persistent Session: `session = requests.Session()` and reuse it. Sessions pool connections via urllib3. For thread-safety: create one Session per thread. For async: use httpx.AsyncClient or aiohttp.ClientSession for async connection pooling.\nCode signature: status:closed", "file_path": "https://github.com/requests/requests/issues/3605", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "2b586a68-0b44-49f7-91cb-8541de4b682d", "source_type": "github_issue", "domain": "Requests/HTTP/APIs", "library": "requests", "title": "PERF: requests connection not reused between calls — new TCP connection each time", "content": "Problem: Each requests.get() call opens a new TCP connection, causing high latency.\n\nFix/Discussion: requests.get/post etc. create a new Session per call. Fix: use a persistent Session: `session = requests.Session()` and reuse it. Sessions pool connections via urllib3. For thread-safety: create one Session per thread. For async: use httpx.AsyncClient or aiohttp.ClientSession for async connection pooling.", "code_signature": "status:closed", "tags": "retry, oauth, json, rate-limiting", "url": "https://github.com/requests/requests/issues/3605", "author": "contributor_1152", "date_published": "02-10-2022", "votes_or_stars": 356, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:57aac642-3e63-4030-be1b-4c959eb4a824": {"content": "How to handle database migrations with Alembic?\nWorkflow: (1) Change SQLAlchemy models. (2) `alembic revision --autogenerate -m 'description'`. (3) Review generated migration script. (4) `alembic upgrade head` to apply. Always review autogenerated migrations — they miss some changes (indexes, constraints). Use `alembic downgrade -1` to rollback. Store migrations in version control.\nCode signature: Use alembic revision --autogenerate to create migrations from model changes.", "file_path": "https://stackoverflow.com/questions/9436429", "function_name": "Use alembic revision --autogenerate to create migrations from model changes.", "type": "stack_overflow", "metadata": {"record_id": "57aac642-3e63-4030-be1b-4c959eb4a824", "source_type": "stack_overflow", "domain": "SQLAlchemy/Databases", "library": "connection-pool", "title": "How to handle database migrations with Alembic?", "content": "Workflow: (1) Change SQLAlchemy models. (2) `alembic revision --autogenerate -m 'description'`. (3) Review generated migration script. (4) `alembic upgrade head` to apply. Always review autogenerated migrations — they miss some changes (indexes, constraints). Use `alembic downgrade -1` to rollback. Store migrations in version control.", "code_signature": "Use alembic revision --autogenerate to create migrations from model changes.", "tags": "migration, sqlalchemy, relationship", "url": "https://stackoverflow.com/questions/9436429", "author": "user_974047", "date_published": "03-02-2023", "votes_or_stars": 2412, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:10799c1c-8c7e-4ac2-b699-485845fd50de": {"content": "How to use df.groupby in Pandas\n`Pandas.df.groupby` groups DataFrame rows by column values. Example usage:\n```python\ndf.groupby('category')['value'].mean()\n```\nThis is useful when aggregation, transformation, filtering by group. Performance tip: avoid apply() on large DataFrames; prefer agg().\nCode signature: df.groupby", "file_path": "https://docs.pandas.org/en/stable/df/groupby.html", "function_name": "df.groupby", "type": "documentation", "metadata": {"record_id": "10799c1c-8c7e-4ac2-b699-485845fd50de", "source_type": "documentation", "domain": "NumPy/Pandas", "library": "Pandas", "title": "How to use df.groupby in Pandas", "content": "`Pandas.df.groupby` groups DataFrame rows by column values. Example usage:\n```python\ndf.groupby('category')['value'].mean()\n```\nThis is useful when aggregation, transformation, filtering by group. Performance tip: avoid apply() on large DataFrames; prefer agg().", "code_signature": "df.groupby", "tags": "groupby, numpy, dataframe, nan, loc", "url": "https://docs.pandas.org/en/stable/df/groupby.html", "author": "Official Docs", "date_published": "09-01-2020", "votes_or_stars": 804, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:97389611-1cd0-4aa2-b56c-15f8cf44d636": {"content": "subprocess.run vs Popen: which should I use?\n`subprocess.run()` is a high-level wrapper that waits for completion — use for most cases. `subprocess.Popen()` gives full control: stream stdout/stderr in real-time, write to stdin, run processes concurrently. For capturing output: `subprocess.run(..., capture_output=True, text=True)`. Always use `shell=False` and pass args as a list to prevent shell injection.\nCode signature: Use subprocess.run() for simple cases; Popen for streaming output or interactive processes.", "file_path": "https://stackoverflow.com/questions/5727085", "function_name": "Use subprocess.run() for simple cases; Popen for streaming output or interactive processes.", "type": "stack_overflow", "metadata": {"record_id": "97389611-1cd0-4aa2-b56c-15f8cf44d636", "source_type": "stack_overflow", "domain": "OS/Subprocess/File I/O", "library": "shutil", "title": "subprocess.run vs Popen: which should I use?", "content": "`subprocess.run()` is a high-level wrapper that waits for completion — use for most cases. `subprocess.Popen()` gives full control: stream stdout/stderr in real-time, write to stdin, run processes concurrently. For capturing output: `subprocess.run(..., capture_output=True, text=True)`. Always use `shell=False` and pass args as a list to prevent shell injection.", "code_signature": "Use subprocess.run() for simple cases; Popen for streaming output or interactive processes.", "tags": "process, subprocess, stdin, glob", "url": "https://stackoverflow.com/questions/5727085", "author": "user_644210", "date_published": "04-07-2024", "votes_or_stars": 676, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:f0b28326-01f0-4c93-a611-910cb0d23cd2": {"content": "How to run background tasks in FastAPI?\nFastAPI's `BackgroundTasks`: add `background_tasks: BackgroundTasks` as a parameter, then `background_tasks.add_task(func, *args)`. Runs after response is sent. For heavy workloads (email, ML inference), use Celery with Redis/RabbitMQ broker. For async background work, `asyncio.create_task()` works within async endpoints.\nCode signature: Use BackgroundTasks for lightweight tasks or Celery for heavy/distributed work.", "file_path": "https://stackoverflow.com/questions/8077999", "function_name": "Use BackgroundTasks for lightweight tasks or Celery for heavy/distributed work.", "type": "stack_overflow", "metadata": {"record_id": "f0b28326-01f0-4c93-a611-910cb0d23cd2", "source_type": "stack_overflow", "domain": "FastAPI/Flask/Django", "library": "django", "title": "How to run background tasks in FastAPI?", "content": "FastAPI's `BackgroundTasks`: add `background_tasks: BackgroundTasks` as a parameter, then `background_tasks.add_task(func, *args)`. Runs after response is sent. For heavy workloads (email, ML inference), use Celery with Redis/RabbitMQ broker. For async background work, `asyncio.create_task()` works within async endpoints.", "code_signature": "Use BackgroundTasks for lightweight tasks or Celery for heavy/distributed work.", "tags": "dependency-injection, pydantic, blueprint, middleware, orm", "url": "https://stackoverflow.com/questions/8077999", "author": "user_202401", "date_published": "11-03-2024", "votes_or_stars": 1867, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:70188618-f097-4cc9-91ad-a7733d0531fb": {"content": "How to add retry logic to requests?\n```python\nfrom requests.adapters import HTTPAdapter\nfrom urllib3.util.retry import Retry\nretry = Retry(total=3, backoff_factor=1, status_forcelist=[500,502,503,504])\nadapter = HTTPAdapter(max_retries=retry)\nsession.mount('https://', adapter)\n```\nCode signature: Mount an HTTPAdapter with urllib3 Retry on the Session.", "file_path": "https://stackoverflow.com/questions/1669324", "function_name": "Mount an HTTPAdapter with urllib3 Retry on the Session.", "type": "stack_overflow", "metadata": {"record_id": "70188618-f097-4cc9-91ad-a7733d0531fb", "source_type": "stack_overflow", "domain": "Requests/HTTP/APIs", "library": "websocket", "title": "How to add retry logic to requests?", "content": "```python\nfrom requests.adapters import HTTPAdapter\nfrom urllib3.util.retry import Retry\nretry = Retry(total=3, backoff_factor=1, status_forcelist=[500,502,503,504])\nadapter = HTTPAdapter(max_retries=retry)\nsession.mount('https://', adapter)\n```", "code_signature": "Mount an HTTPAdapter with urllib3 Retry on the Session.", "tags": "requests, websocket, authentication, rate-limiting, oauth, json", "url": "https://stackoverflow.com/questions/1669324", "author": "user_952590", "date_published": "14-06-2021", "votes_or_stars": 1504, "relevance_label": "low", "difficulty_level": "advanced"}}, "kb:f216e043-af85-4e07-a09e-6df0fdcae388": {"content": "threading Thread Synchronization — official reference\nOfficial documentation for threading Thread Synchronization. The method signature is `threading.Lock()`. creates a mutual-exclusion lock for thread-safe access. Raises `RuntimeError` when lock is released without being acquired. See also: threading.RLock, threading.Semaphore, threading.Event.\nCode signature: threading.Lock()", "file_path": "https://docs.threading.org/en/stable/threading/Lock.html", "function_name": "threading.Lock()", "type": "documentation", "metadata": {"record_id": "f216e043-af85-4e07-a09e-6df0fdcae388", "source_type": "documentation", "domain": "Asyncio/Threading", "library": "threading", "title": "threading Thread Synchronization — official reference", "content": "Official documentation for threading Thread Synchronization. The method signature is `threading.Lock()`. creates a mutual-exclusion lock for thread-safe access. Raises `RuntimeError` when lock is released without being acquired. See also: threading.RLock, threading.Semaphore, threading.Event.", "code_signature": "threading.Lock()", "tags": "semaphore, coroutine, event-loop, executor, task, asyncio", "url": "https://docs.threading.org/en/stable/threading/Lock.html", "author": "Official Docs", "date_published": "28-03-2020", "votes_or_stars": 4818, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:cda0ae59-c9b0-4d7c-9c49-1fcbdf9b2be8": {"content": "BUG: FastAPI dependency_overrides not working in pytest\nProblem: Dependency overrides set in conftest are not applied during test execution.\n\nFix/Discussion: Set overrides on the app object before TestClient is created. Use `app.dependency_overrides[dep] = mock_dep` in a pytest fixture with function scope. Clear overrides after test: `app.dependency_overrides = {}`. Ensure TestClient is created after overrides are set.\nCode signature: status:closed", "file_path": "https://github.com/fastapi/fastapi/issues/8479", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "cda0ae59-c9b0-4d7c-9c49-1fcbdf9b2be8", "source_type": "github_issue", "domain": "FastAPI/Flask/Django", "library": "fastapi", "title": "BUG: FastAPI dependency_overrides not working in pytest", "content": "Problem: Dependency overrides set in conftest are not applied during test execution.\n\nFix/Discussion: Set overrides on the app object before TestClient is created. Use `app.dependency_overrides[dep] = mock_dep` in a pytest fixture with function scope. Clear overrides after test: `app.dependency_overrides = {}`. Ensure TestClient is created after overrides are set.", "code_signature": "status:closed", "tags": "template, response, middleware", "url": "https://github.com/fastapi/fastapi/issues/8479", "author": "contributor_1994", "date_published": "07-12-2018", "votes_or_stars": 291, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:0c7cda54-dd7e-480c-81b7-91a6364eb7fb": {"content": "How to use create_engine in SQLAlchemy\n`SQLAlchemy.create_engine` creates a database engine representing a connection pool. Example usage:\n```python\nengine = create_engine('postgresql://user:pass@host/db', echo=True)\n```\nThis is useful when establishing the central database connection for an application. Performance tip: set pool_size and max_overflow based on expected concurrency.\nCode signature: create_engine", "file_path": "https://docs.sqlalchemy.org/en/stable/create_engine.html", "function_name": "create_engine", "type": "documentation", "metadata": {"record_id": "0c7cda54-dd7e-480c-81b7-91a6364eb7fb", "source_type": "documentation", "domain": "SQLAlchemy/Databases", "library": "SQLAlchemy", "title": "How to use create_engine in SQLAlchemy", "content": "`SQLAlchemy.create_engine` creates a database engine representing a connection pool. Example usage:\n```python\nengine = create_engine('postgresql://user:pass@host/db', echo=True)\n```\nThis is useful when establishing the central database connection for an application. Performance tip: set pool_size and max_overflow based on expected concurrency.", "code_signature": "create_engine", "tags": "sqlalchemy, relationship, foreign-key, query, alembic", "url": "https://docs.sqlalchemy.org/en/stable/create_engine.html", "author": "Official Docs", "date_published": "07-07-2021", "votes_or_stars": 2049, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:414b2724-b97f-46d4-b90f-0501c4c76160": {"content": "BUG: SettingWithCopyWarning when modifying sliced DataFrame\nProblem: Modifying a slice of a DataFrame raises SettingWithCopyWarning and may not update original.\n\nFix/Discussion: Always use .loc or .copy() when slicing. Use `df.loc[mask, 'col'] = val` for assignment. Create explicit copies: `subset = df[mask].copy()`. In Pandas 2.0+, Copy-on-Write makes this behavior consistent — enable with `pd.options.mode.copy_on_write = True`.\nCode signature: status:closed", "file_path": "https://github.com/pandas/pandas/issues/3264", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "414b2724-b97f-46d4-b90f-0501c4c76160", "source_type": "github_issue", "domain": "NumPy/Pandas", "library": "pandas", "title": "BUG: SettingWithCopyWarning when modifying sliced DataFrame", "content": "Problem: Modifying a slice of a DataFrame raises SettingWithCopyWarning and may not update original.\n\nFix/Discussion: Always use .loc or .copy() when slicing. Use `df.loc[mask, 'col'] = val` for assignment. Create explicit copies: `subset = df[mask].copy()`. In Pandas 2.0+, Copy-on-Write makes this behavior consistent — enable with `pd.options.mode.copy_on_write = True`.", "code_signature": "status:closed", "tags": "vectorization, reshape, array, nan", "url": "https://github.com/pandas/pandas/issues/3264", "author": "contributor_6628", "date_published": "06-09-2022", "votes_or_stars": 17, "relevance_label": "low", "difficulty_level": "intermediate"}}, "kb:92e59781-bde4-4cf7-bb46-2af160e8bb11": {"content": "os Environment Variables — official reference\nOfficial documentation for os Environment Variables. The method signature is `os.environ(key)`. provides a mapping of the current environment variables. Raises `KeyError` when key is accessed with [] notation and is not set. See also: dotenv, os.getenv, os.putenv.\nCode signature: os.environ(key)", "file_path": "https://docs.os.org/en/stable/os/environ.html", "function_name": "os.environ(key)", "type": "documentation", "metadata": {"record_id": "92e59781-bde4-4cf7-bb46-2af160e8bb11", "source_type": "documentation", "domain": "OS/Subprocess/File I/O", "library": "os", "title": "os Environment Variables — official reference", "content": "Official documentation for os Environment Variables. The method signature is `os.environ(key)`. provides a mapping of the current environment variables. Raises `KeyError` when key is accessed with [] notation and is not set. See also: dotenv, os.getenv, os.putenv.", "code_signature": "os.environ(key)", "tags": "pathlib, tempfile, shutil", "url": "https://docs.os.org/en/stable/os/environ.html", "author": "Official Docs", "date_published": "07-06-2023", "votes_or_stars": 2228, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:99b7ed07-d69e-4363-b4fb-93d6fb8e9e4c": {"content": "SQLAlchemy — ORM Session Query\nThe `session.query` function in SQLAlchemy constructs a SELECT query against mapped entities. It accepts `*entities` as a parameter and returns Query object. Common use cases include retrieving ORM-mapped objects from the database. Note that prefer session.execute(select(...)) in SQLAlchemy 2.0+.\nCode signature: session.query(*entities) -> Query object", "file_path": "https://docs.sqlalchemy.org/en/stable/session/query.html", "function_name": "session.query(*entities) -> Query object", "type": "documentation", "metadata": {"record_id": "99b7ed07-d69e-4363-b4fb-93d6fb8e9e4c", "source_type": "documentation", "domain": "SQLAlchemy/Databases", "library": "SQLAlchemy", "title": "SQLAlchemy — ORM Session Query", "content": "The `session.query` function in SQLAlchemy constructs a SELECT query against mapped entities. It accepts `*entities` as a parameter and returns Query object. Common use cases include retrieving ORM-mapped objects from the database. Note that prefer session.execute(select(...)) in SQLAlchemy 2.0+.", "code_signature": "session.query(*entities) -> Query object", "tags": "migration, sqlite, foreign-key, alembic, session, query", "url": "https://docs.sqlalchemy.org/en/stable/session/query.html", "author": "Official Docs", "date_published": "03-02-2019", "votes_or_stars": 3023, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:59900996-3094-41b7-a4bd-4f2490ef43f0": {"content": "Python: how to safely write to a file atomically?\n```python\nimport tempfile, os\nwith tempfile.NamedTemporaryFile('w', delete=False, dir='.') as tmp:\n    tmp.write(data)\n    tmp.flush()\n    os.fsync(tmp.fileno())\nos.replace(tmp.name, target_path)\n```\n`os.replace()` is atomic on POSIX. This prevents partial writes from corrupting files.\nCode signature: Write to a temp file then rename — rename is atomic on POSIX systems.", "file_path": "https://stackoverflow.com/questions/5394880", "function_name": "Write to a temp file then rename — rename is atomic on POSIX systems.", "type": "stack_overflow", "metadata": {"record_id": "59900996-3094-41b7-a4bd-4f2490ef43f0", "source_type": "stack_overflow", "domain": "OS/Subprocess/File I/O", "library": "stdin", "title": "Python: how to safely write to a file atomically?", "content": "```python\nimport tempfile, os\nwith tempfile.NamedTemporaryFile('w', delete=False, dir='.') as tmp:\n    tmp.write(data)\n    tmp.flush()\n    os.fsync(tmp.fileno())\nos.replace(tmp.name, target_path)\n```\n`os.replace()` is atomic on POSIX. This prevents partial writes from corrupting files.", "code_signature": "Write to a temp file then rename — rename is atomic on POSIX systems.", "tags": "tempfile, stdout, env-variable, pathlib", "url": "https://stackoverflow.com/questions/5394880", "author": "user_179430", "date_published": "16-12-2020", "votes_or_stars": 548, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:2bb8faad-a3ed-411e-bd6a-2f7fad441514": {"content": "requests — HTTP Session Management\nThe `requests.Session` function in requests creates a persistent HTTP session with connection pooling. It accepts `` as a parameter and returns Session object. Common use cases include making multiple requests to the same host efficiently. Note that always close or use Session as context manager to release connections.\nCode signature: requests.Session() -> Session object", "file_path": "https://docs.requests.org/en/stable/requests/Session.html", "function_name": "requests.Session() -> Session object", "type": "documentation", "metadata": {"record_id": "2bb8faad-a3ed-411e-bd6a-2f7fad441514", "source_type": "documentation", "domain": "Requests/HTTP/APIs", "library": "requests", "title": "requests — HTTP Session Management", "content": "The `requests.Session` function in requests creates a persistent HTTP session with connection pooling. It accepts `` as a parameter and returns Session object. Common use cases include making multiple requests to the same host efficiently. Note that always close or use Session as context manager to release connections.", "code_signature": "requests.Session() -> Session object", "tags": "httpx, websocket, oauth", "url": "https://docs.requests.org/en/stable/requests/Session.html", "author": "Official Docs", "date_published": "02-10-2024", "votes_or_stars": 83, "relevance_label": "low", "difficulty_level": "advanced"}}, "kb:e85de13e-1ec5-4551-9d96-40bdd5179d98": {"content": "FastAPI — Path Operations\nThe `@app.get` function in FastAPI registers a GET endpoint at the specified path. It accepts `path, response_model` as a parameter and returns Response. Common use cases include building REST API endpoints that return JSON data. Note that use response_model to enforce output schema validation.\nCode signature: @app.get(path, response_model) -> Response", "file_path": "https://docs.fastapi.org/en/stable/@app/get.html", "function_name": "@app.get(path, response_model) -> Response", "type": "documentation", "metadata": {"record_id": "e85de13e-1ec5-4551-9d96-40bdd5179d98", "source_type": "documentation", "domain": "FastAPI/Flask/Django", "library": "FastAPI", "title": "FastAPI — Path Operations", "content": "The `@app.get` function in FastAPI registers a GET endpoint at the specified path. It accepts `path, response_model` as a parameter and returns Response. Common use cases include building REST API endpoints that return JSON data. Note that use response_model to enforce output schema validation.", "code_signature": "@app.get(path, response_model) -> Response", "tags": "fastapi, orm, template, rest, blueprint", "url": "https://docs.fastapi.org/en/stable/@app/get.html", "author": "Official Docs", "date_published": "11-09-2019", "votes_or_stars": 2837, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:b7224d05-7bb0-463e-a1ca-357d9b4ba7e9": {"content": "How to add retry logic to requests?\n```python\nfrom requests.adapters import HTTPAdapter\nfrom urllib3.util.retry import Retry\nretry = Retry(total=3, backoff_factor=1, status_forcelist=[500,502,503,504])\nadapter = HTTPAdapter(max_retries=retry)\nsession.mount('https://', adapter)\n```\nCode signature: Mount an HTTPAdapter with urllib3 Retry on the Session.", "file_path": "https://stackoverflow.com/questions/1669324", "function_name": "Mount an HTTPAdapter with urllib3 Retry on the Session.", "type": "stack_overflow", "metadata": {"record_id": "b7224d05-7bb0-463e-a1ca-357d9b4ba7e9", "source_type": "stack_overflow", "domain": "Requests/HTTP/APIs", "library": "websocket", "title": "How to add retry logic to requests?", "content": "```python\nfrom requests.adapters import HTTPAdapter\nfrom urllib3.util.retry import Retry\nretry = Retry(total=3, backoff_factor=1, status_forcelist=[500,502,503,504])\nadapter = HTTPAdapter(max_retries=retry)\nsession.mount('https://', adapter)\n```", "code_signature": "Mount an HTTPAdapter with urllib3 Retry on the Session.", "tags": "aiohttp, rate-limiting, authentication, headers, requests", "url": "https://stackoverflow.com/questions/1669324", "author": "user_952590", "date_published": "04-01-2018", "votes_or_stars": 1532, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:c4899a6e-3e3b-44a9-8e48-1f7dd469e1dd": {"content": "Django ORM Queries — official reference\nOfficial documentation for Django ORM Queries. The method signature is `QuerySet.filter(**kwargs)`. filters QuerySet rows matching given field lookups. Raises `FieldError` when an unknown field lookup is provided. See also: QuerySet.exclude, QuerySet.annotate, Q objects.\nCode signature: QuerySet.filter(**kwargs)", "file_path": "https://docs.django.org/en/stable/QuerySet/filter.html", "function_name": "QuerySet.filter(**kwargs)", "type": "documentation", "metadata": {"record_id": "c4899a6e-3e3b-44a9-8e48-1f7dd469e1dd", "source_type": "documentation", "domain": "FastAPI/Flask/Django", "library": "Django", "title": "Django ORM Queries — official reference", "content": "Official documentation for Django ORM Queries. The method signature is `QuerySet.filter(**kwargs)`. filters QuerySet rows matching given field lookups. Raises `FieldError` when an unknown field lookup is provided. See also: QuerySet.exclude, QuerySet.annotate, Q objects.", "code_signature": "QuerySet.filter(**kwargs)", "tags": "pydantic, response, dependency-injection", "url": "https://docs.django.org/en/stable/QuerySet/filter.html", "author": "Official Docs", "date_published": "27-01-2019", "votes_or_stars": 2394, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:a675f774-2210-44e0-a1d2-b1bb12af4974": {"content": "BUG: subprocess.Popen hangs when stdout and stderr buffers fill\nProblem: subprocess.Popen hangs indefinitely when child process produces large output.\n\nFix/Discussion: This is a classic deadlock: parent waits for child, child waits for parent to read buffer. Fix: use `subprocess.run(..., capture_output=True)` which handles this correctly. Or: `stdout, stderr = proc.communicate()`. Never use `proc.stdout.read()` and `proc.wait()` separately.\nCode signature: status:closed", "file_path": "https://github.com/cpython/cpython/issues/8586", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "a675f774-2210-44e0-a1d2-b1bb12af4974", "source_type": "github_issue", "domain": "OS/Subprocess/File I/O", "library": "cpython", "title": "BUG: subprocess.Popen hangs when stdout and stderr buffers fill", "content": "Problem: subprocess.Popen hangs indefinitely when child process produces large output.\n\nFix/Discussion: This is a classic deadlock: parent waits for child, child waits for parent to read buffer. Fix: use `subprocess.run(..., capture_output=True)` which handles this correctly. Or: `stdout, stderr = proc.communicate()`. Never use `proc.stdout.read()` and `proc.wait()` separately.", "code_signature": "status:closed", "tags": "os, subprocess, glob, file-io, pathlib", "url": "https://github.com/cpython/cpython/issues/8586", "author": "contributor_7711", "date_published": "25-07-2019", "votes_or_stars": 416, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:cec444fb-7f1f-4a5f-9119-917294c4aa42": {"content": "PERF: groupby().apply() extremely slow on large DataFrames\nProblem: groupby().apply() with a custom function is 100x slower than expected on 10M row DataFrames.\n\nFix/Discussion: groupby().apply() serializes each group and has high overhead. Fix: vectorize with numpy, use groupby().transform() for same-shape output, or groupby().agg() with named aggregations. For complex logic, consider Polars or Dask. Numba's @jit can accelerate custom group functions.\nCode signature: status:closed", "file_path": "https://github.com/pandas/pandas/issues/8446", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "cec444fb-7f1f-4a5f-9119-917294c4aa42", "source_type": "github_issue", "domain": "NumPy/Pandas", "library": "pandas", "title": "PERF: groupby().apply() extremely slow on large DataFrames", "content": "Problem: groupby().apply() with a custom function is 100x slower than expected on 10M row DataFrames.\n\nFix/Discussion: groupby().apply() serializes each group and has high overhead. Fix: vectorize with numpy, use groupby().transform() for same-shape output, or groupby().agg() with named aggregations. For complex logic, consider Polars or Dask. Numba's @jit can accelerate custom group functions.", "code_signature": "status:closed", "tags": "loc, pandas, numpy, merge", "url": "https://github.com/pandas/pandas/issues/8446", "author": "contributor_6648", "date_published": "31-03-2018", "votes_or_stars": 328, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:da390c79-c17e-4909-b836-d87dca03a80f": {"content": "requests — HTTP Session Management\nThe `requests.Session` function in requests creates a persistent HTTP session with connection pooling. It accepts `` as a parameter and returns Session object. Common use cases include making multiple requests to the same host efficiently. Note that always close or use Session as context manager to release connections.\nCode signature: requests.Session() -> Session object", "file_path": "https://docs.requests.org/en/stable/requests/Session.html", "function_name": "requests.Session() -> Session object", "type": "documentation", "metadata": {"record_id": "da390c79-c17e-4909-b836-d87dca03a80f", "source_type": "documentation", "domain": "Requests/HTTP/APIs", "library": "requests", "title": "requests — HTTP Session Management", "content": "The `requests.Session` function in requests creates a persistent HTTP session with connection pooling. It accepts `` as a parameter and returns Session object. Common use cases include making multiple requests to the same host efficiently. Note that always close or use Session as context manager to release connections.", "code_signature": "requests.Session() -> Session object", "tags": "websocket, oauth, httpx, aiohttp", "url": "https://docs.requests.org/en/stable/requests/Session.html", "author": "Official Docs", "date_published": "26-02-2019", "votes_or_stars": 55, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:1498bbde-1f06-4925-9b79-b6068526ae40": {"content": "How to upload a file with requests?\n```python\nwith open('file.pdf', 'rb') as f:\n    r = requests.post(url, files={'file': ('file.pdf', f, 'application/pdf')})\n```\nFor multiple files: pass a list of tuples. For multipart form data with fields: combine `files` and `data` parameters. Don't set Content-Type manually — requests sets it with boundary.\nCode signature: Use the files parameter with a file-like object or tuple.", "file_path": "https://stackoverflow.com/questions/8896868", "function_name": "Use the files parameter with a file-like object or tuple.", "type": "stack_overflow", "metadata": {"record_id": "1498bbde-1f06-4925-9b79-b6068526ae40", "source_type": "stack_overflow", "domain": "Requests/HTTP/APIs", "library": "requests", "title": "How to upload a file with requests?", "content": "```python\nwith open('file.pdf', 'rb') as f:\n    r = requests.post(url, files={'file': ('file.pdf', f, 'application/pdf')})\n```\nFor multiple files: pass a list of tuples. For multipart form data with fields: combine `files` and `data` parameters. Don't set Content-Type manually — requests sets it with boundary.", "code_signature": "Use the files parameter with a file-like object or tuple.", "tags": "httpx, authentication, http, rate-limiting", "url": "https://stackoverflow.com/questions/8896868", "author": "user_243238", "date_published": "15-08-2024", "votes_or_stars": 1552, "relevance_label": "low", "difficulty_level": "advanced"}}, "kb:c575b8e3-cb3a-4690-871f-94c40e969f7d": {"content": "PERF: Django template rendering bottleneck in production\nProblem: Template rendering takes 500ms+ for complex pages with many database queries.\n\nFix/Discussion: Primary cause is N+1 queries in template. Solutions: (1) Use select_related/prefetch_related. (2) Cache rendered templates with `cache` template tag. (3) Use `django-cachalot` for query caching. (4) Move heavy computation to views, pass pre-computed context. (5) Consider server-side caching with Redis.\nCode signature: status:closed", "file_path": "https://github.com/django/django/issues/3243", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "c575b8e3-cb3a-4690-871f-94c40e969f7d", "source_type": "github_issue", "domain": "FastAPI/Flask/Django", "library": "django", "title": "PERF: Django template rendering bottleneck in production", "content": "Problem: Template rendering takes 500ms+ for complex pages with many database queries.\n\nFix/Discussion: Primary cause is N+1 queries in template. Solutions: (1) Use select_related/prefetch_related. (2) Cache rendered templates with `cache` template tag. (3) Use `django-cachalot` for query caching. (4) Move heavy computation to views, pass pre-computed context. (5) Consider server-side caching with Redis.", "code_signature": "status:closed", "tags": "blueprint, template, fastapi, flask", "url": "https://github.com/django/django/issues/3243", "author": "contributor_6988", "date_published": "30-06-2020", "votes_or_stars": 78, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:ddeb70d4-7d80-4daa-b549-ddc7ea153f2d": {"content": "How to use create_engine in SQLAlchemy\n`SQLAlchemy.create_engine` creates a database engine representing a connection pool. Example usage:\n```python\nengine = create_engine('postgresql://user:pass@host/db', echo=True)\n```\nThis is useful when establishing the central database connection for an application. Performance tip: set pool_size and max_overflow based on expected concurrency.\nCode signature: create_engine", "file_path": "https://docs.sqlalchemy.org/en/stable/create_engine.html", "function_name": "create_engine", "type": "documentation", "metadata": {"record_id": "ddeb70d4-7d80-4daa-b549-ddc7ea153f2d", "source_type": "documentation", "domain": "SQLAlchemy/Databases", "library": "SQLAlchemy", "title": "How to use create_engine in SQLAlchemy", "content": "`SQLAlchemy.create_engine` creates a database engine representing a connection pool. Example usage:\n```python\nengine = create_engine('postgresql://user:pass@host/db', echo=True)\n```\nThis is useful when establishing the central database connection for an application. Performance tip: set pool_size and max_overflow based on expected concurrency.", "code_signature": "create_engine", "tags": "sqlalchemy, connection-pool, session, query, alembic", "url": "https://docs.sqlalchemy.org/en/stable/create_engine.html", "author": "Official Docs", "date_published": "06-02-2020", "votes_or_stars": 2045, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:59bd33a3-1e17-477e-82d9-31df0bb1026c": {"content": "asyncio — Concurrent Coroutines\nThe `asyncio.gather` function in asyncio runs multiple coroutines concurrently and collects results. It accepts `*coros, return_exceptions=False` as a parameter and returns list of results. Common use cases include parallel I/O operations such as multiple API calls. Note that set return_exceptions=True to prevent one failure from cancelling all tasks.\nCode signature: asyncio.gather(*coros, return_exceptions=False) -> list of results", "file_path": "https://docs.asyncio.org/en/stable/asyncio/gather.html", "function_name": "asyncio.gather(*coros, return_exceptions=False) -> list of results", "type": "documentation", "metadata": {"record_id": "59bd33a3-1e17-477e-82d9-31df0bb1026c", "source_type": "documentation", "domain": "Asyncio/Threading", "library": "asyncio", "title": "asyncio — Concurrent Coroutines", "content": "The `asyncio.gather` function in asyncio runs multiple coroutines concurrently and collects results. It accepts `*coros, return_exceptions=False` as a parameter and returns list of results. Common use cases include parallel I/O operations such as multiple API calls. Note that set return_exceptions=True to prevent one failure from cancelling all tasks.", "code_signature": "asyncio.gather(*coros, return_exceptions=False) -> list of results", "tags": "await, gather, async, coroutine, task, lock", "url": "https://docs.asyncio.org/en/stable/asyncio/gather.html", "author": "Official Docs", "date_published": "03-12-2021", "votes_or_stars": 497, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:b060690a-8ce6-4cbe-bb6c-893599b735e6": {"content": "SQLAlchemy: how to avoid DetachedInstanceError?\nDetachedInstanceError occurs when accessing a relationship after the session is closed. Solutions: (1) Use `joinedload()` or `subqueryload()` to eagerly load relationships. (2) Keep session open while accessing objects. (3) Use `expire_on_commit=False` on sessionmaker. (4) Convert to dict/DTO before closing session.\nCode signature: Access lazy-loaded relationships within the session scope, or use eager loading.", "file_path": "https://stackoverflow.com/questions/1247595", "function_name": "Access lazy-loaded relationships within the session scope, or use eager loading.", "type": "stack_overflow", "metadata": {"record_id": "b060690a-8ce6-4cbe-bb6c-893599b735e6", "source_type": "stack_overflow", "domain": "SQLAlchemy/Databases", "library": "sqlalchemy", "title": "SQLAlchemy: how to avoid DetachedInstanceError?", "content": "DetachedInstanceError occurs when accessing a relationship after the session is closed. Solutions: (1) Use `joinedload()` or `subqueryload()` to eagerly load relationships. (2) Keep session open while accessing objects. (3) Use `expire_on_commit=False` on sessionmaker. (4) Convert to dict/DTO before closing session.", "code_signature": "Access lazy-loaded relationships within the session scope, or use eager loading.", "tags": "transaction, query, foreign-key, alembic, orm", "url": "https://stackoverflow.com/questions/1247595", "author": "user_107793", "date_published": "24-04-2023", "votes_or_stars": 403, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:731d576e-b223-46e6-b609-0db74d5e5c4e": {"content": "asyncio — Concurrent Coroutines\nThe `asyncio.gather` function in asyncio runs multiple coroutines concurrently and collects results. It accepts `*coros, return_exceptions=False` as a parameter and returns list of results. Common use cases include parallel I/O operations such as multiple API calls. Note that set return_exceptions=True to prevent one failure from cancelling all tasks.\nCode signature: asyncio.gather(*coros, return_exceptions=False) -> list of results", "file_path": "https://docs.asyncio.org/en/stable/asyncio/gather.html", "function_name": "asyncio.gather(*coros, return_exceptions=False) -> list of results", "type": "documentation", "metadata": {"record_id": "731d576e-b223-46e6-b609-0db74d5e5c4e", "source_type": "documentation", "domain": "Asyncio/Threading", "library": "asyncio", "title": "asyncio — Concurrent Coroutines", "content": "The `asyncio.gather` function in asyncio runs multiple coroutines concurrently and collects results. It accepts `*coros, return_exceptions=False` as a parameter and returns list of results. Common use cases include parallel I/O operations such as multiple API calls. Note that set return_exceptions=True to prevent one failure from cancelling all tasks.", "code_signature": "asyncio.gather(*coros, return_exceptions=False) -> list of results", "tags": "async, executor, gather, threading, asyncio, task", "url": "https://docs.asyncio.org/en/stable/asyncio/gather.html", "author": "Official Docs", "date_published": "02-03-2022", "votes_or_stars": 527, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:c5e6a1ef-b0bf-44cc-8cb1-50deaf9a9c20": {"content": "How to use Path.glob in pathlib\n`pathlib.Path.glob` yields all paths matching the given glob pattern. Example usage:\n```python\npy_files = list(Path('src').glob('**/*.py'))\n```\nThis is useful when finding files recursively, batch file processing. Performance tip: iterate the generator lazily rather than converting to list for large directories.\nCode signature: Path.glob", "file_path": "https://docs.pathlib.org/en/stable/Path/glob.html", "function_name": "Path.glob", "type": "documentation", "metadata": {"record_id": "c5e6a1ef-b0bf-44cc-8cb1-50deaf9a9c20", "source_type": "documentation", "domain": "OS/Subprocess/File I/O", "library": "pathlib", "title": "How to use Path.glob in pathlib", "content": "`pathlib.Path.glob` yields all paths matching the given glob pattern. Example usage:\n```python\npy_files = list(Path('src').glob('**/*.py'))\n```\nThis is useful when finding files recursively, batch file processing. Performance tip: iterate the generator lazily rather than converting to list for large directories.", "code_signature": "Path.glob", "tags": "pathlib, stderr, file-io, pipe", "url": "https://docs.pathlib.org/en/stable/Path/glob.html", "author": "Official Docs", "date_published": "18-08-2022", "votes_or_stars": 3229, "relevance_label": "low", "difficulty_level": "advanced"}}, "kb:455df171-2eb0-44ea-98d7-42f158a3ebc8": {"content": "How to stream large HTTP responses with requests?\n```python\nwith requests.get(url, stream=True) as r:\n    r.raise_for_status()\n    with open('file', 'wb') as f:\n        for chunk in r.iter_content(chunk_size=8192):\n            f.write(chunk)\n```\nAlways use as context manager with stream=True to ensure connection is released.\nCode signature: Use stream=True and iterate over response.iter_content() or iter_lines().", "file_path": "https://stackoverflow.com/questions/3592974", "function_name": "Use stream=True and iterate over response.iter_content() or iter_lines().", "type": "stack_overflow", "metadata": {"record_id": "455df171-2eb0-44ea-98d7-42f158a3ebc8", "source_type": "stack_overflow", "domain": "Requests/HTTP/APIs", "library": "json", "title": "How to stream large HTTP responses with requests?", "content": "```python\nwith requests.get(url, stream=True) as r:\n    r.raise_for_status()\n    with open('file', 'wb') as f:\n        for chunk in r.iter_content(chunk_size=8192):\n            f.write(chunk)\n```\nAlways use as context manager with stream=True to ensure connection is released.", "code_signature": "Use stream=True and iterate over response.iter_content() or iter_lines().", "tags": "rate-limiting, requests, rest-api, json, oauth, authentication", "url": "https://stackoverflow.com/questions/3592974", "author": "user_980733", "date_published": "16-05-2019", "votes_or_stars": 1670, "relevance_label": "low", "difficulty_level": "intermediate"}}, "kb:4a214a3f-9bd8-45a0-a57f-bca545fed3de": {"content": "subprocess.run vs Popen: which should I use?\n`subprocess.run()` is a high-level wrapper that waits for completion — use for most cases. `subprocess.Popen()` gives full control: stream stdout/stderr in real-time, write to stdin, run processes concurrently. For capturing output: `subprocess.run(..., capture_output=True, text=True)`. Always use `shell=False` and pass args as a list to prevent shell injection.\nCode signature: Use subprocess.run() for simple cases; Popen for streaming output or interactive processes.", "file_path": "https://stackoverflow.com/questions/5727085", "function_name": "Use subprocess.run() for simple cases; Popen for streaming output or interactive processes.", "type": "stack_overflow", "metadata": {"record_id": "4a214a3f-9bd8-45a0-a57f-bca545fed3de", "source_type": "stack_overflow", "domain": "OS/Subprocess/File I/O", "library": "shutil", "title": "subprocess.run vs Popen: which should I use?", "content": "`subprocess.run()` is a high-level wrapper that waits for completion — use for most cases. `subprocess.Popen()` gives full control: stream stdout/stderr in real-time, write to stdin, run processes concurrently. For capturing output: `subprocess.run(..., capture_output=True, text=True)`. Always use `shell=False` and pass args as a list to prevent shell injection.", "code_signature": "Use subprocess.run() for simple cases; Popen for streaming output or interactive processes.", "tags": "pipe, os, stdout, subprocess, stdin", "url": "https://stackoverflow.com/questions/5727085", "author": "user_644210", "date_published": "06-07-2024", "votes_or_stars": 625, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:25662505-da49-4c8e-abff-d834aa268a30": {"content": "FastAPI — Path Operations\nThe `@app.get` function in FastAPI registers a GET endpoint at the specified path. It accepts `path, response_model` as a parameter and returns Response. Common use cases include building REST API endpoints that return JSON data. Note that use response_model to enforce output schema validation.\nCode signature: @app.get(path, response_model) -> Response", "file_path": "https://docs.fastapi.org/en/stable/@app/get.html", "function_name": "@app.get(path, response_model) -> Response", "type": "documentation", "metadata": {"record_id": "25662505-da49-4c8e-abff-d834aa268a30", "source_type": "documentation", "domain": "FastAPI/Flask/Django", "library": "FastAPI", "title": "FastAPI — Path Operations", "content": "The `@app.get` function in FastAPI registers a GET endpoint at the specified path. It accepts `path, response_model` as a parameter and returns Response. Common use cases include building REST API endpoints that return JSON data. Note that use response_model to enforce output schema validation.", "code_signature": "@app.get(path, response_model) -> Response", "tags": "pydantic, orm, middleware, flask", "url": "https://docs.fastapi.org/en/stable/@app/get.html", "author": "Official Docs", "date_published": "21-06-2024", "votes_or_stars": 2877, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:29576394-5fc7-4a08-9abe-3ead1c0b8ce9": {"content": "asyncio — Concurrent Coroutines\nThe `asyncio.gather` function in asyncio runs multiple coroutines concurrently and collects results. It accepts `*coros, return_exceptions=False` as a parameter and returns list of results. Common use cases include parallel I/O operations such as multiple API calls. Note that set return_exceptions=True to prevent one failure from cancelling all tasks.\nCode signature: asyncio.gather(*coros, return_exceptions=False) -> list of results", "file_path": "https://docs.asyncio.org/en/stable/asyncio/gather.html", "function_name": "asyncio.gather(*coros, return_exceptions=False) -> list of results", "type": "documentation", "metadata": {"record_id": "29576394-5fc7-4a08-9abe-3ead1c0b8ce9", "source_type": "documentation", "domain": "Asyncio/Threading", "library": "asyncio", "title": "asyncio — Concurrent Coroutines", "content": "The `asyncio.gather` function in asyncio runs multiple coroutines concurrently and collects results. It accepts `*coros, return_exceptions=False` as a parameter and returns list of results. Common use cases include parallel I/O operations such as multiple API calls. Note that set return_exceptions=True to prevent one failure from cancelling all tasks.", "code_signature": "asyncio.gather(*coros, return_exceptions=False) -> list of results", "tags": "semaphore, lock, asyncio, event-loop", "url": "https://docs.asyncio.org/en/stable/asyncio/gather.html", "author": "Official Docs", "date_published": "29-01-2022", "votes_or_stars": 499, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:c9ac3f35-39a7-426b-9a0e-12c32cee9606": {"content": "PERF: groupby().apply() extremely slow on large DataFrames\nProblem: groupby().apply() with a custom function is 100x slower than expected on 10M row DataFrames.\n\nFix/Discussion: groupby().apply() serializes each group and has high overhead. Fix: vectorize with numpy, use groupby().transform() for same-shape output, or groupby().agg() with named aggregations. For complex logic, consider Polars or Dask. Numba's @jit can accelerate custom group functions.\nCode signature: status:closed", "file_path": "https://github.com/pandas/pandas/issues/8446", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "c9ac3f35-39a7-426b-9a0e-12c32cee9606", "source_type": "github_issue", "domain": "NumPy/Pandas", "library": "pandas", "title": "PERF: groupby().apply() extremely slow on large DataFrames", "content": "Problem: groupby().apply() with a custom function is 100x slower than expected on 10M row DataFrames.\n\nFix/Discussion: groupby().apply() serializes each group and has high overhead. Fix: vectorize with numpy, use groupby().transform() for same-shape output, or groupby().agg() with named aggregations. For complex logic, consider Polars or Dask. Numba's @jit can accelerate custom group functions.", "code_signature": "status:closed", "tags": "pandas, pivot, loc, nan, numpy, vectorization", "url": "https://github.com/pandas/pandas/issues/8446", "author": "contributor_6648", "date_published": "04-09-2020", "votes_or_stars": 306, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:6dff8917-f605-4c95-a2a8-b79044b127c7": {"content": "How to stream large HTTP responses with requests?\n```python\nwith requests.get(url, stream=True) as r:\n    r.raise_for_status()\n    with open('file', 'wb') as f:\n        for chunk in r.iter_content(chunk_size=8192):\n            f.write(chunk)\n```\nAlways use as context manager with stream=True to ensure connection is released.\nCode signature: Use stream=True and iterate over response.iter_content() or iter_lines().", "file_path": "https://stackoverflow.com/questions/3592974", "function_name": "Use stream=True and iterate over response.iter_content() or iter_lines().", "type": "stack_overflow", "metadata": {"record_id": "6dff8917-f605-4c95-a2a8-b79044b127c7", "source_type": "stack_overflow", "domain": "Requests/HTTP/APIs", "library": "json", "title": "How to stream large HTTP responses with requests?", "content": "```python\nwith requests.get(url, stream=True) as r:\n    r.raise_for_status()\n    with open('file', 'wb') as f:\n        for chunk in r.iter_content(chunk_size=8192):\n            f.write(chunk)\n```\nAlways use as context manager with stream=True to ensure connection is released.", "code_signature": "Use stream=True and iterate over response.iter_content() or iter_lines().", "tags": "http, aiohttp, headers", "url": "https://stackoverflow.com/questions/3592974", "author": "user_980733", "date_published": "27-10-2020", "votes_or_stars": 1704, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:d9769f33-aaa6-4f73-a14c-70cd5da9de12": {"content": "BUG: requests hangs indefinitely on slow server responses\nProblem: requests.get() hangs forever when server accepts connection but never sends response.\n\nFix/Discussion: Always set both connect and read timeouts: `requests.get(url, timeout=(3.05, 27))`. Tuple: (connect_timeout, read_timeout). Or single value for both. Mount a Session with default timeouts using a custom HTTPAdapter subclass. Never use `timeout=None` in production code.\nCode signature: status:closed", "file_path": "https://github.com/requests/requests/issues/4019", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "d9769f33-aaa6-4f73-a14c-70cd5da9de12", "source_type": "github_issue", "domain": "Requests/HTTP/APIs", "library": "requests", "title": "BUG: requests hangs indefinitely on slow server responses", "content": "Problem: requests.get() hangs forever when server accepts connection but never sends response.\n\nFix/Discussion: Always set both connect and read timeouts: `requests.get(url, timeout=(3.05, 27))`. Tuple: (connect_timeout, read_timeout). Or single value for both. Mount a Session with default timeouts using a custom HTTPAdapter subclass. Never use `timeout=None` in production code.", "code_signature": "status:closed", "tags": "json, httpx, authentication, oauth", "url": "https://github.com/requests/requests/issues/4019", "author": "contributor_4599", "date_published": "22-11-2023", "votes_or_stars": 239, "relevance_label": "low", "difficulty_level": "intermediate"}}, "kb:c8d9e799-0af8-4dc8-94d7-ccb9881793a4": {"content": "How to stream large HTTP responses with requests?\n```python\nwith requests.get(url, stream=True) as r:\n    r.raise_for_status()\n    with open('file', 'wb') as f:\n        for chunk in r.iter_content(chunk_size=8192):\n            f.write(chunk)\n```\nAlways use as context manager with stream=True to ensure connection is released.\nCode signature: Use stream=True and iterate over response.iter_content() or iter_lines().", "file_path": "https://stackoverflow.com/questions/3592974", "function_name": "Use stream=True and iterate over response.iter_content() or iter_lines().", "type": "stack_overflow", "metadata": {"record_id": "c8d9e799-0af8-4dc8-94d7-ccb9881793a4", "source_type": "stack_overflow", "domain": "Requests/HTTP/APIs", "library": "json", "title": "How to stream large HTTP responses with requests?", "content": "```python\nwith requests.get(url, stream=True) as r:\n    r.raise_for_status()\n    with open('file', 'wb') as f:\n        for chunk in r.iter_content(chunk_size=8192):\n            f.write(chunk)\n```\nAlways use as context manager with stream=True to ensure connection is released.", "code_signature": "Use stream=True and iterate over response.iter_content() or iter_lines().", "tags": "headers, websocket, oauth, retry, rest-api", "url": "https://stackoverflow.com/questions/3592974", "author": "user_980733", "date_published": "11-09-2019", "votes_or_stars": 1701, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:a79b3495-b0df-4010-a8c2-a68b326d11e9": {"content": "How to use df.groupby in Pandas\n`Pandas.df.groupby` groups DataFrame rows by column values. Example usage:\n```python\ndf.groupby('category')['value'].mean()\n```\nThis is useful when aggregation, transformation, filtering by group. Performance tip: avoid apply() on large DataFrames; prefer agg().\nCode signature: df.groupby", "file_path": "https://docs.pandas.org/en/stable/df/groupby.html", "function_name": "df.groupby", "type": "documentation", "metadata": {"record_id": "a79b3495-b0df-4010-a8c2-a68b326d11e9", "source_type": "documentation", "domain": "NumPy/Pandas", "library": "Pandas", "title": "How to use df.groupby in Pandas", "content": "`Pandas.df.groupby` groups DataFrame rows by column values. Example usage:\n```python\ndf.groupby('category')['value'].mean()\n```\nThis is useful when aggregation, transformation, filtering by group. Performance tip: avoid apply() on large DataFrames; prefer agg().", "code_signature": "df.groupby", "tags": "loc, groupby, dtype, nan, reshape, pivot", "url": "https://docs.pandas.org/en/stable/df/groupby.html", "author": "Official Docs", "date_published": "09-08-2020", "votes_or_stars": 840, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:26445d92-1bde-4d0e-97fe-3fc858c959c0": {"content": "NumPy broadcasting error: operands could not be broadcast with shapes\nNumPy broadcasts shapes right-to-left. Arrays (3,4) and (4,) are compatible. Arrays (3,4) and (3,) are not — reshape to (3,1). Use `arr.reshape(-1,1)` to add a dimension. Always print `a.shape, b.shape` before operations to debug. `np.expand_dims(arr, axis=1)` is cleaner than reshape for adding dimensions.\nCode signature: Shapes must be compatible: dimensions must match or one of them must be 1.", "file_path": "https://stackoverflow.com/questions/4860684", "function_name": "Shapes must be compatible: dimensions must match or one of them must be 1.", "type": "stack_overflow", "metadata": {"record_id": "26445d92-1bde-4d0e-97fe-3fc858c959c0", "source_type": "stack_overflow", "domain": "NumPy/Pandas", "library": "numpy", "title": "NumPy broadcasting error: operands could not be broadcast with shapes", "content": "NumPy broadcasts shapes right-to-left. Arrays (3,4) and (4,) are compatible. Arrays (3,4) and (3,) are not — reshape to (3,1). Use `arr.reshape(-1,1)` to add a dimension. Always print `a.shape, b.shape` before operations to debug. `np.expand_dims(arr, axis=1)` is cleaner than reshape for adding dimensions.", "code_signature": "Shapes must be compatible: dimensions must match or one of them must be 1.", "tags": "dtype, groupby, reshape, broadcasting, vectorization", "url": "https://stackoverflow.com/questions/4860684", "author": "user_627024", "date_published": "30-06-2023", "votes_or_stars": 1389, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:2a079ef7-d245-47bc-a929-bc9678d7d3df": {"content": "PERF: groupby().apply() extremely slow on large DataFrames\nProblem: groupby().apply() with a custom function is 100x slower than expected on 10M row DataFrames.\n\nFix/Discussion: groupby().apply() serializes each group and has high overhead. Fix: vectorize with numpy, use groupby().transform() for same-shape output, or groupby().agg() with named aggregations. For complex logic, consider Polars or Dask. Numba's @jit can accelerate custom group functions.\nCode signature: status:closed", "file_path": "https://github.com/pandas/pandas/issues/8446", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "2a079ef7-d245-47bc-a929-bc9678d7d3df", "source_type": "github_issue", "domain": "NumPy/Pandas", "library": "pandas", "title": "PERF: groupby().apply() extremely slow on large DataFrames", "content": "Problem: groupby().apply() with a custom function is 100x slower than expected on 10M row DataFrames.\n\nFix/Discussion: groupby().apply() serializes each group and has high overhead. Fix: vectorize with numpy, use groupby().transform() for same-shape output, or groupby().agg() with named aggregations. For complex logic, consider Polars or Dask. Numba's @jit can accelerate custom group functions.", "code_signature": "status:closed", "tags": "merge, loc, groupby, vectorization, iloc", "url": "https://github.com/pandas/pandas/issues/8446", "author": "contributor_6648", "date_published": "07-12-2023", "votes_or_stars": 308, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:298c71ac-3e40-41e3-bf39-337fa9306350": {"content": "How to run asyncio code from synchronous Python?\n`asyncio.run(main())` creates an event loop, runs the coroutine, closes the loop. Never call asyncio.run() from inside a running event loop (use await instead). In Jupyter notebooks (which have a running loop), use `await coroutine()` directly or `nest_asyncio.apply()`. For mixing sync/async, use `loop.run_until_complete()`.\nCode signature: Use asyncio.run() in Python 3.7+ to execute a coroutine from sync code.", "file_path": "https://stackoverflow.com/questions/2140194", "function_name": "Use asyncio.run() in Python 3.7+ to execute a coroutine from sync code.", "type": "stack_overflow", "metadata": {"record_id": "298c71ac-3e40-41e3-bf39-337fa9306350", "source_type": "stack_overflow", "domain": "Asyncio/Threading", "library": "gather", "title": "How to run asyncio code from synchronous Python?", "content": "`asyncio.run(main())` creates an event loop, runs the coroutine, closes the loop. Never call asyncio.run() from inside a running event loop (use await instead). In Jupyter notebooks (which have a running loop), use `await coroutine()` directly or `nest_asyncio.apply()`. For mixing sync/async, use `loop.run_until_complete()`.", "code_signature": "Use asyncio.run() in Python 3.7+ to execute a coroutine from sync code.", "tags": "threading, asyncio, race-condition, gather, executor", "url": "https://stackoverflow.com/questions/2140194", "author": "user_718011", "date_published": "17-08-2024", "votes_or_stars": 284, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:29bbcd8a-a977-44e0-991e-e078119d7356": {"content": "How to handle database migrations with Alembic?\nWorkflow: (1) Change SQLAlchemy models. (2) `alembic revision --autogenerate -m 'description'`. (3) Review generated migration script. (4) `alembic upgrade head` to apply. Always review autogenerated migrations — they miss some changes (indexes, constraints). Use `alembic downgrade -1` to rollback. Store migrations in version control.\nCode signature: Use alembic revision --autogenerate to create migrations from model changes.", "file_path": "https://stackoverflow.com/questions/9436429", "function_name": "Use alembic revision --autogenerate to create migrations from model changes.", "type": "stack_overflow", "metadata": {"record_id": "29bbcd8a-a977-44e0-991e-e078119d7356", "source_type": "stack_overflow", "domain": "SQLAlchemy/Databases", "library": "connection-pool", "title": "How to handle database migrations with Alembic?", "content": "Workflow: (1) Change SQLAlchemy models. (2) `alembic revision --autogenerate -m 'description'`. (3) Review generated migration script. (4) `alembic upgrade head` to apply. Always review autogenerated migrations — they miss some changes (indexes, constraints). Use `alembic downgrade -1` to rollback. Store migrations in version control.", "code_signature": "Use alembic revision --autogenerate to create migrations from model changes.", "tags": "sqlite, session, postgresql, alembic", "url": "https://stackoverflow.com/questions/9436429", "author": "user_974047", "date_published": "23-08-2024", "votes_or_stars": 2380, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:7a23d587-6972-4a2d-aecd-07c681def55a": {"content": "How to implement a thread pool in Python?\n```python\nfrom concurrent.futures import ThreadPoolExecutor\nwith ThreadPoolExecutor(max_workers=8) as ex:\n    futures = [ex.submit(task, arg) for arg in args]\n    results = [f.result() for f in futures]\n```\nFor CPU-bound: use ProcessPoolExecutor instead. For async integration: `await loop.run_in_executor(executor, func, *args)`.\nCode signature: Use concurrent.futures.ThreadPoolExecutor for I/O-bound tasks.", "file_path": "https://stackoverflow.com/questions/3646552", "function_name": "Use concurrent.futures.ThreadPoolExecutor for I/O-bound tasks.", "type": "stack_overflow", "metadata": {"record_id": "7a23d587-6972-4a2d-aecd-07c681def55a", "source_type": "stack_overflow", "domain": "Asyncio/Threading", "library": "coroutine", "title": "How to implement a thread pool in Python?", "content": "```python\nfrom concurrent.futures import ThreadPoolExecutor\nwith ThreadPoolExecutor(max_workers=8) as ex:\n    futures = [ex.submit(task, arg) for arg in args]\n    results = [f.result() for f in futures]\n```\nFor CPU-bound: use ProcessPoolExecutor instead. For async integration: `await loop.run_in_executor(executor, func, *args)`.", "code_signature": "Use concurrent.futures.ThreadPoolExecutor for I/O-bound tasks.", "tags": "await, asyncio, semaphore, coroutine", "url": "https://stackoverflow.com/questions/3646552", "author": "user_469469", "date_published": "08-12-2022", "votes_or_stars": 1496, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:93ca9cc2-d8fe-49b6-8352-c572de8472ac": {"content": "How to limit concurrency with asyncio?\n```python\nsem = asyncio.Semaphore(10)\nasync def limited(url):\n    async with sem:\n        return await fetch(url)\nresults = await asyncio.gather(*[limited(u) for u in urls])\n```\nThis prevents overwhelming APIs or databases with too many simultaneous connections.\nCode signature: Use asyncio.Semaphore to cap the number of concurrent coroutines.", "file_path": "https://stackoverflow.com/questions/2375453", "function_name": "Use asyncio.Semaphore to cap the number of concurrent coroutines.", "type": "stack_overflow", "metadata": {"record_id": "93ca9cc2-d8fe-49b6-8352-c572de8472ac", "source_type": "stack_overflow", "domain": "Asyncio/Threading", "library": "asyncio", "title": "How to limit concurrency with asyncio?", "content": "```python\nsem = asyncio.Semaphore(10)\nasync def limited(url):\n    async with sem:\n        return await fetch(url)\nresults = await asyncio.gather(*[limited(u) for u in urls])\n```\nThis prevents overwhelming APIs or databases with too many simultaneous connections.", "code_signature": "Use asyncio.Semaphore to cap the number of concurrent coroutines.", "tags": "coroutine, gather, race-condition, lock, async, executor", "url": "https://stackoverflow.com/questions/2375453", "author": "user_449589", "date_published": "29-03-2020", "votes_or_stars": 2446, "relevance_label": "low", "difficulty_level": "intermediate"}}, "kb:fb024e4d-4a9f-48dd-b5ef-9d3fb286b2ce": {"content": "os Environment Variables — official reference\nOfficial documentation for os Environment Variables. The method signature is `os.environ(key)`. provides a mapping of the current environment variables. Raises `KeyError` when key is accessed with [] notation and is not set. See also: dotenv, os.getenv, os.putenv.\nCode signature: os.environ(key)", "file_path": "https://docs.os.org/en/stable/os/environ.html", "function_name": "os.environ(key)", "type": "documentation", "metadata": {"record_id": "fb024e4d-4a9f-48dd-b5ef-9d3fb286b2ce", "source_type": "documentation", "domain": "OS/Subprocess/File I/O", "library": "os", "title": "os Environment Variables — official reference", "content": "Official documentation for os Environment Variables. The method signature is `os.environ(key)`. provides a mapping of the current environment variables. Raises `KeyError` when key is accessed with [] notation and is not set. See also: dotenv, os.getenv, os.putenv.", "code_signature": "os.environ(key)", "tags": "env-variable, stdin, os, stdout, stderr", "url": "https://docs.os.org/en/stable/os/environ.html", "author": "Official Docs", "date_published": "24-11-2024", "votes_or_stars": 2207, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:087941b2-a1f4-4c2e-b0d5-a7b6b5344591": {"content": "Django N+1 query problem: how to fix?\nN+1 occurs when accessing related objects in a loop. Diagnose with `django-debug-toolbar` or `connection.queries`. Fix ForeignKey: `User.objects.select_related('profile').all()`. Fix M2M: `Post.objects.prefetch_related('tags').all()`. Use `only()` to fetch specific fields and reduce data transfer.\nCode signature: Use select_related() for ForeignKey and prefetch_related() for ManyToMany relationships.", "file_path": "https://stackoverflow.com/questions/8106470", "function_name": "Use select_related() for ForeignKey and prefetch_related() for ManyToMany relationships.", "type": "stack_overflow", "metadata": {"record_id": "087941b2-a1f4-4c2e-b0d5-a7b6b5344591", "source_type": "stack_overflow", "domain": "FastAPI/Flask/Django", "library": "request", "title": "Django N+1 query problem: how to fix?", "content": "N+1 occurs when accessing related objects in a loop. Diagnose with `django-debug-toolbar` or `connection.queries`. Fix ForeignKey: `User.objects.select_related('profile').all()`. Fix M2M: `Post.objects.prefetch_related('tags').all()`. Use `only()` to fetch specific fields and reduce data transfer.", "code_signature": "Use select_related() for ForeignKey and prefetch_related() for ManyToMany relationships.", "tags": "fastapi, orm, rest, routing", "url": "https://stackoverflow.com/questions/8106470", "author": "user_441071", "date_published": "13-03-2022", "votes_or_stars": 1765, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:9c911cbb-8c8b-4fb1-b2b7-3653f7fd735c": {"content": "asyncio — Concurrent Coroutines\nThe `asyncio.gather` function in asyncio runs multiple coroutines concurrently and collects results. It accepts `*coros, return_exceptions=False` as a parameter and returns list of results. Common use cases include parallel I/O operations such as multiple API calls. Note that set return_exceptions=True to prevent one failure from cancelling all tasks.\nCode signature: asyncio.gather(*coros, return_exceptions=False) -> list of results", "file_path": "https://docs.asyncio.org/en/stable/asyncio/gather.html", "function_name": "asyncio.gather(*coros, return_exceptions=False) -> list of results", "type": "documentation", "metadata": {"record_id": "9c911cbb-8c8b-4fb1-b2b7-3653f7fd735c", "source_type": "documentation", "domain": "Asyncio/Threading", "library": "asyncio", "title": "asyncio — Concurrent Coroutines", "content": "The `asyncio.gather` function in asyncio runs multiple coroutines concurrently and collects results. It accepts `*coros, return_exceptions=False` as a parameter and returns list of results. Common use cases include parallel I/O operations such as multiple API calls. Note that set return_exceptions=True to prevent one failure from cancelling all tasks.", "code_signature": "asyncio.gather(*coros, return_exceptions=False) -> list of results", "tags": "event-loop, task, threading", "url": "https://docs.asyncio.org/en/stable/asyncio/gather.html", "author": "Official Docs", "date_published": "08-10-2019", "votes_or_stars": 495, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:42d26a4b-5b8c-41d1-9109-e96eed682a27": {"content": "BUG: asyncio task leaks when exception is not handled\nProblem: Unhandled exceptions in fire-and-forget asyncio tasks are silently ignored, causing task leaks.\n\nFix/Discussion: Always await tasks or add exception handlers. Use `task.add_done_callback()` to log exceptions. In Python 3.11+, use TaskGroup for structured concurrency — exceptions propagate automatically. Set `asyncio.get_event_loop().set_exception_handler()` for global unhandled task exception logging.\nCode signature: status:closed", "file_path": "https://github.com/cpython/cpython/issues/5591", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "42d26a4b-5b8c-41d1-9109-e96eed682a27", "source_type": "github_issue", "domain": "Asyncio/Threading", "library": "cpython", "title": "BUG: asyncio task leaks when exception is not handled", "content": "Problem: Unhandled exceptions in fire-and-forget asyncio tasks are silently ignored, causing task leaks.\n\nFix/Discussion: Always await tasks or add exception handlers. Use `task.add_done_callback()` to log exceptions. In Python 3.11+, use TaskGroup for structured concurrency — exceptions propagate automatically. Set `asyncio.get_event_loop().set_exception_handler()` for global unhandled task exception logging.", "code_signature": "status:closed", "tags": "task, race-condition, await, coroutine", "url": "https://github.com/cpython/cpython/issues/5591", "author": "contributor_1630", "date_published": "07-12-2018", "votes_or_stars": 362, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:41246a85-f999-41f4-8e0b-9cc95fdd1f71": {"content": "BUG: subprocess.Popen hangs when stdout and stderr buffers fill\nProblem: subprocess.Popen hangs indefinitely when child process produces large output.\n\nFix/Discussion: This is a classic deadlock: parent waits for child, child waits for parent to read buffer. Fix: use `subprocess.run(..., capture_output=True)` which handles this correctly. Or: `stdout, stderr = proc.communicate()`. Never use `proc.stdout.read()` and `proc.wait()` separately.\nCode signature: status:closed", "file_path": "https://github.com/cpython/cpython/issues/8586", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "41246a85-f999-41f4-8e0b-9cc95fdd1f71", "source_type": "github_issue", "domain": "OS/Subprocess/File I/O", "library": "cpython", "title": "BUG: subprocess.Popen hangs when stdout and stderr buffers fill", "content": "Problem: subprocess.Popen hangs indefinitely when child process produces large output.\n\nFix/Discussion: This is a classic deadlock: parent waits for child, child waits for parent to read buffer. Fix: use `subprocess.run(..., capture_output=True)` which handles this correctly. Or: `stdout, stderr = proc.communicate()`. Never use `proc.stdout.read()` and `proc.wait()` separately.", "code_signature": "status:closed", "tags": "tempfile, stderr, glob, pathlib, file-io", "url": "https://github.com/cpython/cpython/issues/8586", "author": "contributor_7711", "date_published": "29-04-2018", "votes_or_stars": 388, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:afbd07bc-543a-4807-8512-12a0c9f79aa3": {"content": "How to use Path.glob in pathlib\n`pathlib.Path.glob` yields all paths matching the given glob pattern. Example usage:\n```python\npy_files = list(Path('src').glob('**/*.py'))\n```\nThis is useful when finding files recursively, batch file processing. Performance tip: iterate the generator lazily rather than converting to list for large directories.\nCode signature: Path.glob", "file_path": "https://docs.pathlib.org/en/stable/Path/glob.html", "function_name": "Path.glob", "type": "documentation", "metadata": {"record_id": "afbd07bc-543a-4807-8512-12a0c9f79aa3", "source_type": "documentation", "domain": "OS/Subprocess/File I/O", "library": "pathlib", "title": "How to use Path.glob in pathlib", "content": "`pathlib.Path.glob` yields all paths matching the given glob pattern. Example usage:\n```python\npy_files = list(Path('src').glob('**/*.py'))\n```\nThis is useful when finding files recursively, batch file processing. Performance tip: iterate the generator lazily rather than converting to list for large directories.", "code_signature": "Path.glob", "tags": "glob, file-io, shutil, stderr", "url": "https://docs.pathlib.org/en/stable/Path/glob.html", "author": "Official Docs", "date_published": "18-06-2020", "votes_or_stars": 3229, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:84263a6b-4c5f-41c4-963b-0638be04667b": {"content": "How to apply a function to each row in Pandas without using iterrows?\n`iterrows()` is slow because it creates a Series per row. Prefer `df.apply(func, axis=1)` for row-wise operations, or better yet, vectorize using NumPy: `np.where(condition, a, b)`. For complex logic, `df.assign()` with vectorized expressions is cleanest. Benchmark: vectorized > apply > itertuples > iterrows.\nCode signature: Use df.apply(func, axis=1) or vectorized NumPy operations for best performance.", "file_path": "https://stackoverflow.com/questions/5446912", "function_name": "Use df.apply(func, axis=1) or vectorized NumPy operations for best performance.", "type": "stack_overflow", "metadata": {"record_id": "84263a6b-4c5f-41c4-963b-0638be04667b", "source_type": "stack_overflow", "domain": "NumPy/Pandas", "library": "dataframe", "title": "How to apply a function to each row in Pandas without using iterrows?", "content": "`iterrows()` is slow because it creates a Series per row. Prefer `df.apply(func, axis=1)` for row-wise operations, or better yet, vectorize using NumPy: `np.where(condition, a, b)`. For complex logic, `df.assign()` with vectorized expressions is cleanest. Benchmark: vectorized > apply > itertuples > iterrows.", "code_signature": "Use df.apply(func, axis=1) or vectorized NumPy operations for best performance.", "tags": "reshape, loc, merge, numpy", "url": "https://stackoverflow.com/questions/5446912", "author": "user_563306", "date_published": "19-04-2021", "votes_or_stars": 2262, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:7b2dd68d-f182-41c0-afbd-3dfc3bcf7925": {"content": "How to handle rate limiting with the requests library?\n```python\nfrom tenacity import retry, wait_exponential, stop_after_attempt\n@retry(wait=wait_exponential(min=1, max=60), stop=stop_after_attempt(5))\ndef call_api(url):\n    r = requests.get(url)\n    r.raise_for_status()\n    return r.json()\n```\nCheck `Retry-After` header in 429 responses. For async: use `aiohttp` with tenacity.\nCode signature: Implement exponential backoff with the tenacity or backoff library.", "file_path": "https://stackoverflow.com/questions/1604451", "function_name": "Implement exponential backoff with the tenacity or backoff library.", "type": "stack_overflow", "metadata": {"record_id": "7b2dd68d-f182-41c0-afbd-3dfc3bcf7925", "source_type": "stack_overflow", "domain": "Requests/HTTP/APIs", "library": "retry", "title": "How to handle rate limiting with the requests library?", "content": "```python\nfrom tenacity import retry, wait_exponential, stop_after_attempt\n@retry(wait=wait_exponential(min=1, max=60), stop=stop_after_attempt(5))\ndef call_api(url):\n    r = requests.get(url)\n    r.raise_for_status()\n    return r.json()\n```\nCheck `Retry-After` header in 429 responses. For async: use `aiohttp` with tenacity.", "code_signature": "Implement exponential backoff with the tenacity or backoff library.", "tags": "headers, aiohttp, authentication, websocket", "url": "https://stackoverflow.com/questions/1604451", "author": "user_885136", "date_published": "29-02-2020", "votes_or_stars": 635, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:d9509852-5f82-495f-a512-f3fff68cd440": {"content": "BUG: threading.Timer not cancellable after it fires\nProblem: Calling cancel() on a Timer that has already executed does not raise an error but has no effect.\n\nFix/Discussion: threading.Timer.cancel() only works before the timer fires. Store Timer reference and check `timer.finished.is_set()` to know if it's already run. For repeating timers, use a loop with `threading.Event` for clean cancellation: `stop_event = threading.Event(); stop_event.wait(interval)`.\nCode signature: status:closed", "file_path": "https://github.com/cpython/cpython/issues/4111", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "d9509852-5f82-495f-a512-f3fff68cd440", "source_type": "github_issue", "domain": "Asyncio/Threading", "library": "cpython", "title": "BUG: threading.Timer not cancellable after it fires", "content": "Problem: Calling cancel() on a Timer that has already executed does not raise an error but has no effect.\n\nFix/Discussion: threading.Timer.cancel() only works before the timer fires. Store Timer reference and check `timer.finished.is_set()` to know if it's already run. For repeating timers, use a loop with `threading.Event` for clean cancellation: `stop_event = threading.Event(); stop_event.wait(interval)`.", "code_signature": "status:closed", "tags": "lock, race-condition, await", "url": "https://github.com/cpython/cpython/issues/4111", "author": "contributor_7884", "date_published": "18-09-2018", "votes_or_stars": 55, "relevance_label": "low", "difficulty_level": "intermediate"}}, "kb:1a432f07-4363-4bb4-8641-eed995ef193c": {"content": "How to read a large file line by line in Python without loading it all into memory?\n```python\nwith open('large.txt', 'r') as f:\n    for line in f:\n        process(line.strip())\n```\nThis reads one line at a time. For binary files: use `f.read(chunk_size)` in a loop. For CSV: use `pd.read_csv(file, chunksize=10000)`. Never use `f.readlines()` on large files.\nCode signature: Iterate over the file object directly — it yields lines lazily.", "file_path": "https://stackoverflow.com/questions/2737893", "function_name": "Iterate over the file object directly — it yields lines lazily.", "type": "stack_overflow", "metadata": {"record_id": "1a432f07-4363-4bb4-8641-eed995ef193c", "source_type": "stack_overflow", "domain": "OS/Subprocess/File I/O", "library": "shutil", "title": "How to read a large file line by line in Python without loading it all into memory?", "content": "```python\nwith open('large.txt', 'r') as f:\n    for line in f:\n        process(line.strip())\n```\nThis reads one line at a time. For binary files: use `f.read(chunk_size)` in a loop. For CSV: use `pd.read_csv(file, chunksize=10000)`. Never use `f.readlines()` on large files.", "code_signature": "Iterate over the file object directly — it yields lines lazily.", "tags": "env-variable, process, pathlib, file-io, stdin", "url": "https://stackoverflow.com/questions/2737893", "author": "user_994539", "date_published": "18-09-2019", "votes_or_stars": 2278, "relevance_label": "low", "difficulty_level": "intermediate"}}, "kb:ad8b7449-079a-41f5-86d4-c0c63abeb4d8": {"content": "subprocess — Process Execution\nThe `subprocess.run` function in subprocess runs a subprocess and waits for it to complete. It accepts `args, capture_output=False, timeout=None` as a parameter and returns CompletedProcess. Common use cases include executing shell commands, scripts, or external programs from Python. Note that use shell=False (default) to avoid shell injection vulnerabilities.\nCode signature: subprocess.run(args, capture_output=False, timeout=None) -> CompletedProcess", "file_path": "https://docs.subprocess.org/en/stable/subprocess/run.html", "function_name": "subprocess.run(args, capture_output=False, timeout=None) -> CompletedProcess", "type": "documentation", "metadata": {"record_id": "ad8b7449-079a-41f5-86d4-c0c63abeb4d8", "source_type": "documentation", "domain": "OS/Subprocess/File I/O", "library": "subprocess", "title": "subprocess — Process Execution", "content": "The `subprocess.run` function in subprocess runs a subprocess and waits for it to complete. It accepts `args, capture_output=False, timeout=None` as a parameter and returns CompletedProcess. Common use cases include executing shell commands, scripts, or external programs from Python. Note that use shell=False (default) to avoid shell injection vulnerabilities.", "code_signature": "subprocess.run(args, capture_output=False, timeout=None) -> CompletedProcess", "tags": "tempfile, file-io, pathlib, process, env-variable, subprocess", "url": "https://docs.subprocess.org/en/stable/subprocess/run.html", "author": "Official Docs", "date_published": "27-03-2024", "votes_or_stars": 435, "relevance_label": "low", "difficulty_level": "beginner"}}, "kb:d8bf5d99-d897-415f-b932-a8ba4d527d1c": {"content": "How to handle rate limiting with the requests library?\n```python\nfrom tenacity import retry, wait_exponential, stop_after_attempt\n@retry(wait=wait_exponential(min=1, max=60), stop=stop_after_attempt(5))\ndef call_api(url):\n    r = requests.get(url)\n    r.raise_for_status()\n    return r.json()\n```\nCheck `Retry-After` header in 429 responses. For async: use `aiohttp` with tenacity.\nCode signature: Implement exponential backoff with the tenacity or backoff library.", "file_path": "https://stackoverflow.com/questions/1604451", "function_name": "Implement exponential backoff with the tenacity or backoff library.", "type": "stack_overflow", "metadata": {"record_id": "d8bf5d99-d897-415f-b932-a8ba4d527d1c", "source_type": "stack_overflow", "domain": "Requests/HTTP/APIs", "library": "retry", "title": "How to handle rate limiting with the requests library?", "content": "```python\nfrom tenacity import retry, wait_exponential, stop_after_attempt\n@retry(wait=wait_exponential(min=1, max=60), stop=stop_after_attempt(5))\ndef call_api(url):\n    r = requests.get(url)\n    r.raise_for_status()\n    return r.json()\n```\nCheck `Retry-After` header in 429 responses. For async: use `aiohttp` with tenacity.", "code_signature": "Implement exponential backoff with the tenacity or backoff library.", "tags": "json, timeout, oauth, aiohttp, rest-api", "url": "https://stackoverflow.com/questions/1604451", "author": "user_885136", "date_published": "26-04-2018", "votes_or_stars": 634, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:d5c5e941-aeb1-404d-a13d-3a798300cf28": {"content": "SQLAlchemy: how to avoid DetachedInstanceError?\nDetachedInstanceError occurs when accessing a relationship after the session is closed. Solutions: (1) Use `joinedload()` or `subqueryload()` to eagerly load relationships. (2) Keep session open while accessing objects. (3) Use `expire_on_commit=False` on sessionmaker. (4) Convert to dict/DTO before closing session.\nCode signature: Access lazy-loaded relationships within the session scope, or use eager loading.", "file_path": "https://stackoverflow.com/questions/1247595", "function_name": "Access lazy-loaded relationships within the session scope, or use eager loading.", "type": "stack_overflow", "metadata": {"record_id": "d5c5e941-aeb1-404d-a13d-3a798300cf28", "source_type": "stack_overflow", "domain": "SQLAlchemy/Databases", "library": "sqlalchemy", "title": "SQLAlchemy: how to avoid DetachedInstanceError?", "content": "DetachedInstanceError occurs when accessing a relationship after the session is closed. Solutions: (1) Use `joinedload()` or `subqueryload()` to eagerly load relationships. (2) Keep session open while accessing objects. (3) Use `expire_on_commit=False` on sessionmaker. (4) Convert to dict/DTO before closing session.", "code_signature": "Access lazy-loaded relationships within the session scope, or use eager loading.", "tags": "sqlite, connection-pool, orm, relationship", "url": "https://stackoverflow.com/questions/1247595", "author": "user_107793", "date_published": "04-07-2021", "votes_or_stars": 423, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:0c200797-2510-46b4-9a3b-be2ab8ad0dd5": {"content": "BUG: SettingWithCopyWarning when modifying sliced DataFrame\nProblem: Modifying a slice of a DataFrame raises SettingWithCopyWarning and may not update original.\n\nFix/Discussion: Always use .loc or .copy() when slicing. Use `df.loc[mask, 'col'] = val` for assignment. Create explicit copies: `subset = df[mask].copy()`. In Pandas 2.0+, Copy-on-Write makes this behavior consistent — enable with `pd.options.mode.copy_on_write = True`.\nCode signature: status:closed", "file_path": "https://github.com/pandas/pandas/issues/3264", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "0c200797-2510-46b4-9a3b-be2ab8ad0dd5", "source_type": "github_issue", "domain": "NumPy/Pandas", "library": "pandas", "title": "BUG: SettingWithCopyWarning when modifying sliced DataFrame", "content": "Problem: Modifying a slice of a DataFrame raises SettingWithCopyWarning and may not update original.\n\nFix/Discussion: Always use .loc or .copy() when slicing. Use `df.loc[mask, 'col'] = val` for assignment. Create explicit copies: `subset = df[mask].copy()`. In Pandas 2.0+, Copy-on-Write makes this behavior consistent — enable with `pd.options.mode.copy_on_write = True`.", "code_signature": "status:closed", "tags": "iloc, vectorization, reshape, merge, broadcasting, array", "url": "https://github.com/pandas/pandas/issues/3264", "author": "contributor_6628", "date_published": "19-02-2023", "votes_or_stars": 1, "relevance_label": "low", "difficulty_level": "intermediate"}}, "kb:9e8df60c-12e1-4813-8f09-dfc27a312c67": {"content": "BUG: threading.Timer not cancellable after it fires\nProblem: Calling cancel() on a Timer that has already executed does not raise an error but has no effect.\n\nFix/Discussion: threading.Timer.cancel() only works before the timer fires. Store Timer reference and check `timer.finished.is_set()` to know if it's already run. For repeating timers, use a loop with `threading.Event` for clean cancellation: `stop_event = threading.Event(); stop_event.wait(interval)`.\nCode signature: status:closed", "file_path": "https://github.com/cpython/cpython/issues/4111", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "9e8df60c-12e1-4813-8f09-dfc27a312c67", "source_type": "github_issue", "domain": "Asyncio/Threading", "library": "cpython", "title": "BUG: threading.Timer not cancellable after it fires", "content": "Problem: Calling cancel() on a Timer that has already executed does not raise an error but has no effect.\n\nFix/Discussion: threading.Timer.cancel() only works before the timer fires. Store Timer reference and check `timer.finished.is_set()` to know if it's already run. For repeating timers, use a loop with `threading.Event` for clean cancellation: `stop_event = threading.Event(); stop_event.wait(interval)`.", "code_signature": "status:closed", "tags": "coroutine, lock, deadlock", "url": "https://github.com/cpython/cpython/issues/4111", "author": "contributor_7884", "date_published": "17-10-2018", "votes_or_stars": 22, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:5a10fbc3-dc8e-4cd1-b7a4-952418a65ad4": {"content": "How to apply a function to each row in Pandas without using iterrows?\n`iterrows()` is slow because it creates a Series per row. Prefer `df.apply(func, axis=1)` for row-wise operations, or better yet, vectorize using NumPy: `np.where(condition, a, b)`. For complex logic, `df.assign()` with vectorized expressions is cleanest. Benchmark: vectorized > apply > itertuples > iterrows.\nCode signature: Use df.apply(func, axis=1) or vectorized NumPy operations for best performance.", "file_path": "https://stackoverflow.com/questions/5446912", "function_name": "Use df.apply(func, axis=1) or vectorized NumPy operations for best performance.", "type": "stack_overflow", "metadata": {"record_id": "5a10fbc3-dc8e-4cd1-b7a4-952418a65ad4", "source_type": "stack_overflow", "domain": "NumPy/Pandas", "library": "dataframe", "title": "How to apply a function to each row in Pandas without using iterrows?", "content": "`iterrows()` is slow because it creates a Series per row. Prefer `df.apply(func, axis=1)` for row-wise operations, or better yet, vectorize using NumPy: `np.where(condition, a, b)`. For complex logic, `df.assign()` with vectorized expressions is cleanest. Benchmark: vectorized > apply > itertuples > iterrows.", "code_signature": "Use df.apply(func, axis=1) or vectorized NumPy operations for best performance.", "tags": "broadcasting, pandas, pivot, reshape", "url": "https://stackoverflow.com/questions/5446912", "author": "user_563306", "date_published": "06-03-2024", "votes_or_stars": 2231, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:47a2d21b-cbc4-4ca9-ad2b-4df40eb1c260": {"content": "How to use df.groupby in Pandas\n`Pandas.df.groupby` groups DataFrame rows by column values. Example usage:\n```python\ndf.groupby('category')['value'].mean()\n```\nThis is useful when aggregation, transformation, filtering by group. Performance tip: avoid apply() on large DataFrames; prefer agg().\nCode signature: df.groupby", "file_path": "https://docs.pandas.org/en/stable/df/groupby.html", "function_name": "df.groupby", "type": "documentation", "metadata": {"record_id": "47a2d21b-cbc4-4ca9-ad2b-4df40eb1c260", "source_type": "documentation", "domain": "NumPy/Pandas", "library": "Pandas", "title": "How to use df.groupby in Pandas", "content": "`Pandas.df.groupby` groups DataFrame rows by column values. Example usage:\n```python\ndf.groupby('category')['value'].mean()\n```\nThis is useful when aggregation, transformation, filtering by group. Performance tip: avoid apply() on large DataFrames; prefer agg().", "code_signature": "df.groupby", "tags": "broadcasting, array, dtype, reshape", "url": "https://docs.pandas.org/en/stable/df/groupby.html", "author": "Official Docs", "date_published": "03-05-2022", "votes_or_stars": 790, "relevance_label": "low", "difficulty_level": "beginner"}}, "kb:86c5df99-2952-40a1-9180-f25ed7b6f862": {"content": "How to implement a thread pool in Python?\n```python\nfrom concurrent.futures import ThreadPoolExecutor\nwith ThreadPoolExecutor(max_workers=8) as ex:\n    futures = [ex.submit(task, arg) for arg in args]\n    results = [f.result() for f in futures]\n```\nFor CPU-bound: use ProcessPoolExecutor instead. For async integration: `await loop.run_in_executor(executor, func, *args)`.\nCode signature: Use concurrent.futures.ThreadPoolExecutor for I/O-bound tasks.", "file_path": "https://stackoverflow.com/questions/3646552", "function_name": "Use concurrent.futures.ThreadPoolExecutor for I/O-bound tasks.", "type": "stack_overflow", "metadata": {"record_id": "86c5df99-2952-40a1-9180-f25ed7b6f862", "source_type": "stack_overflow", "domain": "Asyncio/Threading", "library": "coroutine", "title": "How to implement a thread pool in Python?", "content": "```python\nfrom concurrent.futures import ThreadPoolExecutor\nwith ThreadPoolExecutor(max_workers=8) as ex:\n    futures = [ex.submit(task, arg) for arg in args]\n    results = [f.result() for f in futures]\n```\nFor CPU-bound: use ProcessPoolExecutor instead. For async integration: `await loop.run_in_executor(executor, func, *args)`.", "code_signature": "Use concurrent.futures.ThreadPoolExecutor for I/O-bound tasks.", "tags": "gather, event-loop, await, async, coroutine", "url": "https://stackoverflow.com/questions/3646552", "author": "user_469469", "date_published": "28-07-2021", "votes_or_stars": 1510, "relevance_label": "low", "difficulty_level": "beginner"}}, "kb:58ab33cc-9272-4cc1-8f86-de101eabb950": {"content": "How to apply a function to each row in Pandas without using iterrows?\n`iterrows()` is slow because it creates a Series per row. Prefer `df.apply(func, axis=1)` for row-wise operations, or better yet, vectorize using NumPy: `np.where(condition, a, b)`. For complex logic, `df.assign()` with vectorized expressions is cleanest. Benchmark: vectorized > apply > itertuples > iterrows.\nCode signature: Use df.apply(func, axis=1) or vectorized NumPy operations for best performance.", "file_path": "https://stackoverflow.com/questions/5446912", "function_name": "Use df.apply(func, axis=1) or vectorized NumPy operations for best performance.", "type": "stack_overflow", "metadata": {"record_id": "58ab33cc-9272-4cc1-8f86-de101eabb950", "source_type": "stack_overflow", "domain": "NumPy/Pandas", "library": "dataframe", "title": "How to apply a function to each row in Pandas without using iterrows?", "content": "`iterrows()` is slow because it creates a Series per row. Prefer `df.apply(func, axis=1)` for row-wise operations, or better yet, vectorize using NumPy: `np.where(condition, a, b)`. For complex logic, `df.assign()` with vectorized expressions is cleanest. Benchmark: vectorized > apply > itertuples > iterrows.", "code_signature": "Use df.apply(func, axis=1) or vectorized NumPy operations for best performance.", "tags": "array, broadcasting, loc, vectorization", "url": "https://stackoverflow.com/questions/5446912", "author": "user_563306", "date_published": "22-02-2024", "votes_or_stars": 2286, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:0bb3297f-260e-41e3-b3b6-2b02080f179c": {"content": "How to run background tasks in FastAPI?\nFastAPI's `BackgroundTasks`: add `background_tasks: BackgroundTasks` as a parameter, then `background_tasks.add_task(func, *args)`. Runs after response is sent. For heavy workloads (email, ML inference), use Celery with Redis/RabbitMQ broker. For async background work, `asyncio.create_task()` works within async endpoints.\nCode signature: Use BackgroundTasks for lightweight tasks or Celery for heavy/distributed work.", "file_path": "https://stackoverflow.com/questions/8077999", "function_name": "Use BackgroundTasks for lightweight tasks or Celery for heavy/distributed work.", "type": "stack_overflow", "metadata": {"record_id": "0bb3297f-260e-41e3-b3b6-2b02080f179c", "source_type": "stack_overflow", "domain": "FastAPI/Flask/Django", "library": "django", "title": "How to run background tasks in FastAPI?", "content": "FastAPI's `BackgroundTasks`: add `background_tasks: BackgroundTasks` as a parameter, then `background_tasks.add_task(func, *args)`. Runs after response is sent. For heavy workloads (email, ML inference), use Celery with Redis/RabbitMQ broker. For async background work, `asyncio.create_task()` works within async endpoints.", "code_signature": "Use BackgroundTasks for lightweight tasks or Celery for heavy/distributed work.", "tags": "request, response, blueprint, rest, orm, flask", "url": "https://stackoverflow.com/questions/8077999", "author": "user_202401", "date_published": "08-01-2020", "votes_or_stars": 1866, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:22afa720-de0b-4747-b3ea-2c66e4338894": {"content": "FEAT: Django: add support for async ORM queries\nProblem: Django ORM is synchronous; using it in async views requires sync_to_async wrapper.\n\nFix/Discussion: Django 4.1+ added native async ORM support. Use `await Model.objects.filter().afirst()`, `async for obj in qs:`. For older versions: wrap with `sync_to_async`: `get_user = sync_to_async(User.objects.get)\nuser = await get_user(pk=1)`.\nCode signature: status:closed", "file_path": "https://github.com/django/django/issues/8618", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "22afa720-de0b-4747-b3ea-2c66e4338894", "source_type": "github_issue", "domain": "FastAPI/Flask/Django", "library": "django", "title": "FEAT: Django: add support for async ORM queries", "content": "Problem: Django ORM is synchronous; using it in async views requires sync_to_async wrapper.\n\nFix/Discussion: Django 4.1+ added native async ORM support. Use `await Model.objects.filter().afirst()`, `async for obj in qs:`. For older versions: wrap with `sync_to_async`: `get_user = sync_to_async(User.objects.get)\nuser = await get_user(pk=1)`.", "code_signature": "status:closed", "tags": "template, flask, pydantic", "url": "https://github.com/django/django/issues/8618", "author": "contributor_8921", "date_published": "31-12-2023", "votes_or_stars": 14, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:f07585c3-fe6c-4eec-b3ec-862da8d60482": {"content": "How to upload a file with requests?\n```python\nwith open('file.pdf', 'rb') as f:\n    r = requests.post(url, files={'file': ('file.pdf', f, 'application/pdf')})\n```\nFor multiple files: pass a list of tuples. For multipart form data with fields: combine `files` and `data` parameters. Don't set Content-Type manually — requests sets it with boundary.\nCode signature: Use the files parameter with a file-like object or tuple.", "file_path": "https://stackoverflow.com/questions/8896868", "function_name": "Use the files parameter with a file-like object or tuple.", "type": "stack_overflow", "metadata": {"record_id": "f07585c3-fe6c-4eec-b3ec-862da8d60482", "source_type": "stack_overflow", "domain": "Requests/HTTP/APIs", "library": "requests", "title": "How to upload a file with requests?", "content": "```python\nwith open('file.pdf', 'rb') as f:\n    r = requests.post(url, files={'file': ('file.pdf', f, 'application/pdf')})\n```\nFor multiple files: pass a list of tuples. For multipart form data with fields: combine `files` and `data` parameters. Don't set Content-Type manually — requests sets it with boundary.", "code_signature": "Use the files parameter with a file-like object or tuple.", "tags": "websocket, json, requests", "url": "https://stackoverflow.com/questions/8896868", "author": "user_243238", "date_published": "20-05-2021", "votes_or_stars": 1600, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:34e5d3e4-58d2-4db9-8feb-149909e460c4": {"content": "How to stream large HTTP responses with requests?\n```python\nwith requests.get(url, stream=True) as r:\n    r.raise_for_status()\n    with open('file', 'wb') as f:\n        for chunk in r.iter_content(chunk_size=8192):\n            f.write(chunk)\n```\nAlways use as context manager with stream=True to ensure connection is released.\nCode signature: Use stream=True and iterate over response.iter_content() or iter_lines().", "file_path": "https://stackoverflow.com/questions/3592974", "function_name": "Use stream=True and iterate over response.iter_content() or iter_lines().", "type": "stack_overflow", "metadata": {"record_id": "34e5d3e4-58d2-4db9-8feb-149909e460c4", "source_type": "stack_overflow", "domain": "Requests/HTTP/APIs", "library": "json", "title": "How to stream large HTTP responses with requests?", "content": "```python\nwith requests.get(url, stream=True) as r:\n    r.raise_for_status()\n    with open('file', 'wb') as f:\n        for chunk in r.iter_content(chunk_size=8192):\n            f.write(chunk)\n```\nAlways use as context manager with stream=True to ensure connection is released.", "code_signature": "Use stream=True and iterate over response.iter_content() or iter_lines().", "tags": "httpx, rate-limiting, retry", "url": "https://stackoverflow.com/questions/3592974", "author": "user_980733", "date_published": "30-06-2022", "votes_or_stars": 1648, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:95f3bf70-841e-49af-854b-48bcfff64692": {"content": "How to watch a directory for file changes in Python?\n```python\nfrom watchdog.observers import Observer\nfrom watchdog.events import FileSystemEventHandler\nhandler = FileSystemEventHandler()\nobserver = Observer()\nobserver.schedule(handler, path='.', recursive=True)\nobserver.start()\n```\nFor simple polling: `os.stat(file).st_mtime`. Linux-native: `inotify`. macOS-native: `FSEvents`.\nCode signature: Use the watchdog library for cross-platform directory monitoring.", "file_path": "https://stackoverflow.com/questions/5213115", "function_name": "Use the watchdog library for cross-platform directory monitoring.", "type": "stack_overflow", "metadata": {"record_id": "95f3bf70-841e-49af-854b-48bcfff64692", "source_type": "stack_overflow", "domain": "OS/Subprocess/File I/O", "library": "env-variable", "title": "How to watch a directory for file changes in Python?", "content": "```python\nfrom watchdog.observers import Observer\nfrom watchdog.events import FileSystemEventHandler\nhandler = FileSystemEventHandler()\nobserver = Observer()\nobserver.schedule(handler, path='.', recursive=True)\nobserver.start()\n```\nFor simple polling: `os.stat(file).st_mtime`. Linux-native: `inotify`. macOS-native: `FSEvents`.", "code_signature": "Use the watchdog library for cross-platform directory monitoring.", "tags": "shutil, subprocess, pipe, os", "url": "https://stackoverflow.com/questions/5213115", "author": "user_959314", "date_published": "23-10-2024", "votes_or_stars": 2097, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:3f96e9cb-db54-4bb1-a847-322a38b22230": {"content": "How to upload a file with requests?\n```python\nwith open('file.pdf', 'rb') as f:\n    r = requests.post(url, files={'file': ('file.pdf', f, 'application/pdf')})\n```\nFor multiple files: pass a list of tuples. For multipart form data with fields: combine `files` and `data` parameters. Don't set Content-Type manually — requests sets it with boundary.\nCode signature: Use the files parameter with a file-like object or tuple.", "file_path": "https://stackoverflow.com/questions/8896868", "function_name": "Use the files parameter with a file-like object or tuple.", "type": "stack_overflow", "metadata": {"record_id": "3f96e9cb-db54-4bb1-a847-322a38b22230", "source_type": "stack_overflow", "domain": "Requests/HTTP/APIs", "library": "requests", "title": "How to upload a file with requests?", "content": "```python\nwith open('file.pdf', 'rb') as f:\n    r = requests.post(url, files={'file': ('file.pdf', f, 'application/pdf')})\n```\nFor multiple files: pass a list of tuples. For multipart form data with fields: combine `files` and `data` parameters. Don't set Content-Type manually — requests sets it with boundary.", "code_signature": "Use the files parameter with a file-like object or tuple.", "tags": "json, headers, rest-api, requests, httpx", "url": "https://stackoverflow.com/questions/8896868", "author": "user_243238", "date_published": "24-12-2022", "votes_or_stars": 1568, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:3f2fbb75-7595-4a1f-8126-db757d005876": {"content": "os Environment Variables — official reference\nOfficial documentation for os Environment Variables. The method signature is `os.environ(key)`. provides a mapping of the current environment variables. Raises `KeyError` when key is accessed with [] notation and is not set. See also: dotenv, os.getenv, os.putenv.\nCode signature: os.environ(key)", "file_path": "https://docs.os.org/en/stable/os/environ.html", "function_name": "os.environ(key)", "type": "documentation", "metadata": {"record_id": "3f2fbb75-7595-4a1f-8126-db757d005876", "source_type": "documentation", "domain": "OS/Subprocess/File I/O", "library": "os", "title": "os Environment Variables — official reference", "content": "Official documentation for os Environment Variables. The method signature is `os.environ(key)`. provides a mapping of the current environment variables. Raises `KeyError` when key is accessed with [] notation and is not set. See also: dotenv, os.getenv, os.putenv.", "code_signature": "os.environ(key)", "tags": "pathlib, process, pipe, env-variable, tempfile, stderr", "url": "https://docs.os.org/en/stable/os/environ.html", "author": "Official Docs", "date_published": "17-04-2018", "votes_or_stars": 2227, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:aa0c9f8f-6338-4c57-9876-ea45f5efac1d": {"content": "Why is my Pandas merge creating duplicate rows?\nDuplicate rows after merge usually mean the join key is not unique. Diagnose with `df.duplicated(subset=['key']).sum()`. Use `how='left'` carefully when right table has multiple matches. Set `validate='one_to_many'` or `'many_to_one'` to enforce cardinality. Consider deduplicating with `df.drop_duplicates(subset=['key'])` before merging.\nCode signature: Your merge key has duplicates in one or both DataFrames. Use validate='one_to_one' to catch this early.", "file_path": "https://stackoverflow.com/questions/2321324", "function_name": "Your merge key has duplicates in one or both DataFrames. Use validate='one_to_one' to catch this early.", "type": "stack_overflow", "metadata": {"record_id": "aa0c9f8f-6338-4c57-9876-ea45f5efac1d", "source_type": "stack_overflow", "domain": "NumPy/Pandas", "library": "array", "title": "Why is my Pandas merge creating duplicate rows?", "content": "Duplicate rows after merge usually mean the join key is not unique. Diagnose with `df.duplicated(subset=['key']).sum()`. Use `how='left'` carefully when right table has multiple matches. Set `validate='one_to_many'` or `'many_to_one'` to enforce cardinality. Consider deduplicating with `df.drop_duplicates(subset=['key'])` before merging.", "code_signature": "Your merge key has duplicates in one or both DataFrames. Use validate='one_to_one' to catch this early.", "tags": "reshape, merge, nan, dtype, groupby", "url": "https://stackoverflow.com/questions/2321324", "author": "user_99814", "date_published": "28-03-2023", "votes_or_stars": 259, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:a2f228c0-ee6e-4e89-8475-46fba4433bb9": {"content": "PERF: concurrent.futures.ProcessPoolExecutor slow startup on Windows\nProblem: ProcessPoolExecutor takes 3-5 seconds to start on Windows due to process spawning overhead.\n\nFix/Discussion: Windows uses 'spawn' start method (vs 'fork' on Linux). Mitigations: (1) Reuse executor across calls — don't create/destroy in loops. (2) Use 'forkserver' or 'fork' on Linux/macOS. (3) For short tasks, ThreadPoolExecutor may be faster. (4) Use `initializer` param to pre-load modules.\nCode signature: status:closed", "file_path": "https://github.com/cpython/cpython/issues/6647", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "a2f228c0-ee6e-4e89-8475-46fba4433bb9", "source_type": "github_issue", "domain": "Asyncio/Threading", "library": "cpython", "title": "PERF: concurrent.futures.ProcessPoolExecutor slow startup on Windows", "content": "Problem: ProcessPoolExecutor takes 3-5 seconds to start on Windows due to process spawning overhead.\n\nFix/Discussion: Windows uses 'spawn' start method (vs 'fork' on Linux). Mitigations: (1) Reuse executor across calls — don't create/destroy in loops. (2) Use 'forkserver' or 'fork' on Linux/macOS. (3) For short tasks, ThreadPoolExecutor may be faster. (4) Use `initializer` param to pre-load modules.", "code_signature": "status:closed", "tags": "gather, deadlock, semaphore, asyncio, await, coroutine", "url": "https://github.com/cpython/cpython/issues/6647", "author": "contributor_4097", "date_published": "09-11-2021", "votes_or_stars": 284, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:0d62d5b3-41ec-4e07-bb98-68e3e00a99e7": {"content": "NumPy — Vectorization\nThe `np.vectorize` function in NumPy generalizes a scalar function to operate on arrays. It accepts `pyfunc` as a parameter and returns vectorized function. Common use cases include applying Python functions element-wise without explicit loops. Note that np.vectorize is a convenience wrapper; it does not improve performance like true ufuncs.\nCode signature: np.vectorize(pyfunc) -> vectorized function", "file_path": "https://docs.numpy.org/en/stable/np/vectorize.html", "function_name": "np.vectorize(pyfunc) -> vectorized function", "type": "documentation", "metadata": {"record_id": "0d62d5b3-41ec-4e07-bb98-68e3e00a99e7", "source_type": "documentation", "domain": "NumPy/Pandas", "library": "NumPy", "title": "NumPy — Vectorization", "content": "The `np.vectorize` function in NumPy generalizes a scalar function to operate on arrays. It accepts `pyfunc` as a parameter and returns vectorized function. Common use cases include applying Python functions element-wise without explicit loops. Note that np.vectorize is a convenience wrapper; it does not improve performance like true ufuncs.", "code_signature": "np.vectorize(pyfunc) -> vectorized function", "tags": "pandas, broadcasting, pivot, loc, iloc, numpy", "url": "https://docs.numpy.org/en/stable/np/vectorize.html", "author": "Official Docs", "date_published": "03-09-2020", "votes_or_stars": 1319, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:95c5c3f7-e14f-4e38-930a-6d4119db0866": {"content": "Why is my Pandas merge creating duplicate rows?\nDuplicate rows after merge usually mean the join key is not unique. Diagnose with `df.duplicated(subset=['key']).sum()`. Use `how='left'` carefully when right table has multiple matches. Set `validate='one_to_many'` or `'many_to_one'` to enforce cardinality. Consider deduplicating with `df.drop_duplicates(subset=['key'])` before merging.\nCode signature: Your merge key has duplicates in one or both DataFrames. Use validate='one_to_one' to catch this early.", "file_path": "https://stackoverflow.com/questions/2321324", "function_name": "Your merge key has duplicates in one or both DataFrames. Use validate='one_to_one' to catch this early.", "type": "stack_overflow", "metadata": {"record_id": "95c5c3f7-e14f-4e38-930a-6d4119db0866", "source_type": "stack_overflow", "domain": "NumPy/Pandas", "library": "array", "title": "Why is my Pandas merge creating duplicate rows?", "content": "Duplicate rows after merge usually mean the join key is not unique. Diagnose with `df.duplicated(subset=['key']).sum()`. Use `how='left'` carefully when right table has multiple matches. Set `validate='one_to_many'` or `'many_to_one'` to enforce cardinality. Consider deduplicating with `df.drop_duplicates(subset=['key'])` before merging.", "code_signature": "Your merge key has duplicates in one or both DataFrames. Use validate='one_to_one' to catch this early.", "tags": "groupby, pandas, loc, dtype", "url": "https://stackoverflow.com/questions/2321324", "author": "user_99814", "date_published": "29-10-2020", "votes_or_stars": 263, "relevance_label": "low", "difficulty_level": "advanced"}}, "kb:1e0539c2-6d4b-4ad9-9873-5a591b3a4c1b": {"content": "How to read a large file line by line in Python without loading it all into memory?\n```python\nwith open('large.txt', 'r') as f:\n    for line in f:\n        process(line.strip())\n```\nThis reads one line at a time. For binary files: use `f.read(chunk_size)` in a loop. For CSV: use `pd.read_csv(file, chunksize=10000)`. Never use `f.readlines()` on large files.\nCode signature: Iterate over the file object directly — it yields lines lazily.", "file_path": "https://stackoverflow.com/questions/2737893", "function_name": "Iterate over the file object directly — it yields lines lazily.", "type": "stack_overflow", "metadata": {"record_id": "1e0539c2-6d4b-4ad9-9873-5a591b3a4c1b", "source_type": "stack_overflow", "domain": "OS/Subprocess/File I/O", "library": "shutil", "title": "How to read a large file line by line in Python without loading it all into memory?", "content": "```python\nwith open('large.txt', 'r') as f:\n    for line in f:\n        process(line.strip())\n```\nThis reads one line at a time. For binary files: use `f.read(chunk_size)` in a loop. For CSV: use `pd.read_csv(file, chunksize=10000)`. Never use `f.readlines()` on large files.", "code_signature": "Iterate over the file object directly — it yields lines lazily.", "tags": "shutil, process, tempfile", "url": "https://stackoverflow.com/questions/2737893", "author": "user_994539", "date_published": "22-07-2023", "votes_or_stars": 2286, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:2ea631a8-ded5-4c8a-abb8-44c9946b4eb3": {"content": "NumPy broadcasting error: operands could not be broadcast with shapes\nNumPy broadcasts shapes right-to-left. Arrays (3,4) and (4,) are compatible. Arrays (3,4) and (3,) are not — reshape to (3,1). Use `arr.reshape(-1,1)` to add a dimension. Always print `a.shape, b.shape` before operations to debug. `np.expand_dims(arr, axis=1)` is cleaner than reshape for adding dimensions.\nCode signature: Shapes must be compatible: dimensions must match or one of them must be 1.", "file_path": "https://stackoverflow.com/questions/4860684", "function_name": "Shapes must be compatible: dimensions must match or one of them must be 1.", "type": "stack_overflow", "metadata": {"record_id": "2ea631a8-ded5-4c8a-abb8-44c9946b4eb3", "source_type": "stack_overflow", "domain": "NumPy/Pandas", "library": "numpy", "title": "NumPy broadcasting error: operands could not be broadcast with shapes", "content": "NumPy broadcasts shapes right-to-left. Arrays (3,4) and (4,) are compatible. Arrays (3,4) and (3,) are not — reshape to (3,1). Use `arr.reshape(-1,1)` to add a dimension. Always print `a.shape, b.shape` before operations to debug. `np.expand_dims(arr, axis=1)` is cleaner than reshape for adding dimensions.", "code_signature": "Shapes must be compatible: dimensions must match or one of them must be 1.", "tags": "pivot, pandas, array, nan, reshape", "url": "https://stackoverflow.com/questions/4860684", "author": "user_627024", "date_published": "21-06-2020", "votes_or_stars": 1394, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:2ca5e7fc-715e-44cb-a230-f11b46389d20": {"content": "How to implement database connection pooling with SQLAlchemy?\n`create_engine(url, pool_size=10, max_overflow=20, pool_timeout=30, pool_recycle=1800)`. Use `pool_pre_ping=True` to detect stale connections. For async: use `AsyncEngine` with `create_async_engine()`. Monitor with `engine.pool.status()`. NullPool disables pooling for scripts/serverless environments.\nCode signature: create_engine() has built-in pooling. Configure pool_size and max_overflow.", "file_path": "https://stackoverflow.com/questions/7358113", "function_name": "create_engine() has built-in pooling. Configure pool_size and max_overflow.", "type": "stack_overflow", "metadata": {"record_id": "2ca5e7fc-715e-44cb-a230-f11b46389d20", "source_type": "stack_overflow", "domain": "SQLAlchemy/Databases", "library": "session", "title": "How to implement database connection pooling with SQLAlchemy?", "content": "`create_engine(url, pool_size=10, max_overflow=20, pool_timeout=30, pool_recycle=1800)`. Use `pool_pre_ping=True` to detect stale connections. For async: use `AsyncEngine` with `create_async_engine()`. Monitor with `engine.pool.status()`. NullPool disables pooling for scripts/serverless environments.", "code_signature": "create_engine() has built-in pooling. Configure pool_size and max_overflow.", "tags": "orm, migration, foreign-key", "url": "https://stackoverflow.com/questions/7358113", "author": "user_12260", "date_published": "31-03-2024", "votes_or_stars": 255, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:3632dc6a-86ae-4d9f-97d8-8a05d2e12c0d": {"content": "Python: how to safely write to a file atomically?\n```python\nimport tempfile, os\nwith tempfile.NamedTemporaryFile('w', delete=False, dir='.') as tmp:\n    tmp.write(data)\n    tmp.flush()\n    os.fsync(tmp.fileno())\nos.replace(tmp.name, target_path)\n```\n`os.replace()` is atomic on POSIX. This prevents partial writes from corrupting files.\nCode signature: Write to a temp file then rename — rename is atomic on POSIX systems.", "file_path": "https://stackoverflow.com/questions/5394880", "function_name": "Write to a temp file then rename — rename is atomic on POSIX systems.", "type": "stack_overflow", "metadata": {"record_id": "3632dc6a-86ae-4d9f-97d8-8a05d2e12c0d", "source_type": "stack_overflow", "domain": "OS/Subprocess/File I/O", "library": "stdin", "title": "Python: how to safely write to a file atomically?", "content": "```python\nimport tempfile, os\nwith tempfile.NamedTemporaryFile('w', delete=False, dir='.') as tmp:\n    tmp.write(data)\n    tmp.flush()\n    os.fsync(tmp.fileno())\nos.replace(tmp.name, target_path)\n```\n`os.replace()` is atomic on POSIX. This prevents partial writes from corrupting files.", "code_signature": "Write to a temp file then rename — rename is atomic on POSIX systems.", "tags": "stderr, env-variable, pipe", "url": "https://stackoverflow.com/questions/5394880", "author": "user_179430", "date_published": "27-12-2021", "votes_or_stars": 534, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:93f300fe-5395-4eb2-b34c-29723fb08900": {"content": "How to implement a thread pool in Python?\n```python\nfrom concurrent.futures import ThreadPoolExecutor\nwith ThreadPoolExecutor(max_workers=8) as ex:\n    futures = [ex.submit(task, arg) for arg in args]\n    results = [f.result() for f in futures]\n```\nFor CPU-bound: use ProcessPoolExecutor instead. For async integration: `await loop.run_in_executor(executor, func, *args)`.\nCode signature: Use concurrent.futures.ThreadPoolExecutor for I/O-bound tasks.", "file_path": "https://stackoverflow.com/questions/3646552", "function_name": "Use concurrent.futures.ThreadPoolExecutor for I/O-bound tasks.", "type": "stack_overflow", "metadata": {"record_id": "93f300fe-5395-4eb2-b34c-29723fb08900", "source_type": "stack_overflow", "domain": "Asyncio/Threading", "library": "coroutine", "title": "How to implement a thread pool in Python?", "content": "```python\nfrom concurrent.futures import ThreadPoolExecutor\nwith ThreadPoolExecutor(max_workers=8) as ex:\n    futures = [ex.submit(task, arg) for arg in args]\n    results = [f.result() for f in futures]\n```\nFor CPU-bound: use ProcessPoolExecutor instead. For async integration: `await loop.run_in_executor(executor, func, *args)`.", "code_signature": "Use concurrent.futures.ThreadPoolExecutor for I/O-bound tasks.", "tags": "threading, coroutine, asyncio, deadlock, async, gather", "url": "https://stackoverflow.com/questions/3646552", "author": "user_469469", "date_published": "25-03-2018", "votes_or_stars": 1517, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:1f05c4b3-59f5-40c5-b7de-edcd82e8ce5a": {"content": "BUG: subprocess.Popen hangs when stdout and stderr buffers fill\nProblem: subprocess.Popen hangs indefinitely when child process produces large output.\n\nFix/Discussion: This is a classic deadlock: parent waits for child, child waits for parent to read buffer. Fix: use `subprocess.run(..., capture_output=True)` which handles this correctly. Or: `stdout, stderr = proc.communicate()`. Never use `proc.stdout.read()` and `proc.wait()` separately.\nCode signature: status:closed", "file_path": "https://github.com/cpython/cpython/issues/8586", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "1f05c4b3-59f5-40c5-b7de-edcd82e8ce5a", "source_type": "github_issue", "domain": "OS/Subprocess/File I/O", "library": "cpython", "title": "BUG: subprocess.Popen hangs when stdout and stderr buffers fill", "content": "Problem: subprocess.Popen hangs indefinitely when child process produces large output.\n\nFix/Discussion: This is a classic deadlock: parent waits for child, child waits for parent to read buffer. Fix: use `subprocess.run(..., capture_output=True)` which handles this correctly. Or: `stdout, stderr = proc.communicate()`. Never use `proc.stdout.read()` and `proc.wait()` separately.", "code_signature": "status:closed", "tags": "subprocess, tempfile, stdout", "url": "https://github.com/cpython/cpython/issues/8586", "author": "contributor_7711", "date_published": "31-12-2020", "votes_or_stars": 418, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:67df1921-0f03-4d78-bb59-21f31fb001ea": {"content": "How to implement database connection pooling with SQLAlchemy?\n`create_engine(url, pool_size=10, max_overflow=20, pool_timeout=30, pool_recycle=1800)`. Use `pool_pre_ping=True` to detect stale connections. For async: use `AsyncEngine` with `create_async_engine()`. Monitor with `engine.pool.status()`. NullPool disables pooling for scripts/serverless environments.\nCode signature: create_engine() has built-in pooling. Configure pool_size and max_overflow.", "file_path": "https://stackoverflow.com/questions/7358113", "function_name": "create_engine() has built-in pooling. Configure pool_size and max_overflow.", "type": "stack_overflow", "metadata": {"record_id": "67df1921-0f03-4d78-bb59-21f31fb001ea", "source_type": "stack_overflow", "domain": "SQLAlchemy/Databases", "library": "session", "title": "How to implement database connection pooling with SQLAlchemy?", "content": "`create_engine(url, pool_size=10, max_overflow=20, pool_timeout=30, pool_recycle=1800)`. Use `pool_pre_ping=True` to detect stale connections. For async: use `AsyncEngine` with `create_async_engine()`. Monitor with `engine.pool.status()`. NullPool disables pooling for scripts/serverless environments.", "code_signature": "create_engine() has built-in pooling. Configure pool_size and max_overflow.", "tags": "postgresql, connection-pool, migration, sqlalchemy", "url": "https://stackoverflow.com/questions/7358113", "author": "user_12260", "date_published": "15-10-2019", "votes_or_stars": 231, "relevance_label": "low", "difficulty_level": "advanced"}}, "kb:d05fe93a-9dcd-478b-902b-f7db46b2d7f8": {"content": "asyncio — Concurrent Coroutines\nThe `asyncio.gather` function in asyncio runs multiple coroutines concurrently and collects results. It accepts `*coros, return_exceptions=False` as a parameter and returns list of results. Common use cases include parallel I/O operations such as multiple API calls. Note that set return_exceptions=True to prevent one failure from cancelling all tasks.\nCode signature: asyncio.gather(*coros, return_exceptions=False) -> list of results", "file_path": "https://docs.asyncio.org/en/stable/asyncio/gather.html", "function_name": "asyncio.gather(*coros, return_exceptions=False) -> list of results", "type": "documentation", "metadata": {"record_id": "d05fe93a-9dcd-478b-902b-f7db46b2d7f8", "source_type": "documentation", "domain": "Asyncio/Threading", "library": "asyncio", "title": "asyncio — Concurrent Coroutines", "content": "The `asyncio.gather` function in asyncio runs multiple coroutines concurrently and collects results. It accepts `*coros, return_exceptions=False` as a parameter and returns list of results. Common use cases include parallel I/O operations such as multiple API calls. Note that set return_exceptions=True to prevent one failure from cancelling all tasks.", "code_signature": "asyncio.gather(*coros, return_exceptions=False) -> list of results", "tags": "async, deadlock, task, race-condition, asyncio", "url": "https://docs.asyncio.org/en/stable/asyncio/gather.html", "author": "Official Docs", "date_published": "16-11-2024", "votes_or_stars": 496, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:0f27b876-55de-4447-a40f-bdf0dec253c9": {"content": "How to stream large HTTP responses with requests?\n```python\nwith requests.get(url, stream=True) as r:\n    r.raise_for_status()\n    with open('file', 'wb') as f:\n        for chunk in r.iter_content(chunk_size=8192):\n            f.write(chunk)\n```\nAlways use as context manager with stream=True to ensure connection is released.\nCode signature: Use stream=True and iterate over response.iter_content() or iter_lines().", "file_path": "https://stackoverflow.com/questions/3592974", "function_name": "Use stream=True and iterate over response.iter_content() or iter_lines().", "type": "stack_overflow", "metadata": {"record_id": "0f27b876-55de-4447-a40f-bdf0dec253c9", "source_type": "stack_overflow", "domain": "Requests/HTTP/APIs", "library": "json", "title": "How to stream large HTTP responses with requests?", "content": "```python\nwith requests.get(url, stream=True) as r:\n    r.raise_for_status()\n    with open('file', 'wb') as f:\n        for chunk in r.iter_content(chunk_size=8192):\n            f.write(chunk)\n```\nAlways use as context manager with stream=True to ensure connection is released.", "code_signature": "Use stream=True and iterate over response.iter_content() or iter_lines().", "tags": "requests, aiohttp, json, timeout", "url": "https://stackoverflow.com/questions/3592974", "author": "user_980733", "date_published": "22-07-2022", "votes_or_stars": 1667, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:47f0ff8c-f61c-4c3a-bd32-620d2c8566fe": {"content": "BUG: pd.read_csv() silently truncates large integers\nProblem: Integer columns with values > 2^53 are silently corrupted when read from CSV.\n\nFix/Discussion: Floating point cannot represent integers > 2^53 exactly. Fix: `pd.read_csv(f, dtype={'col': str})` then convert: `df['col'] = df['col'].astype('Int64')`. Use `dtype_backend='numpy_nullable'` in Pandas 2.0+ for better integer handling.\nCode signature: status:closed", "file_path": "https://github.com/pandas/pandas/issues/4449", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "47f0ff8c-f61c-4c3a-bd32-620d2c8566fe", "source_type": "github_issue", "domain": "NumPy/Pandas", "library": "pandas", "title": "BUG: pd.read_csv() silently truncates large integers", "content": "Problem: Integer columns with values > 2^53 are silently corrupted when read from CSV.\n\nFix/Discussion: Floating point cannot represent integers > 2^53 exactly. Fix: `pd.read_csv(f, dtype={'col': str})` then convert: `df['col'] = df['col'].astype('Int64')`. Use `dtype_backend='numpy_nullable'` in Pandas 2.0+ for better integer handling.", "code_signature": "status:closed", "tags": "broadcasting, dataframe, nan", "url": "https://github.com/pandas/pandas/issues/4449", "author": "contributor_726", "date_published": "19-08-2022", "votes_or_stars": 526, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:030f185b-c8d4-4f42-a30f-ad9597649467": {"content": "SQLAlchemy — ORM Session Query\nThe `session.query` function in SQLAlchemy constructs a SELECT query against mapped entities. It accepts `*entities` as a parameter and returns Query object. Common use cases include retrieving ORM-mapped objects from the database. Note that prefer session.execute(select(...)) in SQLAlchemy 2.0+.\nCode signature: session.query(*entities) -> Query object", "file_path": "https://docs.sqlalchemy.org/en/stable/session/query.html", "function_name": "session.query(*entities) -> Query object", "type": "documentation", "metadata": {"record_id": "030f185b-c8d4-4f42-a30f-ad9597649467", "source_type": "documentation", "domain": "SQLAlchemy/Databases", "library": "SQLAlchemy", "title": "SQLAlchemy — ORM Session Query", "content": "The `session.query` function in SQLAlchemy constructs a SELECT query against mapped entities. It accepts `*entities` as a parameter and returns Query object. Common use cases include retrieving ORM-mapped objects from the database. Note that prefer session.execute(select(...)) in SQLAlchemy 2.0+.", "code_signature": "session.query(*entities) -> Query object", "tags": "migration, session, connection-pool, relationship, postgresql, alembic", "url": "https://docs.sqlalchemy.org/en/stable/session/query.html", "author": "Official Docs", "date_published": "16-02-2022", "votes_or_stars": 3035, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:282382b6-8282-48aa-ba92-cf6ba821ac15": {"content": "BUG: FastAPI dependency_overrides not working in pytest\nProblem: Dependency overrides set in conftest are not applied during test execution.\n\nFix/Discussion: Set overrides on the app object before TestClient is created. Use `app.dependency_overrides[dep] = mock_dep` in a pytest fixture with function scope. Clear overrides after test: `app.dependency_overrides = {}`. Ensure TestClient is created after overrides are set.\nCode signature: status:closed", "file_path": "https://github.com/fastapi/fastapi/issues/8479", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "282382b6-8282-48aa-ba92-cf6ba821ac15", "source_type": "github_issue", "domain": "FastAPI/Flask/Django", "library": "fastapi", "title": "BUG: FastAPI dependency_overrides not working in pytest", "content": "Problem: Dependency overrides set in conftest are not applied during test execution.\n\nFix/Discussion: Set overrides on the app object before TestClient is created. Use `app.dependency_overrides[dep] = mock_dep` in a pytest fixture with function scope. Clear overrides after test: `app.dependency_overrides = {}`. Ensure TestClient is created after overrides are set.", "code_signature": "status:closed", "tags": "orm, template, dependency-injection", "url": "https://github.com/fastapi/fastapi/issues/8479", "author": "contributor_1994", "date_published": "26-01-2024", "votes_or_stars": 287, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:238490e3-fbff-43e5-b7e5-5e2bb544f877": {"content": "requests — HTTP Session Management\nThe `requests.Session` function in requests creates a persistent HTTP session with connection pooling. It accepts `` as a parameter and returns Session object. Common use cases include making multiple requests to the same host efficiently. Note that always close or use Session as context manager to release connections.\nCode signature: requests.Session() -> Session object", "file_path": "https://docs.requests.org/en/stable/requests/Session.html", "function_name": "requests.Session() -> Session object", "type": "documentation", "metadata": {"record_id": "238490e3-fbff-43e5-b7e5-5e2bb544f877", "source_type": "documentation", "domain": "Requests/HTTP/APIs", "library": "requests", "title": "requests — HTTP Session Management", "content": "The `requests.Session` function in requests creates a persistent HTTP session with connection pooling. It accepts `` as a parameter and returns Session object. Common use cases include making multiple requests to the same host efficiently. Note that always close or use Session as context manager to release connections.", "code_signature": "requests.Session() -> Session object", "tags": "aiohttp, requests, authentication, retry, oauth", "url": "https://docs.requests.org/en/stable/requests/Session.html", "author": "Official Docs", "date_published": "01-03-2019", "votes_or_stars": 95, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:9a380bfe-e6ea-497d-8113-5bddf1cbe09e": {"content": "How to run asyncio code from synchronous Python?\n`asyncio.run(main())` creates an event loop, runs the coroutine, closes the loop. Never call asyncio.run() from inside a running event loop (use await instead). In Jupyter notebooks (which have a running loop), use `await coroutine()` directly or `nest_asyncio.apply()`. For mixing sync/async, use `loop.run_until_complete()`.\nCode signature: Use asyncio.run() in Python 3.7+ to execute a coroutine from sync code.", "file_path": "https://stackoverflow.com/questions/2140194", "function_name": "Use asyncio.run() in Python 3.7+ to execute a coroutine from sync code.", "type": "stack_overflow", "metadata": {"record_id": "9a380bfe-e6ea-497d-8113-5bddf1cbe09e", "source_type": "stack_overflow", "domain": "Asyncio/Threading", "library": "gather", "title": "How to run asyncio code from synchronous Python?", "content": "`asyncio.run(main())` creates an event loop, runs the coroutine, closes the loop. Never call asyncio.run() from inside a running event loop (use await instead). In Jupyter notebooks (which have a running loop), use `await coroutine()` directly or `nest_asyncio.apply()`. For mixing sync/async, use `loop.run_until_complete()`.", "code_signature": "Use asyncio.run() in Python 3.7+ to execute a coroutine from sync code.", "tags": "executor, event-loop, task, async, coroutine", "url": "https://stackoverflow.com/questions/2140194", "author": "user_718011", "date_published": "02-04-2022", "votes_or_stars": 270, "relevance_label": "low", "difficulty_level": "intermediate"}}, "kb:9300cd84-36c1-49fa-ba7a-264474932d10": {"content": "How to implement database connection pooling with SQLAlchemy?\n`create_engine(url, pool_size=10, max_overflow=20, pool_timeout=30, pool_recycle=1800)`. Use `pool_pre_ping=True` to detect stale connections. For async: use `AsyncEngine` with `create_async_engine()`. Monitor with `engine.pool.status()`. NullPool disables pooling for scripts/serverless environments.\nCode signature: create_engine() has built-in pooling. Configure pool_size and max_overflow.", "file_path": "https://stackoverflow.com/questions/7358113", "function_name": "create_engine() has built-in pooling. Configure pool_size and max_overflow.", "type": "stack_overflow", "metadata": {"record_id": "9300cd84-36c1-49fa-ba7a-264474932d10", "source_type": "stack_overflow", "domain": "SQLAlchemy/Databases", "library": "session", "title": "How to implement database connection pooling with SQLAlchemy?", "content": "`create_engine(url, pool_size=10, max_overflow=20, pool_timeout=30, pool_recycle=1800)`. Use `pool_pre_ping=True` to detect stale connections. For async: use `AsyncEngine` with `create_async_engine()`. Monitor with `engine.pool.status()`. NullPool disables pooling for scripts/serverless environments.", "code_signature": "create_engine() has built-in pooling. Configure pool_size and max_overflow.", "tags": "relationship, transaction, sqlalchemy, orm, query", "url": "https://stackoverflow.com/questions/7358113", "author": "user_12260", "date_published": "26-11-2018", "votes_or_stars": 258, "relevance_label": "low", "difficulty_level": "beginner"}}, "kb:721a2b66-ae12-4d7a-b8f1-ba12bd3775b0": {"content": "How to implement a thread pool in Python?\n```python\nfrom concurrent.futures import ThreadPoolExecutor\nwith ThreadPoolExecutor(max_workers=8) as ex:\n    futures = [ex.submit(task, arg) for arg in args]\n    results = [f.result() for f in futures]\n```\nFor CPU-bound: use ProcessPoolExecutor instead. For async integration: `await loop.run_in_executor(executor, func, *args)`.\nCode signature: Use concurrent.futures.ThreadPoolExecutor for I/O-bound tasks.", "file_path": "https://stackoverflow.com/questions/3646552", "function_name": "Use concurrent.futures.ThreadPoolExecutor for I/O-bound tasks.", "type": "stack_overflow", "metadata": {"record_id": "721a2b66-ae12-4d7a-b8f1-ba12bd3775b0", "source_type": "stack_overflow", "domain": "Asyncio/Threading", "library": "coroutine", "title": "How to implement a thread pool in Python?", "content": "```python\nfrom concurrent.futures import ThreadPoolExecutor\nwith ThreadPoolExecutor(max_workers=8) as ex:\n    futures = [ex.submit(task, arg) for arg in args]\n    results = [f.result() for f in futures]\n```\nFor CPU-bound: use ProcessPoolExecutor instead. For async integration: `await loop.run_in_executor(executor, func, *args)`.", "code_signature": "Use concurrent.futures.ThreadPoolExecutor for I/O-bound tasks.", "tags": "task, deadlock, async, coroutine, event-loop, threading", "url": "https://stackoverflow.com/questions/3646552", "author": "user_469469", "date_published": "23-03-2024", "votes_or_stars": 1502, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:13861343-8461-470a-a740-2b6d225d80c2": {"content": "How to use @app.route in Flask\n`Flask.@app.route` maps a URL rule to a view function. Example usage:\n```python\n@app.route('/users/<int:uid>')\ndef get_user(uid):\n    return jsonify(user)\n```\nThis is useful when defining HTTP endpoints in a Flask application. Performance tip: use Blueprints to organize large applications.\nCode signature: @app.route", "file_path": "https://docs.flask.org/en/stable/@app/route.html", "function_name": "@app.route", "type": "documentation", "metadata": {"record_id": "13861343-8461-470a-a740-2b6d225d80c2", "source_type": "documentation", "domain": "FastAPI/Flask/Django", "library": "Flask", "title": "How to use @app.route in Flask", "content": "`Flask.@app.route` maps a URL rule to a view function. Example usage:\n```python\n@app.route('/users/<int:uid>')\ndef get_user(uid):\n    return jsonify(user)\n```\nThis is useful when defining HTTP endpoints in a Flask application. Performance tip: use Blueprints to organize large applications.", "code_signature": "@app.route", "tags": "fastapi, response, flask", "url": "https://docs.flask.org/en/stable/@app/route.html", "author": "Official Docs", "date_published": "10-10-2024", "votes_or_stars": 2998, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:71e1c987-5b27-4ff2-886a-088965e8a6d8": {"content": "How to run asyncio code from synchronous Python?\n`asyncio.run(main())` creates an event loop, runs the coroutine, closes the loop. Never call asyncio.run() from inside a running event loop (use await instead). In Jupyter notebooks (which have a running loop), use `await coroutine()` directly or `nest_asyncio.apply()`. For mixing sync/async, use `loop.run_until_complete()`.\nCode signature: Use asyncio.run() in Python 3.7+ to execute a coroutine from sync code.", "file_path": "https://stackoverflow.com/questions/2140194", "function_name": "Use asyncio.run() in Python 3.7+ to execute a coroutine from sync code.", "type": "stack_overflow", "metadata": {"record_id": "71e1c987-5b27-4ff2-886a-088965e8a6d8", "source_type": "stack_overflow", "domain": "Asyncio/Threading", "library": "gather", "title": "How to run asyncio code from synchronous Python?", "content": "`asyncio.run(main())` creates an event loop, runs the coroutine, closes the loop. Never call asyncio.run() from inside a running event loop (use await instead). In Jupyter notebooks (which have a running loop), use `await coroutine()` directly or `nest_asyncio.apply()`. For mixing sync/async, use `loop.run_until_complete()`.", "code_signature": "Use asyncio.run() in Python 3.7+ to execute a coroutine from sync code.", "tags": "threading, async, event-loop", "url": "https://stackoverflow.com/questions/2140194", "author": "user_718011", "date_published": "06-04-2019", "votes_or_stars": 314, "relevance_label": "low", "difficulty_level": "advanced"}}, "kb:caca9b78-a1e2-466a-b770-8b8f16b7d024": {"content": "BUG: asyncio task leaks when exception is not handled\nProblem: Unhandled exceptions in fire-and-forget asyncio tasks are silently ignored, causing task leaks.\n\nFix/Discussion: Always await tasks or add exception handlers. Use `task.add_done_callback()` to log exceptions. In Python 3.11+, use TaskGroup for structured concurrency — exceptions propagate automatically. Set `asyncio.get_event_loop().set_exception_handler()` for global unhandled task exception logging.\nCode signature: status:closed", "file_path": "https://github.com/cpython/cpython/issues/5591", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "caca9b78-a1e2-466a-b770-8b8f16b7d024", "source_type": "github_issue", "domain": "Asyncio/Threading", "library": "cpython", "title": "BUG: asyncio task leaks when exception is not handled", "content": "Problem: Unhandled exceptions in fire-and-forget asyncio tasks are silently ignored, causing task leaks.\n\nFix/Discussion: Always await tasks or add exception handlers. Use `task.add_done_callback()` to log exceptions. In Python 3.11+, use TaskGroup for structured concurrency — exceptions propagate automatically. Set `asyncio.get_event_loop().set_exception_handler()` for global unhandled task exception logging.", "code_signature": "status:closed", "tags": "async, asyncio, race-condition", "url": "https://github.com/cpython/cpython/issues/5591", "author": "contributor_1630", "date_published": "05-04-2021", "votes_or_stars": 337, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:fac33366-b59c-4eea-bf05-24ce1a308bcb": {"content": "How to limit concurrency with asyncio?\n```python\nsem = asyncio.Semaphore(10)\nasync def limited(url):\n    async with sem:\n        return await fetch(url)\nresults = await asyncio.gather(*[limited(u) for u in urls])\n```\nThis prevents overwhelming APIs or databases with too many simultaneous connections.\nCode signature: Use asyncio.Semaphore to cap the number of concurrent coroutines.", "file_path": "https://stackoverflow.com/questions/2375453", "function_name": "Use asyncio.Semaphore to cap the number of concurrent coroutines.", "type": "stack_overflow", "metadata": {"record_id": "fac33366-b59c-4eea-bf05-24ce1a308bcb", "source_type": "stack_overflow", "domain": "Asyncio/Threading", "library": "asyncio", "title": "How to limit concurrency with asyncio?", "content": "```python\nsem = asyncio.Semaphore(10)\nasync def limited(url):\n    async with sem:\n        return await fetch(url)\nresults = await asyncio.gather(*[limited(u) for u in urls])\n```\nThis prevents overwhelming APIs or databases with too many simultaneous connections.", "code_signature": "Use asyncio.Semaphore to cap the number of concurrent coroutines.", "tags": "semaphore, gather, asyncio, threading, race-condition", "url": "https://stackoverflow.com/questions/2375453", "author": "user_449589", "date_published": "30-01-2022", "votes_or_stars": 2471, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:e847c87e-e352-4e5f-b383-62bc7fe45416": {"content": "How to implement database connection pooling with SQLAlchemy?\n`create_engine(url, pool_size=10, max_overflow=20, pool_timeout=30, pool_recycle=1800)`. Use `pool_pre_ping=True` to detect stale connections. For async: use `AsyncEngine` with `create_async_engine()`. Monitor with `engine.pool.status()`. NullPool disables pooling for scripts/serverless environments.\nCode signature: create_engine() has built-in pooling. Configure pool_size and max_overflow.", "file_path": "https://stackoverflow.com/questions/7358113", "function_name": "create_engine() has built-in pooling. Configure pool_size and max_overflow.", "type": "stack_overflow", "metadata": {"record_id": "e847c87e-e352-4e5f-b383-62bc7fe45416", "source_type": "stack_overflow", "domain": "SQLAlchemy/Databases", "library": "session", "title": "How to implement database connection pooling with SQLAlchemy?", "content": "`create_engine(url, pool_size=10, max_overflow=20, pool_timeout=30, pool_recycle=1800)`. Use `pool_pre_ping=True` to detect stale connections. For async: use `AsyncEngine` with `create_async_engine()`. Monitor with `engine.pool.status()`. NullPool disables pooling for scripts/serverless environments.", "code_signature": "create_engine() has built-in pooling. Configure pool_size and max_overflow.", "tags": "postgresql, foreign-key, sqlite, transaction, connection-pool, relationship", "url": "https://stackoverflow.com/questions/7358113", "author": "user_12260", "date_published": "15-06-2023", "votes_or_stars": 252, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:36a3439b-7f31-49b0-8e9c-6346be00e4ec": {"content": "How to apply a function to each row in Pandas without using iterrows?\n`iterrows()` is slow because it creates a Series per row. Prefer `df.apply(func, axis=1)` for row-wise operations, or better yet, vectorize using NumPy: `np.where(condition, a, b)`. For complex logic, `df.assign()` with vectorized expressions is cleanest. Benchmark: vectorized > apply > itertuples > iterrows.\nCode signature: Use df.apply(func, axis=1) or vectorized NumPy operations for best performance.", "file_path": "https://stackoverflow.com/questions/5446912", "function_name": "Use df.apply(func, axis=1) or vectorized NumPy operations for best performance.", "type": "stack_overflow", "metadata": {"record_id": "36a3439b-7f31-49b0-8e9c-6346be00e4ec", "source_type": "stack_overflow", "domain": "NumPy/Pandas", "library": "dataframe", "title": "How to apply a function to each row in Pandas without using iterrows?", "content": "`iterrows()` is slow because it creates a Series per row. Prefer `df.apply(func, axis=1)` for row-wise operations, or better yet, vectorize using NumPy: `np.where(condition, a, b)`. For complex logic, `df.assign()` with vectorized expressions is cleanest. Benchmark: vectorized > apply > itertuples > iterrows.", "code_signature": "Use df.apply(func, axis=1) or vectorized NumPy operations for best performance.", "tags": "vectorization, pivot, dtype, iloc, nan", "url": "https://stackoverflow.com/questions/5446912", "author": "user_563306", "date_published": "15-05-2024", "votes_or_stars": 2269, "relevance_label": "low", "difficulty_level": "beginner"}}, "kb:f4a396da-886f-4f96-969d-5920cbeed74c": {"content": "How to use Path.glob in pathlib\n`pathlib.Path.glob` yields all paths matching the given glob pattern. Example usage:\n```python\npy_files = list(Path('src').glob('**/*.py'))\n```\nThis is useful when finding files recursively, batch file processing. Performance tip: iterate the generator lazily rather than converting to list for large directories.\nCode signature: Path.glob", "file_path": "https://docs.pathlib.org/en/stable/Path/glob.html", "function_name": "Path.glob", "type": "documentation", "metadata": {"record_id": "f4a396da-886f-4f96-969d-5920cbeed74c", "source_type": "documentation", "domain": "OS/Subprocess/File I/O", "library": "pathlib", "title": "How to use Path.glob in pathlib", "content": "`pathlib.Path.glob` yields all paths matching the given glob pattern. Example usage:\n```python\npy_files = list(Path('src').glob('**/*.py'))\n```\nThis is useful when finding files recursively, batch file processing. Performance tip: iterate the generator lazily rather than converting to list for large directories.", "code_signature": "Path.glob", "tags": "pipe, pathlib, shutil, os", "url": "https://docs.pathlib.org/en/stable/Path/glob.html", "author": "Official Docs", "date_published": "02-12-2022", "votes_or_stars": 3200, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:cc552f4b-8382-4dfb-83c2-37f28c38a042": {"content": "BUG: requests hangs indefinitely on slow server responses\nProblem: requests.get() hangs forever when server accepts connection but never sends response.\n\nFix/Discussion: Always set both connect and read timeouts: `requests.get(url, timeout=(3.05, 27))`. Tuple: (connect_timeout, read_timeout). Or single value for both. Mount a Session with default timeouts using a custom HTTPAdapter subclass. Never use `timeout=None` in production code.\nCode signature: status:closed", "file_path": "https://github.com/requests/requests/issues/4019", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "cc552f4b-8382-4dfb-83c2-37f28c38a042", "source_type": "github_issue", "domain": "Requests/HTTP/APIs", "library": "requests", "title": "BUG: requests hangs indefinitely on slow server responses", "content": "Problem: requests.get() hangs forever when server accepts connection but never sends response.\n\nFix/Discussion: Always set both connect and read timeouts: `requests.get(url, timeout=(3.05, 27))`. Tuple: (connect_timeout, read_timeout). Or single value for both. Mount a Session with default timeouts using a custom HTTPAdapter subclass. Never use `timeout=None` in production code.", "code_signature": "status:closed", "tags": "oauth, json, websocket, rate-limiting, authentication, retry", "url": "https://github.com/requests/requests/issues/4019", "author": "contributor_4599", "date_published": "17-07-2023", "votes_or_stars": 252, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:ba3d1ef3-b32b-4eb0-be13-6a1c5418f99f": {"content": "How to implement JWT authentication in FastAPI?\n```python\nfrom fastapi.security import OAuth2PasswordBearer\nfrom jose import jwt\noauth2_scheme = OAuth2PasswordBearer(tokenUrl='token')\n```\nCreate access tokens with `jwt.encode(payload, SECRET_KEY, algorithm='HS256')`. Verify with `jwt.decode(token, SECRET_KEY, algorithms=['HS256'])`. Store SECRET_KEY in environment variables, never in code.\nCode signature: Use python-jose for JWT and passlib for password hashing with OAuth2PasswordBearer.", "file_path": "https://stackoverflow.com/questions/7754864", "function_name": "Use python-jose for JWT and passlib for password hashing with OAuth2PasswordBearer.", "type": "stack_overflow", "metadata": {"record_id": "ba3d1ef3-b32b-4eb0-be13-6a1c5418f99f", "source_type": "stack_overflow", "domain": "FastAPI/Flask/Django", "library": "fastapi", "title": "How to implement JWT authentication in FastAPI?", "content": "```python\nfrom fastapi.security import OAuth2PasswordBearer\nfrom jose import jwt\noauth2_scheme = OAuth2PasswordBearer(tokenUrl='token')\n```\nCreate access tokens with `jwt.encode(payload, SECRET_KEY, algorithm='HS256')`. Verify with `jwt.decode(token, SECRET_KEY, algorithms=['HS256'])`. Store SECRET_KEY in environment variables, never in code.", "code_signature": "Use python-jose for JWT and passlib for password hashing with OAuth2PasswordBearer.", "tags": "fastapi, response, request, routing, pydantic, blueprint", "url": "https://stackoverflow.com/questions/7754864", "author": "user_773587", "date_published": "13-08-2019", "votes_or_stars": 422, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:e294c205-bb43-4802-84fb-ad1ffecdf401": {"content": "How to run asyncio code from synchronous Python?\n`asyncio.run(main())` creates an event loop, runs the coroutine, closes the loop. Never call asyncio.run() from inside a running event loop (use await instead). In Jupyter notebooks (which have a running loop), use `await coroutine()` directly or `nest_asyncio.apply()`. For mixing sync/async, use `loop.run_until_complete()`.\nCode signature: Use asyncio.run() in Python 3.7+ to execute a coroutine from sync code.", "file_path": "https://stackoverflow.com/questions/2140194", "function_name": "Use asyncio.run() in Python 3.7+ to execute a coroutine from sync code.", "type": "stack_overflow", "metadata": {"record_id": "e294c205-bb43-4802-84fb-ad1ffecdf401", "source_type": "stack_overflow", "domain": "Asyncio/Threading", "library": "gather", "title": "How to run asyncio code from synchronous Python?", "content": "`asyncio.run(main())` creates an event loop, runs the coroutine, closes the loop. Never call asyncio.run() from inside a running event loop (use await instead). In Jupyter notebooks (which have a running loop), use `await coroutine()` directly or `nest_asyncio.apply()`. For mixing sync/async, use `loop.run_until_complete()`.", "code_signature": "Use asyncio.run() in Python 3.7+ to execute a coroutine from sync code.", "tags": "async, gather, threading, race-condition, semaphore, lock", "url": "https://stackoverflow.com/questions/2140194", "author": "user_718011", "date_published": "29-09-2020", "votes_or_stars": 261, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:006a29f0-a78b-4837-ba6c-332c536d8ed9": {"content": "How do I reshape a wide DataFrame to long format?\n`pd.melt(df, id_vars=['id'], value_vars=['col1','col2'])` unpivots columns to rows. `df.stack()` moves column index to row index. For the reverse, use `df.pivot()` or `df.unstack()`. Always reset_index() after stack/unstack to get a clean DataFrame.\nCode signature: Use pd.melt() or df.stack() depending on your structure.", "file_path": "https://stackoverflow.com/questions/6229731", "function_name": "Use pd.melt() or df.stack() depending on your structure.", "type": "stack_overflow", "metadata": {"record_id": "006a29f0-a78b-4837-ba6c-332c536d8ed9", "source_type": "stack_overflow", "domain": "NumPy/Pandas", "library": "nan", "title": "How do I reshape a wide DataFrame to long format?", "content": "`pd.melt(df, id_vars=['id'], value_vars=['col1','col2'])` unpivots columns to rows. `df.stack()` moves column index to row index. For the reverse, use `df.pivot()` or `df.unstack()`. Always reset_index() after stack/unstack to get a clean DataFrame.", "code_signature": "Use pd.melt() or df.stack() depending on your structure.", "tags": "numpy, vectorization, loc, dtype, merge, pandas", "url": "https://stackoverflow.com/questions/6229731", "author": "user_428373", "date_published": "24-09-2018", "votes_or_stars": 853, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:2fe70f01-90ee-4bcb-8edd-e91e708e9120": {"content": "asyncio — Concurrent Coroutines\nThe `asyncio.gather` function in asyncio runs multiple coroutines concurrently and collects results. It accepts `*coros, return_exceptions=False` as a parameter and returns list of results. Common use cases include parallel I/O operations such as multiple API calls. Note that set return_exceptions=True to prevent one failure from cancelling all tasks.\nCode signature: asyncio.gather(*coros, return_exceptions=False) -> list of results", "file_path": "https://docs.asyncio.org/en/stable/asyncio/gather.html", "function_name": "asyncio.gather(*coros, return_exceptions=False) -> list of results", "type": "documentation", "metadata": {"record_id": "2fe70f01-90ee-4bcb-8edd-e91e708e9120", "source_type": "documentation", "domain": "Asyncio/Threading", "library": "asyncio", "title": "asyncio — Concurrent Coroutines", "content": "The `asyncio.gather` function in asyncio runs multiple coroutines concurrently and collects results. It accepts `*coros, return_exceptions=False` as a parameter and returns list of results. Common use cases include parallel I/O operations such as multiple API calls. Note that set return_exceptions=True to prevent one failure from cancelling all tasks.", "code_signature": "asyncio.gather(*coros, return_exceptions=False) -> list of results", "tags": "event-loop, coroutine, race-condition, async", "url": "https://docs.asyncio.org/en/stable/asyncio/gather.html", "author": "Official Docs", "date_published": "21-07-2019", "votes_or_stars": 494, "relevance_label": "low", "difficulty_level": "intermediate"}}, "kb:a9a19674-7148-44d1-8fed-124c13240e47": {"content": "BUG: requests hangs indefinitely on slow server responses\nProblem: requests.get() hangs forever when server accepts connection but never sends response.\n\nFix/Discussion: Always set both connect and read timeouts: `requests.get(url, timeout=(3.05, 27))`. Tuple: (connect_timeout, read_timeout). Or single value for both. Mount a Session with default timeouts using a custom HTTPAdapter subclass. Never use `timeout=None` in production code.\nCode signature: status:closed", "file_path": "https://github.com/requests/requests/issues/4019", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "a9a19674-7148-44d1-8fed-124c13240e47", "source_type": "github_issue", "domain": "Requests/HTTP/APIs", "library": "requests", "title": "BUG: requests hangs indefinitely on slow server responses", "content": "Problem: requests.get() hangs forever when server accepts connection but never sends response.\n\nFix/Discussion: Always set both connect and read timeouts: `requests.get(url, timeout=(3.05, 27))`. Tuple: (connect_timeout, read_timeout). Or single value for both. Mount a Session with default timeouts using a custom HTTPAdapter subclass. Never use `timeout=None` in production code.", "code_signature": "status:closed", "tags": "json, oauth, requests, timeout, retry, rest-api", "url": "https://github.com/requests/requests/issues/4019", "author": "contributor_4599", "date_published": "20-06-2024", "votes_or_stars": 239, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:357e640d-6453-4df9-9105-1a9fbae9374a": {"content": "How to use df.groupby in Pandas\n`Pandas.df.groupby` groups DataFrame rows by column values. Example usage:\n```python\ndf.groupby('category')['value'].mean()\n```\nThis is useful when aggregation, transformation, filtering by group. Performance tip: avoid apply() on large DataFrames; prefer agg().\nCode signature: df.groupby", "file_path": "https://docs.pandas.org/en/stable/df/groupby.html", "function_name": "df.groupby", "type": "documentation", "metadata": {"record_id": "357e640d-6453-4df9-9105-1a9fbae9374a", "source_type": "documentation", "domain": "NumPy/Pandas", "library": "Pandas", "title": "How to use df.groupby in Pandas", "content": "`Pandas.df.groupby` groups DataFrame rows by column values. Example usage:\n```python\ndf.groupby('category')['value'].mean()\n```\nThis is useful when aggregation, transformation, filtering by group. Performance tip: avoid apply() on large DataFrames; prefer agg().", "code_signature": "df.groupby", "tags": "pandas, broadcasting, iloc, dataframe", "url": "https://docs.pandas.org/en/stable/df/groupby.html", "author": "Official Docs", "date_published": "20-08-2023", "votes_or_stars": 839, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:b87dce59-e3b5-4790-b74a-7262291c9142": {"content": "Why is my Pandas merge creating duplicate rows?\nDuplicate rows after merge usually mean the join key is not unique. Diagnose with `df.duplicated(subset=['key']).sum()`. Use `how='left'` carefully when right table has multiple matches. Set `validate='one_to_many'` or `'many_to_one'` to enforce cardinality. Consider deduplicating with `df.drop_duplicates(subset=['key'])` before merging.\nCode signature: Your merge key has duplicates in one or both DataFrames. Use validate='one_to_one' to catch this early.", "file_path": "https://stackoverflow.com/questions/2321324", "function_name": "Your merge key has duplicates in one or both DataFrames. Use validate='one_to_one' to catch this early.", "type": "stack_overflow", "metadata": {"record_id": "b87dce59-e3b5-4790-b74a-7262291c9142", "source_type": "stack_overflow", "domain": "NumPy/Pandas", "library": "array", "title": "Why is my Pandas merge creating duplicate rows?", "content": "Duplicate rows after merge usually mean the join key is not unique. Diagnose with `df.duplicated(subset=['key']).sum()`. Use `how='left'` carefully when right table has multiple matches. Set `validate='one_to_many'` or `'many_to_one'` to enforce cardinality. Consider deduplicating with `df.drop_duplicates(subset=['key'])` before merging.", "code_signature": "Your merge key has duplicates in one or both DataFrames. Use validate='one_to_one' to catch this early.", "tags": "loc, array, vectorization, dtype", "url": "https://stackoverflow.com/questions/2321324", "author": "user_99814", "date_published": "18-07-2023", "votes_or_stars": 241, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:acf1dc2f-5eb9-4ffd-ad18-f7e6ddfe6f3d": {"content": "How to use df.groupby in Pandas\n`Pandas.df.groupby` groups DataFrame rows by column values. Example usage:\n```python\ndf.groupby('category')['value'].mean()\n```\nThis is useful when aggregation, transformation, filtering by group. Performance tip: avoid apply() on large DataFrames; prefer agg().\nCode signature: df.groupby", "file_path": "https://docs.pandas.org/en/stable/df/groupby.html", "function_name": "df.groupby", "type": "documentation", "metadata": {"record_id": "acf1dc2f-5eb9-4ffd-ad18-f7e6ddfe6f3d", "source_type": "documentation", "domain": "NumPy/Pandas", "library": "Pandas", "title": "How to use df.groupby in Pandas", "content": "`Pandas.df.groupby` groups DataFrame rows by column values. Example usage:\n```python\ndf.groupby('category')['value'].mean()\n```\nThis is useful when aggregation, transformation, filtering by group. Performance tip: avoid apply() on large DataFrames; prefer agg().", "code_signature": "df.groupby", "tags": "dataframe, iloc, numpy, pandas", "url": "https://docs.pandas.org/en/stable/df/groupby.html", "author": "Official Docs", "date_published": "09-01-2022", "votes_or_stars": 843, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:7e4303bf-d441-4cd1-8bc9-20085ea86ae1": {"content": "How to limit concurrency with asyncio?\n```python\nsem = asyncio.Semaphore(10)\nasync def limited(url):\n    async with sem:\n        return await fetch(url)\nresults = await asyncio.gather(*[limited(u) for u in urls])\n```\nThis prevents overwhelming APIs or databases with too many simultaneous connections.\nCode signature: Use asyncio.Semaphore to cap the number of concurrent coroutines.", "file_path": "https://stackoverflow.com/questions/2375453", "function_name": "Use asyncio.Semaphore to cap the number of concurrent coroutines.", "type": "stack_overflow", "metadata": {"record_id": "7e4303bf-d441-4cd1-8bc9-20085ea86ae1", "source_type": "stack_overflow", "domain": "Asyncio/Threading", "library": "asyncio", "title": "How to limit concurrency with asyncio?", "content": "```python\nsem = asyncio.Semaphore(10)\nasync def limited(url):\n    async with sem:\n        return await fetch(url)\nresults = await asyncio.gather(*[limited(u) for u in urls])\n```\nThis prevents overwhelming APIs or databases with too many simultaneous connections.", "code_signature": "Use asyncio.Semaphore to cap the number of concurrent coroutines.", "tags": "deadlock, semaphore, threading", "url": "https://stackoverflow.com/questions/2375453", "author": "user_449589", "date_published": "07-04-2023", "votes_or_stars": 2469, "relevance_label": "low", "difficulty_level": "intermediate"}}, "kb:03ccea44-56f6-482a-a510-731165bb24bf": {"content": "BUG: Flask session not persisting between requests in tests\nProblem: Session data set in one request is not available in the next request during testing.\n\nFix/Discussion: Use Flask test client's session_transaction() context manager: `with client.session_transaction() as sess:\n    sess['user_id'] = 1`. Ensure SECRET_KEY is set. For testing: `app.config['TESTING'] = True; app.config['SECRET_KEY'] = 'test'`.\nCode signature: status:closed", "file_path": "https://github.com/flask/flask/issues/2141", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "03ccea44-56f6-482a-a510-731165bb24bf", "source_type": "github_issue", "domain": "FastAPI/Flask/Django", "library": "flask", "title": "BUG: Flask session not persisting between requests in tests", "content": "Problem: Session data set in one request is not available in the next request during testing.\n\nFix/Discussion: Use Flask test client's session_transaction() context manager: `with client.session_transaction() as sess:\n    sess['user_id'] = 1`. Ensure SECRET_KEY is set. For testing: `app.config['TESTING'] = True; app.config['SECRET_KEY'] = 'test'`.", "code_signature": "status:closed", "tags": "flask, request, dependency-injection, orm", "url": "https://github.com/flask/flask/issues/2141", "author": "contributor_5020", "date_published": "04-06-2024", "votes_or_stars": 178, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:980aff58-5d00-48e2-94ff-71505d61e331": {"content": "How to run background tasks in FastAPI?\nFastAPI's `BackgroundTasks`: add `background_tasks: BackgroundTasks` as a parameter, then `background_tasks.add_task(func, *args)`. Runs after response is sent. For heavy workloads (email, ML inference), use Celery with Redis/RabbitMQ broker. For async background work, `asyncio.create_task()` works within async endpoints.\nCode signature: Use BackgroundTasks for lightweight tasks or Celery for heavy/distributed work.", "file_path": "https://stackoverflow.com/questions/8077999", "function_name": "Use BackgroundTasks for lightweight tasks or Celery for heavy/distributed work.", "type": "stack_overflow", "metadata": {"record_id": "980aff58-5d00-48e2-94ff-71505d61e331", "source_type": "stack_overflow", "domain": "FastAPI/Flask/Django", "library": "django", "title": "How to run background tasks in FastAPI?", "content": "FastAPI's `BackgroundTasks`: add `background_tasks: BackgroundTasks` as a parameter, then `background_tasks.add_task(func, *args)`. Runs after response is sent. For heavy workloads (email, ML inference), use Celery with Redis/RabbitMQ broker. For async background work, `asyncio.create_task()` works within async endpoints.", "code_signature": "Use BackgroundTasks for lightweight tasks or Celery for heavy/distributed work.", "tags": "response, routing, middleware, flask, pydantic, rest", "url": "https://stackoverflow.com/questions/8077999", "author": "user_202401", "date_published": "15-04-2024", "votes_or_stars": 1844, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:bb2562b4-7c63-40fb-82a4-7742438f8fb1": {"content": "subprocess.run vs Popen: which should I use?\n`subprocess.run()` is a high-level wrapper that waits for completion — use for most cases. `subprocess.Popen()` gives full control: stream stdout/stderr in real-time, write to stdin, run processes concurrently. For capturing output: `subprocess.run(..., capture_output=True, text=True)`. Always use `shell=False` and pass args as a list to prevent shell injection.\nCode signature: Use subprocess.run() for simple cases; Popen for streaming output or interactive processes.", "file_path": "https://stackoverflow.com/questions/5727085", "function_name": "Use subprocess.run() for simple cases; Popen for streaming output or interactive processes.", "type": "stack_overflow", "metadata": {"record_id": "bb2562b4-7c63-40fb-82a4-7742438f8fb1", "source_type": "stack_overflow", "domain": "OS/Subprocess/File I/O", "library": "shutil", "title": "subprocess.run vs Popen: which should I use?", "content": "`subprocess.run()` is a high-level wrapper that waits for completion — use for most cases. `subprocess.Popen()` gives full control: stream stdout/stderr in real-time, write to stdin, run processes concurrently. For capturing output: `subprocess.run(..., capture_output=True, text=True)`. Always use `shell=False` and pass args as a list to prevent shell injection.", "code_signature": "Use subprocess.run() for simple cases; Popen for streaming output or interactive processes.", "tags": "stdout, stderr, process, pathlib, glob", "url": "https://stackoverflow.com/questions/5727085", "author": "user_644210", "date_published": "27-01-2024", "votes_or_stars": 659, "relevance_label": "low", "difficulty_level": "beginner"}}, "kb:efa2f3fd-b837-406e-8f60-72e5fccf4fae": {"content": "SQLAlchemy bulk insert: what's the fastest way?\n`session.bulk_insert_mappings(Model, list_of_dicts)` skips ORM overhead. Even faster: `session.execute(insert(Model), data)`. Avoid `session.add()` in loops. For PostgreSQL, use `insert().on_conflict_do_nothing()` for upserts. Batch in chunks of 1000-5000 rows for optimal performance.\nCode signature: Use session.bulk_insert_mappings() or execute(insert(), list_of_dicts) for maximum speed.", "file_path": "https://stackoverflow.com/questions/5977931", "function_name": "Use session.bulk_insert_mappings() or execute(insert(), list_of_dicts) for maximum speed.", "type": "stack_overflow", "metadata": {"record_id": "efa2f3fd-b837-406e-8f60-72e5fccf4fae", "source_type": "stack_overflow", "domain": "SQLAlchemy/Databases", "library": "query", "title": "SQLAlchemy bulk insert: what's the fastest way?", "content": "`session.bulk_insert_mappings(Model, list_of_dicts)` skips ORM overhead. Even faster: `session.execute(insert(Model), data)`. Avoid `session.add()` in loops. For PostgreSQL, use `insert().on_conflict_do_nothing()` for upserts. Batch in chunks of 1000-5000 rows for optimal performance.", "code_signature": "Use session.bulk_insert_mappings() or execute(insert(), list_of_dicts) for maximum speed.", "tags": "postgresql, sqlalchemy, foreign-key, transaction, migration, relationship", "url": "https://stackoverflow.com/questions/5977931", "author": "user_238275", "date_published": "09-01-2024", "votes_or_stars": 671, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:04443eff-569d-4e4f-86ba-5d348ed9f4fb": {"content": "How to use create_engine in SQLAlchemy\n`SQLAlchemy.create_engine` creates a database engine representing a connection pool. Example usage:\n```python\nengine = create_engine('postgresql://user:pass@host/db', echo=True)\n```\nThis is useful when establishing the central database connection for an application. Performance tip: set pool_size and max_overflow based on expected concurrency.\nCode signature: create_engine", "file_path": "https://docs.sqlalchemy.org/en/stable/create_engine.html", "function_name": "create_engine", "type": "documentation", "metadata": {"record_id": "04443eff-569d-4e4f-86ba-5d348ed9f4fb", "source_type": "documentation", "domain": "SQLAlchemy/Databases", "library": "SQLAlchemy", "title": "How to use create_engine in SQLAlchemy", "content": "`SQLAlchemy.create_engine` creates a database engine representing a connection pool. Example usage:\n```python\nengine = create_engine('postgresql://user:pass@host/db', echo=True)\n```\nThis is useful when establishing the central database connection for an application. Performance tip: set pool_size and max_overflow based on expected concurrency.", "code_signature": "create_engine", "tags": "foreign-key, query, sqlite, sqlalchemy, orm, transaction", "url": "https://docs.sqlalchemy.org/en/stable/create_engine.html", "author": "Official Docs", "date_published": "15-05-2019", "votes_or_stars": 2069, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:04c924de-fba3-496f-81a5-1d8943ffb894": {"content": "NumPy broadcasting error: operands could not be broadcast with shapes\nNumPy broadcasts shapes right-to-left. Arrays (3,4) and (4,) are compatible. Arrays (3,4) and (3,) are not — reshape to (3,1). Use `arr.reshape(-1,1)` to add a dimension. Always print `a.shape, b.shape` before operations to debug. `np.expand_dims(arr, axis=1)` is cleaner than reshape for adding dimensions.\nCode signature: Shapes must be compatible: dimensions must match or one of them must be 1.", "file_path": "https://stackoverflow.com/questions/4860684", "function_name": "Shapes must be compatible: dimensions must match or one of them must be 1.", "type": "stack_overflow", "metadata": {"record_id": "04c924de-fba3-496f-81a5-1d8943ffb894", "source_type": "stack_overflow", "domain": "NumPy/Pandas", "library": "numpy", "title": "NumPy broadcasting error: operands could not be broadcast with shapes", "content": "NumPy broadcasts shapes right-to-left. Arrays (3,4) and (4,) are compatible. Arrays (3,4) and (3,) are not — reshape to (3,1). Use `arr.reshape(-1,1)` to add a dimension. Always print `a.shape, b.shape` before operations to debug. `np.expand_dims(arr, axis=1)` is cleaner than reshape for adding dimensions.", "code_signature": "Shapes must be compatible: dimensions must match or one of them must be 1.", "tags": "iloc, nan, array", "url": "https://stackoverflow.com/questions/4860684", "author": "user_627024", "date_published": "20-11-2018", "votes_or_stars": 1423, "relevance_label": "low", "difficulty_level": "intermediate"}}, "kb:b3d286b0-9a5e-40ab-9522-876c12fb0bf3": {"content": "Django N+1 query problem: how to fix?\nN+1 occurs when accessing related objects in a loop. Diagnose with `django-debug-toolbar` or `connection.queries`. Fix ForeignKey: `User.objects.select_related('profile').all()`. Fix M2M: `Post.objects.prefetch_related('tags').all()`. Use `only()` to fetch specific fields and reduce data transfer.\nCode signature: Use select_related() for ForeignKey and prefetch_related() for ManyToMany relationships.", "file_path": "https://stackoverflow.com/questions/8106470", "function_name": "Use select_related() for ForeignKey and prefetch_related() for ManyToMany relationships.", "type": "stack_overflow", "metadata": {"record_id": "b3d286b0-9a5e-40ab-9522-876c12fb0bf3", "source_type": "stack_overflow", "domain": "FastAPI/Flask/Django", "library": "request", "title": "Django N+1 query problem: how to fix?", "content": "N+1 occurs when accessing related objects in a loop. Diagnose with `django-debug-toolbar` or `connection.queries`. Fix ForeignKey: `User.objects.select_related('profile').all()`. Fix M2M: `Post.objects.prefetch_related('tags').all()`. Use `only()` to fetch specific fields and reduce data transfer.", "code_signature": "Use select_related() for ForeignKey and prefetch_related() for ManyToMany relationships.", "tags": "pydantic, response, orm, request, dependency-injection, routing", "url": "https://stackoverflow.com/questions/8106470", "author": "user_441071", "date_published": "28-09-2018", "votes_or_stars": 1766, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:2dd0e1ed-8f8a-49f5-9044-a272b00a62f3": {"content": "NumPy — Vectorization\nThe `np.vectorize` function in NumPy generalizes a scalar function to operate on arrays. It accepts `pyfunc` as a parameter and returns vectorized function. Common use cases include applying Python functions element-wise without explicit loops. Note that np.vectorize is a convenience wrapper; it does not improve performance like true ufuncs.\nCode signature: np.vectorize(pyfunc) -> vectorized function", "file_path": "https://docs.numpy.org/en/stable/np/vectorize.html", "function_name": "np.vectorize(pyfunc) -> vectorized function", "type": "documentation", "metadata": {"record_id": "2dd0e1ed-8f8a-49f5-9044-a272b00a62f3", "source_type": "documentation", "domain": "NumPy/Pandas", "library": "NumPy", "title": "NumPy — Vectorization", "content": "The `np.vectorize` function in NumPy generalizes a scalar function to operate on arrays. It accepts `pyfunc` as a parameter and returns vectorized function. Common use cases include applying Python functions element-wise without explicit loops. Note that np.vectorize is a convenience wrapper; it does not improve performance like true ufuncs.", "code_signature": "np.vectorize(pyfunc) -> vectorized function", "tags": "nan, iloc, broadcasting, reshape, vectorization, loc", "url": "https://docs.numpy.org/en/stable/np/vectorize.html", "author": "Official Docs", "date_published": "19-02-2019", "votes_or_stars": 1314, "relevance_label": "low", "difficulty_level": "beginner"}}, "kb:22ab2a0c-1727-417c-b081-c406db97e024": {"content": "How to use httpx.AsyncClient in httpx\n`httpx.httpx.AsyncClient` provides an async HTTP client for non-blocking requests. Example usage:\n```python\nasync with httpx.AsyncClient() as client:\n    r = await client.get(url)\n    data = r.json()\n```\nThis is useful when making concurrent HTTP requests in async applications. Performance tip: set timeout=httpx.Timeout(10.0) to prevent hanging requests.\nCode signature: httpx.AsyncClient", "file_path": "https://docs.httpx.org/en/stable/httpx/AsyncClient.html", "function_name": "httpx.AsyncClient", "type": "documentation", "metadata": {"record_id": "22ab2a0c-1727-417c-b081-c406db97e024", "source_type": "documentation", "domain": "Requests/HTTP/APIs", "library": "httpx", "title": "How to use httpx.AsyncClient in httpx", "content": "`httpx.httpx.AsyncClient` provides an async HTTP client for non-blocking requests. Example usage:\n```python\nasync with httpx.AsyncClient() as client:\n    r = await client.get(url)\n    data = r.json()\n```\nThis is useful when making concurrent HTTP requests in async applications. Performance tip: set timeout=httpx.Timeout(10.0) to prevent hanging requests.", "code_signature": "httpx.AsyncClient", "tags": "headers, websocket, authentication", "url": "https://docs.httpx.org/en/stable/httpx/AsyncClient.html", "author": "Official Docs", "date_published": "26-10-2019", "votes_or_stars": 4227, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:7666e514-00b8-4d04-be5c-1c817d021812": {"content": "How to implement JWT authentication in FastAPI?\n```python\nfrom fastapi.security import OAuth2PasswordBearer\nfrom jose import jwt\noauth2_scheme = OAuth2PasswordBearer(tokenUrl='token')\n```\nCreate access tokens with `jwt.encode(payload, SECRET_KEY, algorithm='HS256')`. Verify with `jwt.decode(token, SECRET_KEY, algorithms=['HS256'])`. Store SECRET_KEY in environment variables, never in code.\nCode signature: Use python-jose for JWT and passlib for password hashing with OAuth2PasswordBearer.", "file_path": "https://stackoverflow.com/questions/7754864", "function_name": "Use python-jose for JWT and passlib for password hashing with OAuth2PasswordBearer.", "type": "stack_overflow", "metadata": {"record_id": "7666e514-00b8-4d04-be5c-1c817d021812", "source_type": "stack_overflow", "domain": "FastAPI/Flask/Django", "library": "fastapi", "title": "How to implement JWT authentication in FastAPI?", "content": "```python\nfrom fastapi.security import OAuth2PasswordBearer\nfrom jose import jwt\noauth2_scheme = OAuth2PasswordBearer(tokenUrl='token')\n```\nCreate access tokens with `jwt.encode(payload, SECRET_KEY, algorithm='HS256')`. Verify with `jwt.decode(token, SECRET_KEY, algorithms=['HS256'])`. Store SECRET_KEY in environment variables, never in code.", "code_signature": "Use python-jose for JWT and passlib for password hashing with OAuth2PasswordBearer.", "tags": "fastapi, middleware, request, template, rest", "url": "https://stackoverflow.com/questions/7754864", "author": "user_773587", "date_published": "29-08-2019", "votes_or_stars": 428, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:7d8e5fc9-1ca1-4671-ae9e-81009fec0bb8": {"content": "Python: how to safely write to a file atomically?\n```python\nimport tempfile, os\nwith tempfile.NamedTemporaryFile('w', delete=False, dir='.') as tmp:\n    tmp.write(data)\n    tmp.flush()\n    os.fsync(tmp.fileno())\nos.replace(tmp.name, target_path)\n```\n`os.replace()` is atomic on POSIX. This prevents partial writes from corrupting files.\nCode signature: Write to a temp file then rename — rename is atomic on POSIX systems.", "file_path": "https://stackoverflow.com/questions/5394880", "function_name": "Write to a temp file then rename — rename is atomic on POSIX systems.", "type": "stack_overflow", "metadata": {"record_id": "7d8e5fc9-1ca1-4671-ae9e-81009fec0bb8", "source_type": "stack_overflow", "domain": "OS/Subprocess/File I/O", "library": "stdin", "title": "Python: how to safely write to a file atomically?", "content": "```python\nimport tempfile, os\nwith tempfile.NamedTemporaryFile('w', delete=False, dir='.') as tmp:\n    tmp.write(data)\n    tmp.flush()\n    os.fsync(tmp.fileno())\nos.replace(tmp.name, target_path)\n```\n`os.replace()` is atomic on POSIX. This prevents partial writes from corrupting files.", "code_signature": "Write to a temp file then rename — rename is atomic on POSIX systems.", "tags": "process, os, pipe, pathlib", "url": "https://stackoverflow.com/questions/5394880", "author": "user_179430", "date_published": "10-04-2022", "votes_or_stars": 534, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:05fc4fdf-b2aa-4725-b387-02de7d80ae95": {"content": "How to implement JWT authentication in FastAPI?\n```python\nfrom fastapi.security import OAuth2PasswordBearer\nfrom jose import jwt\noauth2_scheme = OAuth2PasswordBearer(tokenUrl='token')\n```\nCreate access tokens with `jwt.encode(payload, SECRET_KEY, algorithm='HS256')`. Verify with `jwt.decode(token, SECRET_KEY, algorithms=['HS256'])`. Store SECRET_KEY in environment variables, never in code.\nCode signature: Use python-jose for JWT and passlib for password hashing with OAuth2PasswordBearer.", "file_path": "https://stackoverflow.com/questions/7754864", "function_name": "Use python-jose for JWT and passlib for password hashing with OAuth2PasswordBearer.", "type": "stack_overflow", "metadata": {"record_id": "05fc4fdf-b2aa-4725-b387-02de7d80ae95", "source_type": "stack_overflow", "domain": "FastAPI/Flask/Django", "library": "fastapi", "title": "How to implement JWT authentication in FastAPI?", "content": "```python\nfrom fastapi.security import OAuth2PasswordBearer\nfrom jose import jwt\noauth2_scheme = OAuth2PasswordBearer(tokenUrl='token')\n```\nCreate access tokens with `jwt.encode(payload, SECRET_KEY, algorithm='HS256')`. Verify with `jwt.decode(token, SECRET_KEY, algorithms=['HS256'])`. Store SECRET_KEY in environment variables, never in code.", "code_signature": "Use python-jose for JWT and passlib for password hashing with OAuth2PasswordBearer.", "tags": "rest, django, request", "url": "https://stackoverflow.com/questions/7754864", "author": "user_773587", "date_published": "07-03-2020", "votes_or_stars": 408, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:d0fb3a65-bbed-4b3a-b8fc-e53e5b1193fd": {"content": "How to watch a directory for file changes in Python?\n```python\nfrom watchdog.observers import Observer\nfrom watchdog.events import FileSystemEventHandler\nhandler = FileSystemEventHandler()\nobserver = Observer()\nobserver.schedule(handler, path='.', recursive=True)\nobserver.start()\n```\nFor simple polling: `os.stat(file).st_mtime`. Linux-native: `inotify`. macOS-native: `FSEvents`.\nCode signature: Use the watchdog library for cross-platform directory monitoring.", "file_path": "https://stackoverflow.com/questions/5213115", "function_name": "Use the watchdog library for cross-platform directory monitoring.", "type": "stack_overflow", "metadata": {"record_id": "d0fb3a65-bbed-4b3a-b8fc-e53e5b1193fd", "source_type": "stack_overflow", "domain": "OS/Subprocess/File I/O", "library": "env-variable", "title": "How to watch a directory for file changes in Python?", "content": "```python\nfrom watchdog.observers import Observer\nfrom watchdog.events import FileSystemEventHandler\nhandler = FileSystemEventHandler()\nobserver = Observer()\nobserver.schedule(handler, path='.', recursive=True)\nobserver.start()\n```\nFor simple polling: `os.stat(file).st_mtime`. Linux-native: `inotify`. macOS-native: `FSEvents`.", "code_signature": "Use the watchdog library for cross-platform directory monitoring.", "tags": "subprocess, os, tempfile, stdin, glob, process", "url": "https://stackoverflow.com/questions/5213115", "author": "user_959314", "date_published": "15-05-2021", "votes_or_stars": 2052, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:9d8ef251-283c-453b-b1ec-272d2fd64e2f": {"content": "asyncio RuntimeError: This event loop is already running\nThis error is common in Jupyter and FastAPI. Solutions: (1) In async context: just use `await coroutine()`. (2) In Jupyter: `import nest_asyncio; nest_asyncio.apply()`. (3) Run in thread: `asyncio.get_event_loop().run_in_executor(None, sync_func)`. (4) Use `asyncio.ensure_future()` to schedule from within async code.\nCode signature: You're calling asyncio.run() inside an already running event loop. Use await instead.", "file_path": "https://stackoverflow.com/questions/2229110", "function_name": "You're calling asyncio.run() inside an already running event loop. Use await instead.", "type": "stack_overflow", "metadata": {"record_id": "9d8ef251-283c-453b-b1ec-272d2fd64e2f", "source_type": "stack_overflow", "domain": "Asyncio/Threading", "library": "async", "title": "asyncio RuntimeError: This event loop is already running", "content": "This error is common in Jupyter and FastAPI. Solutions: (1) In async context: just use `await coroutine()`. (2) In Jupyter: `import nest_asyncio; nest_asyncio.apply()`. (3) Run in thread: `asyncio.get_event_loop().run_in_executor(None, sync_func)`. (4) Use `asyncio.ensure_future()` to schedule from within async code.", "code_signature": "You're calling asyncio.run() inside an already running event loop. Use await instead.", "tags": "lock, async, deadlock", "url": "https://stackoverflow.com/questions/2229110", "author": "user_573750", "date_published": "08-11-2021", "votes_or_stars": 2337, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:0a2290da-452b-4659-99e2-4a4aaf59dcbc": {"content": "How to use httpx.AsyncClient in httpx\n`httpx.httpx.AsyncClient` provides an async HTTP client for non-blocking requests. Example usage:\n```python\nasync with httpx.AsyncClient() as client:\n    r = await client.get(url)\n    data = r.json()\n```\nThis is useful when making concurrent HTTP requests in async applications. Performance tip: set timeout=httpx.Timeout(10.0) to prevent hanging requests.\nCode signature: httpx.AsyncClient", "file_path": "https://docs.httpx.org/en/stable/httpx/AsyncClient.html", "function_name": "httpx.AsyncClient", "type": "documentation", "metadata": {"record_id": "0a2290da-452b-4659-99e2-4a4aaf59dcbc", "source_type": "documentation", "domain": "Requests/HTTP/APIs", "library": "httpx", "title": "How to use httpx.AsyncClient in httpx", "content": "`httpx.httpx.AsyncClient` provides an async HTTP client for non-blocking requests. Example usage:\n```python\nasync with httpx.AsyncClient() as client:\n    r = await client.get(url)\n    data = r.json()\n```\nThis is useful when making concurrent HTTP requests in async applications. Performance tip: set timeout=httpx.Timeout(10.0) to prevent hanging requests.", "code_signature": "httpx.AsyncClient", "tags": "requests, httpx, websocket, rate-limiting", "url": "https://docs.httpx.org/en/stable/httpx/AsyncClient.html", "author": "Official Docs", "date_published": "28-06-2023", "votes_or_stars": 4209, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:b7805e5d-461d-48a3-85a7-e6bdc79c55a5": {"content": "PERF: concurrent.futures.ProcessPoolExecutor slow startup on Windows\nProblem: ProcessPoolExecutor takes 3-5 seconds to start on Windows due to process spawning overhead.\n\nFix/Discussion: Windows uses 'spawn' start method (vs 'fork' on Linux). Mitigations: (1) Reuse executor across calls — don't create/destroy in loops. (2) Use 'forkserver' or 'fork' on Linux/macOS. (3) For short tasks, ThreadPoolExecutor may be faster. (4) Use `initializer` param to pre-load modules.\nCode signature: status:closed", "file_path": "https://github.com/cpython/cpython/issues/6647", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "b7805e5d-461d-48a3-85a7-e6bdc79c55a5", "source_type": "github_issue", "domain": "Asyncio/Threading", "library": "cpython", "title": "PERF: concurrent.futures.ProcessPoolExecutor slow startup on Windows", "content": "Problem: ProcessPoolExecutor takes 3-5 seconds to start on Windows due to process spawning overhead.\n\nFix/Discussion: Windows uses 'spawn' start method (vs 'fork' on Linux). Mitigations: (1) Reuse executor across calls — don't create/destroy in loops. (2) Use 'forkserver' or 'fork' on Linux/macOS. (3) For short tasks, ThreadPoolExecutor may be faster. (4) Use `initializer` param to pre-load modules.", "code_signature": "status:closed", "tags": "executor, await, semaphore, task", "url": "https://github.com/cpython/cpython/issues/6647", "author": "contributor_4097", "date_published": "06-07-2024", "votes_or_stars": 285, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:63107430-d315-48a1-8856-6406fdca5c1b": {"content": "How to use df.groupby in Pandas\n`Pandas.df.groupby` groups DataFrame rows by column values. Example usage:\n```python\ndf.groupby('category')['value'].mean()\n```\nThis is useful when aggregation, transformation, filtering by group. Performance tip: avoid apply() on large DataFrames; prefer agg().\nCode signature: df.groupby", "file_path": "https://docs.pandas.org/en/stable/df/groupby.html", "function_name": "df.groupby", "type": "documentation", "metadata": {"record_id": "63107430-d315-48a1-8856-6406fdca5c1b", "source_type": "documentation", "domain": "NumPy/Pandas", "library": "Pandas", "title": "How to use df.groupby in Pandas", "content": "`Pandas.df.groupby` groups DataFrame rows by column values. Example usage:\n```python\ndf.groupby('category')['value'].mean()\n```\nThis is useful when aggregation, transformation, filtering by group. Performance tip: avoid apply() on large DataFrames; prefer agg().", "code_signature": "df.groupby", "tags": "loc, broadcasting, dtype, nan", "url": "https://docs.pandas.org/en/stable/df/groupby.html", "author": "Official Docs", "date_published": "14-08-2022", "votes_or_stars": 794, "relevance_label": "low", "difficulty_level": "intermediate"}}, "kb:2c6ac981-a3e1-4ed7-a6b7-40270f566e71": {"content": "BUG: httpx: SSL certificate error on Windows despite valid cert\nProblem: httpx raises SSLCertVerificationError on Windows when system CA store has the cert.\n\nFix/Discussion: httpx uses its own CA bundle (certifi) by default, not the system store. Fix: `httpx.Client(verify=certifi.where())` — already default. For custom certs: `httpx.Client(verify='/path/to/ca-bundle.pem')`. Install `pip install truststore` and use `ssl.create_default_context(ssl.Purpose.SERVER_AUTH)` to use system store.\nCode signature: status:closed", "file_path": "https://github.com/httpx/httpx/issues/8949", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "2c6ac981-a3e1-4ed7-a6b7-40270f566e71", "source_type": "github_issue", "domain": "Requests/HTTP/APIs", "library": "httpx", "title": "BUG: httpx: SSL certificate error on Windows despite valid cert", "content": "Problem: httpx raises SSLCertVerificationError on Windows when system CA store has the cert.\n\nFix/Discussion: httpx uses its own CA bundle (certifi) by default, not the system store. Fix: `httpx.Client(verify=certifi.where())` — already default. For custom certs: `httpx.Client(verify='/path/to/ca-bundle.pem')`. Install `pip install truststore` and use `ssl.create_default_context(ssl.Purpose.SERVER_AUTH)` to use system store.", "code_signature": "status:closed", "tags": "websocket, oauth, aiohttp, authentication, headers, retry", "url": "https://github.com/httpx/httpx/issues/8949", "author": "contributor_1420", "date_published": "24-12-2020", "votes_or_stars": 439, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:eee5503a-7e1b-4511-a80c-2512d1280dfc": {"content": "BUG: os.path.join ignores prefix when component starts with /\nProblem: os.path.join('base', '/absolute') returns '/absolute', ignoring 'base'.\n\nFix/Discussion: This is documented behavior: os.path.join discards previous components when it encounters an absolute path. Fix: use `pathlib.Path(base) / component.lstrip('/')`. Validate user-provided paths with `Path(path).resolve().is_relative_to(base_dir)` to prevent path traversal.\nCode signature: status:closed", "file_path": "https://github.com/cpython/cpython/issues/7712", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "eee5503a-7e1b-4511-a80c-2512d1280dfc", "source_type": "github_issue", "domain": "OS/Subprocess/File I/O", "library": "cpython", "title": "BUG: os.path.join ignores prefix when component starts with /", "content": "Problem: os.path.join('base', '/absolute') returns '/absolute', ignoring 'base'.\n\nFix/Discussion: This is documented behavior: os.path.join discards previous components when it encounters an absolute path. Fix: use `pathlib.Path(base) / component.lstrip('/')`. Validate user-provided paths with `Path(path).resolve().is_relative_to(base_dir)` to prevent path traversal.", "code_signature": "status:closed", "tags": "glob, file-io, shutil, stdout", "url": "https://github.com/cpython/cpython/issues/7712", "author": "contributor_8802", "date_published": "25-08-2018", "votes_or_stars": 420, "relevance_label": "low", "difficulty_level": "intermediate"}}, "kb:172c30bb-470b-4502-8d0f-4f617d9f4021": {"content": "How to use df.groupby in Pandas\n`Pandas.df.groupby` groups DataFrame rows by column values. Example usage:\n```python\ndf.groupby('category')['value'].mean()\n```\nThis is useful when aggregation, transformation, filtering by group. Performance tip: avoid apply() on large DataFrames; prefer agg().\nCode signature: df.groupby", "file_path": "https://docs.pandas.org/en/stable/df/groupby.html", "function_name": "df.groupby", "type": "documentation", "metadata": {"record_id": "172c30bb-470b-4502-8d0f-4f617d9f4021", "source_type": "documentation", "domain": "NumPy/Pandas", "library": "Pandas", "title": "How to use df.groupby in Pandas", "content": "`Pandas.df.groupby` groups DataFrame rows by column values. Example usage:\n```python\ndf.groupby('category')['value'].mean()\n```\nThis is useful when aggregation, transformation, filtering by group. Performance tip: avoid apply() on large DataFrames; prefer agg().", "code_signature": "df.groupby", "tags": "pandas, reshape, vectorization, array, nan, broadcasting", "url": "https://docs.pandas.org/en/stable/df/groupby.html", "author": "Official Docs", "date_published": "14-11-2020", "votes_or_stars": 800, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:abb238ed-0a28-4d96-9611-26fd2342ca15": {"content": "How do I efficiently fill NaN values in a Pandas DataFrame?\nFor large DataFrames, avoid looping. Use `df.fillna({'col': val})` to fill specific columns. `df.ffill()` propagates last valid observation forward. For time-series data, `df.interpolate()` provides smoother results. Always check `df.isna().sum()` before and after to verify.\nCode signature: Use df.fillna(value) or df.ffill()/df.bfill() for forward/backward fill.", "file_path": "https://stackoverflow.com/questions/6438436", "function_name": "Use df.fillna(value) or df.ffill()/df.bfill() for forward/backward fill.", "type": "stack_overflow", "metadata": {"record_id": "abb238ed-0a28-4d96-9611-26fd2342ca15", "source_type": "stack_overflow", "domain": "NumPy/Pandas", "library": "loc", "title": "How do I efficiently fill NaN values in a Pandas DataFrame?", "content": "For large DataFrames, avoid looping. Use `df.fillna({'col': val})` to fill specific columns. `df.ffill()` propagates last valid observation forward. For time-series data, `df.interpolate()` provides smoother results. Always check `df.isna().sum()` before and after to verify.", "code_signature": "Use df.fillna(value) or df.ffill()/df.bfill() for forward/backward fill.", "tags": "nan, merge, groupby, broadcasting", "url": "https://stackoverflow.com/questions/6438436", "author": "user_522340", "date_published": "24-09-2019", "votes_or_stars": 6, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:0d3e111a-c6d3-46ca-9f9c-3e9b0e4f8afb": {"content": "Flask: how to return JSON responses properly?\n`return jsonify({'key': 'value'})` sets Content-Type to application/json. Flask 1.0+ automatically converts returned dicts to JSON responses. For custom status codes: `return jsonify(data), 201`. For errors: use `abort(404)` or return `({'error': 'Not found'}, 404)`.\nCode signature: Use jsonify() or return a dict directly (Flask 1.0+).", "file_path": "https://stackoverflow.com/questions/8930103", "function_name": "Use jsonify() or return a dict directly (Flask 1.0+).", "type": "stack_overflow", "metadata": {"record_id": "0d3e111a-c6d3-46ca-9f9c-3e9b0e4f8afb", "source_type": "stack_overflow", "domain": "FastAPI/Flask/Django", "library": "django", "title": "Flask: how to return JSON responses properly?", "content": "`return jsonify({'key': 'value'})` sets Content-Type to application/json. Flask 1.0+ automatically converts returned dicts to JSON responses. For custom status codes: `return jsonify(data), 201`. For errors: use `abort(404)` or return `({'error': 'Not found'}, 404)`.", "code_signature": "Use jsonify() or return a dict directly (Flask 1.0+).", "tags": "django, middleware, flask, template, pydantic, rest", "url": "https://stackoverflow.com/questions/8930103", "author": "user_264801", "date_published": "26-09-2018", "votes_or_stars": 2200, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:4df53127-d34e-4836-8bbf-f6679084b714": {"content": "BUG: SQLAlchemy connection pool exhaustion under load\nProblem: Application hangs under concurrent load — all connections are checked out and pool is exhausted.\n\nFix/Discussion: Increase `pool_size` and `max_overflow`. Ensure sessions are always closed: use `contextmanager` or `with Session() as session:`. Set `pool_timeout` to fail fast. Enable `pool_pre_ping=True`. Monitor with `engine.pool.checkedout()`. In async: use `AsyncSession` with `async_scoped_session`.\nCode signature: status:closed", "file_path": "https://github.com/sqlalchemy/sqlalchemy/issues/106", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "4df53127-d34e-4836-8bbf-f6679084b714", "source_type": "github_issue", "domain": "SQLAlchemy/Databases", "library": "sqlalchemy", "title": "BUG: SQLAlchemy connection pool exhaustion under load", "content": "Problem: Application hangs under concurrent load — all connections are checked out and pool is exhausted.\n\nFix/Discussion: Increase `pool_size` and `max_overflow`. Ensure sessions are always closed: use `contextmanager` or `with Session() as session:`. Set `pool_timeout` to fail fast. Enable `pool_pre_ping=True`. Monitor with `engine.pool.checkedout()`. In async: use `AsyncSession` with `async_scoped_session`.", "code_signature": "status:closed", "tags": "query, sqlalchemy, connection-pool", "url": "https://github.com/sqlalchemy/sqlalchemy/issues/106", "author": "contributor_5078", "date_published": "19-09-2020", "votes_or_stars": 257, "relevance_label": "low", "difficulty_level": "advanced"}}, "kb:766483a2-d627-4b91-a616-dd4d3cfdc656": {"content": "BUG: asyncio task leaks when exception is not handled\nProblem: Unhandled exceptions in fire-and-forget asyncio tasks are silently ignored, causing task leaks.\n\nFix/Discussion: Always await tasks or add exception handlers. Use `task.add_done_callback()` to log exceptions. In Python 3.11+, use TaskGroup for structured concurrency — exceptions propagate automatically. Set `asyncio.get_event_loop().set_exception_handler()` for global unhandled task exception logging.\nCode signature: status:closed", "file_path": "https://github.com/cpython/cpython/issues/5591", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "766483a2-d627-4b91-a616-dd4d3cfdc656", "source_type": "github_issue", "domain": "Asyncio/Threading", "library": "cpython", "title": "BUG: asyncio task leaks when exception is not handled", "content": "Problem: Unhandled exceptions in fire-and-forget asyncio tasks are silently ignored, causing task leaks.\n\nFix/Discussion: Always await tasks or add exception handlers. Use `task.add_done_callback()` to log exceptions. In Python 3.11+, use TaskGroup for structured concurrency — exceptions propagate automatically. Set `asyncio.get_event_loop().set_exception_handler()` for global unhandled task exception logging.", "code_signature": "status:closed", "tags": "race-condition, deadlock, gather, asyncio, event-loop", "url": "https://github.com/cpython/cpython/issues/5591", "author": "contributor_1630", "date_published": "07-05-2019", "votes_or_stars": 319, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:ec45a005-e5fb-417c-afaa-f74d59552ec6": {"content": "How to use asyncio.create_task in asyncio\n`asyncio.asyncio.create_task` schedules a coroutine as a Task in the event loop. Example usage:\n```python\ntask = asyncio.create_task(send_email(user))\nawait task\n```\nThis is useful when fire-and-forget background tasks, concurrent task management. Performance tip: use TaskGroup (Python 3.11+) for structured concurrency.\nCode signature: asyncio.create_task", "file_path": "https://docs.asyncio.org/en/stable/asyncio/create_task.html", "function_name": "asyncio.create_task", "type": "documentation", "metadata": {"record_id": "ec45a005-e5fb-417c-afaa-f74d59552ec6", "source_type": "documentation", "domain": "Asyncio/Threading", "library": "asyncio", "title": "How to use asyncio.create_task in asyncio", "content": "`asyncio.asyncio.create_task` schedules a coroutine as a Task in the event loop. Example usage:\n```python\ntask = asyncio.create_task(send_email(user))\nawait task\n```\nThis is useful when fire-and-forget background tasks, concurrent task management. Performance tip: use TaskGroup (Python 3.11+) for structured concurrency.", "code_signature": "asyncio.create_task", "tags": "lock, coroutine, async, event-loop, threading", "url": "https://docs.asyncio.org/en/stable/asyncio/create_task.html", "author": "Official Docs", "date_published": "11-03-2021", "votes_or_stars": 4139, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:30d1253a-bd21-4e94-809a-6b0fc4aa5aac": {"content": "subprocess — Process Execution\nThe `subprocess.run` function in subprocess runs a subprocess and waits for it to complete. It accepts `args, capture_output=False, timeout=None` as a parameter and returns CompletedProcess. Common use cases include executing shell commands, scripts, or external programs from Python. Note that use shell=False (default) to avoid shell injection vulnerabilities.\nCode signature: subprocess.run(args, capture_output=False, timeout=None) -> CompletedProcess", "file_path": "https://docs.subprocess.org/en/stable/subprocess/run.html", "function_name": "subprocess.run(args, capture_output=False, timeout=None) -> CompletedProcess", "type": "documentation", "metadata": {"record_id": "30d1253a-bd21-4e94-809a-6b0fc4aa5aac", "source_type": "documentation", "domain": "OS/Subprocess/File I/O", "library": "subprocess", "title": "subprocess — Process Execution", "content": "The `subprocess.run` function in subprocess runs a subprocess and waits for it to complete. It accepts `args, capture_output=False, timeout=None` as a parameter and returns CompletedProcess. Common use cases include executing shell commands, scripts, or external programs from Python. Note that use shell=False (default) to avoid shell injection vulnerabilities.", "code_signature": "subprocess.run(args, capture_output=False, timeout=None) -> CompletedProcess", "tags": "pipe, file-io, os, stdin, process, stdout", "url": "https://docs.subprocess.org/en/stable/subprocess/run.html", "author": "Official Docs", "date_published": "12-12-2018", "votes_or_stars": 431, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:d1949a96-c9db-4ac9-9718-19a9afc56981": {"content": "FEAT: pathlib: add support for reading/writing with encoding shorthand\nProblem: pathlib.Path.read_text() does not default to UTF-8, causing issues across platforms.\n\nFix/Discussion: Always pass encoding explicitly: `Path('file').read_text(encoding='utf-8')`. Python 3.10+ added `encoding='locale'` default change proposal. Best practice: always specify encoding in all file I/O operations for portability.\nCode signature: status:closed", "file_path": "https://github.com/cpython/cpython/issues/2707", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "d1949a96-c9db-4ac9-9718-19a9afc56981", "source_type": "github_issue", "domain": "OS/Subprocess/File I/O", "library": "cpython", "title": "FEAT: pathlib: add support for reading/writing with encoding shorthand", "content": "Problem: pathlib.Path.read_text() does not default to UTF-8, causing issues across platforms.\n\nFix/Discussion: Always pass encoding explicitly: `Path('file').read_text(encoding='utf-8')`. Python 3.10+ added `encoding='locale'` default change proposal. Best practice: always specify encoding in all file I/O operations for portability.", "code_signature": "status:closed", "tags": "subprocess, glob, process, stderr, shutil, env-variable", "url": "https://github.com/cpython/cpython/issues/2707", "author": "contributor_7877", "date_published": "27-05-2018", "votes_or_stars": 439, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:1d8542ab-318d-4d06-816b-dbf491935ebe": {"content": "BUG: os.path.join ignores prefix when component starts with /\nProblem: os.path.join('base', '/absolute') returns '/absolute', ignoring 'base'.\n\nFix/Discussion: This is documented behavior: os.path.join discards previous components when it encounters an absolute path. Fix: use `pathlib.Path(base) / component.lstrip('/')`. Validate user-provided paths with `Path(path).resolve().is_relative_to(base_dir)` to prevent path traversal.\nCode signature: status:closed", "file_path": "https://github.com/cpython/cpython/issues/7712", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "1d8542ab-318d-4d06-816b-dbf491935ebe", "source_type": "github_issue", "domain": "OS/Subprocess/File I/O", "library": "cpython", "title": "BUG: os.path.join ignores prefix when component starts with /", "content": "Problem: os.path.join('base', '/absolute') returns '/absolute', ignoring 'base'.\n\nFix/Discussion: This is documented behavior: os.path.join discards previous components when it encounters an absolute path. Fix: use `pathlib.Path(base) / component.lstrip('/')`. Validate user-provided paths with `Path(path).resolve().is_relative_to(base_dir)` to prevent path traversal.", "code_signature": "status:closed", "tags": "subprocess, env-variable, pathlib", "url": "https://github.com/cpython/cpython/issues/7712", "author": "contributor_8802", "date_published": "23-07-2022", "votes_or_stars": 415, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:b2fc5023-2e48-404e-8c12-123548fc0373": {"content": "How to use asyncio.create_task in asyncio\n`asyncio.asyncio.create_task` schedules a coroutine as a Task in the event loop. Example usage:\n```python\ntask = asyncio.create_task(send_email(user))\nawait task\n```\nThis is useful when fire-and-forget background tasks, concurrent task management. Performance tip: use TaskGroup (Python 3.11+) for structured concurrency.\nCode signature: asyncio.create_task", "file_path": "https://docs.asyncio.org/en/stable/asyncio/create_task.html", "function_name": "asyncio.create_task", "type": "documentation", "metadata": {"record_id": "b2fc5023-2e48-404e-8c12-123548fc0373", "source_type": "documentation", "domain": "Asyncio/Threading", "library": "asyncio", "title": "How to use asyncio.create_task in asyncio", "content": "`asyncio.asyncio.create_task` schedules a coroutine as a Task in the event loop. Example usage:\n```python\ntask = asyncio.create_task(send_email(user))\nawait task\n```\nThis is useful when fire-and-forget background tasks, concurrent task management. Performance tip: use TaskGroup (Python 3.11+) for structured concurrency.", "code_signature": "asyncio.create_task", "tags": "asyncio, semaphore, threading, task, race-condition, gather", "url": "https://docs.asyncio.org/en/stable/asyncio/create_task.html", "author": "Official Docs", "date_published": "29-03-2023", "votes_or_stars": 4136, "relevance_label": "low", "difficulty_level": "beginner"}}, "kb:71741ba9-0799-476f-ac94-f53e8f912073": {"content": "How to add retry logic to requests?\n```python\nfrom requests.adapters import HTTPAdapter\nfrom urllib3.util.retry import Retry\nretry = Retry(total=3, backoff_factor=1, status_forcelist=[500,502,503,504])\nadapter = HTTPAdapter(max_retries=retry)\nsession.mount('https://', adapter)\n```\nCode signature: Mount an HTTPAdapter with urllib3 Retry on the Session.", "file_path": "https://stackoverflow.com/questions/1669324", "function_name": "Mount an HTTPAdapter with urllib3 Retry on the Session.", "type": "stack_overflow", "metadata": {"record_id": "71741ba9-0799-476f-ac94-f53e8f912073", "source_type": "stack_overflow", "domain": "Requests/HTTP/APIs", "library": "websocket", "title": "How to add retry logic to requests?", "content": "```python\nfrom requests.adapters import HTTPAdapter\nfrom urllib3.util.retry import Retry\nretry = Retry(total=3, backoff_factor=1, status_forcelist=[500,502,503,504])\nadapter = HTTPAdapter(max_retries=retry)\nsession.mount('https://', adapter)\n```", "code_signature": "Mount an HTTPAdapter with urllib3 Retry on the Session.", "tags": "httpx, requests, websocket, rest-api, http, retry", "url": "https://stackoverflow.com/questions/1669324", "author": "user_952590", "date_published": "25-06-2022", "votes_or_stars": 1521, "relevance_label": "low", "difficulty_level": "intermediate"}}, "kb:9dd9ac98-82e7-4d15-b157-f7405e231153": {"content": "How to watch a directory for file changes in Python?\n```python\nfrom watchdog.observers import Observer\nfrom watchdog.events import FileSystemEventHandler\nhandler = FileSystemEventHandler()\nobserver = Observer()\nobserver.schedule(handler, path='.', recursive=True)\nobserver.start()\n```\nFor simple polling: `os.stat(file).st_mtime`. Linux-native: `inotify`. macOS-native: `FSEvents`.\nCode signature: Use the watchdog library for cross-platform directory monitoring.", "file_path": "https://stackoverflow.com/questions/5213115", "function_name": "Use the watchdog library for cross-platform directory monitoring.", "type": "stack_overflow", "metadata": {"record_id": "9dd9ac98-82e7-4d15-b157-f7405e231153", "source_type": "stack_overflow", "domain": "OS/Subprocess/File I/O", "library": "env-variable", "title": "How to watch a directory for file changes in Python?", "content": "```python\nfrom watchdog.observers import Observer\nfrom watchdog.events import FileSystemEventHandler\nhandler = FileSystemEventHandler()\nobserver = Observer()\nobserver.schedule(handler, path='.', recursive=True)\nobserver.start()\n```\nFor simple polling: `os.stat(file).st_mtime`. Linux-native: `inotify`. macOS-native: `FSEvents`.", "code_signature": "Use the watchdog library for cross-platform directory monitoring.", "tags": "process, subprocess, env-variable", "url": "https://stackoverflow.com/questions/5213115", "author": "user_959314", "date_published": "08-04-2022", "votes_or_stars": 2096, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:46aaa0f5-da8d-4db1-a69c-99cabd92801c": {"content": "NumPy broadcasting error: operands could not be broadcast with shapes\nNumPy broadcasts shapes right-to-left. Arrays (3,4) and (4,) are compatible. Arrays (3,4) and (3,) are not — reshape to (3,1). Use `arr.reshape(-1,1)` to add a dimension. Always print `a.shape, b.shape` before operations to debug. `np.expand_dims(arr, axis=1)` is cleaner than reshape for adding dimensions.\nCode signature: Shapes must be compatible: dimensions must match or one of them must be 1.", "file_path": "https://stackoverflow.com/questions/4860684", "function_name": "Shapes must be compatible: dimensions must match or one of them must be 1.", "type": "stack_overflow", "metadata": {"record_id": "46aaa0f5-da8d-4db1-a69c-99cabd92801c", "source_type": "stack_overflow", "domain": "NumPy/Pandas", "library": "numpy", "title": "NumPy broadcasting error: operands could not be broadcast with shapes", "content": "NumPy broadcasts shapes right-to-left. Arrays (3,4) and (4,) are compatible. Arrays (3,4) and (3,) are not — reshape to (3,1). Use `arr.reshape(-1,1)` to add a dimension. Always print `a.shape, b.shape` before operations to debug. `np.expand_dims(arr, axis=1)` is cleaner than reshape for adding dimensions.", "code_signature": "Shapes must be compatible: dimensions must match or one of them must be 1.", "tags": "dtype, nan, broadcasting, vectorization, array", "url": "https://stackoverflow.com/questions/4860684", "author": "user_627024", "date_published": "06-02-2021", "votes_or_stars": 1404, "relevance_label": "low", "difficulty_level": "intermediate"}}, "kb:da7164ee-6532-4124-ad14-c467a7306a90": {"content": "How to use httpx.AsyncClient in httpx\n`httpx.httpx.AsyncClient` provides an async HTTP client for non-blocking requests. Example usage:\n```python\nasync with httpx.AsyncClient() as client:\n    r = await client.get(url)\n    data = r.json()\n```\nThis is useful when making concurrent HTTP requests in async applications. Performance tip: set timeout=httpx.Timeout(10.0) to prevent hanging requests.\nCode signature: httpx.AsyncClient", "file_path": "https://docs.httpx.org/en/stable/httpx/AsyncClient.html", "function_name": "httpx.AsyncClient", "type": "documentation", "metadata": {"record_id": "da7164ee-6532-4124-ad14-c467a7306a90", "source_type": "documentation", "domain": "Requests/HTTP/APIs", "library": "httpx", "title": "How to use httpx.AsyncClient in httpx", "content": "`httpx.httpx.AsyncClient` provides an async HTTP client for non-blocking requests. Example usage:\n```python\nasync with httpx.AsyncClient() as client:\n    r = await client.get(url)\n    data = r.json()\n```\nThis is useful when making concurrent HTTP requests in async applications. Performance tip: set timeout=httpx.Timeout(10.0) to prevent hanging requests.", "code_signature": "httpx.AsyncClient", "tags": "aiohttp, websocket, headers, requests, httpx, oauth", "url": "https://docs.httpx.org/en/stable/httpx/AsyncClient.html", "author": "Official Docs", "date_published": "26-07-2024", "votes_or_stars": 4183, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:3dc3d5ed-0f3f-4ec5-afe3-da7c5d5b0675": {"content": "How to add retry logic to requests?\n```python\nfrom requests.adapters import HTTPAdapter\nfrom urllib3.util.retry import Retry\nretry = Retry(total=3, backoff_factor=1, status_forcelist=[500,502,503,504])\nadapter = HTTPAdapter(max_retries=retry)\nsession.mount('https://', adapter)\n```\nCode signature: Mount an HTTPAdapter with urllib3 Retry on the Session.", "file_path": "https://stackoverflow.com/questions/1669324", "function_name": "Mount an HTTPAdapter with urllib3 Retry on the Session.", "type": "stack_overflow", "metadata": {"record_id": "3dc3d5ed-0f3f-4ec5-afe3-da7c5d5b0675", "source_type": "stack_overflow", "domain": "Requests/HTTP/APIs", "library": "websocket", "title": "How to add retry logic to requests?", "content": "```python\nfrom requests.adapters import HTTPAdapter\nfrom urllib3.util.retry import Retry\nretry = Retry(total=3, backoff_factor=1, status_forcelist=[500,502,503,504])\nadapter = HTTPAdapter(max_retries=retry)\nsession.mount('https://', adapter)\n```", "code_signature": "Mount an HTTPAdapter with urllib3 Retry on the Session.", "tags": "rate-limiting, timeout, json, oauth", "url": "https://stackoverflow.com/questions/1669324", "author": "user_952590", "date_published": "07-01-2019", "votes_or_stars": 1474, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:3d142493-1f46-40df-aba4-fad1a20cabf7": {"content": "How to run asyncio code from synchronous Python?\n`asyncio.run(main())` creates an event loop, runs the coroutine, closes the loop. Never call asyncio.run() from inside a running event loop (use await instead). In Jupyter notebooks (which have a running loop), use `await coroutine()` directly or `nest_asyncio.apply()`. For mixing sync/async, use `loop.run_until_complete()`.\nCode signature: Use asyncio.run() in Python 3.7+ to execute a coroutine from sync code.", "file_path": "https://stackoverflow.com/questions/2140194", "function_name": "Use asyncio.run() in Python 3.7+ to execute a coroutine from sync code.", "type": "stack_overflow", "metadata": {"record_id": "3d142493-1f46-40df-aba4-fad1a20cabf7", "source_type": "stack_overflow", "domain": "Asyncio/Threading", "library": "gather", "title": "How to run asyncio code from synchronous Python?", "content": "`asyncio.run(main())` creates an event loop, runs the coroutine, closes the loop. Never call asyncio.run() from inside a running event loop (use await instead). In Jupyter notebooks (which have a running loop), use `await coroutine()` directly or `nest_asyncio.apply()`. For mixing sync/async, use `loop.run_until_complete()`.", "code_signature": "Use asyncio.run() in Python 3.7+ to execute a coroutine from sync code.", "tags": "race-condition, async, executor, asyncio", "url": "https://stackoverflow.com/questions/2140194", "author": "user_718011", "date_published": "05-07-2021", "votes_or_stars": 293, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:23fa46a8-9dab-4566-9934-f4638b7d42f8": {"content": "NumPy — Linear Algebra\nThe `np.einsum` function in NumPy evaluates Einstein summation convention on operands. It accepts `subscripts, *operands` as a parameter and returns ndarray. Common use cases include matrix multiplication, dot products, tensor contractions. Note that prefer np.einsum over loops for large arrays.\nCode signature: np.einsum(subscripts, *operands) -> ndarray", "file_path": "https://docs.numpy.org/en/stable/np/einsum.html", "function_name": "np.einsum(subscripts, *operands) -> ndarray", "type": "documentation", "metadata": {"record_id": "23fa46a8-9dab-4566-9934-f4638b7d42f8", "source_type": "documentation", "domain": "NumPy/Pandas", "library": "NumPy", "title": "NumPy — Linear Algebra", "content": "The `np.einsum` function in NumPy evaluates Einstein summation convention on operands. It accepts `subscripts, *operands` as a parameter and returns ndarray. Common use cases include matrix multiplication, dot products, tensor contractions. Note that prefer np.einsum over loops for large arrays.", "code_signature": "np.einsum(subscripts, *operands) -> ndarray", "tags": "iloc, numpy, nan, pivot, dtype, vectorization", "url": "https://docs.numpy.org/en/stable/np/einsum.html", "author": "Official Docs", "date_published": "14-11-2023", "votes_or_stars": 1900, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:cf383109-aac6-4e11-834c-589146856d47": {"content": "NumPy — Linear Algebra\nThe `np.einsum` function in NumPy evaluates Einstein summation convention on operands. It accepts `subscripts, *operands` as a parameter and returns ndarray. Common use cases include matrix multiplication, dot products, tensor contractions. Note that prefer np.einsum over loops for large arrays.\nCode signature: np.einsum(subscripts, *operands) -> ndarray", "file_path": "https://docs.numpy.org/en/stable/np/einsum.html", "function_name": "np.einsum(subscripts, *operands) -> ndarray", "type": "documentation", "metadata": {"record_id": "cf383109-aac6-4e11-834c-589146856d47", "source_type": "documentation", "domain": "NumPy/Pandas", "library": "NumPy", "title": "NumPy — Linear Algebra", "content": "The `np.einsum` function in NumPy evaluates Einstein summation convention on operands. It accepts `subscripts, *operands` as a parameter and returns ndarray. Common use cases include matrix multiplication, dot products, tensor contractions. Note that prefer np.einsum over loops for large arrays.", "code_signature": "np.einsum(subscripts, *operands) -> ndarray", "tags": "merge, nan, pivot, dataframe, array", "url": "https://docs.numpy.org/en/stable/np/einsum.html", "author": "Official Docs", "date_published": "05-01-2020", "votes_or_stars": 1907, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:c7f156ad-3100-4c6d-9418-081489637a85": {"content": "Why is my Pandas merge creating duplicate rows?\nDuplicate rows after merge usually mean the join key is not unique. Diagnose with `df.duplicated(subset=['key']).sum()`. Use `how='left'` carefully when right table has multiple matches. Set `validate='one_to_many'` or `'many_to_one'` to enforce cardinality. Consider deduplicating with `df.drop_duplicates(subset=['key'])` before merging.\nCode signature: Your merge key has duplicates in one or both DataFrames. Use validate='one_to_one' to catch this early.", "file_path": "https://stackoverflow.com/questions/2321324", "function_name": "Your merge key has duplicates in one or both DataFrames. Use validate='one_to_one' to catch this early.", "type": "stack_overflow", "metadata": {"record_id": "c7f156ad-3100-4c6d-9418-081489637a85", "source_type": "stack_overflow", "domain": "NumPy/Pandas", "library": "array", "title": "Why is my Pandas merge creating duplicate rows?", "content": "Duplicate rows after merge usually mean the join key is not unique. Diagnose with `df.duplicated(subset=['key']).sum()`. Use `how='left'` carefully when right table has multiple matches. Set `validate='one_to_many'` or `'many_to_one'` to enforce cardinality. Consider deduplicating with `df.drop_duplicates(subset=['key'])` before merging.", "code_signature": "Your merge key has duplicates in one or both DataFrames. Use validate='one_to_one' to catch this early.", "tags": "loc, vectorization, pivot", "url": "https://stackoverflow.com/questions/2321324", "author": "user_99814", "date_published": "02-06-2019", "votes_or_stars": 257, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:006dca6d-2032-416c-b983-191e6b66fb13": {"content": "NumPy — Linear Algebra\nThe `np.einsum` function in NumPy evaluates Einstein summation convention on operands. It accepts `subscripts, *operands` as a parameter and returns ndarray. Common use cases include matrix multiplication, dot products, tensor contractions. Note that prefer np.einsum over loops for large arrays.\nCode signature: np.einsum(subscripts, *operands) -> ndarray", "file_path": "https://docs.numpy.org/en/stable/np/einsum.html", "function_name": "np.einsum(subscripts, *operands) -> ndarray", "type": "documentation", "metadata": {"record_id": "006dca6d-2032-416c-b983-191e6b66fb13", "source_type": "documentation", "domain": "NumPy/Pandas", "library": "NumPy", "title": "NumPy — Linear Algebra", "content": "The `np.einsum` function in NumPy evaluates Einstein summation convention on operands. It accepts `subscripts, *operands` as a parameter and returns ndarray. Common use cases include matrix multiplication, dot products, tensor contractions. Note that prefer np.einsum over loops for large arrays.", "code_signature": "np.einsum(subscripts, *operands) -> ndarray", "tags": "iloc, dtype, pivot, numpy", "url": "https://docs.numpy.org/en/stable/np/einsum.html", "author": "Official Docs", "date_published": "24-05-2020", "votes_or_stars": 1878, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:7e41dca6-9fd7-4103-8552-961b5278c34a": {"content": "BUG: SettingWithCopyWarning when modifying sliced DataFrame\nProblem: Modifying a slice of a DataFrame raises SettingWithCopyWarning and may not update original.\n\nFix/Discussion: Always use .loc or .copy() when slicing. Use `df.loc[mask, 'col'] = val` for assignment. Create explicit copies: `subset = df[mask].copy()`. In Pandas 2.0+, Copy-on-Write makes this behavior consistent — enable with `pd.options.mode.copy_on_write = True`.\nCode signature: status:closed", "file_path": "https://github.com/pandas/pandas/issues/3264", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "7e41dca6-9fd7-4103-8552-961b5278c34a", "source_type": "github_issue", "domain": "NumPy/Pandas", "library": "pandas", "title": "BUG: SettingWithCopyWarning when modifying sliced DataFrame", "content": "Problem: Modifying a slice of a DataFrame raises SettingWithCopyWarning and may not update original.\n\nFix/Discussion: Always use .loc or .copy() when slicing. Use `df.loc[mask, 'col'] = val` for assignment. Create explicit copies: `subset = df[mask].copy()`. In Pandas 2.0+, Copy-on-Write makes this behavior consistent — enable with `pd.options.mode.copy_on_write = True`.", "code_signature": "status:closed", "tags": "broadcasting, reshape, pivot, iloc", "url": "https://github.com/pandas/pandas/issues/3264", "author": "contributor_6628", "date_published": "11-12-2021", "votes_or_stars": 47, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:f7b80187-a2d4-497e-a8ed-dd2ed0ade291": {"content": "os Environment Variables — official reference\nOfficial documentation for os Environment Variables. The method signature is `os.environ(key)`. provides a mapping of the current environment variables. Raises `KeyError` when key is accessed with [] notation and is not set. See also: dotenv, os.getenv, os.putenv.\nCode signature: os.environ(key)", "file_path": "https://docs.os.org/en/stable/os/environ.html", "function_name": "os.environ(key)", "type": "documentation", "metadata": {"record_id": "f7b80187-a2d4-497e-a8ed-dd2ed0ade291", "source_type": "documentation", "domain": "OS/Subprocess/File I/O", "library": "os", "title": "os Environment Variables — official reference", "content": "Official documentation for os Environment Variables. The method signature is `os.environ(key)`. provides a mapping of the current environment variables. Raises `KeyError` when key is accessed with [] notation and is not set. See also: dotenv, os.getenv, os.putenv.", "code_signature": "os.environ(key)", "tags": "glob, stdin, tempfile", "url": "https://docs.os.org/en/stable/os/environ.html", "author": "Official Docs", "date_published": "25-05-2020", "votes_or_stars": 2211, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:b621d7ac-5646-4e95-b752-cc4b61153390": {"content": "How to implement a thread pool in Python?\n```python\nfrom concurrent.futures import ThreadPoolExecutor\nwith ThreadPoolExecutor(max_workers=8) as ex:\n    futures = [ex.submit(task, arg) for arg in args]\n    results = [f.result() for f in futures]\n```\nFor CPU-bound: use ProcessPoolExecutor instead. For async integration: `await loop.run_in_executor(executor, func, *args)`.\nCode signature: Use concurrent.futures.ThreadPoolExecutor for I/O-bound tasks.", "file_path": "https://stackoverflow.com/questions/3646552", "function_name": "Use concurrent.futures.ThreadPoolExecutor for I/O-bound tasks.", "type": "stack_overflow", "metadata": {"record_id": "b621d7ac-5646-4e95-b752-cc4b61153390", "source_type": "stack_overflow", "domain": "Asyncio/Threading", "library": "coroutine", "title": "How to implement a thread pool in Python?", "content": "```python\nfrom concurrent.futures import ThreadPoolExecutor\nwith ThreadPoolExecutor(max_workers=8) as ex:\n    futures = [ex.submit(task, arg) for arg in args]\n    results = [f.result() for f in futures]\n```\nFor CPU-bound: use ProcessPoolExecutor instead. For async integration: `await loop.run_in_executor(executor, func, *args)`.", "code_signature": "Use concurrent.futures.ThreadPoolExecutor for I/O-bound tasks.", "tags": "threading, lock, gather, executor, asyncio", "url": "https://stackoverflow.com/questions/3646552", "author": "user_469469", "date_published": "19-08-2021", "votes_or_stars": 1542, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:8204956f-c0aa-44de-9d82-f36f07ffe643": {"content": "subprocess.run vs Popen: which should I use?\n`subprocess.run()` is a high-level wrapper that waits for completion — use for most cases. `subprocess.Popen()` gives full control: stream stdout/stderr in real-time, write to stdin, run processes concurrently. For capturing output: `subprocess.run(..., capture_output=True, text=True)`. Always use `shell=False` and pass args as a list to prevent shell injection.\nCode signature: Use subprocess.run() for simple cases; Popen for streaming output or interactive processes.", "file_path": "https://stackoverflow.com/questions/5727085", "function_name": "Use subprocess.run() for simple cases; Popen for streaming output or interactive processes.", "type": "stack_overflow", "metadata": {"record_id": "8204956f-c0aa-44de-9d82-f36f07ffe643", "source_type": "stack_overflow", "domain": "OS/Subprocess/File I/O", "library": "shutil", "title": "subprocess.run vs Popen: which should I use?", "content": "`subprocess.run()` is a high-level wrapper that waits for completion — use for most cases. `subprocess.Popen()` gives full control: stream stdout/stderr in real-time, write to stdin, run processes concurrently. For capturing output: `subprocess.run(..., capture_output=True, text=True)`. Always use `shell=False` and pass args as a list to prevent shell injection.", "code_signature": "Use subprocess.run() for simple cases; Popen for streaming output or interactive processes.", "tags": "tempfile, glob, env-variable, pipe, stdin", "url": "https://stackoverflow.com/questions/5727085", "author": "user_644210", "date_published": "04-03-2020", "votes_or_stars": 673, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:161933f8-36a8-48c3-ab16-904cc06e314d": {"content": "How to use create_engine in SQLAlchemy\n`SQLAlchemy.create_engine` creates a database engine representing a connection pool. Example usage:\n```python\nengine = create_engine('postgresql://user:pass@host/db', echo=True)\n```\nThis is useful when establishing the central database connection for an application. Performance tip: set pool_size and max_overflow based on expected concurrency.\nCode signature: create_engine", "file_path": "https://docs.sqlalchemy.org/en/stable/create_engine.html", "function_name": "create_engine", "type": "documentation", "metadata": {"record_id": "161933f8-36a8-48c3-ab16-904cc06e314d", "source_type": "documentation", "domain": "SQLAlchemy/Databases", "library": "SQLAlchemy", "title": "How to use create_engine in SQLAlchemy", "content": "`SQLAlchemy.create_engine` creates a database engine representing a connection pool. Example usage:\n```python\nengine = create_engine('postgresql://user:pass@host/db', echo=True)\n```\nThis is useful when establishing the central database connection for an application. Performance tip: set pool_size and max_overflow based on expected concurrency.", "code_signature": "create_engine", "tags": "transaction, sqlite, connection-pool, relationship, query, orm", "url": "https://docs.sqlalchemy.org/en/stable/create_engine.html", "author": "Official Docs", "date_published": "16-01-2019", "votes_or_stars": 2077, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:d0eb7b65-638b-4c47-b12e-5a87362ed04e": {"content": "NumPy broadcasting error: operands could not be broadcast with shapes\nNumPy broadcasts shapes right-to-left. Arrays (3,4) and (4,) are compatible. Arrays (3,4) and (3,) are not — reshape to (3,1). Use `arr.reshape(-1,1)` to add a dimension. Always print `a.shape, b.shape` before operations to debug. `np.expand_dims(arr, axis=1)` is cleaner than reshape for adding dimensions.\nCode signature: Shapes must be compatible: dimensions must match or one of them must be 1.", "file_path": "https://stackoverflow.com/questions/4860684", "function_name": "Shapes must be compatible: dimensions must match or one of them must be 1.", "type": "stack_overflow", "metadata": {"record_id": "d0eb7b65-638b-4c47-b12e-5a87362ed04e", "source_type": "stack_overflow", "domain": "NumPy/Pandas", "library": "numpy", "title": "NumPy broadcasting error: operands could not be broadcast with shapes", "content": "NumPy broadcasts shapes right-to-left. Arrays (3,4) and (4,) are compatible. Arrays (3,4) and (3,) are not — reshape to (3,1). Use `arr.reshape(-1,1)` to add a dimension. Always print `a.shape, b.shape` before operations to debug. `np.expand_dims(arr, axis=1)` is cleaner than reshape for adding dimensions.", "code_signature": "Shapes must be compatible: dimensions must match or one of them must be 1.", "tags": "groupby, dtype, merge, nan", "url": "https://stackoverflow.com/questions/4860684", "author": "user_627024", "date_published": "27-02-2022", "votes_or_stars": 1407, "relevance_label": "low", "difficulty_level": "intermediate"}}, "kb:2eac6192-1e87-483e-aac7-847e68f1a627": {"content": "FEAT: Django: add support for async ORM queries\nProblem: Django ORM is synchronous; using it in async views requires sync_to_async wrapper.\n\nFix/Discussion: Django 4.1+ added native async ORM support. Use `await Model.objects.filter().afirst()`, `async for obj in qs:`. For older versions: wrap with `sync_to_async`: `get_user = sync_to_async(User.objects.get)\nuser = await get_user(pk=1)`.\nCode signature: status:closed", "file_path": "https://github.com/django/django/issues/8618", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "2eac6192-1e87-483e-aac7-847e68f1a627", "source_type": "github_issue", "domain": "FastAPI/Flask/Django", "library": "django", "title": "FEAT: Django: add support for async ORM queries", "content": "Problem: Django ORM is synchronous; using it in async views requires sync_to_async wrapper.\n\nFix/Discussion: Django 4.1+ added native async ORM support. Use `await Model.objects.filter().afirst()`, `async for obj in qs:`. For older versions: wrap with `sync_to_async`: `get_user = sync_to_async(User.objects.get)\nuser = await get_user(pk=1)`.", "code_signature": "status:closed", "tags": "orm, middleware, template, django", "url": "https://github.com/django/django/issues/8618", "author": "contributor_8921", "date_published": "31-07-2020", "votes_or_stars": 13, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:3a7b0aab-8309-4397-bbba-c21a7df3e520": {"content": "How to run asyncio code from synchronous Python?\n`asyncio.run(main())` creates an event loop, runs the coroutine, closes the loop. Never call asyncio.run() from inside a running event loop (use await instead). In Jupyter notebooks (which have a running loop), use `await coroutine()` directly or `nest_asyncio.apply()`. For mixing sync/async, use `loop.run_until_complete()`.\nCode signature: Use asyncio.run() in Python 3.7+ to execute a coroutine from sync code.", "file_path": "https://stackoverflow.com/questions/2140194", "function_name": "Use asyncio.run() in Python 3.7+ to execute a coroutine from sync code.", "type": "stack_overflow", "metadata": {"record_id": "3a7b0aab-8309-4397-bbba-c21a7df3e520", "source_type": "stack_overflow", "domain": "Asyncio/Threading", "library": "gather", "title": "How to run asyncio code from synchronous Python?", "content": "`asyncio.run(main())` creates an event loop, runs the coroutine, closes the loop. Never call asyncio.run() from inside a running event loop (use await instead). In Jupyter notebooks (which have a running loop), use `await coroutine()` directly or `nest_asyncio.apply()`. For mixing sync/async, use `loop.run_until_complete()`.", "code_signature": "Use asyncio.run() in Python 3.7+ to execute a coroutine from sync code.", "tags": "deadlock, semaphore, coroutine, executor, task", "url": "https://stackoverflow.com/questions/2140194", "author": "user_718011", "date_published": "05-07-2019", "votes_or_stars": 317, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:d28af518-33c8-4eb6-8bfe-dafd753a5ae6": {"content": "How to use Path.glob in pathlib\n`pathlib.Path.glob` yields all paths matching the given glob pattern. Example usage:\n```python\npy_files = list(Path('src').glob('**/*.py'))\n```\nThis is useful when finding files recursively, batch file processing. Performance tip: iterate the generator lazily rather than converting to list for large directories.\nCode signature: Path.glob", "file_path": "https://docs.pathlib.org/en/stable/Path/glob.html", "function_name": "Path.glob", "type": "documentation", "metadata": {"record_id": "d28af518-33c8-4eb6-8bfe-dafd753a5ae6", "source_type": "documentation", "domain": "OS/Subprocess/File I/O", "library": "pathlib", "title": "How to use Path.glob in pathlib", "content": "`pathlib.Path.glob` yields all paths matching the given glob pattern. Example usage:\n```python\npy_files = list(Path('src').glob('**/*.py'))\n```\nThis is useful when finding files recursively, batch file processing. Performance tip: iterate the generator lazily rather than converting to list for large directories.", "code_signature": "Path.glob", "tags": "glob, stdout, shutil", "url": "https://docs.pathlib.org/en/stable/Path/glob.html", "author": "Official Docs", "date_published": "16-12-2022", "votes_or_stars": 3216, "relevance_label": "low", "difficulty_level": "beginner"}}, "kb:74d11c46-c50c-44b4-8cf4-6a27e512a42c": {"content": "NumPy broadcasting error: operands could not be broadcast with shapes\nNumPy broadcasts shapes right-to-left. Arrays (3,4) and (4,) are compatible. Arrays (3,4) and (3,) are not — reshape to (3,1). Use `arr.reshape(-1,1)` to add a dimension. Always print `a.shape, b.shape` before operations to debug. `np.expand_dims(arr, axis=1)` is cleaner than reshape for adding dimensions.\nCode signature: Shapes must be compatible: dimensions must match or one of them must be 1.", "file_path": "https://stackoverflow.com/questions/4860684", "function_name": "Shapes must be compatible: dimensions must match or one of them must be 1.", "type": "stack_overflow", "metadata": {"record_id": "74d11c46-c50c-44b4-8cf4-6a27e512a42c", "source_type": "stack_overflow", "domain": "NumPy/Pandas", "library": "numpy", "title": "NumPy broadcasting error: operands could not be broadcast with shapes", "content": "NumPy broadcasts shapes right-to-left. Arrays (3,4) and (4,) are compatible. Arrays (3,4) and (3,) are not — reshape to (3,1). Use `arr.reshape(-1,1)` to add a dimension. Always print `a.shape, b.shape` before operations to debug. `np.expand_dims(arr, axis=1)` is cleaner than reshape for adding dimensions.", "code_signature": "Shapes must be compatible: dimensions must match or one of them must be 1.", "tags": "numpy, vectorization, iloc", "url": "https://stackoverflow.com/questions/4860684", "author": "user_627024", "date_published": "19-08-2020", "votes_or_stars": 1394, "relevance_label": "low", "difficulty_level": "intermediate"}}, "kb:c246aa92-8043-46b0-a000-4d71d8bbcdb4": {"content": "PERF: groupby().apply() extremely slow on large DataFrames\nProblem: groupby().apply() with a custom function is 100x slower than expected on 10M row DataFrames.\n\nFix/Discussion: groupby().apply() serializes each group and has high overhead. Fix: vectorize with numpy, use groupby().transform() for same-shape output, or groupby().agg() with named aggregations. For complex logic, consider Polars or Dask. Numba's @jit can accelerate custom group functions.\nCode signature: status:closed", "file_path": "https://github.com/pandas/pandas/issues/8446", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "c246aa92-8043-46b0-a000-4d71d8bbcdb4", "source_type": "github_issue", "domain": "NumPy/Pandas", "library": "pandas", "title": "PERF: groupby().apply() extremely slow on large DataFrames", "content": "Problem: groupby().apply() with a custom function is 100x slower than expected on 10M row DataFrames.\n\nFix/Discussion: groupby().apply() serializes each group and has high overhead. Fix: vectorize with numpy, use groupby().transform() for same-shape output, or groupby().agg() with named aggregations. For complex logic, consider Polars or Dask. Numba's @jit can accelerate custom group functions.", "code_signature": "status:closed", "tags": "array, pivot, groupby, dataframe", "url": "https://github.com/pandas/pandas/issues/8446", "author": "contributor_6648", "date_published": "09-02-2024", "votes_or_stars": 330, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:b52ad89a-c580-4cb1-bd4a-d32a017d56c0": {"content": "NumPy broadcasting error: operands could not be broadcast with shapes\nNumPy broadcasts shapes right-to-left. Arrays (3,4) and (4,) are compatible. Arrays (3,4) and (3,) are not — reshape to (3,1). Use `arr.reshape(-1,1)` to add a dimension. Always print `a.shape, b.shape` before operations to debug. `np.expand_dims(arr, axis=1)` is cleaner than reshape for adding dimensions.\nCode signature: Shapes must be compatible: dimensions must match or one of them must be 1.", "file_path": "https://stackoverflow.com/questions/4860684", "function_name": "Shapes must be compatible: dimensions must match or one of them must be 1.", "type": "stack_overflow", "metadata": {"record_id": "b52ad89a-c580-4cb1-bd4a-d32a017d56c0", "source_type": "stack_overflow", "domain": "NumPy/Pandas", "library": "numpy", "title": "NumPy broadcasting error: operands could not be broadcast with shapes", "content": "NumPy broadcasts shapes right-to-left. Arrays (3,4) and (4,) are compatible. Arrays (3,4) and (3,) are not — reshape to (3,1). Use `arr.reshape(-1,1)` to add a dimension. Always print `a.shape, b.shape` before operations to debug. `np.expand_dims(arr, axis=1)` is cleaner than reshape for adding dimensions.", "code_signature": "Shapes must be compatible: dimensions must match or one of them must be 1.", "tags": "numpy, loc, vectorization, dtype, array", "url": "https://stackoverflow.com/questions/4860684", "author": "user_627024", "date_published": "28-03-2023", "votes_or_stars": 1420, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:b144f275-8dd1-4438-8cb3-d6bb02832107": {"content": "BUG: SQLAlchemy connection pool exhaustion under load\nProblem: Application hangs under concurrent load — all connections are checked out and pool is exhausted.\n\nFix/Discussion: Increase `pool_size` and `max_overflow`. Ensure sessions are always closed: use `contextmanager` or `with Session() as session:`. Set `pool_timeout` to fail fast. Enable `pool_pre_ping=True`. Monitor with `engine.pool.checkedout()`. In async: use `AsyncSession` with `async_scoped_session`.\nCode signature: status:closed", "file_path": "https://github.com/sqlalchemy/sqlalchemy/issues/106", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "b144f275-8dd1-4438-8cb3-d6bb02832107", "source_type": "github_issue", "domain": "SQLAlchemy/Databases", "library": "sqlalchemy", "title": "BUG: SQLAlchemy connection pool exhaustion under load", "content": "Problem: Application hangs under concurrent load — all connections are checked out and pool is exhausted.\n\nFix/Discussion: Increase `pool_size` and `max_overflow`. Ensure sessions are always closed: use `contextmanager` or `with Session() as session:`. Set `pool_timeout` to fail fast. Enable `pool_pre_ping=True`. Monitor with `engine.pool.checkedout()`. In async: use `AsyncSession` with `async_scoped_session`.", "code_signature": "status:closed", "tags": "session, relationship, sqlite, transaction, alembic", "url": "https://github.com/sqlalchemy/sqlalchemy/issues/106", "author": "contributor_5078", "date_published": "21-03-2022", "votes_or_stars": 285, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:efa695d4-b35c-4bfd-b0a7-338b7aacf2a2": {"content": "SQLAlchemy — ORM Session Query\nThe `session.query` function in SQLAlchemy constructs a SELECT query against mapped entities. It accepts `*entities` as a parameter and returns Query object. Common use cases include retrieving ORM-mapped objects from the database. Note that prefer session.execute(select(...)) in SQLAlchemy 2.0+.\nCode signature: session.query(*entities) -> Query object", "file_path": "https://docs.sqlalchemy.org/en/stable/session/query.html", "function_name": "session.query(*entities) -> Query object", "type": "documentation", "metadata": {"record_id": "efa695d4-b35c-4bfd-b0a7-338b7aacf2a2", "source_type": "documentation", "domain": "SQLAlchemy/Databases", "library": "SQLAlchemy", "title": "SQLAlchemy — ORM Session Query", "content": "The `session.query` function in SQLAlchemy constructs a SELECT query against mapped entities. It accepts `*entities` as a parameter and returns Query object. Common use cases include retrieving ORM-mapped objects from the database. Note that prefer session.execute(select(...)) in SQLAlchemy 2.0+.", "code_signature": "session.query(*entities) -> Query object", "tags": "query, relationship, connection-pool, orm", "url": "https://docs.sqlalchemy.org/en/stable/session/query.html", "author": "Official Docs", "date_published": "19-12-2023", "votes_or_stars": 3019, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:cb5d41d5-d752-4554-94b2-2f0534b7cc13": {"content": "requests — HTTP Session Management\nThe `requests.Session` function in requests creates a persistent HTTP session with connection pooling. It accepts `` as a parameter and returns Session object. Common use cases include making multiple requests to the same host efficiently. Note that always close or use Session as context manager to release connections.\nCode signature: requests.Session() -> Session object", "file_path": "https://docs.requests.org/en/stable/requests/Session.html", "function_name": "requests.Session() -> Session object", "type": "documentation", "metadata": {"record_id": "cb5d41d5-d752-4554-94b2-2f0534b7cc13", "source_type": "documentation", "domain": "Requests/HTTP/APIs", "library": "requests", "title": "requests — HTTP Session Management", "content": "The `requests.Session` function in requests creates a persistent HTTP session with connection pooling. It accepts `` as a parameter and returns Session object. Common use cases include making multiple requests to the same host efficiently. Note that always close or use Session as context manager to release connections.", "code_signature": "requests.Session() -> Session object", "tags": "rest-api, headers, httpx, authentication, rate-limiting", "url": "https://docs.requests.org/en/stable/requests/Session.html", "author": "Official Docs", "date_published": "26-02-2020", "votes_or_stars": 85, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:71540fda-9e64-4fbd-a7ca-01e121574e01": {"content": "BUG: asyncio task leaks when exception is not handled\nProblem: Unhandled exceptions in fire-and-forget asyncio tasks are silently ignored, causing task leaks.\n\nFix/Discussion: Always await tasks or add exception handlers. Use `task.add_done_callback()` to log exceptions. In Python 3.11+, use TaskGroup for structured concurrency — exceptions propagate automatically. Set `asyncio.get_event_loop().set_exception_handler()` for global unhandled task exception logging.\nCode signature: status:closed", "file_path": "https://github.com/cpython/cpython/issues/5591", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "71540fda-9e64-4fbd-a7ca-01e121574e01", "source_type": "github_issue", "domain": "Asyncio/Threading", "library": "cpython", "title": "BUG: asyncio task leaks when exception is not handled", "content": "Problem: Unhandled exceptions in fire-and-forget asyncio tasks are silently ignored, causing task leaks.\n\nFix/Discussion: Always await tasks or add exception handlers. Use `task.add_done_callback()` to log exceptions. In Python 3.11+, use TaskGroup for structured concurrency — exceptions propagate automatically. Set `asyncio.get_event_loop().set_exception_handler()` for global unhandled task exception logging.", "code_signature": "status:closed", "tags": "async, deadlock, threading, gather, await, race-condition", "url": "https://github.com/cpython/cpython/issues/5591", "author": "contributor_1630", "date_published": "04-07-2022", "votes_or_stars": 341, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:625d013b-8d4a-415c-8b42-f9679009269b": {"content": "BUG: httpx: SSL certificate error on Windows despite valid cert\nProblem: httpx raises SSLCertVerificationError on Windows when system CA store has the cert.\n\nFix/Discussion: httpx uses its own CA bundle (certifi) by default, not the system store. Fix: `httpx.Client(verify=certifi.where())` — already default. For custom certs: `httpx.Client(verify='/path/to/ca-bundle.pem')`. Install `pip install truststore` and use `ssl.create_default_context(ssl.Purpose.SERVER_AUTH)` to use system store.\nCode signature: status:closed", "file_path": "https://github.com/httpx/httpx/issues/8949", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "625d013b-8d4a-415c-8b42-f9679009269b", "source_type": "github_issue", "domain": "Requests/HTTP/APIs", "library": "httpx", "title": "BUG: httpx: SSL certificate error on Windows despite valid cert", "content": "Problem: httpx raises SSLCertVerificationError on Windows when system CA store has the cert.\n\nFix/Discussion: httpx uses its own CA bundle (certifi) by default, not the system store. Fix: `httpx.Client(verify=certifi.where())` — already default. For custom certs: `httpx.Client(verify='/path/to/ca-bundle.pem')`. Install `pip install truststore` and use `ssl.create_default_context(ssl.Purpose.SERVER_AUTH)` to use system store.", "code_signature": "status:closed", "tags": "websocket, timeout, rate-limiting, http, authentication", "url": "https://github.com/httpx/httpx/issues/8949", "author": "contributor_1420", "date_published": "05-09-2024", "votes_or_stars": 457, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:07fa91b3-6595-4ce2-80d0-71d1590a5a67": {"content": "How to use Path.glob in pathlib\n`pathlib.Path.glob` yields all paths matching the given glob pattern. Example usage:\n```python\npy_files = list(Path('src').glob('**/*.py'))\n```\nThis is useful when finding files recursively, batch file processing. Performance tip: iterate the generator lazily rather than converting to list for large directories.\nCode signature: Path.glob", "file_path": "https://docs.pathlib.org/en/stable/Path/glob.html", "function_name": "Path.glob", "type": "documentation", "metadata": {"record_id": "07fa91b3-6595-4ce2-80d0-71d1590a5a67", "source_type": "documentation", "domain": "OS/Subprocess/File I/O", "library": "pathlib", "title": "How to use Path.glob in pathlib", "content": "`pathlib.Path.glob` yields all paths matching the given glob pattern. Example usage:\n```python\npy_files = list(Path('src').glob('**/*.py'))\n```\nThis is useful when finding files recursively, batch file processing. Performance tip: iterate the generator lazily rather than converting to list for large directories.", "code_signature": "Path.glob", "tags": "shutil, tempfile, subprocess, os, env-variable", "url": "https://docs.pathlib.org/en/stable/Path/glob.html", "author": "Official Docs", "date_published": "19-05-2021", "votes_or_stars": 3232, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:c8be90b4-cacb-4c99-a2f3-1b72290d9da1": {"content": "PERF: Django template rendering bottleneck in production\nProblem: Template rendering takes 500ms+ for complex pages with many database queries.\n\nFix/Discussion: Primary cause is N+1 queries in template. Solutions: (1) Use select_related/prefetch_related. (2) Cache rendered templates with `cache` template tag. (3) Use `django-cachalot` for query caching. (4) Move heavy computation to views, pass pre-computed context. (5) Consider server-side caching with Redis.\nCode signature: status:closed", "file_path": "https://github.com/django/django/issues/3243", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "c8be90b4-cacb-4c99-a2f3-1b72290d9da1", "source_type": "github_issue", "domain": "FastAPI/Flask/Django", "library": "django", "title": "PERF: Django template rendering bottleneck in production", "content": "Problem: Template rendering takes 500ms+ for complex pages with many database queries.\n\nFix/Discussion: Primary cause is N+1 queries in template. Solutions: (1) Use select_related/prefetch_related. (2) Cache rendered templates with `cache` template tag. (3) Use `django-cachalot` for query caching. (4) Move heavy computation to views, pass pre-computed context. (5) Consider server-side caching with Redis.", "code_signature": "status:closed", "tags": "blueprint, rest, dependency-injection, middleware, request", "url": "https://github.com/django/django/issues/3243", "author": "contributor_6988", "date_published": "31-05-2023", "votes_or_stars": 97, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:029108a5-53a9-482c-9382-2a05eceda026": {"content": "How to handle rate limiting with the requests library?\n```python\nfrom tenacity import retry, wait_exponential, stop_after_attempt\n@retry(wait=wait_exponential(min=1, max=60), stop=stop_after_attempt(5))\ndef call_api(url):\n    r = requests.get(url)\n    r.raise_for_status()\n    return r.json()\n```\nCheck `Retry-After` header in 429 responses. For async: use `aiohttp` with tenacity.\nCode signature: Implement exponential backoff with the tenacity or backoff library.", "file_path": "https://stackoverflow.com/questions/1604451", "function_name": "Implement exponential backoff with the tenacity or backoff library.", "type": "stack_overflow", "metadata": {"record_id": "029108a5-53a9-482c-9382-2a05eceda026", "source_type": "stack_overflow", "domain": "Requests/HTTP/APIs", "library": "retry", "title": "How to handle rate limiting with the requests library?", "content": "```python\nfrom tenacity import retry, wait_exponential, stop_after_attempt\n@retry(wait=wait_exponential(min=1, max=60), stop=stop_after_attempt(5))\ndef call_api(url):\n    r = requests.get(url)\n    r.raise_for_status()\n    return r.json()\n```\nCheck `Retry-After` header in 429 responses. For async: use `aiohttp` with tenacity.", "code_signature": "Implement exponential backoff with the tenacity or backoff library.", "tags": "headers, retry, requests, httpx, json", "url": "https://stackoverflow.com/questions/1604451", "author": "user_885136", "date_published": "21-02-2022", "votes_or_stars": 620, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:c04a4dad-06a4-45aa-a558-4a2480674340": {"content": "How to use create_engine in SQLAlchemy\n`SQLAlchemy.create_engine` creates a database engine representing a connection pool. Example usage:\n```python\nengine = create_engine('postgresql://user:pass@host/db', echo=True)\n```\nThis is useful when establishing the central database connection for an application. Performance tip: set pool_size and max_overflow based on expected concurrency.\nCode signature: create_engine", "file_path": "https://docs.sqlalchemy.org/en/stable/create_engine.html", "function_name": "create_engine", "type": "documentation", "metadata": {"record_id": "c04a4dad-06a4-45aa-a558-4a2480674340", "source_type": "documentation", "domain": "SQLAlchemy/Databases", "library": "SQLAlchemy", "title": "How to use create_engine in SQLAlchemy", "content": "`SQLAlchemy.create_engine` creates a database engine representing a connection pool. Example usage:\n```python\nengine = create_engine('postgresql://user:pass@host/db', echo=True)\n```\nThis is useful when establishing the central database connection for an application. Performance tip: set pool_size and max_overflow based on expected concurrency.", "code_signature": "create_engine", "tags": "alembic, orm, connection-pool, relationship, postgresql, transaction", "url": "https://docs.sqlalchemy.org/en/stable/create_engine.html", "author": "Official Docs", "date_published": "25-05-2021", "votes_or_stars": 2040, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:5559f38e-0040-465a-b9de-e3b3f79fb548": {"content": "BUG: httpx: SSL certificate error on Windows despite valid cert\nProblem: httpx raises SSLCertVerificationError on Windows when system CA store has the cert.\n\nFix/Discussion: httpx uses its own CA bundle (certifi) by default, not the system store. Fix: `httpx.Client(verify=certifi.where())` — already default. For custom certs: `httpx.Client(verify='/path/to/ca-bundle.pem')`. Install `pip install truststore` and use `ssl.create_default_context(ssl.Purpose.SERVER_AUTH)` to use system store.\nCode signature: status:closed", "file_path": "https://github.com/httpx/httpx/issues/8949", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "5559f38e-0040-465a-b9de-e3b3f79fb548", "source_type": "github_issue", "domain": "Requests/HTTP/APIs", "library": "httpx", "title": "BUG: httpx: SSL certificate error on Windows despite valid cert", "content": "Problem: httpx raises SSLCertVerificationError on Windows when system CA store has the cert.\n\nFix/Discussion: httpx uses its own CA bundle (certifi) by default, not the system store. Fix: `httpx.Client(verify=certifi.where())` — already default. For custom certs: `httpx.Client(verify='/path/to/ca-bundle.pem')`. Install `pip install truststore` and use `ssl.create_default_context(ssl.Purpose.SERVER_AUTH)` to use system store.", "code_signature": "status:closed", "tags": "requests, headers, retry, httpx", "url": "https://github.com/httpx/httpx/issues/8949", "author": "contributor_1420", "date_published": "31-01-2019", "votes_or_stars": 483, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:f9014d5b-ee99-4599-95c6-5be9b361e458": {"content": "Python: how to safely write to a file atomically?\n```python\nimport tempfile, os\nwith tempfile.NamedTemporaryFile('w', delete=False, dir='.') as tmp:\n    tmp.write(data)\n    tmp.flush()\n    os.fsync(tmp.fileno())\nos.replace(tmp.name, target_path)\n```\n`os.replace()` is atomic on POSIX. This prevents partial writes from corrupting files.\nCode signature: Write to a temp file then rename — rename is atomic on POSIX systems.", "file_path": "https://stackoverflow.com/questions/5394880", "function_name": "Write to a temp file then rename — rename is atomic on POSIX systems.", "type": "stack_overflow", "metadata": {"record_id": "f9014d5b-ee99-4599-95c6-5be9b361e458", "source_type": "stack_overflow", "domain": "OS/Subprocess/File I/O", "library": "stdin", "title": "Python: how to safely write to a file atomically?", "content": "```python\nimport tempfile, os\nwith tempfile.NamedTemporaryFile('w', delete=False, dir='.') as tmp:\n    tmp.write(data)\n    tmp.flush()\n    os.fsync(tmp.fileno())\nos.replace(tmp.name, target_path)\n```\n`os.replace()` is atomic on POSIX. This prevents partial writes from corrupting files.", "code_signature": "Write to a temp file then rename — rename is atomic on POSIX systems.", "tags": "env-variable, process, stderr, pathlib, tempfile", "url": "https://stackoverflow.com/questions/5394880", "author": "user_179430", "date_published": "23-09-2018", "votes_or_stars": 525, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:0465694a-6bf9-48aa-9601-c9346c53fe6d": {"content": "Pandas Merging DataFrames — official reference\nOfficial documentation for Pandas Merging DataFrames. The method signature is `pd.merge(left, right, on, how='inner')`. merges two DataFrames using SQL-style joins. Raises `MergeError` when merge keys produce duplicate columns. See also: df.join, df.concat.\nCode signature: pd.merge(left, right, on, how='inner')", "file_path": "https://docs.pandas.org/en/stable/pd/merge.html", "function_name": "pd.merge(left, right, on, how='inner')", "type": "documentation", "metadata": {"record_id": "0465694a-6bf9-48aa-9601-c9346c53fe6d", "source_type": "documentation", "domain": "NumPy/Pandas", "library": "Pandas", "title": "Pandas Merging DataFrames — official reference", "content": "Official documentation for Pandas Merging DataFrames. The method signature is `pd.merge(left, right, on, how='inner')`. merges two DataFrames using SQL-style joins. Raises `MergeError` when merge keys produce duplicate columns. See also: df.join, df.concat.", "code_signature": "pd.merge(left, right, on, how='inner')", "tags": "pandas, merge, nan, broadcasting, dataframe", "url": "https://docs.pandas.org/en/stable/pd/merge.html", "author": "Official Docs", "date_published": "24-10-2019", "votes_or_stars": 4531, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:c50f985c-0734-4bd5-9524-eecee35d76c9": {"content": "How to handle database migrations with Alembic?\nWorkflow: (1) Change SQLAlchemy models. (2) `alembic revision --autogenerate -m 'description'`. (3) Review generated migration script. (4) `alembic upgrade head` to apply. Always review autogenerated migrations — they miss some changes (indexes, constraints). Use `alembic downgrade -1` to rollback. Store migrations in version control.\nCode signature: Use alembic revision --autogenerate to create migrations from model changes.", "file_path": "https://stackoverflow.com/questions/9436429", "function_name": "Use alembic revision --autogenerate to create migrations from model changes.", "type": "stack_overflow", "metadata": {"record_id": "c50f985c-0734-4bd5-9524-eecee35d76c9", "source_type": "stack_overflow", "domain": "SQLAlchemy/Databases", "library": "connection-pool", "title": "How to handle database migrations with Alembic?", "content": "Workflow: (1) Change SQLAlchemy models. (2) `alembic revision --autogenerate -m 'description'`. (3) Review generated migration script. (4) `alembic upgrade head` to apply. Always review autogenerated migrations — they miss some changes (indexes, constraints). Use `alembic downgrade -1` to rollback. Store migrations in version control.", "code_signature": "Use alembic revision --autogenerate to create migrations from model changes.", "tags": "relationship, sqlite, foreign-key", "url": "https://stackoverflow.com/questions/9436429", "author": "user_974047", "date_published": "29-11-2021", "votes_or_stars": 2407, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:09f7988f-0057-4fd7-8c4c-a01996c4cbf9": {"content": "How to add retry logic to requests?\n```python\nfrom requests.adapters import HTTPAdapter\nfrom urllib3.util.retry import Retry\nretry = Retry(total=3, backoff_factor=1, status_forcelist=[500,502,503,504])\nadapter = HTTPAdapter(max_retries=retry)\nsession.mount('https://', adapter)\n```\nCode signature: Mount an HTTPAdapter with urllib3 Retry on the Session.", "file_path": "https://stackoverflow.com/questions/1669324", "function_name": "Mount an HTTPAdapter with urllib3 Retry on the Session.", "type": "stack_overflow", "metadata": {"record_id": "09f7988f-0057-4fd7-8c4c-a01996c4cbf9", "source_type": "stack_overflow", "domain": "Requests/HTTP/APIs", "library": "websocket", "title": "How to add retry logic to requests?", "content": "```python\nfrom requests.adapters import HTTPAdapter\nfrom urllib3.util.retry import Retry\nretry = Retry(total=3, backoff_factor=1, status_forcelist=[500,502,503,504])\nadapter = HTTPAdapter(max_retries=retry)\nsession.mount('https://', adapter)\n```", "code_signature": "Mount an HTTPAdapter with urllib3 Retry on the Session.", "tags": "rate-limiting, retry, httpx", "url": "https://stackoverflow.com/questions/1669324", "author": "user_952590", "date_published": "23-05-2023", "votes_or_stars": 1528, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:a1f174ec-d7d2-4371-872b-f0ddd4f7e498": {"content": "How to limit concurrency with asyncio?\n```python\nsem = asyncio.Semaphore(10)\nasync def limited(url):\n    async with sem:\n        return await fetch(url)\nresults = await asyncio.gather(*[limited(u) for u in urls])\n```\nThis prevents overwhelming APIs or databases with too many simultaneous connections.\nCode signature: Use asyncio.Semaphore to cap the number of concurrent coroutines.", "file_path": "https://stackoverflow.com/questions/2375453", "function_name": "Use asyncio.Semaphore to cap the number of concurrent coroutines.", "type": "stack_overflow", "metadata": {"record_id": "a1f174ec-d7d2-4371-872b-f0ddd4f7e498", "source_type": "stack_overflow", "domain": "Asyncio/Threading", "library": "asyncio", "title": "How to limit concurrency with asyncio?", "content": "```python\nsem = asyncio.Semaphore(10)\nasync def limited(url):\n    async with sem:\n        return await fetch(url)\nresults = await asyncio.gather(*[limited(u) for u in urls])\n```\nThis prevents overwhelming APIs or databases with too many simultaneous connections.", "code_signature": "Use asyncio.Semaphore to cap the number of concurrent coroutines.", "tags": "deadlock, coroutine, threading, async, semaphore, gather", "url": "https://stackoverflow.com/questions/2375453", "author": "user_449589", "date_published": "21-01-2021", "votes_or_stars": 2468, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:e3fe41c1-d66d-42c8-acf2-8edaebd3fc8e": {"content": "How to read a large file line by line in Python without loading it all into memory?\n```python\nwith open('large.txt', 'r') as f:\n    for line in f:\n        process(line.strip())\n```\nThis reads one line at a time. For binary files: use `f.read(chunk_size)` in a loop. For CSV: use `pd.read_csv(file, chunksize=10000)`. Never use `f.readlines()` on large files.\nCode signature: Iterate over the file object directly — it yields lines lazily.", "file_path": "https://stackoverflow.com/questions/2737893", "function_name": "Iterate over the file object directly — it yields lines lazily.", "type": "stack_overflow", "metadata": {"record_id": "e3fe41c1-d66d-42c8-acf2-8edaebd3fc8e", "source_type": "stack_overflow", "domain": "OS/Subprocess/File I/O", "library": "shutil", "title": "How to read a large file line by line in Python without loading it all into memory?", "content": "```python\nwith open('large.txt', 'r') as f:\n    for line in f:\n        process(line.strip())\n```\nThis reads one line at a time. For binary files: use `f.read(chunk_size)` in a loop. For CSV: use `pd.read_csv(file, chunksize=10000)`. Never use `f.readlines()` on large files.", "code_signature": "Iterate over the file object directly — it yields lines lazily.", "tags": "subprocess, tempfile, glob, stdin, pathlib, env-variable", "url": "https://stackoverflow.com/questions/2737893", "author": "user_994539", "date_published": "11-01-2023", "votes_or_stars": 2291, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:25eea788-55ab-48c2-9cac-146da9d69050": {"content": "How to stream large HTTP responses with requests?\n```python\nwith requests.get(url, stream=True) as r:\n    r.raise_for_status()\n    with open('file', 'wb') as f:\n        for chunk in r.iter_content(chunk_size=8192):\n            f.write(chunk)\n```\nAlways use as context manager with stream=True to ensure connection is released.\nCode signature: Use stream=True and iterate over response.iter_content() or iter_lines().", "file_path": "https://stackoverflow.com/questions/3592974", "function_name": "Use stream=True and iterate over response.iter_content() or iter_lines().", "type": "stack_overflow", "metadata": {"record_id": "25eea788-55ab-48c2-9cac-146da9d69050", "source_type": "stack_overflow", "domain": "Requests/HTTP/APIs", "library": "json", "title": "How to stream large HTTP responses with requests?", "content": "```python\nwith requests.get(url, stream=True) as r:\n    r.raise_for_status()\n    with open('file', 'wb') as f:\n        for chunk in r.iter_content(chunk_size=8192):\n            f.write(chunk)\n```\nAlways use as context manager with stream=True to ensure connection is released.", "code_signature": "Use stream=True and iterate over response.iter_content() or iter_lines().", "tags": "requests, websocket, retry, rest-api, oauth, json", "url": "https://stackoverflow.com/questions/3592974", "author": "user_980733", "date_published": "22-06-2023", "votes_or_stars": 1645, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:22e4cded-af64-493a-a183-d66d3c1241e4": {"content": "subprocess — Process Execution\nThe `subprocess.run` function in subprocess runs a subprocess and waits for it to complete. It accepts `args, capture_output=False, timeout=None` as a parameter and returns CompletedProcess. Common use cases include executing shell commands, scripts, or external programs from Python. Note that use shell=False (default) to avoid shell injection vulnerabilities.\nCode signature: subprocess.run(args, capture_output=False, timeout=None) -> CompletedProcess", "file_path": "https://docs.subprocess.org/en/stable/subprocess/run.html", "function_name": "subprocess.run(args, capture_output=False, timeout=None) -> CompletedProcess", "type": "documentation", "metadata": {"record_id": "22e4cded-af64-493a-a183-d66d3c1241e4", "source_type": "documentation", "domain": "OS/Subprocess/File I/O", "library": "subprocess", "title": "subprocess — Process Execution", "content": "The `subprocess.run` function in subprocess runs a subprocess and waits for it to complete. It accepts `args, capture_output=False, timeout=None` as a parameter and returns CompletedProcess. Common use cases include executing shell commands, scripts, or external programs from Python. Note that use shell=False (default) to avoid shell injection vulnerabilities.", "code_signature": "subprocess.run(args, capture_output=False, timeout=None) -> CompletedProcess", "tags": "env-variable, stdin, stdout, file-io", "url": "https://docs.subprocess.org/en/stable/subprocess/run.html", "author": "Official Docs", "date_published": "27-04-2019", "votes_or_stars": 438, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:dac5c811-9aaf-4bbf-8e19-13c621dc08b4": {"content": "Why is my Pandas merge creating duplicate rows?\nDuplicate rows after merge usually mean the join key is not unique. Diagnose with `df.duplicated(subset=['key']).sum()`. Use `how='left'` carefully when right table has multiple matches. Set `validate='one_to_many'` or `'many_to_one'` to enforce cardinality. Consider deduplicating with `df.drop_duplicates(subset=['key'])` before merging.\nCode signature: Your merge key has duplicates in one or both DataFrames. Use validate='one_to_one' to catch this early.", "file_path": "https://stackoverflow.com/questions/2321324", "function_name": "Your merge key has duplicates in one or both DataFrames. Use validate='one_to_one' to catch this early.", "type": "stack_overflow", "metadata": {"record_id": "dac5c811-9aaf-4bbf-8e19-13c621dc08b4", "source_type": "stack_overflow", "domain": "NumPy/Pandas", "library": "array", "title": "Why is my Pandas merge creating duplicate rows?", "content": "Duplicate rows after merge usually mean the join key is not unique. Diagnose with `df.duplicated(subset=['key']).sum()`. Use `how='left'` carefully when right table has multiple matches. Set `validate='one_to_many'` or `'many_to_one'` to enforce cardinality. Consider deduplicating with `df.drop_duplicates(subset=['key'])` before merging.", "code_signature": "Your merge key has duplicates in one or both DataFrames. Use validate='one_to_one' to catch this early.", "tags": "groupby, vectorization, broadcasting, pivot", "url": "https://stackoverflow.com/questions/2321324", "author": "user_99814", "date_published": "31-08-2022", "votes_or_stars": 275, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:ac892288-ba31-4e9b-bf5c-133736aaeda8": {"content": "BUG: pd.read_csv() silently truncates large integers\nProblem: Integer columns with values > 2^53 are silently corrupted when read from CSV.\n\nFix/Discussion: Floating point cannot represent integers > 2^53 exactly. Fix: `pd.read_csv(f, dtype={'col': str})` then convert: `df['col'] = df['col'].astype('Int64')`. Use `dtype_backend='numpy_nullable'` in Pandas 2.0+ for better integer handling.\nCode signature: status:closed", "file_path": "https://github.com/pandas/pandas/issues/4449", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "ac892288-ba31-4e9b-bf5c-133736aaeda8", "source_type": "github_issue", "domain": "NumPy/Pandas", "library": "pandas", "title": "BUG: pd.read_csv() silently truncates large integers", "content": "Problem: Integer columns with values > 2^53 are silently corrupted when read from CSV.\n\nFix/Discussion: Floating point cannot represent integers > 2^53 exactly. Fix: `pd.read_csv(f, dtype={'col': str})` then convert: `df['col'] = df['col'].astype('Int64')`. Use `dtype_backend='numpy_nullable'` in Pandas 2.0+ for better integer handling.", "code_signature": "status:closed", "tags": "loc, pivot, dataframe", "url": "https://github.com/pandas/pandas/issues/4449", "author": "contributor_726", "date_published": "14-11-2021", "votes_or_stars": 482, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:5f4b4a82-489a-443f-b0d6-d5350dd3ff31": {"content": "PERF: requests connection not reused between calls — new TCP connection each time\nProblem: Each requests.get() call opens a new TCP connection, causing high latency.\n\nFix/Discussion: requests.get/post etc. create a new Session per call. Fix: use a persistent Session: `session = requests.Session()` and reuse it. Sessions pool connections via urllib3. For thread-safety: create one Session per thread. For async: use httpx.AsyncClient or aiohttp.ClientSession for async connection pooling.\nCode signature: status:closed", "file_path": "https://github.com/requests/requests/issues/3605", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "5f4b4a82-489a-443f-b0d6-d5350dd3ff31", "source_type": "github_issue", "domain": "Requests/HTTP/APIs", "library": "requests", "title": "PERF: requests connection not reused between calls — new TCP connection each time", "content": "Problem: Each requests.get() call opens a new TCP connection, causing high latency.\n\nFix/Discussion: requests.get/post etc. create a new Session per call. Fix: use a persistent Session: `session = requests.Session()` and reuse it. Sessions pool connections via urllib3. For thread-safety: create one Session per thread. For async: use httpx.AsyncClient or aiohttp.ClientSession for async connection pooling.", "code_signature": "status:closed", "tags": "rate-limiting, websocket, requests, aiohttp", "url": "https://github.com/requests/requests/issues/3605", "author": "contributor_1152", "date_published": "26-09-2021", "votes_or_stars": 345, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:8d72ebfc-b68b-4678-a7fd-e0e752b15ef0": {"content": "SQLAlchemy — ORM Session Query\nThe `session.query` function in SQLAlchemy constructs a SELECT query against mapped entities. It accepts `*entities` as a parameter and returns Query object. Common use cases include retrieving ORM-mapped objects from the database. Note that prefer session.execute(select(...)) in SQLAlchemy 2.0+.\nCode signature: session.query(*entities) -> Query object", "file_path": "https://docs.sqlalchemy.org/en/stable/session/query.html", "function_name": "session.query(*entities) -> Query object", "type": "documentation", "metadata": {"record_id": "8d72ebfc-b68b-4678-a7fd-e0e752b15ef0", "source_type": "documentation", "domain": "SQLAlchemy/Databases", "library": "SQLAlchemy", "title": "SQLAlchemy — ORM Session Query", "content": "The `session.query` function in SQLAlchemy constructs a SELECT query against mapped entities. It accepts `*entities` as a parameter and returns Query object. Common use cases include retrieving ORM-mapped objects from the database. Note that prefer session.execute(select(...)) in SQLAlchemy 2.0+.", "code_signature": "session.query(*entities) -> Query object", "tags": "relationship, alembic, postgresql, sqlite, orm", "url": "https://docs.sqlalchemy.org/en/stable/session/query.html", "author": "Official Docs", "date_published": "10-12-2020", "votes_or_stars": 3050, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:a965decc-237e-4901-8567-80b9dcaaef4e": {"content": "PERF: Django template rendering bottleneck in production\nProblem: Template rendering takes 500ms+ for complex pages with many database queries.\n\nFix/Discussion: Primary cause is N+1 queries in template. Solutions: (1) Use select_related/prefetch_related. (2) Cache rendered templates with `cache` template tag. (3) Use `django-cachalot` for query caching. (4) Move heavy computation to views, pass pre-computed context. (5) Consider server-side caching with Redis.\nCode signature: status:closed", "file_path": "https://github.com/django/django/issues/3243", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "a965decc-237e-4901-8567-80b9dcaaef4e", "source_type": "github_issue", "domain": "FastAPI/Flask/Django", "library": "django", "title": "PERF: Django template rendering bottleneck in production", "content": "Problem: Template rendering takes 500ms+ for complex pages with many database queries.\n\nFix/Discussion: Primary cause is N+1 queries in template. Solutions: (1) Use select_related/prefetch_related. (2) Cache rendered templates with `cache` template tag. (3) Use `django-cachalot` for query caching. (4) Move heavy computation to views, pass pre-computed context. (5) Consider server-side caching with Redis.", "code_signature": "status:closed", "tags": "routing, request, middleware, fastapi, pydantic, dependency-injection", "url": "https://github.com/django/django/issues/3243", "author": "contributor_6988", "date_published": "03-08-2024", "votes_or_stars": 86, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:0d80bb59-870f-407a-afda-193ace40ba0e": {"content": "FastAPI — Path Operations\nThe `@app.get` function in FastAPI registers a GET endpoint at the specified path. It accepts `path, response_model` as a parameter and returns Response. Common use cases include building REST API endpoints that return JSON data. Note that use response_model to enforce output schema validation.\nCode signature: @app.get(path, response_model) -> Response", "file_path": "https://docs.fastapi.org/en/stable/@app/get.html", "function_name": "@app.get(path, response_model) -> Response", "type": "documentation", "metadata": {"record_id": "0d80bb59-870f-407a-afda-193ace40ba0e", "source_type": "documentation", "domain": "FastAPI/Flask/Django", "library": "FastAPI", "title": "FastAPI — Path Operations", "content": "The `@app.get` function in FastAPI registers a GET endpoint at the specified path. It accepts `path, response_model` as a parameter and returns Response. Common use cases include building REST API endpoints that return JSON data. Note that use response_model to enforce output schema validation.", "code_signature": "@app.get(path, response_model) -> Response", "tags": "routing, orm, middleware, django, pydantic", "url": "https://docs.fastapi.org/en/stable/@app/get.html", "author": "Official Docs", "date_published": "21-03-2024", "votes_or_stars": 2896, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:2f66e1d9-7be7-4020-bb2d-b8b3e9eaeb63": {"content": "How to handle database migrations with Alembic?\nWorkflow: (1) Change SQLAlchemy models. (2) `alembic revision --autogenerate -m 'description'`. (3) Review generated migration script. (4) `alembic upgrade head` to apply. Always review autogenerated migrations — they miss some changes (indexes, constraints). Use `alembic downgrade -1` to rollback. Store migrations in version control.\nCode signature: Use alembic revision --autogenerate to create migrations from model changes.", "file_path": "https://stackoverflow.com/questions/9436429", "function_name": "Use alembic revision --autogenerate to create migrations from model changes.", "type": "stack_overflow", "metadata": {"record_id": "2f66e1d9-7be7-4020-bb2d-b8b3e9eaeb63", "source_type": "stack_overflow", "domain": "SQLAlchemy/Databases", "library": "connection-pool", "title": "How to handle database migrations with Alembic?", "content": "Workflow: (1) Change SQLAlchemy models. (2) `alembic revision --autogenerate -m 'description'`. (3) Review generated migration script. (4) `alembic upgrade head` to apply. Always review autogenerated migrations — they miss some changes (indexes, constraints). Use `alembic downgrade -1` to rollback. Store migrations in version control.", "code_signature": "Use alembic revision --autogenerate to create migrations from model changes.", "tags": "postgresql, orm, connection-pool, relationship, query, sqlalchemy", "url": "https://stackoverflow.com/questions/9436429", "author": "user_974047", "date_published": "28-08-2023", "votes_or_stars": 2374, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:2ffd8892-4d55-4193-bbaa-f87d7bedacf5": {"content": "threading Thread Synchronization — official reference\nOfficial documentation for threading Thread Synchronization. The method signature is `threading.Lock()`. creates a mutual-exclusion lock for thread-safe access. Raises `RuntimeError` when lock is released without being acquired. See also: threading.RLock, threading.Semaphore, threading.Event.\nCode signature: threading.Lock()", "file_path": "https://docs.threading.org/en/stable/threading/Lock.html", "function_name": "threading.Lock()", "type": "documentation", "metadata": {"record_id": "2ffd8892-4d55-4193-bbaa-f87d7bedacf5", "source_type": "documentation", "domain": "Asyncio/Threading", "library": "threading", "title": "threading Thread Synchronization — official reference", "content": "Official documentation for threading Thread Synchronization. The method signature is `threading.Lock()`. creates a mutual-exclusion lock for thread-safe access. Raises `RuntimeError` when lock is released without being acquired. See also: threading.RLock, threading.Semaphore, threading.Event.", "code_signature": "threading.Lock()", "tags": "threading, lock, asyncio, task, event-loop", "url": "https://docs.threading.org/en/stable/threading/Lock.html", "author": "Official Docs", "date_published": "10-08-2023", "votes_or_stars": 4847, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:50cba774-4ce6-4404-9a0a-2294d99494fc": {"content": "FastAPI — Path Operations\nThe `@app.get` function in FastAPI registers a GET endpoint at the specified path. It accepts `path, response_model` as a parameter and returns Response. Common use cases include building REST API endpoints that return JSON data. Note that use response_model to enforce output schema validation.\nCode signature: @app.get(path, response_model) -> Response", "file_path": "https://docs.fastapi.org/en/stable/@app/get.html", "function_name": "@app.get(path, response_model) -> Response", "type": "documentation", "metadata": {"record_id": "50cba774-4ce6-4404-9a0a-2294d99494fc", "source_type": "documentation", "domain": "FastAPI/Flask/Django", "library": "FastAPI", "title": "FastAPI — Path Operations", "content": "The `@app.get` function in FastAPI registers a GET endpoint at the specified path. It accepts `path, response_model` as a parameter and returns Response. Common use cases include building REST API endpoints that return JSON data. Note that use response_model to enforce output schema validation.", "code_signature": "@app.get(path, response_model) -> Response", "tags": "orm, routing, dependency-injection, middleware, rest", "url": "https://docs.fastapi.org/en/stable/@app/get.html", "author": "Official Docs", "date_published": "06-08-2024", "votes_or_stars": 2845, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:0a2bbac6-e752-4dfe-98ee-87861362be89": {"content": "requests.exceptions.SSLError: certificate verify failed\nNever use `verify=False` in production — it disables SSL verification entirely. Solutions: (1) Update certifi: `pip install --upgrade certifi`. (2) Provide CA bundle: `requests.get(url, verify='/path/to/ca-bundle.crt')`. (3) For self-signed certs in dev: `verify=False` only, and never commit this. (4) Check system time — expired system clock causes SSL failures.\nCode signature: Your SSL certificate chain is incomplete or self-signed. Fix the cert, don't disable verification.", "file_path": "https://stackoverflow.com/questions/7907400", "function_name": "Your SSL certificate chain is incomplete or self-signed. Fix the cert, don't disable verification.", "type": "stack_overflow", "metadata": {"record_id": "0a2bbac6-e752-4dfe-98ee-87861362be89", "source_type": "stack_overflow", "domain": "Requests/HTTP/APIs", "library": "websocket", "title": "requests.exceptions.SSLError: certificate verify failed", "content": "Never use `verify=False` in production — it disables SSL verification entirely. Solutions: (1) Update certifi: `pip install --upgrade certifi`. (2) Provide CA bundle: `requests.get(url, verify='/path/to/ca-bundle.crt')`. (3) For self-signed certs in dev: `verify=False` only, and never commit this. (4) Check system time — expired system clock causes SSL failures.", "code_signature": "Your SSL certificate chain is incomplete or self-signed. Fix the cert, don't disable verification.", "tags": "retry, websocket, oauth, authentication", "url": "https://stackoverflow.com/questions/7907400", "author": "user_851204", "date_published": "15-11-2021", "votes_or_stars": 1398, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:9a7df6e1-8f23-474a-812e-be3c3b31007a": {"content": "subprocess — Process Execution\nThe `subprocess.run` function in subprocess runs a subprocess and waits for it to complete. It accepts `args, capture_output=False, timeout=None` as a parameter and returns CompletedProcess. Common use cases include executing shell commands, scripts, or external programs from Python. Note that use shell=False (default) to avoid shell injection vulnerabilities.\nCode signature: subprocess.run(args, capture_output=False, timeout=None) -> CompletedProcess", "file_path": "https://docs.subprocess.org/en/stable/subprocess/run.html", "function_name": "subprocess.run(args, capture_output=False, timeout=None) -> CompletedProcess", "type": "documentation", "metadata": {"record_id": "9a7df6e1-8f23-474a-812e-be3c3b31007a", "source_type": "documentation", "domain": "OS/Subprocess/File I/O", "library": "subprocess", "title": "subprocess — Process Execution", "content": "The `subprocess.run` function in subprocess runs a subprocess and waits for it to complete. It accepts `args, capture_output=False, timeout=None` as a parameter and returns CompletedProcess. Common use cases include executing shell commands, scripts, or external programs from Python. Note that use shell=False (default) to avoid shell injection vulnerabilities.", "code_signature": "subprocess.run(args, capture_output=False, timeout=None) -> CompletedProcess", "tags": "stderr, env-variable, process, os", "url": "https://docs.subprocess.org/en/stable/subprocess/run.html", "author": "Official Docs", "date_published": "09-12-2024", "votes_or_stars": 439, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:be4a87f3-aced-478f-8535-cd88f932316e": {"content": "asyncio RuntimeError: This event loop is already running\nThis error is common in Jupyter and FastAPI. Solutions: (1) In async context: just use `await coroutine()`. (2) In Jupyter: `import nest_asyncio; nest_asyncio.apply()`. (3) Run in thread: `asyncio.get_event_loop().run_in_executor(None, sync_func)`. (4) Use `asyncio.ensure_future()` to schedule from within async code.\nCode signature: You're calling asyncio.run() inside an already running event loop. Use await instead.", "file_path": "https://stackoverflow.com/questions/2229110", "function_name": "You're calling asyncio.run() inside an already running event loop. Use await instead.", "type": "stack_overflow", "metadata": {"record_id": "be4a87f3-aced-478f-8535-cd88f932316e", "source_type": "stack_overflow", "domain": "Asyncio/Threading", "library": "async", "title": "asyncio RuntimeError: This event loop is already running", "content": "This error is common in Jupyter and FastAPI. Solutions: (1) In async context: just use `await coroutine()`. (2) In Jupyter: `import nest_asyncio; nest_asyncio.apply()`. (3) Run in thread: `asyncio.get_event_loop().run_in_executor(None, sync_func)`. (4) Use `asyncio.ensure_future()` to schedule from within async code.", "code_signature": "You're calling asyncio.run() inside an already running event loop. Use await instead.", "tags": "await, event-loop, coroutine, lock", "url": "https://stackoverflow.com/questions/2229110", "author": "user_573750", "date_published": "10-09-2022", "votes_or_stars": 2316, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:317ad2eb-0d17-4dda-8ed0-2bfc913cfbc0": {"content": "asyncio — Concurrent Coroutines\nThe `asyncio.gather` function in asyncio runs multiple coroutines concurrently and collects results. It accepts `*coros, return_exceptions=False` as a parameter and returns list of results. Common use cases include parallel I/O operations such as multiple API calls. Note that set return_exceptions=True to prevent one failure from cancelling all tasks.\nCode signature: asyncio.gather(*coros, return_exceptions=False) -> list of results", "file_path": "https://docs.asyncio.org/en/stable/asyncio/gather.html", "function_name": "asyncio.gather(*coros, return_exceptions=False) -> list of results", "type": "documentation", "metadata": {"record_id": "317ad2eb-0d17-4dda-8ed0-2bfc913cfbc0", "source_type": "documentation", "domain": "Asyncio/Threading", "library": "asyncio", "title": "asyncio — Concurrent Coroutines", "content": "The `asyncio.gather` function in asyncio runs multiple coroutines concurrently and collects results. It accepts `*coros, return_exceptions=False` as a parameter and returns list of results. Common use cases include parallel I/O operations such as multiple API calls. Note that set return_exceptions=True to prevent one failure from cancelling all tasks.", "code_signature": "asyncio.gather(*coros, return_exceptions=False) -> list of results", "tags": "executor, race-condition, task, event-loop, threading", "url": "https://docs.asyncio.org/en/stable/asyncio/gather.html", "author": "Official Docs", "date_published": "22-09-2024", "votes_or_stars": 508, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:1cbd138e-25ee-4b03-b7b6-6f1071cba1c3": {"content": "NumPy — Linear Algebra\nThe `np.einsum` function in NumPy evaluates Einstein summation convention on operands. It accepts `subscripts, *operands` as a parameter and returns ndarray. Common use cases include matrix multiplication, dot products, tensor contractions. Note that prefer np.einsum over loops for large arrays.\nCode signature: np.einsum(subscripts, *operands) -> ndarray", "file_path": "https://docs.numpy.org/en/stable/np/einsum.html", "function_name": "np.einsum(subscripts, *operands) -> ndarray", "type": "documentation", "metadata": {"record_id": "1cbd138e-25ee-4b03-b7b6-6f1071cba1c3", "source_type": "documentation", "domain": "NumPy/Pandas", "library": "NumPy", "title": "NumPy — Linear Algebra", "content": "The `np.einsum` function in NumPy evaluates Einstein summation convention on operands. It accepts `subscripts, *operands` as a parameter and returns ndarray. Common use cases include matrix multiplication, dot products, tensor contractions. Note that prefer np.einsum over loops for large arrays.", "code_signature": "np.einsum(subscripts, *operands) -> ndarray", "tags": "reshape, merge, iloc, pandas, dtype, nan", "url": "https://docs.numpy.org/en/stable/np/einsum.html", "author": "Official Docs", "date_published": "09-02-2018", "votes_or_stars": 1891, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:6ac658f3-89ad-4d69-941d-be828d65d6e1": {"content": "requests.exceptions.SSLError: certificate verify failed\nNever use `verify=False` in production — it disables SSL verification entirely. Solutions: (1) Update certifi: `pip install --upgrade certifi`. (2) Provide CA bundle: `requests.get(url, verify='/path/to/ca-bundle.crt')`. (3) For self-signed certs in dev: `verify=False` only, and never commit this. (4) Check system time — expired system clock causes SSL failures.\nCode signature: Your SSL certificate chain is incomplete or self-signed. Fix the cert, don't disable verification.", "file_path": "https://stackoverflow.com/questions/7907400", "function_name": "Your SSL certificate chain is incomplete or self-signed. Fix the cert, don't disable verification.", "type": "stack_overflow", "metadata": {"record_id": "6ac658f3-89ad-4d69-941d-be828d65d6e1", "source_type": "stack_overflow", "domain": "Requests/HTTP/APIs", "library": "websocket", "title": "requests.exceptions.SSLError: certificate verify failed", "content": "Never use `verify=False` in production — it disables SSL verification entirely. Solutions: (1) Update certifi: `pip install --upgrade certifi`. (2) Provide CA bundle: `requests.get(url, verify='/path/to/ca-bundle.crt')`. (3) For self-signed certs in dev: `verify=False` only, and never commit this. (4) Check system time — expired system clock causes SSL failures.", "code_signature": "Your SSL certificate chain is incomplete or self-signed. Fix the cert, don't disable verification.", "tags": "rate-limiting, requests, http", "url": "https://stackoverflow.com/questions/7907400", "author": "user_851204", "date_published": "12-03-2022", "votes_or_stars": 1349, "relevance_label": "low", "difficulty_level": "advanced"}}, "kb:f644c190-9f7e-4e44-9392-4cc5512b4770": {"content": "BUG: httpx: SSL certificate error on Windows despite valid cert\nProblem: httpx raises SSLCertVerificationError on Windows when system CA store has the cert.\n\nFix/Discussion: httpx uses its own CA bundle (certifi) by default, not the system store. Fix: `httpx.Client(verify=certifi.where())` — already default. For custom certs: `httpx.Client(verify='/path/to/ca-bundle.pem')`. Install `pip install truststore` and use `ssl.create_default_context(ssl.Purpose.SERVER_AUTH)` to use system store.\nCode signature: status:closed", "file_path": "https://github.com/httpx/httpx/issues/8949", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "f644c190-9f7e-4e44-9392-4cc5512b4770", "source_type": "github_issue", "domain": "Requests/HTTP/APIs", "library": "httpx", "title": "BUG: httpx: SSL certificate error on Windows despite valid cert", "content": "Problem: httpx raises SSLCertVerificationError on Windows when system CA store has the cert.\n\nFix/Discussion: httpx uses its own CA bundle (certifi) by default, not the system store. Fix: `httpx.Client(verify=certifi.where())` — already default. For custom certs: `httpx.Client(verify='/path/to/ca-bundle.pem')`. Install `pip install truststore` and use `ssl.create_default_context(ssl.Purpose.SERVER_AUTH)` to use system store.", "code_signature": "status:closed", "tags": "aiohttp, retry, oauth, json, headers, websocket", "url": "https://github.com/httpx/httpx/issues/8949", "author": "contributor_1420", "date_published": "27-06-2024", "votes_or_stars": 468, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:34026b9b-531a-43c6-8ea0-f9cad8334dfa": {"content": "Python: how to safely write to a file atomically?\n```python\nimport tempfile, os\nwith tempfile.NamedTemporaryFile('w', delete=False, dir='.') as tmp:\n    tmp.write(data)\n    tmp.flush()\n    os.fsync(tmp.fileno())\nos.replace(tmp.name, target_path)\n```\n`os.replace()` is atomic on POSIX. This prevents partial writes from corrupting files.\nCode signature: Write to a temp file then rename — rename is atomic on POSIX systems.", "file_path": "https://stackoverflow.com/questions/5394880", "function_name": "Write to a temp file then rename — rename is atomic on POSIX systems.", "type": "stack_overflow", "metadata": {"record_id": "34026b9b-531a-43c6-8ea0-f9cad8334dfa", "source_type": "stack_overflow", "domain": "OS/Subprocess/File I/O", "library": "stdin", "title": "Python: how to safely write to a file atomically?", "content": "```python\nimport tempfile, os\nwith tempfile.NamedTemporaryFile('w', delete=False, dir='.') as tmp:\n    tmp.write(data)\n    tmp.flush()\n    os.fsync(tmp.fileno())\nos.replace(tmp.name, target_path)\n```\n`os.replace()` is atomic on POSIX. This prevents partial writes from corrupting files.", "code_signature": "Write to a temp file then rename — rename is atomic on POSIX systems.", "tags": "os, file-io, stdin", "url": "https://stackoverflow.com/questions/5394880", "author": "user_179430", "date_published": "14-08-2024", "votes_or_stars": 531, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:ba2b0233-152a-4dae-919a-b3909900801a": {"content": "How to watch a directory for file changes in Python?\n```python\nfrom watchdog.observers import Observer\nfrom watchdog.events import FileSystemEventHandler\nhandler = FileSystemEventHandler()\nobserver = Observer()\nobserver.schedule(handler, path='.', recursive=True)\nobserver.start()\n```\nFor simple polling: `os.stat(file).st_mtime`. Linux-native: `inotify`. macOS-native: `FSEvents`.\nCode signature: Use the watchdog library for cross-platform directory monitoring.", "file_path": "https://stackoverflow.com/questions/5213115", "function_name": "Use the watchdog library for cross-platform directory monitoring.", "type": "stack_overflow", "metadata": {"record_id": "ba2b0233-152a-4dae-919a-b3909900801a", "source_type": "stack_overflow", "domain": "OS/Subprocess/File I/O", "library": "env-variable", "title": "How to watch a directory for file changes in Python?", "content": "```python\nfrom watchdog.observers import Observer\nfrom watchdog.events import FileSystemEventHandler\nhandler = FileSystemEventHandler()\nobserver = Observer()\nobserver.schedule(handler, path='.', recursive=True)\nobserver.start()\n```\nFor simple polling: `os.stat(file).st_mtime`. Linux-native: `inotify`. macOS-native: `FSEvents`.", "code_signature": "Use the watchdog library for cross-platform directory monitoring.", "tags": "subprocess, tempfile, stderr, process, pathlib, os", "url": "https://stackoverflow.com/questions/5213115", "author": "user_959314", "date_published": "03-05-2023", "votes_or_stars": 2082, "relevance_label": "low", "difficulty_level": "beginner"}}, "kb:8758ff68-729a-48e9-9f3c-e5f89a2819f6": {"content": "NumPy — Vectorization\nThe `np.vectorize` function in NumPy generalizes a scalar function to operate on arrays. It accepts `pyfunc` as a parameter and returns vectorized function. Common use cases include applying Python functions element-wise without explicit loops. Note that np.vectorize is a convenience wrapper; it does not improve performance like true ufuncs.\nCode signature: np.vectorize(pyfunc) -> vectorized function", "file_path": "https://docs.numpy.org/en/stable/np/vectorize.html", "function_name": "np.vectorize(pyfunc) -> vectorized function", "type": "documentation", "metadata": {"record_id": "8758ff68-729a-48e9-9f3c-e5f89a2819f6", "source_type": "documentation", "domain": "NumPy/Pandas", "library": "NumPy", "title": "NumPy — Vectorization", "content": "The `np.vectorize` function in NumPy generalizes a scalar function to operate on arrays. It accepts `pyfunc` as a parameter and returns vectorized function. Common use cases include applying Python functions element-wise without explicit loops. Note that np.vectorize is a convenience wrapper; it does not improve performance like true ufuncs.", "code_signature": "np.vectorize(pyfunc) -> vectorized function", "tags": "reshape, numpy, dataframe, merge, groupby", "url": "https://docs.numpy.org/en/stable/np/vectorize.html", "author": "Official Docs", "date_published": "23-07-2021", "votes_or_stars": 1323, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:7d257eec-c7c8-47e9-a1be-a389a403246f": {"content": "How to use httpx.AsyncClient in httpx\n`httpx.httpx.AsyncClient` provides an async HTTP client for non-blocking requests. Example usage:\n```python\nasync with httpx.AsyncClient() as client:\n    r = await client.get(url)\n    data = r.json()\n```\nThis is useful when making concurrent HTTP requests in async applications. Performance tip: set timeout=httpx.Timeout(10.0) to prevent hanging requests.\nCode signature: httpx.AsyncClient", "file_path": "https://docs.httpx.org/en/stable/httpx/AsyncClient.html", "function_name": "httpx.AsyncClient", "type": "documentation", "metadata": {"record_id": "7d257eec-c7c8-47e9-a1be-a389a403246f", "source_type": "documentation", "domain": "Requests/HTTP/APIs", "library": "httpx", "title": "How to use httpx.AsyncClient in httpx", "content": "`httpx.httpx.AsyncClient` provides an async HTTP client for non-blocking requests. Example usage:\n```python\nasync with httpx.AsyncClient() as client:\n    r = await client.get(url)\n    data = r.json()\n```\nThis is useful when making concurrent HTTP requests in async applications. Performance tip: set timeout=httpx.Timeout(10.0) to prevent hanging requests.", "code_signature": "httpx.AsyncClient", "tags": "websocket, requests, oauth, authentication, rest-api", "url": "https://docs.httpx.org/en/stable/httpx/AsyncClient.html", "author": "Official Docs", "date_published": "30-05-2018", "votes_or_stars": 4226, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:51594211-b89b-43c8-9e4b-b128285cd467": {"content": "NumPy — Linear Algebra\nThe `np.einsum` function in NumPy evaluates Einstein summation convention on operands. It accepts `subscripts, *operands` as a parameter and returns ndarray. Common use cases include matrix multiplication, dot products, tensor contractions. Note that prefer np.einsum over loops for large arrays.\nCode signature: np.einsum(subscripts, *operands) -> ndarray", "file_path": "https://docs.numpy.org/en/stable/np/einsum.html", "function_name": "np.einsum(subscripts, *operands) -> ndarray", "type": "documentation", "metadata": {"record_id": "51594211-b89b-43c8-9e4b-b128285cd467", "source_type": "documentation", "domain": "NumPy/Pandas", "library": "NumPy", "title": "NumPy — Linear Algebra", "content": "The `np.einsum` function in NumPy evaluates Einstein summation convention on operands. It accepts `subscripts, *operands` as a parameter and returns ndarray. Common use cases include matrix multiplication, dot products, tensor contractions. Note that prefer np.einsum over loops for large arrays.", "code_signature": "np.einsum(subscripts, *operands) -> ndarray", "tags": "reshape, pandas, dataframe", "url": "https://docs.numpy.org/en/stable/np/einsum.html", "author": "Official Docs", "date_published": "06-02-2019", "votes_or_stars": 1872, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:4a622da2-9373-4007-85d5-9ad97d3491e4": {"content": "How to use @app.route in Flask\n`Flask.@app.route` maps a URL rule to a view function. Example usage:\n```python\n@app.route('/users/<int:uid>')\ndef get_user(uid):\n    return jsonify(user)\n```\nThis is useful when defining HTTP endpoints in a Flask application. Performance tip: use Blueprints to organize large applications.\nCode signature: @app.route", "file_path": "https://docs.flask.org/en/stable/@app/route.html", "function_name": "@app.route", "type": "documentation", "metadata": {"record_id": "4a622da2-9373-4007-85d5-9ad97d3491e4", "source_type": "documentation", "domain": "FastAPI/Flask/Django", "library": "Flask", "title": "How to use @app.route in Flask", "content": "`Flask.@app.route` maps a URL rule to a view function. Example usage:\n```python\n@app.route('/users/<int:uid>')\ndef get_user(uid):\n    return jsonify(user)\n```\nThis is useful when defining HTTP endpoints in a Flask application. Performance tip: use Blueprints to organize large applications.", "code_signature": "@app.route", "tags": "template, fastapi, django", "url": "https://docs.flask.org/en/stable/@app/route.html", "author": "Official Docs", "date_published": "19-07-2022", "votes_or_stars": 3006, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:eb8fdfa3-4b1a-4393-b8e9-e3525c3cd093": {"content": "FEAT: Django: add support for async ORM queries\nProblem: Django ORM is synchronous; using it in async views requires sync_to_async wrapper.\n\nFix/Discussion: Django 4.1+ added native async ORM support. Use `await Model.objects.filter().afirst()`, `async for obj in qs:`. For older versions: wrap with `sync_to_async`: `get_user = sync_to_async(User.objects.get)\nuser = await get_user(pk=1)`.\nCode signature: status:closed", "file_path": "https://github.com/django/django/issues/8618", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "eb8fdfa3-4b1a-4393-b8e9-e3525c3cd093", "source_type": "github_issue", "domain": "FastAPI/Flask/Django", "library": "django", "title": "FEAT: Django: add support for async ORM queries", "content": "Problem: Django ORM is synchronous; using it in async views requires sync_to_async wrapper.\n\nFix/Discussion: Django 4.1+ added native async ORM support. Use `await Model.objects.filter().afirst()`, `async for obj in qs:`. For older versions: wrap with `sync_to_async`: `get_user = sync_to_async(User.objects.get)\nuser = await get_user(pk=1)`.", "code_signature": "status:closed", "tags": "orm, pydantic, rest, routing, template", "url": "https://github.com/django/django/issues/8618", "author": "contributor_8921", "date_published": "09-02-2020", "votes_or_stars": 1, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:f6ce017a-2eeb-4f13-8440-030141f8c09b": {"content": "How to limit concurrency with asyncio?\n```python\nsem = asyncio.Semaphore(10)\nasync def limited(url):\n    async with sem:\n        return await fetch(url)\nresults = await asyncio.gather(*[limited(u) for u in urls])\n```\nThis prevents overwhelming APIs or databases with too many simultaneous connections.\nCode signature: Use asyncio.Semaphore to cap the number of concurrent coroutines.", "file_path": "https://stackoverflow.com/questions/2375453", "function_name": "Use asyncio.Semaphore to cap the number of concurrent coroutines.", "type": "stack_overflow", "metadata": {"record_id": "f6ce017a-2eeb-4f13-8440-030141f8c09b", "source_type": "stack_overflow", "domain": "Asyncio/Threading", "library": "asyncio", "title": "How to limit concurrency with asyncio?", "content": "```python\nsem = asyncio.Semaphore(10)\nasync def limited(url):\n    async with sem:\n        return await fetch(url)\nresults = await asyncio.gather(*[limited(u) for u in urls])\n```\nThis prevents overwhelming APIs or databases with too many simultaneous connections.", "code_signature": "Use asyncio.Semaphore to cap the number of concurrent coroutines.", "tags": "threading, semaphore, gather, race-condition", "url": "https://stackoverflow.com/questions/2375453", "author": "user_449589", "date_published": "14-06-2022", "votes_or_stars": 2455, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:ceab5fb0-8ce0-425d-8ff7-7d8c5b6b1938": {"content": "BUG: requests hangs indefinitely on slow server responses\nProblem: requests.get() hangs forever when server accepts connection but never sends response.\n\nFix/Discussion: Always set both connect and read timeouts: `requests.get(url, timeout=(3.05, 27))`. Tuple: (connect_timeout, read_timeout). Or single value for both. Mount a Session with default timeouts using a custom HTTPAdapter subclass. Never use `timeout=None` in production code.\nCode signature: status:closed", "file_path": "https://github.com/requests/requests/issues/4019", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "ceab5fb0-8ce0-425d-8ff7-7d8c5b6b1938", "source_type": "github_issue", "domain": "Requests/HTTP/APIs", "library": "requests", "title": "BUG: requests hangs indefinitely on slow server responses", "content": "Problem: requests.get() hangs forever when server accepts connection but never sends response.\n\nFix/Discussion: Always set both connect and read timeouts: `requests.get(url, timeout=(3.05, 27))`. Tuple: (connect_timeout, read_timeout). Or single value for both. Mount a Session with default timeouts using a custom HTTPAdapter subclass. Never use `timeout=None` in production code.", "code_signature": "status:closed", "tags": "httpx, oauth, headers, http, aiohttp, rest-api", "url": "https://github.com/requests/requests/issues/4019", "author": "contributor_4599", "date_published": "16-11-2019", "votes_or_stars": 236, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:904dd744-672e-4284-a122-ac444d2b8136": {"content": "BUG: FastAPI dependency_overrides not working in pytest\nProblem: Dependency overrides set in conftest are not applied during test execution.\n\nFix/Discussion: Set overrides on the app object before TestClient is created. Use `app.dependency_overrides[dep] = mock_dep` in a pytest fixture with function scope. Clear overrides after test: `app.dependency_overrides = {}`. Ensure TestClient is created after overrides are set.\nCode signature: status:closed", "file_path": "https://github.com/fastapi/fastapi/issues/8479", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "904dd744-672e-4284-a122-ac444d2b8136", "source_type": "github_issue", "domain": "FastAPI/Flask/Django", "library": "fastapi", "title": "BUG: FastAPI dependency_overrides not working in pytest", "content": "Problem: Dependency overrides set in conftest are not applied during test execution.\n\nFix/Discussion: Set overrides on the app object before TestClient is created. Use `app.dependency_overrides[dep] = mock_dep` in a pytest fixture with function scope. Clear overrides after test: `app.dependency_overrides = {}`. Ensure TestClient is created after overrides are set.", "code_signature": "status:closed", "tags": "routing, middleware, template, blueprint, orm, rest", "url": "https://github.com/fastapi/fastapi/issues/8479", "author": "contributor_1994", "date_published": "10-11-2018", "votes_or_stars": 327, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:29bcb491-deef-4c22-bb92-5faedcd03573": {"content": "PERF: groupby().apply() extremely slow on large DataFrames\nProblem: groupby().apply() with a custom function is 100x slower than expected on 10M row DataFrames.\n\nFix/Discussion: groupby().apply() serializes each group and has high overhead. Fix: vectorize with numpy, use groupby().transform() for same-shape output, or groupby().agg() with named aggregations. For complex logic, consider Polars or Dask. Numba's @jit can accelerate custom group functions.\nCode signature: status:closed", "file_path": "https://github.com/pandas/pandas/issues/8446", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "29bcb491-deef-4c22-bb92-5faedcd03573", "source_type": "github_issue", "domain": "NumPy/Pandas", "library": "pandas", "title": "PERF: groupby().apply() extremely slow on large DataFrames", "content": "Problem: groupby().apply() with a custom function is 100x slower than expected on 10M row DataFrames.\n\nFix/Discussion: groupby().apply() serializes each group and has high overhead. Fix: vectorize with numpy, use groupby().transform() for same-shape output, or groupby().agg() with named aggregations. For complex logic, consider Polars or Dask. Numba's @jit can accelerate custom group functions.", "code_signature": "status:closed", "tags": "pandas, nan, vectorization", "url": "https://github.com/pandas/pandas/issues/8446", "author": "contributor_6648", "date_published": "10-09-2020", "votes_or_stars": 363, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:44ab7b84-a41f-455a-9d8e-64caa1ad5f81": {"content": "Why is my Pandas merge creating duplicate rows?\nDuplicate rows after merge usually mean the join key is not unique. Diagnose with `df.duplicated(subset=['key']).sum()`. Use `how='left'` carefully when right table has multiple matches. Set `validate='one_to_many'` or `'many_to_one'` to enforce cardinality. Consider deduplicating with `df.drop_duplicates(subset=['key'])` before merging.\nCode signature: Your merge key has duplicates in one or both DataFrames. Use validate='one_to_one' to catch this early.", "file_path": "https://stackoverflow.com/questions/2321324", "function_name": "Your merge key has duplicates in one or both DataFrames. Use validate='one_to_one' to catch this early.", "type": "stack_overflow", "metadata": {"record_id": "44ab7b84-a41f-455a-9d8e-64caa1ad5f81", "source_type": "stack_overflow", "domain": "NumPy/Pandas", "library": "array", "title": "Why is my Pandas merge creating duplicate rows?", "content": "Duplicate rows after merge usually mean the join key is not unique. Diagnose with `df.duplicated(subset=['key']).sum()`. Use `how='left'` carefully when right table has multiple matches. Set `validate='one_to_many'` or `'many_to_one'` to enforce cardinality. Consider deduplicating with `df.drop_duplicates(subset=['key'])` before merging.", "code_signature": "Your merge key has duplicates in one or both DataFrames. Use validate='one_to_one' to catch this early.", "tags": "dtype, reshape, numpy", "url": "https://stackoverflow.com/questions/2321324", "author": "user_99814", "date_published": "24-05-2019", "votes_or_stars": 251, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:82526b4f-24c6-4a07-a214-9eccb9b5fa91": {"content": "NumPy — Linear Algebra\nThe `np.einsum` function in NumPy evaluates Einstein summation convention on operands. It accepts `subscripts, *operands` as a parameter and returns ndarray. Common use cases include matrix multiplication, dot products, tensor contractions. Note that prefer np.einsum over loops for large arrays.\nCode signature: np.einsum(subscripts, *operands) -> ndarray", "file_path": "https://docs.numpy.org/en/stable/np/einsum.html", "function_name": "np.einsum(subscripts, *operands) -> ndarray", "type": "documentation", "metadata": {"record_id": "82526b4f-24c6-4a07-a214-9eccb9b5fa91", "source_type": "documentation", "domain": "NumPy/Pandas", "library": "NumPy", "title": "NumPy — Linear Algebra", "content": "The `np.einsum` function in NumPy evaluates Einstein summation convention on operands. It accepts `subscripts, *operands` as a parameter and returns ndarray. Common use cases include matrix multiplication, dot products, tensor contractions. Note that prefer np.einsum over loops for large arrays.", "code_signature": "np.einsum(subscripts, *operands) -> ndarray", "tags": "merge, groupby, pandas, dataframe, broadcasting", "url": "https://docs.numpy.org/en/stable/np/einsum.html", "author": "Official Docs", "date_published": "17-03-2022", "votes_or_stars": 1889, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:727ab28a-70c0-4fe8-8c8a-6fa5f36a2e3e": {"content": "How to implement a thread pool in Python?\n```python\nfrom concurrent.futures import ThreadPoolExecutor\nwith ThreadPoolExecutor(max_workers=8) as ex:\n    futures = [ex.submit(task, arg) for arg in args]\n    results = [f.result() for f in futures]\n```\nFor CPU-bound: use ProcessPoolExecutor instead. For async integration: `await loop.run_in_executor(executor, func, *args)`.\nCode signature: Use concurrent.futures.ThreadPoolExecutor for I/O-bound tasks.", "file_path": "https://stackoverflow.com/questions/3646552", "function_name": "Use concurrent.futures.ThreadPoolExecutor for I/O-bound tasks.", "type": "stack_overflow", "metadata": {"record_id": "727ab28a-70c0-4fe8-8c8a-6fa5f36a2e3e", "source_type": "stack_overflow", "domain": "Asyncio/Threading", "library": "coroutine", "title": "How to implement a thread pool in Python?", "content": "```python\nfrom concurrent.futures import ThreadPoolExecutor\nwith ThreadPoolExecutor(max_workers=8) as ex:\n    futures = [ex.submit(task, arg) for arg in args]\n    results = [f.result() for f in futures]\n```\nFor CPU-bound: use ProcessPoolExecutor instead. For async integration: `await loop.run_in_executor(executor, func, *args)`.", "code_signature": "Use concurrent.futures.ThreadPoolExecutor for I/O-bound tasks.", "tags": "semaphore, task, deadlock, event-loop, asyncio", "url": "https://stackoverflow.com/questions/3646552", "author": "user_469469", "date_published": "03-07-2023", "votes_or_stars": 1495, "relevance_label": "low", "difficulty_level": "intermediate"}}, "kb:9ac3164c-4d05-4220-b485-7c23ce0417a4": {"content": "How do I handle CORS in FastAPI?\n```python\nfrom fastapi.middleware.cors import CORSMiddleware\napp.add_middleware(CORSMiddleware, allow_origins=['*'], allow_methods=['*'], allow_headers=['*'])\n```\nIn production, replace `'*'` with your actual frontend domain. For credentials (cookies), set `allow_credentials=True` and specify exact origins.\nCode signature: Add CORSMiddleware from starlette to your FastAPI app.", "file_path": "https://stackoverflow.com/questions/1527021", "function_name": "Add CORSMiddleware from starlette to your FastAPI app.", "type": "stack_overflow", "metadata": {"record_id": "9ac3164c-4d05-4220-b485-7c23ce0417a4", "source_type": "stack_overflow", "domain": "FastAPI/Flask/Django", "library": "flask", "title": "How do I handle CORS in FastAPI?", "content": "```python\nfrom fastapi.middleware.cors import CORSMiddleware\napp.add_middleware(CORSMiddleware, allow_origins=['*'], allow_methods=['*'], allow_headers=['*'])\n```\nIn production, replace `'*'` with your actual frontend domain. For credentials (cookies), set `allow_credentials=True` and specify exact origins.", "code_signature": "Add CORSMiddleware from starlette to your FastAPI app.", "tags": "blueprint, pydantic, flask, response", "url": "https://stackoverflow.com/questions/1527021", "author": "user_911393", "date_published": "09-11-2024", "votes_or_stars": 918, "relevance_label": "low", "difficulty_level": "advanced"}}, "kb:ae3b1086-da53-44f6-b959-b78f705dda11": {"content": "BUG: subprocess.Popen hangs when stdout and stderr buffers fill\nProblem: subprocess.Popen hangs indefinitely when child process produces large output.\n\nFix/Discussion: This is a classic deadlock: parent waits for child, child waits for parent to read buffer. Fix: use `subprocess.run(..., capture_output=True)` which handles this correctly. Or: `stdout, stderr = proc.communicate()`. Never use `proc.stdout.read()` and `proc.wait()` separately.\nCode signature: status:closed", "file_path": "https://github.com/cpython/cpython/issues/8586", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "ae3b1086-da53-44f6-b959-b78f705dda11", "source_type": "github_issue", "domain": "OS/Subprocess/File I/O", "library": "cpython", "title": "BUG: subprocess.Popen hangs when stdout and stderr buffers fill", "content": "Problem: subprocess.Popen hangs indefinitely when child process produces large output.\n\nFix/Discussion: This is a classic deadlock: parent waits for child, child waits for parent to read buffer. Fix: use `subprocess.run(..., capture_output=True)` which handles this correctly. Or: `stdout, stderr = proc.communicate()`. Never use `proc.stdout.read()` and `proc.wait()` separately.", "code_signature": "status:closed", "tags": "pipe, env-variable, tempfile", "url": "https://github.com/cpython/cpython/issues/8586", "author": "contributor_7711", "date_published": "05-12-2024", "votes_or_stars": 440, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:05a6479f-2e8e-4fdd-9178-a6f2b57b70f1": {"content": "BUG: requests hangs indefinitely on slow server responses\nProblem: requests.get() hangs forever when server accepts connection but never sends response.\n\nFix/Discussion: Always set both connect and read timeouts: `requests.get(url, timeout=(3.05, 27))`. Tuple: (connect_timeout, read_timeout). Or single value for both. Mount a Session with default timeouts using a custom HTTPAdapter subclass. Never use `timeout=None` in production code.\nCode signature: status:closed", "file_path": "https://github.com/requests/requests/issues/4019", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "05a6479f-2e8e-4fdd-9178-a6f2b57b70f1", "source_type": "github_issue", "domain": "Requests/HTTP/APIs", "library": "requests", "title": "BUG: requests hangs indefinitely on slow server responses", "content": "Problem: requests.get() hangs forever when server accepts connection but never sends response.\n\nFix/Discussion: Always set both connect and read timeouts: `requests.get(url, timeout=(3.05, 27))`. Tuple: (connect_timeout, read_timeout). Or single value for both. Mount a Session with default timeouts using a custom HTTPAdapter subclass. Never use `timeout=None` in production code.", "code_signature": "status:closed", "tags": "timeout, oauth, retry", "url": "https://github.com/requests/requests/issues/4019", "author": "contributor_4599", "date_published": "20-07-2022", "votes_or_stars": 275, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:21ac6555-c2b3-40e0-8b7b-97706e8c2e3a": {"content": "BUG: Flask session not persisting between requests in tests\nProblem: Session data set in one request is not available in the next request during testing.\n\nFix/Discussion: Use Flask test client's session_transaction() context manager: `with client.session_transaction() as sess:\n    sess['user_id'] = 1`. Ensure SECRET_KEY is set. For testing: `app.config['TESTING'] = True; app.config['SECRET_KEY'] = 'test'`.\nCode signature: status:closed", "file_path": "https://github.com/flask/flask/issues/2141", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "21ac6555-c2b3-40e0-8b7b-97706e8c2e3a", "source_type": "github_issue", "domain": "FastAPI/Flask/Django", "library": "flask", "title": "BUG: Flask session not persisting between requests in tests", "content": "Problem: Session data set in one request is not available in the next request during testing.\n\nFix/Discussion: Use Flask test client's session_transaction() context manager: `with client.session_transaction() as sess:\n    sess['user_id'] = 1`. Ensure SECRET_KEY is set. For testing: `app.config['TESTING'] = True; app.config['SECRET_KEY'] = 'test'`.", "code_signature": "status:closed", "tags": "pydantic, dependency-injection, orm, fastapi, django", "url": "https://github.com/flask/flask/issues/2141", "author": "contributor_5020", "date_published": "25-10-2019", "votes_or_stars": 141, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:c8a6db61-6368-4231-b04a-28fe31638e97": {"content": "BUG: requests hangs indefinitely on slow server responses\nProblem: requests.get() hangs forever when server accepts connection but never sends response.\n\nFix/Discussion: Always set both connect and read timeouts: `requests.get(url, timeout=(3.05, 27))`. Tuple: (connect_timeout, read_timeout). Or single value for both. Mount a Session with default timeouts using a custom HTTPAdapter subclass. Never use `timeout=None` in production code.\nCode signature: status:closed", "file_path": "https://github.com/requests/requests/issues/4019", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "c8a6db61-6368-4231-b04a-28fe31638e97", "source_type": "github_issue", "domain": "Requests/HTTP/APIs", "library": "requests", "title": "BUG: requests hangs indefinitely on slow server responses", "content": "Problem: requests.get() hangs forever when server accepts connection but never sends response.\n\nFix/Discussion: Always set both connect and read timeouts: `requests.get(url, timeout=(3.05, 27))`. Tuple: (connect_timeout, read_timeout). Or single value for both. Mount a Session with default timeouts using a custom HTTPAdapter subclass. Never use `timeout=None` in production code.", "code_signature": "status:closed", "tags": "headers, requests, websocket, rate-limiting", "url": "https://github.com/requests/requests/issues/4019", "author": "contributor_4599", "date_published": "13-10-2018", "votes_or_stars": 228, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:c810202a-0d3b-4dba-acd5-135e1c706cf1": {"content": "How to implement database connection pooling with SQLAlchemy?\n`create_engine(url, pool_size=10, max_overflow=20, pool_timeout=30, pool_recycle=1800)`. Use `pool_pre_ping=True` to detect stale connections. For async: use `AsyncEngine` with `create_async_engine()`. Monitor with `engine.pool.status()`. NullPool disables pooling for scripts/serverless environments.\nCode signature: create_engine() has built-in pooling. Configure pool_size and max_overflow.", "file_path": "https://stackoverflow.com/questions/7358113", "function_name": "create_engine() has built-in pooling. Configure pool_size and max_overflow.", "type": "stack_overflow", "metadata": {"record_id": "c810202a-0d3b-4dba-acd5-135e1c706cf1", "source_type": "stack_overflow", "domain": "SQLAlchemy/Databases", "library": "session", "title": "How to implement database connection pooling with SQLAlchemy?", "content": "`create_engine(url, pool_size=10, max_overflow=20, pool_timeout=30, pool_recycle=1800)`. Use `pool_pre_ping=True` to detect stale connections. For async: use `AsyncEngine` with `create_async_engine()`. Monitor with `engine.pool.status()`. NullPool disables pooling for scripts/serverless environments.", "code_signature": "create_engine() has built-in pooling. Configure pool_size and max_overflow.", "tags": "orm, transaction, query, foreign-key", "url": "https://stackoverflow.com/questions/7358113", "author": "user_12260", "date_published": "07-01-2020", "votes_or_stars": 256, "relevance_label": "low", "difficulty_level": "beginner"}}, "kb:eef5d852-5b41-4b36-8cf4-ede1ff28b224": {"content": "Pandas Merging DataFrames — official reference\nOfficial documentation for Pandas Merging DataFrames. The method signature is `pd.merge(left, right, on, how='inner')`. merges two DataFrames using SQL-style joins. Raises `MergeError` when merge keys produce duplicate columns. See also: df.join, df.concat.\nCode signature: pd.merge(left, right, on, how='inner')", "file_path": "https://docs.pandas.org/en/stable/pd/merge.html", "function_name": "pd.merge(left, right, on, how='inner')", "type": "documentation", "metadata": {"record_id": "eef5d852-5b41-4b36-8cf4-ede1ff28b224", "source_type": "documentation", "domain": "NumPy/Pandas", "library": "Pandas", "title": "Pandas Merging DataFrames — official reference", "content": "Official documentation for Pandas Merging DataFrames. The method signature is `pd.merge(left, right, on, how='inner')`. merges two DataFrames using SQL-style joins. Raises `MergeError` when merge keys produce duplicate columns. See also: df.join, df.concat.", "code_signature": "pd.merge(left, right, on, how='inner')", "tags": "loc, dtype, pandas, nan", "url": "https://docs.pandas.org/en/stable/pd/merge.html", "author": "Official Docs", "date_published": "08-04-2018", "votes_or_stars": 4523, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:a99f45eb-d38e-4715-a920-68f0f927ae94": {"content": "How to read a large file line by line in Python without loading it all into memory?\n```python\nwith open('large.txt', 'r') as f:\n    for line in f:\n        process(line.strip())\n```\nThis reads one line at a time. For binary files: use `f.read(chunk_size)` in a loop. For CSV: use `pd.read_csv(file, chunksize=10000)`. Never use `f.readlines()` on large files.\nCode signature: Iterate over the file object directly — it yields lines lazily.", "file_path": "https://stackoverflow.com/questions/2737893", "function_name": "Iterate over the file object directly — it yields lines lazily.", "type": "stack_overflow", "metadata": {"record_id": "a99f45eb-d38e-4715-a920-68f0f927ae94", "source_type": "stack_overflow", "domain": "OS/Subprocess/File I/O", "library": "shutil", "title": "How to read a large file line by line in Python without loading it all into memory?", "content": "```python\nwith open('large.txt', 'r') as f:\n    for line in f:\n        process(line.strip())\n```\nThis reads one line at a time. For binary files: use `f.read(chunk_size)` in a loop. For CSV: use `pd.read_csv(file, chunksize=10000)`. Never use `f.readlines()` on large files.", "code_signature": "Iterate over the file object directly — it yields lines lazily.", "tags": "stdout, file-io, stdin, pipe, glob", "url": "https://stackoverflow.com/questions/2737893", "author": "user_994539", "date_published": "31-12-2020", "votes_or_stars": 2254, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:d2f079f0-0717-494e-b70b-97156f6d92bd": {"content": "How to implement database connection pooling with SQLAlchemy?\n`create_engine(url, pool_size=10, max_overflow=20, pool_timeout=30, pool_recycle=1800)`. Use `pool_pre_ping=True` to detect stale connections. For async: use `AsyncEngine` with `create_async_engine()`. Monitor with `engine.pool.status()`. NullPool disables pooling for scripts/serverless environments.\nCode signature: create_engine() has built-in pooling. Configure pool_size and max_overflow.", "file_path": "https://stackoverflow.com/questions/7358113", "function_name": "create_engine() has built-in pooling. Configure pool_size and max_overflow.", "type": "stack_overflow", "metadata": {"record_id": "d2f079f0-0717-494e-b70b-97156f6d92bd", "source_type": "stack_overflow", "domain": "SQLAlchemy/Databases", "library": "session", "title": "How to implement database connection pooling with SQLAlchemy?", "content": "`create_engine(url, pool_size=10, max_overflow=20, pool_timeout=30, pool_recycle=1800)`. Use `pool_pre_ping=True` to detect stale connections. For async: use `AsyncEngine` with `create_async_engine()`. Monitor with `engine.pool.status()`. NullPool disables pooling for scripts/serverless environments.", "code_signature": "create_engine() has built-in pooling. Configure pool_size and max_overflow.", "tags": "sqlalchemy, foreign-key, session, orm, transaction", "url": "https://stackoverflow.com/questions/7358113", "author": "user_12260", "date_published": "15-05-2020", "votes_or_stars": 278, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:a003b14a-d39b-4332-9d9c-1830595bb3e0": {"content": "Flask: how to return JSON responses properly?\n`return jsonify({'key': 'value'})` sets Content-Type to application/json. Flask 1.0+ automatically converts returned dicts to JSON responses. For custom status codes: `return jsonify(data), 201`. For errors: use `abort(404)` or return `({'error': 'Not found'}, 404)`.\nCode signature: Use jsonify() or return a dict directly (Flask 1.0+).", "file_path": "https://stackoverflow.com/questions/8930103", "function_name": "Use jsonify() or return a dict directly (Flask 1.0+).", "type": "stack_overflow", "metadata": {"record_id": "a003b14a-d39b-4332-9d9c-1830595bb3e0", "source_type": "stack_overflow", "domain": "FastAPI/Flask/Django", "library": "django", "title": "Flask: how to return JSON responses properly?", "content": "`return jsonify({'key': 'value'})` sets Content-Type to application/json. Flask 1.0+ automatically converts returned dicts to JSON responses. For custom status codes: `return jsonify(data), 201`. For errors: use `abort(404)` or return `({'error': 'Not found'}, 404)`.", "code_signature": "Use jsonify() or return a dict directly (Flask 1.0+).", "tags": "response, pydantic, template, django", "url": "https://stackoverflow.com/questions/8930103", "author": "user_264801", "date_published": "15-06-2024", "votes_or_stars": 2194, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:e6b5413e-346a-48c0-80d4-d1e5ab497531": {"content": "How to add retry logic to requests?\n```python\nfrom requests.adapters import HTTPAdapter\nfrom urllib3.util.retry import Retry\nretry = Retry(total=3, backoff_factor=1, status_forcelist=[500,502,503,504])\nadapter = HTTPAdapter(max_retries=retry)\nsession.mount('https://', adapter)\n```\nCode signature: Mount an HTTPAdapter with urllib3 Retry on the Session.", "file_path": "https://stackoverflow.com/questions/1669324", "function_name": "Mount an HTTPAdapter with urllib3 Retry on the Session.", "type": "stack_overflow", "metadata": {"record_id": "e6b5413e-346a-48c0-80d4-d1e5ab497531", "source_type": "stack_overflow", "domain": "Requests/HTTP/APIs", "library": "websocket", "title": "How to add retry logic to requests?", "content": "```python\nfrom requests.adapters import HTTPAdapter\nfrom urllib3.util.retry import Retry\nretry = Retry(total=3, backoff_factor=1, status_forcelist=[500,502,503,504])\nadapter = HTTPAdapter(max_retries=retry)\nsession.mount('https://', adapter)\n```", "code_signature": "Mount an HTTPAdapter with urllib3 Retry on the Session.", "tags": "websocket, json, timeout, authentication", "url": "https://stackoverflow.com/questions/1669324", "author": "user_952590", "date_published": "25-04-2021", "votes_or_stars": 1503, "relevance_label": "low", "difficulty_level": "intermediate"}}, "kb:e82e7f8c-6263-4194-b7a6-c09a6bd86d6c": {"content": "Pandas Merging DataFrames — official reference\nOfficial documentation for Pandas Merging DataFrames. The method signature is `pd.merge(left, right, on, how='inner')`. merges two DataFrames using SQL-style joins. Raises `MergeError` when merge keys produce duplicate columns. See also: df.join, df.concat.\nCode signature: pd.merge(left, right, on, how='inner')", "file_path": "https://docs.pandas.org/en/stable/pd/merge.html", "function_name": "pd.merge(left, right, on, how='inner')", "type": "documentation", "metadata": {"record_id": "e82e7f8c-6263-4194-b7a6-c09a6bd86d6c", "source_type": "documentation", "domain": "NumPy/Pandas", "library": "Pandas", "title": "Pandas Merging DataFrames — official reference", "content": "Official documentation for Pandas Merging DataFrames. The method signature is `pd.merge(left, right, on, how='inner')`. merges two DataFrames using SQL-style joins. Raises `MergeError` when merge keys produce duplicate columns. See also: df.join, df.concat.", "code_signature": "pd.merge(left, right, on, how='inner')", "tags": "groupby, merge, loc, numpy, nan", "url": "https://docs.pandas.org/en/stable/pd/merge.html", "author": "Official Docs", "date_published": "02-06-2019", "votes_or_stars": 4488, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:6345c90b-4432-445c-9644-4a549db54988": {"content": "Django N+1 query problem: how to fix?\nN+1 occurs when accessing related objects in a loop. Diagnose with `django-debug-toolbar` or `connection.queries`. Fix ForeignKey: `User.objects.select_related('profile').all()`. Fix M2M: `Post.objects.prefetch_related('tags').all()`. Use `only()` to fetch specific fields and reduce data transfer.\nCode signature: Use select_related() for ForeignKey and prefetch_related() for ManyToMany relationships.", "file_path": "https://stackoverflow.com/questions/8106470", "function_name": "Use select_related() for ForeignKey and prefetch_related() for ManyToMany relationships.", "type": "stack_overflow", "metadata": {"record_id": "6345c90b-4432-445c-9644-4a549db54988", "source_type": "stack_overflow", "domain": "FastAPI/Flask/Django", "library": "request", "title": "Django N+1 query problem: how to fix?", "content": "N+1 occurs when accessing related objects in a loop. Diagnose with `django-debug-toolbar` or `connection.queries`. Fix ForeignKey: `User.objects.select_related('profile').all()`. Fix M2M: `Post.objects.prefetch_related('tags').all()`. Use `only()` to fetch specific fields and reduce data transfer.", "code_signature": "Use select_related() for ForeignKey and prefetch_related() for ManyToMany relationships.", "tags": "request, template, middleware, response, fastapi", "url": "https://stackoverflow.com/questions/8106470", "author": "user_441071", "date_published": "29-05-2023", "votes_or_stars": 1752, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:7b2c720d-ddcb-451b-a6d8-eac513d9e8a4": {"content": "PERF: concurrent.futures.ProcessPoolExecutor slow startup on Windows\nProblem: ProcessPoolExecutor takes 3-5 seconds to start on Windows due to process spawning overhead.\n\nFix/Discussion: Windows uses 'spawn' start method (vs 'fork' on Linux). Mitigations: (1) Reuse executor across calls — don't create/destroy in loops. (2) Use 'forkserver' or 'fork' on Linux/macOS. (3) For short tasks, ThreadPoolExecutor may be faster. (4) Use `initializer` param to pre-load modules.\nCode signature: status:closed", "file_path": "https://github.com/cpython/cpython/issues/6647", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "7b2c720d-ddcb-451b-a6d8-eac513d9e8a4", "source_type": "github_issue", "domain": "Asyncio/Threading", "library": "cpython", "title": "PERF: concurrent.futures.ProcessPoolExecutor slow startup on Windows", "content": "Problem: ProcessPoolExecutor takes 3-5 seconds to start on Windows due to process spawning overhead.\n\nFix/Discussion: Windows uses 'spawn' start method (vs 'fork' on Linux). Mitigations: (1) Reuse executor across calls — don't create/destroy in loops. (2) Use 'forkserver' or 'fork' on Linux/macOS. (3) For short tasks, ThreadPoolExecutor may be faster. (4) Use `initializer` param to pre-load modules.", "code_signature": "status:closed", "tags": "gather, semaphore, event-loop, race-condition", "url": "https://github.com/cpython/cpython/issues/6647", "author": "contributor_4097", "date_published": "18-03-2018", "votes_or_stars": 268, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:36e1f9fb-59a8-4f80-9fdf-4faaa2cfb7ad": {"content": "PERF: requests connection not reused between calls — new TCP connection each time\nProblem: Each requests.get() call opens a new TCP connection, causing high latency.\n\nFix/Discussion: requests.get/post etc. create a new Session per call. Fix: use a persistent Session: `session = requests.Session()` and reuse it. Sessions pool connections via urllib3. For thread-safety: create one Session per thread. For async: use httpx.AsyncClient or aiohttp.ClientSession for async connection pooling.\nCode signature: status:closed", "file_path": "https://github.com/requests/requests/issues/3605", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "36e1f9fb-59a8-4f80-9fdf-4faaa2cfb7ad", "source_type": "github_issue", "domain": "Requests/HTTP/APIs", "library": "requests", "title": "PERF: requests connection not reused between calls — new TCP connection each time", "content": "Problem: Each requests.get() call opens a new TCP connection, causing high latency.\n\nFix/Discussion: requests.get/post etc. create a new Session per call. Fix: use a persistent Session: `session = requests.Session()` and reuse it. Sessions pool connections via urllib3. For thread-safety: create one Session per thread. For async: use httpx.AsyncClient or aiohttp.ClientSession for async connection pooling.", "code_signature": "status:closed", "tags": "headers, rate-limiting, aiohttp", "url": "https://github.com/requests/requests/issues/3605", "author": "contributor_1152", "date_published": "27-08-2023", "votes_or_stars": 366, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:47e31298-9f38-493a-b625-d9d014c76ca6": {"content": "PERF: groupby().apply() extremely slow on large DataFrames\nProblem: groupby().apply() with a custom function is 100x slower than expected on 10M row DataFrames.\n\nFix/Discussion: groupby().apply() serializes each group and has high overhead. Fix: vectorize with numpy, use groupby().transform() for same-shape output, or groupby().agg() with named aggregations. For complex logic, consider Polars or Dask. Numba's @jit can accelerate custom group functions.\nCode signature: status:closed", "file_path": "https://github.com/pandas/pandas/issues/8446", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "47e31298-9f38-493a-b625-d9d014c76ca6", "source_type": "github_issue", "domain": "NumPy/Pandas", "library": "pandas", "title": "PERF: groupby().apply() extremely slow on large DataFrames", "content": "Problem: groupby().apply() with a custom function is 100x slower than expected on 10M row DataFrames.\n\nFix/Discussion: groupby().apply() serializes each group and has high overhead. Fix: vectorize with numpy, use groupby().transform() for same-shape output, or groupby().agg() with named aggregations. For complex logic, consider Polars or Dask. Numba's @jit can accelerate custom group functions.", "code_signature": "status:closed", "tags": "iloc, vectorization, broadcasting, pivot, array, dataframe", "url": "https://github.com/pandas/pandas/issues/8446", "author": "contributor_6648", "date_published": "06-07-2019", "votes_or_stars": 319, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:92bf3772-53ff-461a-a16d-f4e1b2215067": {"content": "PERF: groupby().apply() extremely slow on large DataFrames\nProblem: groupby().apply() with a custom function is 100x slower than expected on 10M row DataFrames.\n\nFix/Discussion: groupby().apply() serializes each group and has high overhead. Fix: vectorize with numpy, use groupby().transform() for same-shape output, or groupby().agg() with named aggregations. For complex logic, consider Polars or Dask. Numba's @jit can accelerate custom group functions.\nCode signature: status:closed", "file_path": "https://github.com/pandas/pandas/issues/8446", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "92bf3772-53ff-461a-a16d-f4e1b2215067", "source_type": "github_issue", "domain": "NumPy/Pandas", "library": "pandas", "title": "PERF: groupby().apply() extremely slow on large DataFrames", "content": "Problem: groupby().apply() with a custom function is 100x slower than expected on 10M row DataFrames.\n\nFix/Discussion: groupby().apply() serializes each group and has high overhead. Fix: vectorize with numpy, use groupby().transform() for same-shape output, or groupby().agg() with named aggregations. For complex logic, consider Polars or Dask. Numba's @jit can accelerate custom group functions.", "code_signature": "status:closed", "tags": "reshape, vectorization, groupby", "url": "https://github.com/pandas/pandas/issues/8446", "author": "contributor_6648", "date_published": "18-09-2022", "votes_or_stars": 333, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:ac797da4-5e01-4c13-a295-d2d020176a93": {"content": "Django N+1 query problem: how to fix?\nN+1 occurs when accessing related objects in a loop. Diagnose with `django-debug-toolbar` or `connection.queries`. Fix ForeignKey: `User.objects.select_related('profile').all()`. Fix M2M: `Post.objects.prefetch_related('tags').all()`. Use `only()` to fetch specific fields and reduce data transfer.\nCode signature: Use select_related() for ForeignKey and prefetch_related() for ManyToMany relationships.", "file_path": "https://stackoverflow.com/questions/8106470", "function_name": "Use select_related() for ForeignKey and prefetch_related() for ManyToMany relationships.", "type": "stack_overflow", "metadata": {"record_id": "ac797da4-5e01-4c13-a295-d2d020176a93", "source_type": "stack_overflow", "domain": "FastAPI/Flask/Django", "library": "request", "title": "Django N+1 query problem: how to fix?", "content": "N+1 occurs when accessing related objects in a loop. Diagnose with `django-debug-toolbar` or `connection.queries`. Fix ForeignKey: `User.objects.select_related('profile').all()`. Fix M2M: `Post.objects.prefetch_related('tags').all()`. Use `only()` to fetch specific fields and reduce data transfer.", "code_signature": "Use select_related() for ForeignKey and prefetch_related() for ManyToMany relationships.", "tags": "orm, dependency-injection, django, rest", "url": "https://stackoverflow.com/questions/8106470", "author": "user_441071", "date_published": "22-02-2021", "votes_or_stars": 1797, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:af7435ad-fadb-4b2f-8083-c9137490d4c6": {"content": "BUG: asyncio task leaks when exception is not handled\nProblem: Unhandled exceptions in fire-and-forget asyncio tasks are silently ignored, causing task leaks.\n\nFix/Discussion: Always await tasks or add exception handlers. Use `task.add_done_callback()` to log exceptions. In Python 3.11+, use TaskGroup for structured concurrency — exceptions propagate automatically. Set `asyncio.get_event_loop().set_exception_handler()` for global unhandled task exception logging.\nCode signature: status:closed", "file_path": "https://github.com/cpython/cpython/issues/5591", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "af7435ad-fadb-4b2f-8083-c9137490d4c6", "source_type": "github_issue", "domain": "Asyncio/Threading", "library": "cpython", "title": "BUG: asyncio task leaks when exception is not handled", "content": "Problem: Unhandled exceptions in fire-and-forget asyncio tasks are silently ignored, causing task leaks.\n\nFix/Discussion: Always await tasks or add exception handlers. Use `task.add_done_callback()` to log exceptions. In Python 3.11+, use TaskGroup for structured concurrency — exceptions propagate automatically. Set `asyncio.get_event_loop().set_exception_handler()` for global unhandled task exception logging.", "code_signature": "status:closed", "tags": "async, executor, asyncio, await", "url": "https://github.com/cpython/cpython/issues/5591", "author": "contributor_1630", "date_published": "16-08-2019", "votes_or_stars": 322, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:d454aad3-124a-4685-b817-c0bf5458a776": {"content": "BUG: pd.read_csv() silently truncates large integers\nProblem: Integer columns with values > 2^53 are silently corrupted when read from CSV.\n\nFix/Discussion: Floating point cannot represent integers > 2^53 exactly. Fix: `pd.read_csv(f, dtype={'col': str})` then convert: `df['col'] = df['col'].astype('Int64')`. Use `dtype_backend='numpy_nullable'` in Pandas 2.0+ for better integer handling.\nCode signature: status:closed", "file_path": "https://github.com/pandas/pandas/issues/4449", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "d454aad3-124a-4685-b817-c0bf5458a776", "source_type": "github_issue", "domain": "NumPy/Pandas", "library": "pandas", "title": "BUG: pd.read_csv() silently truncates large integers", "content": "Problem: Integer columns with values > 2^53 are silently corrupted when read from CSV.\n\nFix/Discussion: Floating point cannot represent integers > 2^53 exactly. Fix: `pd.read_csv(f, dtype={'col': str})` then convert: `df['col'] = df['col'].astype('Int64')`. Use `dtype_backend='numpy_nullable'` in Pandas 2.0+ for better integer handling.", "code_signature": "status:closed", "tags": "pivot, merge, pandas, iloc", "url": "https://github.com/pandas/pandas/issues/4449", "author": "contributor_726", "date_published": "16-02-2020", "votes_or_stars": 475, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:75371beb-9087-4f38-b964-e9bfedc32037": {"content": "How to use Path.glob in pathlib\n`pathlib.Path.glob` yields all paths matching the given glob pattern. Example usage:\n```python\npy_files = list(Path('src').glob('**/*.py'))\n```\nThis is useful when finding files recursively, batch file processing. Performance tip: iterate the generator lazily rather than converting to list for large directories.\nCode signature: Path.glob", "file_path": "https://docs.pathlib.org/en/stable/Path/glob.html", "function_name": "Path.glob", "type": "documentation", "metadata": {"record_id": "75371beb-9087-4f38-b964-e9bfedc32037", "source_type": "documentation", "domain": "OS/Subprocess/File I/O", "library": "pathlib", "title": "How to use Path.glob in pathlib", "content": "`pathlib.Path.glob` yields all paths matching the given glob pattern. Example usage:\n```python\npy_files = list(Path('src').glob('**/*.py'))\n```\nThis is useful when finding files recursively, batch file processing. Performance tip: iterate the generator lazily rather than converting to list for large directories.", "code_signature": "Path.glob", "tags": "stdin, shutil, process", "url": "https://docs.pathlib.org/en/stable/Path/glob.html", "author": "Official Docs", "date_published": "10-05-2024", "votes_or_stars": 3202, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:1bda3b4c-b7d8-4683-bcc6-f6cce6a1e8ab": {"content": "asyncio RuntimeError: This event loop is already running\nThis error is common in Jupyter and FastAPI. Solutions: (1) In async context: just use `await coroutine()`. (2) In Jupyter: `import nest_asyncio; nest_asyncio.apply()`. (3) Run in thread: `asyncio.get_event_loop().run_in_executor(None, sync_func)`. (4) Use `asyncio.ensure_future()` to schedule from within async code.\nCode signature: You're calling asyncio.run() inside an already running event loop. Use await instead.", "file_path": "https://stackoverflow.com/questions/2229110", "function_name": "You're calling asyncio.run() inside an already running event loop. Use await instead.", "type": "stack_overflow", "metadata": {"record_id": "1bda3b4c-b7d8-4683-bcc6-f6cce6a1e8ab", "source_type": "stack_overflow", "domain": "Asyncio/Threading", "library": "async", "title": "asyncio RuntimeError: This event loop is already running", "content": "This error is common in Jupyter and FastAPI. Solutions: (1) In async context: just use `await coroutine()`. (2) In Jupyter: `import nest_asyncio; nest_asyncio.apply()`. (3) Run in thread: `asyncio.get_event_loop().run_in_executor(None, sync_func)`. (4) Use `asyncio.ensure_future()` to schedule from within async code.", "code_signature": "You're calling asyncio.run() inside an already running event loop. Use await instead.", "tags": "task, race-condition, asyncio, executor, event-loop, coroutine", "url": "https://stackoverflow.com/questions/2229110", "author": "user_573750", "date_published": "27-12-2018", "votes_or_stars": 2297, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:fffab104-5045-40d8-9e9d-21e7e8359b5b": {"content": "How to use create_engine in SQLAlchemy\n`SQLAlchemy.create_engine` creates a database engine representing a connection pool. Example usage:\n```python\nengine = create_engine('postgresql://user:pass@host/db', echo=True)\n```\nThis is useful when establishing the central database connection for an application. Performance tip: set pool_size and max_overflow based on expected concurrency.\nCode signature: create_engine", "file_path": "https://docs.sqlalchemy.org/en/stable/create_engine.html", "function_name": "create_engine", "type": "documentation", "metadata": {"record_id": "fffab104-5045-40d8-9e9d-21e7e8359b5b", "source_type": "documentation", "domain": "SQLAlchemy/Databases", "library": "SQLAlchemy", "title": "How to use create_engine in SQLAlchemy", "content": "`SQLAlchemy.create_engine` creates a database engine representing a connection pool. Example usage:\n```python\nengine = create_engine('postgresql://user:pass@host/db', echo=True)\n```\nThis is useful when establishing the central database connection for an application. Performance tip: set pool_size and max_overflow based on expected concurrency.", "code_signature": "create_engine", "tags": "foreign-key, postgresql, sqlite", "url": "https://docs.sqlalchemy.org/en/stable/create_engine.html", "author": "Official Docs", "date_published": "27-09-2019", "votes_or_stars": 2029, "relevance_label": "low", "difficulty_level": "intermediate"}}, "kb:3090a6dc-831f-4edb-b698-6dbf5d309e6b": {"content": "How do I handle CORS in FastAPI?\n```python\nfrom fastapi.middleware.cors import CORSMiddleware\napp.add_middleware(CORSMiddleware, allow_origins=['*'], allow_methods=['*'], allow_headers=['*'])\n```\nIn production, replace `'*'` with your actual frontend domain. For credentials (cookies), set `allow_credentials=True` and specify exact origins.\nCode signature: Add CORSMiddleware from starlette to your FastAPI app.", "file_path": "https://stackoverflow.com/questions/1527021", "function_name": "Add CORSMiddleware from starlette to your FastAPI app.", "type": "stack_overflow", "metadata": {"record_id": "3090a6dc-831f-4edb-b698-6dbf5d309e6b", "source_type": "stack_overflow", "domain": "FastAPI/Flask/Django", "library": "flask", "title": "How do I handle CORS in FastAPI?", "content": "```python\nfrom fastapi.middleware.cors import CORSMiddleware\napp.add_middleware(CORSMiddleware, allow_origins=['*'], allow_methods=['*'], allow_headers=['*'])\n```\nIn production, replace `'*'` with your actual frontend domain. For credentials (cookies), set `allow_credentials=True` and specify exact origins.", "code_signature": "Add CORSMiddleware from starlette to your FastAPI app.", "tags": "rest, template, fastapi", "url": "https://stackoverflow.com/questions/1527021", "author": "user_911393", "date_published": "15-09-2021", "votes_or_stars": 947, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:981051b7-7e41-433c-95c7-015e294ea1f6": {"content": "How to limit concurrency with asyncio?\n```python\nsem = asyncio.Semaphore(10)\nasync def limited(url):\n    async with sem:\n        return await fetch(url)\nresults = await asyncio.gather(*[limited(u) for u in urls])\n```\nThis prevents overwhelming APIs or databases with too many simultaneous connections.\nCode signature: Use asyncio.Semaphore to cap the number of concurrent coroutines.", "file_path": "https://stackoverflow.com/questions/2375453", "function_name": "Use asyncio.Semaphore to cap the number of concurrent coroutines.", "type": "stack_overflow", "metadata": {"record_id": "981051b7-7e41-433c-95c7-015e294ea1f6", "source_type": "stack_overflow", "domain": "Asyncio/Threading", "library": "asyncio", "title": "How to limit concurrency with asyncio?", "content": "```python\nsem = asyncio.Semaphore(10)\nasync def limited(url):\n    async with sem:\n        return await fetch(url)\nresults = await asyncio.gather(*[limited(u) for u in urls])\n```\nThis prevents overwhelming APIs or databases with too many simultaneous connections.", "code_signature": "Use asyncio.Semaphore to cap the number of concurrent coroutines.", "tags": "event-loop, semaphore, executor, async, task, deadlock", "url": "https://stackoverflow.com/questions/2375453", "author": "user_449589", "date_published": "22-10-2021", "votes_or_stars": 2425, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:b2283003-4340-4cee-abaf-efbc870ab37f": {"content": "How to watch a directory for file changes in Python?\n```python\nfrom watchdog.observers import Observer\nfrom watchdog.events import FileSystemEventHandler\nhandler = FileSystemEventHandler()\nobserver = Observer()\nobserver.schedule(handler, path='.', recursive=True)\nobserver.start()\n```\nFor simple polling: `os.stat(file).st_mtime`. Linux-native: `inotify`. macOS-native: `FSEvents`.\nCode signature: Use the watchdog library for cross-platform directory monitoring.", "file_path": "https://stackoverflow.com/questions/5213115", "function_name": "Use the watchdog library for cross-platform directory monitoring.", "type": "stack_overflow", "metadata": {"record_id": "b2283003-4340-4cee-abaf-efbc870ab37f", "source_type": "stack_overflow", "domain": "OS/Subprocess/File I/O", "library": "env-variable", "title": "How to watch a directory for file changes in Python?", "content": "```python\nfrom watchdog.observers import Observer\nfrom watchdog.events import FileSystemEventHandler\nhandler = FileSystemEventHandler()\nobserver = Observer()\nobserver.schedule(handler, path='.', recursive=True)\nobserver.start()\n```\nFor simple polling: `os.stat(file).st_mtime`. Linux-native: `inotify`. macOS-native: `FSEvents`.", "code_signature": "Use the watchdog library for cross-platform directory monitoring.", "tags": "subprocess, os, file-io, process", "url": "https://stackoverflow.com/questions/5213115", "author": "user_959314", "date_published": "16-08-2024", "votes_or_stars": 2054, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:92a5ef98-11c0-4601-a4de-17dcd42293e8": {"content": "How to run background tasks in FastAPI?\nFastAPI's `BackgroundTasks`: add `background_tasks: BackgroundTasks` as a parameter, then `background_tasks.add_task(func, *args)`. Runs after response is sent. For heavy workloads (email, ML inference), use Celery with Redis/RabbitMQ broker. For async background work, `asyncio.create_task()` works within async endpoints.\nCode signature: Use BackgroundTasks for lightweight tasks or Celery for heavy/distributed work.", "file_path": "https://stackoverflow.com/questions/8077999", "function_name": "Use BackgroundTasks for lightweight tasks or Celery for heavy/distributed work.", "type": "stack_overflow", "metadata": {"record_id": "92a5ef98-11c0-4601-a4de-17dcd42293e8", "source_type": "stack_overflow", "domain": "FastAPI/Flask/Django", "library": "django", "title": "How to run background tasks in FastAPI?", "content": "FastAPI's `BackgroundTasks`: add `background_tasks: BackgroundTasks` as a parameter, then `background_tasks.add_task(func, *args)`. Runs after response is sent. For heavy workloads (email, ML inference), use Celery with Redis/RabbitMQ broker. For async background work, `asyncio.create_task()` works within async endpoints.", "code_signature": "Use BackgroundTasks for lightweight tasks or Celery for heavy/distributed work.", "tags": "routing, middleware, flask, response, request", "url": "https://stackoverflow.com/questions/8077999", "author": "user_202401", "date_published": "17-12-2021", "votes_or_stars": 1850, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:ad626ba5-8a11-4242-860e-492ce70f3fac": {"content": "BUG: Flask session not persisting between requests in tests\nProblem: Session data set in one request is not available in the next request during testing.\n\nFix/Discussion: Use Flask test client's session_transaction() context manager: `with client.session_transaction() as sess:\n    sess['user_id'] = 1`. Ensure SECRET_KEY is set. For testing: `app.config['TESTING'] = True; app.config['SECRET_KEY'] = 'test'`.\nCode signature: status:closed", "file_path": "https://github.com/flask/flask/issues/2141", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "ad626ba5-8a11-4242-860e-492ce70f3fac", "source_type": "github_issue", "domain": "FastAPI/Flask/Django", "library": "flask", "title": "BUG: Flask session not persisting between requests in tests", "content": "Problem: Session data set in one request is not available in the next request during testing.\n\nFix/Discussion: Use Flask test client's session_transaction() context manager: `with client.session_transaction() as sess:\n    sess['user_id'] = 1`. Ensure SECRET_KEY is set. For testing: `app.config['TESTING'] = True; app.config['SECRET_KEY'] = 'test'`.", "code_signature": "status:closed", "tags": "response, pydantic, blueprint, rest, fastapi, dependency-injection", "url": "https://github.com/flask/flask/issues/2141", "author": "contributor_5020", "date_published": "27-12-2018", "votes_or_stars": 143, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:9a605cde-011a-42b4-8906-8ade3b4b995d": {"content": "How do I handle CORS in FastAPI?\n```python\nfrom fastapi.middleware.cors import CORSMiddleware\napp.add_middleware(CORSMiddleware, allow_origins=['*'], allow_methods=['*'], allow_headers=['*'])\n```\nIn production, replace `'*'` with your actual frontend domain. For credentials (cookies), set `allow_credentials=True` and specify exact origins.\nCode signature: Add CORSMiddleware from starlette to your FastAPI app.", "file_path": "https://stackoverflow.com/questions/1527021", "function_name": "Add CORSMiddleware from starlette to your FastAPI app.", "type": "stack_overflow", "metadata": {"record_id": "9a605cde-011a-42b4-8906-8ade3b4b995d", "source_type": "stack_overflow", "domain": "FastAPI/Flask/Django", "library": "flask", "title": "How do I handle CORS in FastAPI?", "content": "```python\nfrom fastapi.middleware.cors import CORSMiddleware\napp.add_middleware(CORSMiddleware, allow_origins=['*'], allow_methods=['*'], allow_headers=['*'])\n```\nIn production, replace `'*'` with your actual frontend domain. For credentials (cookies), set `allow_credentials=True` and specify exact origins.", "code_signature": "Add CORSMiddleware from starlette to your FastAPI app.", "tags": "orm, flask, routing, request, blueprint", "url": "https://stackoverflow.com/questions/1527021", "author": "user_911393", "date_published": "10-06-2019", "votes_or_stars": 959, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:228d2dfd-e94e-4c0a-9148-2a471b566650": {"content": "FastAPI — Path Operations\nThe `@app.get` function in FastAPI registers a GET endpoint at the specified path. It accepts `path, response_model` as a parameter and returns Response. Common use cases include building REST API endpoints that return JSON data. Note that use response_model to enforce output schema validation.\nCode signature: @app.get(path, response_model) -> Response", "file_path": "https://docs.fastapi.org/en/stable/@app/get.html", "function_name": "@app.get(path, response_model) -> Response", "type": "documentation", "metadata": {"record_id": "228d2dfd-e94e-4c0a-9148-2a471b566650", "source_type": "documentation", "domain": "FastAPI/Flask/Django", "library": "FastAPI", "title": "FastAPI — Path Operations", "content": "The `@app.get` function in FastAPI registers a GET endpoint at the specified path. It accepts `path, response_model` as a parameter and returns Response. Common use cases include building REST API endpoints that return JSON data. Note that use response_model to enforce output schema validation.", "code_signature": "@app.get(path, response_model) -> Response", "tags": "fastapi, orm, django, routing", "url": "https://docs.fastapi.org/en/stable/@app/get.html", "author": "Official Docs", "date_published": "14-06-2018", "votes_or_stars": 2885, "relevance_label": "low", "difficulty_level": "beginner"}}, "kb:87fabd08-7f90-47e7-9921-ec25dc7c916d": {"content": "BUG: SQLAlchemy connection pool exhaustion under load\nProblem: Application hangs under concurrent load — all connections are checked out and pool is exhausted.\n\nFix/Discussion: Increase `pool_size` and `max_overflow`. Ensure sessions are always closed: use `contextmanager` or `with Session() as session:`. Set `pool_timeout` to fail fast. Enable `pool_pre_ping=True`. Monitor with `engine.pool.checkedout()`. In async: use `AsyncSession` with `async_scoped_session`.\nCode signature: status:closed", "file_path": "https://github.com/sqlalchemy/sqlalchemy/issues/106", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "87fabd08-7f90-47e7-9921-ec25dc7c916d", "source_type": "github_issue", "domain": "SQLAlchemy/Databases", "library": "sqlalchemy", "title": "BUG: SQLAlchemy connection pool exhaustion under load", "content": "Problem: Application hangs under concurrent load — all connections are checked out and pool is exhausted.\n\nFix/Discussion: Increase `pool_size` and `max_overflow`. Ensure sessions are always closed: use `contextmanager` or `with Session() as session:`. Set `pool_timeout` to fail fast. Enable `pool_pre_ping=True`. Monitor with `engine.pool.checkedout()`. In async: use `AsyncSession` with `async_scoped_session`.", "code_signature": "status:closed", "tags": "transaction, alembic, connection-pool, sqlite, session, sqlalchemy", "url": "https://github.com/sqlalchemy/sqlalchemy/issues/106", "author": "contributor_5078", "date_published": "04-11-2021", "votes_or_stars": 266, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:66a980de-8797-4025-9884-d80b56aa19d3": {"content": "subprocess — Process Execution\nThe `subprocess.run` function in subprocess runs a subprocess and waits for it to complete. It accepts `args, capture_output=False, timeout=None` as a parameter and returns CompletedProcess. Common use cases include executing shell commands, scripts, or external programs from Python. Note that use shell=False (default) to avoid shell injection vulnerabilities.\nCode signature: subprocess.run(args, capture_output=False, timeout=None) -> CompletedProcess", "file_path": "https://docs.subprocess.org/en/stable/subprocess/run.html", "function_name": "subprocess.run(args, capture_output=False, timeout=None) -> CompletedProcess", "type": "documentation", "metadata": {"record_id": "66a980de-8797-4025-9884-d80b56aa19d3", "source_type": "documentation", "domain": "OS/Subprocess/File I/O", "library": "subprocess", "title": "subprocess — Process Execution", "content": "The `subprocess.run` function in subprocess runs a subprocess and waits for it to complete. It accepts `args, capture_output=False, timeout=None` as a parameter and returns CompletedProcess. Common use cases include executing shell commands, scripts, or external programs from Python. Note that use shell=False (default) to avoid shell injection vulnerabilities.", "code_signature": "subprocess.run(args, capture_output=False, timeout=None) -> CompletedProcess", "tags": "file-io, env-variable, tempfile, os", "url": "https://docs.subprocess.org/en/stable/subprocess/run.html", "author": "Official Docs", "date_published": "04-01-2019", "votes_or_stars": 457, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:2b1e5c0a-10e6-4d94-b9e6-30559f70ce4d": {"content": "PERF: groupby().apply() extremely slow on large DataFrames\nProblem: groupby().apply() with a custom function is 100x slower than expected on 10M row DataFrames.\n\nFix/Discussion: groupby().apply() serializes each group and has high overhead. Fix: vectorize with numpy, use groupby().transform() for same-shape output, or groupby().agg() with named aggregations. For complex logic, consider Polars or Dask. Numba's @jit can accelerate custom group functions.\nCode signature: status:closed", "file_path": "https://github.com/pandas/pandas/issues/8446", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "2b1e5c0a-10e6-4d94-b9e6-30559f70ce4d", "source_type": "github_issue", "domain": "NumPy/Pandas", "library": "pandas", "title": "PERF: groupby().apply() extremely slow on large DataFrames", "content": "Problem: groupby().apply() with a custom function is 100x slower than expected on 10M row DataFrames.\n\nFix/Discussion: groupby().apply() serializes each group and has high overhead. Fix: vectorize with numpy, use groupby().transform() for same-shape output, or groupby().agg() with named aggregations. For complex logic, consider Polars or Dask. Numba's @jit can accelerate custom group functions.", "code_signature": "status:closed", "tags": "dataframe, array, broadcasting, iloc, groupby, dtype", "url": "https://github.com/pandas/pandas/issues/8446", "author": "contributor_6648", "date_published": "23-06-2019", "votes_or_stars": 308, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:c98fcf5d-8bbf-48f6-be25-78d2cde6f4d5": {"content": "How to watch a directory for file changes in Python?\n```python\nfrom watchdog.observers import Observer\nfrom watchdog.events import FileSystemEventHandler\nhandler = FileSystemEventHandler()\nobserver = Observer()\nobserver.schedule(handler, path='.', recursive=True)\nobserver.start()\n```\nFor simple polling: `os.stat(file).st_mtime`. Linux-native: `inotify`. macOS-native: `FSEvents`.\nCode signature: Use the watchdog library for cross-platform directory monitoring.", "file_path": "https://stackoverflow.com/questions/5213115", "function_name": "Use the watchdog library for cross-platform directory monitoring.", "type": "stack_overflow", "metadata": {"record_id": "c98fcf5d-8bbf-48f6-be25-78d2cde6f4d5", "source_type": "stack_overflow", "domain": "OS/Subprocess/File I/O", "library": "env-variable", "title": "How to watch a directory for file changes in Python?", "content": "```python\nfrom watchdog.observers import Observer\nfrom watchdog.events import FileSystemEventHandler\nhandler = FileSystemEventHandler()\nobserver = Observer()\nobserver.schedule(handler, path='.', recursive=True)\nobserver.start()\n```\nFor simple polling: `os.stat(file).st_mtime`. Linux-native: `inotify`. macOS-native: `FSEvents`.", "code_signature": "Use the watchdog library for cross-platform directory monitoring.", "tags": "pipe, process, file-io, subprocess", "url": "https://stackoverflow.com/questions/5213115", "author": "user_959314", "date_published": "05-10-2019", "votes_or_stars": 2100, "relevance_label": "low", "difficulty_level": "advanced"}}, "kb:eede7887-3dca-4d3b-93b6-54dd355d8a2f": {"content": "BUG: FastAPI dependency_overrides not working in pytest\nProblem: Dependency overrides set in conftest are not applied during test execution.\n\nFix/Discussion: Set overrides on the app object before TestClient is created. Use `app.dependency_overrides[dep] = mock_dep` in a pytest fixture with function scope. Clear overrides after test: `app.dependency_overrides = {}`. Ensure TestClient is created after overrides are set.\nCode signature: status:closed", "file_path": "https://github.com/fastapi/fastapi/issues/8479", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "eede7887-3dca-4d3b-93b6-54dd355d8a2f", "source_type": "github_issue", "domain": "FastAPI/Flask/Django", "library": "fastapi", "title": "BUG: FastAPI dependency_overrides not working in pytest", "content": "Problem: Dependency overrides set in conftest are not applied during test execution.\n\nFix/Discussion: Set overrides on the app object before TestClient is created. Use `app.dependency_overrides[dep] = mock_dep` in a pytest fixture with function scope. Clear overrides after test: `app.dependency_overrides = {}`. Ensure TestClient is created after overrides are set.", "code_signature": "status:closed", "tags": "dependency-injection, request, response", "url": "https://github.com/fastapi/fastapi/issues/8479", "author": "contributor_1994", "date_published": "14-03-2024", "votes_or_stars": 292, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:723e4263-412a-4f6d-b275-3c93cd7c13d4": {"content": "How to use create_engine in SQLAlchemy\n`SQLAlchemy.create_engine` creates a database engine representing a connection pool. Example usage:\n```python\nengine = create_engine('postgresql://user:pass@host/db', echo=True)\n```\nThis is useful when establishing the central database connection for an application. Performance tip: set pool_size and max_overflow based on expected concurrency.\nCode signature: create_engine", "file_path": "https://docs.sqlalchemy.org/en/stable/create_engine.html", "function_name": "create_engine", "type": "documentation", "metadata": {"record_id": "723e4263-412a-4f6d-b275-3c93cd7c13d4", "source_type": "documentation", "domain": "SQLAlchemy/Databases", "library": "SQLAlchemy", "title": "How to use create_engine in SQLAlchemy", "content": "`SQLAlchemy.create_engine` creates a database engine representing a connection pool. Example usage:\n```python\nengine = create_engine('postgresql://user:pass@host/db', echo=True)\n```\nThis is useful when establishing the central database connection for an application. Performance tip: set pool_size and max_overflow based on expected concurrency.", "code_signature": "create_engine", "tags": "orm, alembic, query, connection-pool", "url": "https://docs.sqlalchemy.org/en/stable/create_engine.html", "author": "Official Docs", "date_published": "04-08-2022", "votes_or_stars": 2031, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:3245181a-2687-4a1e-80c1-d4668c2ab2aa": {"content": "How to watch a directory for file changes in Python?\n```python\nfrom watchdog.observers import Observer\nfrom watchdog.events import FileSystemEventHandler\nhandler = FileSystemEventHandler()\nobserver = Observer()\nobserver.schedule(handler, path='.', recursive=True)\nobserver.start()\n```\nFor simple polling: `os.stat(file).st_mtime`. Linux-native: `inotify`. macOS-native: `FSEvents`.\nCode signature: Use the watchdog library for cross-platform directory monitoring.", "file_path": "https://stackoverflow.com/questions/5213115", "function_name": "Use the watchdog library for cross-platform directory monitoring.", "type": "stack_overflow", "metadata": {"record_id": "3245181a-2687-4a1e-80c1-d4668c2ab2aa", "source_type": "stack_overflow", "domain": "OS/Subprocess/File I/O", "library": "env-variable", "title": "How to watch a directory for file changes in Python?", "content": "```python\nfrom watchdog.observers import Observer\nfrom watchdog.events import FileSystemEventHandler\nhandler = FileSystemEventHandler()\nobserver = Observer()\nobserver.schedule(handler, path='.', recursive=True)\nobserver.start()\n```\nFor simple polling: `os.stat(file).st_mtime`. Linux-native: `inotify`. macOS-native: `FSEvents`.", "code_signature": "Use the watchdog library for cross-platform directory monitoring.", "tags": "process, tempfile, pipe, file-io, pathlib, subprocess", "url": "https://stackoverflow.com/questions/5213115", "author": "user_959314", "date_published": "16-06-2020", "votes_or_stars": 2051, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:f9cce29f-d372-441c-be84-cfd2bd0e16b1": {"content": "SQLAlchemy: how to avoid DetachedInstanceError?\nDetachedInstanceError occurs when accessing a relationship after the session is closed. Solutions: (1) Use `joinedload()` or `subqueryload()` to eagerly load relationships. (2) Keep session open while accessing objects. (3) Use `expire_on_commit=False` on sessionmaker. (4) Convert to dict/DTO before closing session.\nCode signature: Access lazy-loaded relationships within the session scope, or use eager loading.", "file_path": "https://stackoverflow.com/questions/1247595", "function_name": "Access lazy-loaded relationships within the session scope, or use eager loading.", "type": "stack_overflow", "metadata": {"record_id": "f9cce29f-d372-441c-be84-cfd2bd0e16b1", "source_type": "stack_overflow", "domain": "SQLAlchemy/Databases", "library": "sqlalchemy", "title": "SQLAlchemy: how to avoid DetachedInstanceError?", "content": "DetachedInstanceError occurs when accessing a relationship after the session is closed. Solutions: (1) Use `joinedload()` or `subqueryload()` to eagerly load relationships. (2) Keep session open while accessing objects. (3) Use `expire_on_commit=False` on sessionmaker. (4) Convert to dict/DTO before closing session.", "code_signature": "Access lazy-loaded relationships within the session scope, or use eager loading.", "tags": "alembic, transaction, orm, relationship, foreign-key", "url": "https://stackoverflow.com/questions/1247595", "author": "user_107793", "date_published": "10-01-2020", "votes_or_stars": 407, "relevance_label": "low", "difficulty_level": "beginner"}}, "kb:e1ff8d17-44a1-41d5-8c95-bbfe2a112f0e": {"content": "BUG: threading.Timer not cancellable after it fires\nProblem: Calling cancel() on a Timer that has already executed does not raise an error but has no effect.\n\nFix/Discussion: threading.Timer.cancel() only works before the timer fires. Store Timer reference and check `timer.finished.is_set()` to know if it's already run. For repeating timers, use a loop with `threading.Event` for clean cancellation: `stop_event = threading.Event(); stop_event.wait(interval)`.\nCode signature: status:closed", "file_path": "https://github.com/cpython/cpython/issues/4111", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "e1ff8d17-44a1-41d5-8c95-bbfe2a112f0e", "source_type": "github_issue", "domain": "Asyncio/Threading", "library": "cpython", "title": "BUG: threading.Timer not cancellable after it fires", "content": "Problem: Calling cancel() on a Timer that has already executed does not raise an error but has no effect.\n\nFix/Discussion: threading.Timer.cancel() only works before the timer fires. Store Timer reference and check `timer.finished.is_set()` to know if it's already run. For repeating timers, use a loop with `threading.Event` for clean cancellation: `stop_event = threading.Event(); stop_event.wait(interval)`.", "code_signature": "status:closed", "tags": "asyncio, threading, event-loop, coroutine, gather, executor", "url": "https://github.com/cpython/cpython/issues/4111", "author": "contributor_7884", "date_published": "02-03-2021", "votes_or_stars": 47, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:78c377f1-6e09-4e4b-8e3b-8caedbee9fe9": {"content": "threading Thread Synchronization — official reference\nOfficial documentation for threading Thread Synchronization. The method signature is `threading.Lock()`. creates a mutual-exclusion lock for thread-safe access. Raises `RuntimeError` when lock is released without being acquired. See also: threading.RLock, threading.Semaphore, threading.Event.\nCode signature: threading.Lock()", "file_path": "https://docs.threading.org/en/stable/threading/Lock.html", "function_name": "threading.Lock()", "type": "documentation", "metadata": {"record_id": "78c377f1-6e09-4e4b-8e3b-8caedbee9fe9", "source_type": "documentation", "domain": "Asyncio/Threading", "library": "threading", "title": "threading Thread Synchronization — official reference", "content": "Official documentation for threading Thread Synchronization. The method signature is `threading.Lock()`. creates a mutual-exclusion lock for thread-safe access. Raises `RuntimeError` when lock is released without being acquired. See also: threading.RLock, threading.Semaphore, threading.Event.", "code_signature": "threading.Lock()", "tags": "semaphore, gather, event-loop", "url": "https://docs.threading.org/en/stable/threading/Lock.html", "author": "Official Docs", "date_published": "05-07-2019", "votes_or_stars": 4820, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:2b8204c3-dfa1-4607-bcea-1fcade87e6ce": {"content": "How do I efficiently fill NaN values in a Pandas DataFrame?\nFor large DataFrames, avoid looping. Use `df.fillna({'col': val})` to fill specific columns. `df.ffill()` propagates last valid observation forward. For time-series data, `df.interpolate()` provides smoother results. Always check `df.isna().sum()` before and after to verify.\nCode signature: Use df.fillna(value) or df.ffill()/df.bfill() for forward/backward fill.", "file_path": "https://stackoverflow.com/questions/6438436", "function_name": "Use df.fillna(value) or df.ffill()/df.bfill() for forward/backward fill.", "type": "stack_overflow", "metadata": {"record_id": "2b8204c3-dfa1-4607-bcea-1fcade87e6ce", "source_type": "stack_overflow", "domain": "NumPy/Pandas", "library": "loc", "title": "How do I efficiently fill NaN values in a Pandas DataFrame?", "content": "For large DataFrames, avoid looping. Use `df.fillna({'col': val})` to fill specific columns. `df.ffill()` propagates last valid observation forward. For time-series data, `df.interpolate()` provides smoother results. Always check `df.isna().sum()` before and after to verify.", "code_signature": "Use df.fillna(value) or df.ffill()/df.bfill() for forward/backward fill.", "tags": "loc, groupby, numpy, dataframe, nan, pivot", "url": "https://stackoverflow.com/questions/6438436", "author": "user_522340", "date_published": "04-10-2020", "votes_or_stars": 24, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:4a50a16a-ae7f-4d18-91d0-0b0e3e9d860e": {"content": "PERF: concurrent.futures.ProcessPoolExecutor slow startup on Windows\nProblem: ProcessPoolExecutor takes 3-5 seconds to start on Windows due to process spawning overhead.\n\nFix/Discussion: Windows uses 'spawn' start method (vs 'fork' on Linux). Mitigations: (1) Reuse executor across calls — don't create/destroy in loops. (2) Use 'forkserver' or 'fork' on Linux/macOS. (3) For short tasks, ThreadPoolExecutor may be faster. (4) Use `initializer` param to pre-load modules.\nCode signature: status:closed", "file_path": "https://github.com/cpython/cpython/issues/6647", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "4a50a16a-ae7f-4d18-91d0-0b0e3e9d860e", "source_type": "github_issue", "domain": "Asyncio/Threading", "library": "cpython", "title": "PERF: concurrent.futures.ProcessPoolExecutor slow startup on Windows", "content": "Problem: ProcessPoolExecutor takes 3-5 seconds to start on Windows due to process spawning overhead.\n\nFix/Discussion: Windows uses 'spawn' start method (vs 'fork' on Linux). Mitigations: (1) Reuse executor across calls — don't create/destroy in loops. (2) Use 'forkserver' or 'fork' on Linux/macOS. (3) For short tasks, ThreadPoolExecutor may be faster. (4) Use `initializer` param to pre-load modules.", "code_signature": "status:closed", "tags": "gather, coroutine, threading, await, event-loop, deadlock", "url": "https://github.com/cpython/cpython/issues/6647", "author": "contributor_4097", "date_published": "04-05-2021", "votes_or_stars": 248, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:836e82c0-9496-4dbe-9641-cb775459b1ab": {"content": "How to handle database migrations with Alembic?\nWorkflow: (1) Change SQLAlchemy models. (2) `alembic revision --autogenerate -m 'description'`. (3) Review generated migration script. (4) `alembic upgrade head` to apply. Always review autogenerated migrations — they miss some changes (indexes, constraints). Use `alembic downgrade -1` to rollback. Store migrations in version control.\nCode signature: Use alembic revision --autogenerate to create migrations from model changes.", "file_path": "https://stackoverflow.com/questions/9436429", "function_name": "Use alembic revision --autogenerate to create migrations from model changes.", "type": "stack_overflow", "metadata": {"record_id": "836e82c0-9496-4dbe-9641-cb775459b1ab", "source_type": "stack_overflow", "domain": "SQLAlchemy/Databases", "library": "connection-pool", "title": "How to handle database migrations with Alembic?", "content": "Workflow: (1) Change SQLAlchemy models. (2) `alembic revision --autogenerate -m 'description'`. (3) Review generated migration script. (4) `alembic upgrade head` to apply. Always review autogenerated migrations — they miss some changes (indexes, constraints). Use `alembic downgrade -1` to rollback. Store migrations in version control.", "code_signature": "Use alembic revision --autogenerate to create migrations from model changes.", "tags": "orm, foreign-key, session, alembic, sqlalchemy", "url": "https://stackoverflow.com/questions/9436429", "author": "user_974047", "date_published": "12-11-2022", "votes_or_stars": 2391, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:98d2598f-3db1-4cba-b312-dd1581805172": {"content": "Django ORM Queries — official reference\nOfficial documentation for Django ORM Queries. The method signature is `QuerySet.filter(**kwargs)`. filters QuerySet rows matching given field lookups. Raises `FieldError` when an unknown field lookup is provided. See also: QuerySet.exclude, QuerySet.annotate, Q objects.\nCode signature: QuerySet.filter(**kwargs)", "file_path": "https://docs.django.org/en/stable/QuerySet/filter.html", "function_name": "QuerySet.filter(**kwargs)", "type": "documentation", "metadata": {"record_id": "98d2598f-3db1-4cba-b312-dd1581805172", "source_type": "documentation", "domain": "FastAPI/Flask/Django", "library": "Django", "title": "Django ORM Queries — official reference", "content": "Official documentation for Django ORM Queries. The method signature is `QuerySet.filter(**kwargs)`. filters QuerySet rows matching given field lookups. Raises `FieldError` when an unknown field lookup is provided. See also: QuerySet.exclude, QuerySet.annotate, Q objects.", "code_signature": "QuerySet.filter(**kwargs)", "tags": "pydantic, dependency-injection, orm, rest", "url": "https://docs.django.org/en/stable/QuerySet/filter.html", "author": "Official Docs", "date_published": "11-06-2020", "votes_or_stars": 2399, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:d29437d2-fe3c-489d-8623-1f14a8e4ddb9": {"content": "How to use httpx.AsyncClient in httpx\n`httpx.httpx.AsyncClient` provides an async HTTP client for non-blocking requests. Example usage:\n```python\nasync with httpx.AsyncClient() as client:\n    r = await client.get(url)\n    data = r.json()\n```\nThis is useful when making concurrent HTTP requests in async applications. Performance tip: set timeout=httpx.Timeout(10.0) to prevent hanging requests.\nCode signature: httpx.AsyncClient", "file_path": "https://docs.httpx.org/en/stable/httpx/AsyncClient.html", "function_name": "httpx.AsyncClient", "type": "documentation", "metadata": {"record_id": "d29437d2-fe3c-489d-8623-1f14a8e4ddb9", "source_type": "documentation", "domain": "Requests/HTTP/APIs", "library": "httpx", "title": "How to use httpx.AsyncClient in httpx", "content": "`httpx.httpx.AsyncClient` provides an async HTTP client for non-blocking requests. Example usage:\n```python\nasync with httpx.AsyncClient() as client:\n    r = await client.get(url)\n    data = r.json()\n```\nThis is useful when making concurrent HTTP requests in async applications. Performance tip: set timeout=httpx.Timeout(10.0) to prevent hanging requests.", "code_signature": "httpx.AsyncClient", "tags": "timeout, rest-api, json", "url": "https://docs.httpx.org/en/stable/httpx/AsyncClient.html", "author": "Official Docs", "date_published": "10-09-2023", "votes_or_stars": 4200, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:13ab219f-96f4-4990-98a9-daaa6b77e73a": {"content": "How to add retry logic to requests?\n```python\nfrom requests.adapters import HTTPAdapter\nfrom urllib3.util.retry import Retry\nretry = Retry(total=3, backoff_factor=1, status_forcelist=[500,502,503,504])\nadapter = HTTPAdapter(max_retries=retry)\nsession.mount('https://', adapter)\n```\nCode signature: Mount an HTTPAdapter with urllib3 Retry on the Session.", "file_path": "https://stackoverflow.com/questions/1669324", "function_name": "Mount an HTTPAdapter with urllib3 Retry on the Session.", "type": "stack_overflow", "metadata": {"record_id": "13ab219f-96f4-4990-98a9-daaa6b77e73a", "source_type": "stack_overflow", "domain": "Requests/HTTP/APIs", "library": "websocket", "title": "How to add retry logic to requests?", "content": "```python\nfrom requests.adapters import HTTPAdapter\nfrom urllib3.util.retry import Retry\nretry = Retry(total=3, backoff_factor=1, status_forcelist=[500,502,503,504])\nadapter = HTTPAdapter(max_retries=retry)\nsession.mount('https://', adapter)\n```", "code_signature": "Mount an HTTPAdapter with urllib3 Retry on the Session.", "tags": "retry, oauth, httpx, websocket, rate-limiting", "url": "https://stackoverflow.com/questions/1669324", "author": "user_952590", "date_published": "29-12-2018", "votes_or_stars": 1528, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:d1105c92-b541-4a20-9ff1-c4149cd83e39": {"content": "Why is my Pandas merge creating duplicate rows?\nDuplicate rows after merge usually mean the join key is not unique. Diagnose with `df.duplicated(subset=['key']).sum()`. Use `how='left'` carefully when right table has multiple matches. Set `validate='one_to_many'` or `'many_to_one'` to enforce cardinality. Consider deduplicating with `df.drop_duplicates(subset=['key'])` before merging.\nCode signature: Your merge key has duplicates in one or both DataFrames. Use validate='one_to_one' to catch this early.", "file_path": "https://stackoverflow.com/questions/2321324", "function_name": "Your merge key has duplicates in one or both DataFrames. Use validate='one_to_one' to catch this early.", "type": "stack_overflow", "metadata": {"record_id": "d1105c92-b541-4a20-9ff1-c4149cd83e39", "source_type": "stack_overflow", "domain": "NumPy/Pandas", "library": "array", "title": "Why is my Pandas merge creating duplicate rows?", "content": "Duplicate rows after merge usually mean the join key is not unique. Diagnose with `df.duplicated(subset=['key']).sum()`. Use `how='left'` carefully when right table has multiple matches. Set `validate='one_to_many'` or `'many_to_one'` to enforce cardinality. Consider deduplicating with `df.drop_duplicates(subset=['key'])` before merging.", "code_signature": "Your merge key has duplicates in one or both DataFrames. Use validate='one_to_one' to catch this early.", "tags": "array, vectorization, dataframe, dtype, iloc", "url": "https://stackoverflow.com/questions/2321324", "author": "user_99814", "date_published": "17-08-2020", "votes_or_stars": 269, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:792d8836-4b23-40ce-a75e-863aa2b63e4d": {"content": "FEAT: pathlib: add support for reading/writing with encoding shorthand\nProblem: pathlib.Path.read_text() does not default to UTF-8, causing issues across platforms.\n\nFix/Discussion: Always pass encoding explicitly: `Path('file').read_text(encoding='utf-8')`. Python 3.10+ added `encoding='locale'` default change proposal. Best practice: always specify encoding in all file I/O operations for portability.\nCode signature: status:closed", "file_path": "https://github.com/cpython/cpython/issues/2707", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "792d8836-4b23-40ce-a75e-863aa2b63e4d", "source_type": "github_issue", "domain": "OS/Subprocess/File I/O", "library": "cpython", "title": "FEAT: pathlib: add support for reading/writing with encoding shorthand", "content": "Problem: pathlib.Path.read_text() does not default to UTF-8, causing issues across platforms.\n\nFix/Discussion: Always pass encoding explicitly: `Path('file').read_text(encoding='utf-8')`. Python 3.10+ added `encoding='locale'` default change proposal. Best practice: always specify encoding in all file I/O operations for portability.", "code_signature": "status:closed", "tags": "process, subprocess, stdout, file-io, env-variable, tempfile", "url": "https://github.com/cpython/cpython/issues/2707", "author": "contributor_7877", "date_published": "11-01-2024", "votes_or_stars": 445, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:2ba4a4ea-64d7-421b-bd2e-644f3924a6f8": {"content": "NumPy — Linear Algebra\nThe `np.einsum` function in NumPy evaluates Einstein summation convention on operands. It accepts `subscripts, *operands` as a parameter and returns ndarray. Common use cases include matrix multiplication, dot products, tensor contractions. Note that prefer np.einsum over loops for large arrays.\nCode signature: np.einsum(subscripts, *operands) -> ndarray", "file_path": "https://docs.numpy.org/en/stable/np/einsum.html", "function_name": "np.einsum(subscripts, *operands) -> ndarray", "type": "documentation", "metadata": {"record_id": "2ba4a4ea-64d7-421b-bd2e-644f3924a6f8", "source_type": "documentation", "domain": "NumPy/Pandas", "library": "NumPy", "title": "NumPy — Linear Algebra", "content": "The `np.einsum` function in NumPy evaluates Einstein summation convention on operands. It accepts `subscripts, *operands` as a parameter and returns ndarray. Common use cases include matrix multiplication, dot products, tensor contractions. Note that prefer np.einsum over loops for large arrays.", "code_signature": "np.einsum(subscripts, *operands) -> ndarray", "tags": "iloc, groupby, array", "url": "https://docs.numpy.org/en/stable/np/einsum.html", "author": "Official Docs", "date_published": "30-10-2022", "votes_or_stars": 1899, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:3531fb57-7942-439a-9999-b07e9cb26212": {"content": "How to handle rate limiting with the requests library?\n```python\nfrom tenacity import retry, wait_exponential, stop_after_attempt\n@retry(wait=wait_exponential(min=1, max=60), stop=stop_after_attempt(5))\ndef call_api(url):\n    r = requests.get(url)\n    r.raise_for_status()\n    return r.json()\n```\nCheck `Retry-After` header in 429 responses. For async: use `aiohttp` with tenacity.\nCode signature: Implement exponential backoff with the tenacity or backoff library.", "file_path": "https://stackoverflow.com/questions/1604451", "function_name": "Implement exponential backoff with the tenacity or backoff library.", "type": "stack_overflow", "metadata": {"record_id": "3531fb57-7942-439a-9999-b07e9cb26212", "source_type": "stack_overflow", "domain": "Requests/HTTP/APIs", "library": "retry", "title": "How to handle rate limiting with the requests library?", "content": "```python\nfrom tenacity import retry, wait_exponential, stop_after_attempt\n@retry(wait=wait_exponential(min=1, max=60), stop=stop_after_attempt(5))\ndef call_api(url):\n    r = requests.get(url)\n    r.raise_for_status()\n    return r.json()\n```\nCheck `Retry-After` header in 429 responses. For async: use `aiohttp` with tenacity.", "code_signature": "Implement exponential backoff with the tenacity or backoff library.", "tags": "rest-api, http, aiohttp, json, oauth, timeout", "url": "https://stackoverflow.com/questions/1604451", "author": "user_885136", "date_published": "26-12-2019", "votes_or_stars": 610, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:e841a792-bc27-4e7c-9cf6-0ff136b816f0": {"content": "Flask: how to return JSON responses properly?\n`return jsonify({'key': 'value'})` sets Content-Type to application/json. Flask 1.0+ automatically converts returned dicts to JSON responses. For custom status codes: `return jsonify(data), 201`. For errors: use `abort(404)` or return `({'error': 'Not found'}, 404)`.\nCode signature: Use jsonify() or return a dict directly (Flask 1.0+).", "file_path": "https://stackoverflow.com/questions/8930103", "function_name": "Use jsonify() or return a dict directly (Flask 1.0+).", "type": "stack_overflow", "metadata": {"record_id": "e841a792-bc27-4e7c-9cf6-0ff136b816f0", "source_type": "stack_overflow", "domain": "FastAPI/Flask/Django", "library": "django", "title": "Flask: how to return JSON responses properly?", "content": "`return jsonify({'key': 'value'})` sets Content-Type to application/json. Flask 1.0+ automatically converts returned dicts to JSON responses. For custom status codes: `return jsonify(data), 201`. For errors: use `abort(404)` or return `({'error': 'Not found'}, 404)`.", "code_signature": "Use jsonify() or return a dict directly (Flask 1.0+).", "tags": "fastapi, rest, django", "url": "https://stackoverflow.com/questions/8930103", "author": "user_264801", "date_published": "19-06-2022", "votes_or_stars": 2208, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:557e7580-e3b0-48fc-b40a-13e4b869a944": {"content": "NumPy — Vectorization\nThe `np.vectorize` function in NumPy generalizes a scalar function to operate on arrays. It accepts `pyfunc` as a parameter and returns vectorized function. Common use cases include applying Python functions element-wise without explicit loops. Note that np.vectorize is a convenience wrapper; it does not improve performance like true ufuncs.\nCode signature: np.vectorize(pyfunc) -> vectorized function", "file_path": "https://docs.numpy.org/en/stable/np/vectorize.html", "function_name": "np.vectorize(pyfunc) -> vectorized function", "type": "documentation", "metadata": {"record_id": "557e7580-e3b0-48fc-b40a-13e4b869a944", "source_type": "documentation", "domain": "NumPy/Pandas", "library": "NumPy", "title": "NumPy — Vectorization", "content": "The `np.vectorize` function in NumPy generalizes a scalar function to operate on arrays. It accepts `pyfunc` as a parameter and returns vectorized function. Common use cases include applying Python functions element-wise without explicit loops. Note that np.vectorize is a convenience wrapper; it does not improve performance like true ufuncs.", "code_signature": "np.vectorize(pyfunc) -> vectorized function", "tags": "pandas, reshape, iloc, array", "url": "https://docs.numpy.org/en/stable/np/vectorize.html", "author": "Official Docs", "date_published": "28-09-2021", "votes_or_stars": 1308, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:23bed06a-87c7-450b-bd26-992e292e3020": {"content": "How to upload a file with requests?\n```python\nwith open('file.pdf', 'rb') as f:\n    r = requests.post(url, files={'file': ('file.pdf', f, 'application/pdf')})\n```\nFor multiple files: pass a list of tuples. For multipart form data with fields: combine `files` and `data` parameters. Don't set Content-Type manually — requests sets it with boundary.\nCode signature: Use the files parameter with a file-like object or tuple.", "file_path": "https://stackoverflow.com/questions/8896868", "function_name": "Use the files parameter with a file-like object or tuple.", "type": "stack_overflow", "metadata": {"record_id": "23bed06a-87c7-450b-bd26-992e292e3020", "source_type": "stack_overflow", "domain": "Requests/HTTP/APIs", "library": "requests", "title": "How to upload a file with requests?", "content": "```python\nwith open('file.pdf', 'rb') as f:\n    r = requests.post(url, files={'file': ('file.pdf', f, 'application/pdf')})\n```\nFor multiple files: pass a list of tuples. For multipart form data with fields: combine `files` and `data` parameters. Don't set Content-Type manually — requests sets it with boundary.", "code_signature": "Use the files parameter with a file-like object or tuple.", "tags": "websocket, rate-limiting, httpx", "url": "https://stackoverflow.com/questions/8896868", "author": "user_243238", "date_published": "28-03-2020", "votes_or_stars": 1576, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:63f56e34-91d4-4018-8358-2c3c6c5b736f": {"content": "FastAPI — Path Operations\nThe `@app.get` function in FastAPI registers a GET endpoint at the specified path. It accepts `path, response_model` as a parameter and returns Response. Common use cases include building REST API endpoints that return JSON data. Note that use response_model to enforce output schema validation.\nCode signature: @app.get(path, response_model) -> Response", "file_path": "https://docs.fastapi.org/en/stable/@app/get.html", "function_name": "@app.get(path, response_model) -> Response", "type": "documentation", "metadata": {"record_id": "63f56e34-91d4-4018-8358-2c3c6c5b736f", "source_type": "documentation", "domain": "FastAPI/Flask/Django", "library": "FastAPI", "title": "FastAPI — Path Operations", "content": "The `@app.get` function in FastAPI registers a GET endpoint at the specified path. It accepts `path, response_model` as a parameter and returns Response. Common use cases include building REST API endpoints that return JSON data. Note that use response_model to enforce output schema validation.", "code_signature": "@app.get(path, response_model) -> Response", "tags": "pydantic, request, rest, django", "url": "https://docs.fastapi.org/en/stable/@app/get.html", "author": "Official Docs", "date_published": "22-09-2023", "votes_or_stars": 2877, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:4ac0fc89-8c96-4a5c-8ea2-2084d6de21af": {"content": "How to implement a thread pool in Python?\n```python\nfrom concurrent.futures import ThreadPoolExecutor\nwith ThreadPoolExecutor(max_workers=8) as ex:\n    futures = [ex.submit(task, arg) for arg in args]\n    results = [f.result() for f in futures]\n```\nFor CPU-bound: use ProcessPoolExecutor instead. For async integration: `await loop.run_in_executor(executor, func, *args)`.\nCode signature: Use concurrent.futures.ThreadPoolExecutor for I/O-bound tasks.", "file_path": "https://stackoverflow.com/questions/3646552", "function_name": "Use concurrent.futures.ThreadPoolExecutor for I/O-bound tasks.", "type": "stack_overflow", "metadata": {"record_id": "4ac0fc89-8c96-4a5c-8ea2-2084d6de21af", "source_type": "stack_overflow", "domain": "Asyncio/Threading", "library": "coroutine", "title": "How to implement a thread pool in Python?", "content": "```python\nfrom concurrent.futures import ThreadPoolExecutor\nwith ThreadPoolExecutor(max_workers=8) as ex:\n    futures = [ex.submit(task, arg) for arg in args]\n    results = [f.result() for f in futures]\n```\nFor CPU-bound: use ProcessPoolExecutor instead. For async integration: `await loop.run_in_executor(executor, func, *args)`.", "code_signature": "Use concurrent.futures.ThreadPoolExecutor for I/O-bound tasks.", "tags": "await, threading, coroutine, event-loop", "url": "https://stackoverflow.com/questions/3646552", "author": "user_469469", "date_published": "20-02-2018", "votes_or_stars": 1537, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:56f2d3b6-2b6c-4091-8235-549360e83ed7": {"content": "SQLAlchemy: how to avoid DetachedInstanceError?\nDetachedInstanceError occurs when accessing a relationship after the session is closed. Solutions: (1) Use `joinedload()` or `subqueryload()` to eagerly load relationships. (2) Keep session open while accessing objects. (3) Use `expire_on_commit=False` on sessionmaker. (4) Convert to dict/DTO before closing session.\nCode signature: Access lazy-loaded relationships within the session scope, or use eager loading.", "file_path": "https://stackoverflow.com/questions/1247595", "function_name": "Access lazy-loaded relationships within the session scope, or use eager loading.", "type": "stack_overflow", "metadata": {"record_id": "56f2d3b6-2b6c-4091-8235-549360e83ed7", "source_type": "stack_overflow", "domain": "SQLAlchemy/Databases", "library": "sqlalchemy", "title": "SQLAlchemy: how to avoid DetachedInstanceError?", "content": "DetachedInstanceError occurs when accessing a relationship after the session is closed. Solutions: (1) Use `joinedload()` or `subqueryload()` to eagerly load relationships. (2) Keep session open while accessing objects. (3) Use `expire_on_commit=False` on sessionmaker. (4) Convert to dict/DTO before closing session.", "code_signature": "Access lazy-loaded relationships within the session scope, or use eager loading.", "tags": "connection-pool, sqlalchemy, session, alembic", "url": "https://stackoverflow.com/questions/1247595", "author": "user_107793", "date_published": "10-04-2022", "votes_or_stars": 393, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:fca72146-1ef6-48bc-a430-f22f8a7af6ac": {"content": "Django ORM Queries — official reference\nOfficial documentation for Django ORM Queries. The method signature is `QuerySet.filter(**kwargs)`. filters QuerySet rows matching given field lookups. Raises `FieldError` when an unknown field lookup is provided. See also: QuerySet.exclude, QuerySet.annotate, Q objects.\nCode signature: QuerySet.filter(**kwargs)", "file_path": "https://docs.django.org/en/stable/QuerySet/filter.html", "function_name": "QuerySet.filter(**kwargs)", "type": "documentation", "metadata": {"record_id": "fca72146-1ef6-48bc-a430-f22f8a7af6ac", "source_type": "documentation", "domain": "FastAPI/Flask/Django", "library": "Django", "title": "Django ORM Queries — official reference", "content": "Official documentation for Django ORM Queries. The method signature is `QuerySet.filter(**kwargs)`. filters QuerySet rows matching given field lookups. Raises `FieldError` when an unknown field lookup is provided. See also: QuerySet.exclude, QuerySet.annotate, Q objects.", "code_signature": "QuerySet.filter(**kwargs)", "tags": "template, middleware, response, django, orm, request", "url": "https://docs.django.org/en/stable/QuerySet/filter.html", "author": "Official Docs", "date_published": "14-10-2018", "votes_or_stars": 2439, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:2dc52c2a-be21-4f8a-bca6-1de8196aa1d1": {"content": "SQLAlchemy — ORM Session Query\nThe `session.query` function in SQLAlchemy constructs a SELECT query against mapped entities. It accepts `*entities` as a parameter and returns Query object. Common use cases include retrieving ORM-mapped objects from the database. Note that prefer session.execute(select(...)) in SQLAlchemy 2.0+.\nCode signature: session.query(*entities) -> Query object", "file_path": "https://docs.sqlalchemy.org/en/stable/session/query.html", "function_name": "session.query(*entities) -> Query object", "type": "documentation", "metadata": {"record_id": "2dc52c2a-be21-4f8a-bca6-1de8196aa1d1", "source_type": "documentation", "domain": "SQLAlchemy/Databases", "library": "SQLAlchemy", "title": "SQLAlchemy — ORM Session Query", "content": "The `session.query` function in SQLAlchemy constructs a SELECT query against mapped entities. It accepts `*entities` as a parameter and returns Query object. Common use cases include retrieving ORM-mapped objects from the database. Note that prefer session.execute(select(...)) in SQLAlchemy 2.0+.", "code_signature": "session.query(*entities) -> Query object", "tags": "postgresql, migration, session, connection-pool, sqlite", "url": "https://docs.sqlalchemy.org/en/stable/session/query.html", "author": "Official Docs", "date_published": "22-06-2019", "votes_or_stars": 3067, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:87131d3a-91d2-499a-a090-e9b73ea524e7": {"content": "NumPy broadcasting error: operands could not be broadcast with shapes\nNumPy broadcasts shapes right-to-left. Arrays (3,4) and (4,) are compatible. Arrays (3,4) and (3,) are not — reshape to (3,1). Use `arr.reshape(-1,1)` to add a dimension. Always print `a.shape, b.shape` before operations to debug. `np.expand_dims(arr, axis=1)` is cleaner than reshape for adding dimensions.\nCode signature: Shapes must be compatible: dimensions must match or one of them must be 1.", "file_path": "https://stackoverflow.com/questions/4860684", "function_name": "Shapes must be compatible: dimensions must match or one of them must be 1.", "type": "stack_overflow", "metadata": {"record_id": "87131d3a-91d2-499a-a090-e9b73ea524e7", "source_type": "stack_overflow", "domain": "NumPy/Pandas", "library": "numpy", "title": "NumPy broadcasting error: operands could not be broadcast with shapes", "content": "NumPy broadcasts shapes right-to-left. Arrays (3,4) and (4,) are compatible. Arrays (3,4) and (3,) are not — reshape to (3,1). Use `arr.reshape(-1,1)` to add a dimension. Always print `a.shape, b.shape` before operations to debug. `np.expand_dims(arr, axis=1)` is cleaner than reshape for adding dimensions.", "code_signature": "Shapes must be compatible: dimensions must match or one of them must be 1.", "tags": "iloc, pandas, loc, array, vectorization", "url": "https://stackoverflow.com/questions/4860684", "author": "user_627024", "date_published": "05-06-2018", "votes_or_stars": 1369, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:566664e9-b6e4-4f52-9003-2d7eb258fc6f": {"content": "How to upload a file with requests?\n```python\nwith open('file.pdf', 'rb') as f:\n    r = requests.post(url, files={'file': ('file.pdf', f, 'application/pdf')})\n```\nFor multiple files: pass a list of tuples. For multipart form data with fields: combine `files` and `data` parameters. Don't set Content-Type manually — requests sets it with boundary.\nCode signature: Use the files parameter with a file-like object or tuple.", "file_path": "https://stackoverflow.com/questions/8896868", "function_name": "Use the files parameter with a file-like object or tuple.", "type": "stack_overflow", "metadata": {"record_id": "566664e9-b6e4-4f52-9003-2d7eb258fc6f", "source_type": "stack_overflow", "domain": "Requests/HTTP/APIs", "library": "requests", "title": "How to upload a file with requests?", "content": "```python\nwith open('file.pdf', 'rb') as f:\n    r = requests.post(url, files={'file': ('file.pdf', f, 'application/pdf')})\n```\nFor multiple files: pass a list of tuples. For multipart form data with fields: combine `files` and `data` parameters. Don't set Content-Type manually — requests sets it with boundary.", "code_signature": "Use the files parameter with a file-like object or tuple.", "tags": "json, authentication, requests", "url": "https://stackoverflow.com/questions/8896868", "author": "user_243238", "date_published": "13-11-2024", "votes_or_stars": 1580, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:b6e48d6b-370f-43ac-948c-8ebac1d9aa1b": {"content": "How to use df.groupby in Pandas\n`Pandas.df.groupby` groups DataFrame rows by column values. Example usage:\n```python\ndf.groupby('category')['value'].mean()\n```\nThis is useful when aggregation, transformation, filtering by group. Performance tip: avoid apply() on large DataFrames; prefer agg().\nCode signature: df.groupby", "file_path": "https://docs.pandas.org/en/stable/df/groupby.html", "function_name": "df.groupby", "type": "documentation", "metadata": {"record_id": "b6e48d6b-370f-43ac-948c-8ebac1d9aa1b", "source_type": "documentation", "domain": "NumPy/Pandas", "library": "Pandas", "title": "How to use df.groupby in Pandas", "content": "`Pandas.df.groupby` groups DataFrame rows by column values. Example usage:\n```python\ndf.groupby('category')['value'].mean()\n```\nThis is useful when aggregation, transformation, filtering by group. Performance tip: avoid apply() on large DataFrames; prefer agg().", "code_signature": "df.groupby", "tags": "merge, numpy, nan, array, dtype, vectorization", "url": "https://docs.pandas.org/en/stable/df/groupby.html", "author": "Official Docs", "date_published": "20-11-2024", "votes_or_stars": 843, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:233606b6-454a-4885-b711-57c9ca449045": {"content": "os Environment Variables — official reference\nOfficial documentation for os Environment Variables. The method signature is `os.environ(key)`. provides a mapping of the current environment variables. Raises `KeyError` when key is accessed with [] notation and is not set. See also: dotenv, os.getenv, os.putenv.\nCode signature: os.environ(key)", "file_path": "https://docs.os.org/en/stable/os/environ.html", "function_name": "os.environ(key)", "type": "documentation", "metadata": {"record_id": "233606b6-454a-4885-b711-57c9ca449045", "source_type": "documentation", "domain": "OS/Subprocess/File I/O", "library": "os", "title": "os Environment Variables — official reference", "content": "Official documentation for os Environment Variables. The method signature is `os.environ(key)`. provides a mapping of the current environment variables. Raises `KeyError` when key is accessed with [] notation and is not set. See also: dotenv, os.getenv, os.putenv.", "code_signature": "os.environ(key)", "tags": "file-io, os, env-variable", "url": "https://docs.os.org/en/stable/os/environ.html", "author": "Official Docs", "date_published": "16-09-2023", "votes_or_stars": 2205, "relevance_label": "low", "difficulty_level": "beginner"}}, "kb:39359f6c-efc0-4e8a-b9a8-80d93fe83cb3": {"content": "Django N+1 query problem: how to fix?\nN+1 occurs when accessing related objects in a loop. Diagnose with `django-debug-toolbar` or `connection.queries`. Fix ForeignKey: `User.objects.select_related('profile').all()`. Fix M2M: `Post.objects.prefetch_related('tags').all()`. Use `only()` to fetch specific fields and reduce data transfer.\nCode signature: Use select_related() for ForeignKey and prefetch_related() for ManyToMany relationships.", "file_path": "https://stackoverflow.com/questions/8106470", "function_name": "Use select_related() for ForeignKey and prefetch_related() for ManyToMany relationships.", "type": "stack_overflow", "metadata": {"record_id": "39359f6c-efc0-4e8a-b9a8-80d93fe83cb3", "source_type": "stack_overflow", "domain": "FastAPI/Flask/Django", "library": "request", "title": "Django N+1 query problem: how to fix?", "content": "N+1 occurs when accessing related objects in a loop. Diagnose with `django-debug-toolbar` or `connection.queries`. Fix ForeignKey: `User.objects.select_related('profile').all()`. Fix M2M: `Post.objects.prefetch_related('tags').all()`. Use `only()` to fetch specific fields and reduce data transfer.", "code_signature": "Use select_related() for ForeignKey and prefetch_related() for ManyToMany relationships.", "tags": "django, pydantic, rest, fastapi", "url": "https://stackoverflow.com/questions/8106470", "author": "user_441071", "date_published": "29-06-2018", "votes_or_stars": 1794, "relevance_label": "low", "difficulty_level": "beginner"}}, "kb:a634a331-d1f3-4555-9850-e48f80e89d8d": {"content": "Pandas Merging DataFrames — official reference\nOfficial documentation for Pandas Merging DataFrames. The method signature is `pd.merge(left, right, on, how='inner')`. merges two DataFrames using SQL-style joins. Raises `MergeError` when merge keys produce duplicate columns. See also: df.join, df.concat.\nCode signature: pd.merge(left, right, on, how='inner')", "file_path": "https://docs.pandas.org/en/stable/pd/merge.html", "function_name": "pd.merge(left, right, on, how='inner')", "type": "documentation", "metadata": {"record_id": "a634a331-d1f3-4555-9850-e48f80e89d8d", "source_type": "documentation", "domain": "NumPy/Pandas", "library": "Pandas", "title": "Pandas Merging DataFrames — official reference", "content": "Official documentation for Pandas Merging DataFrames. The method signature is `pd.merge(left, right, on, how='inner')`. merges two DataFrames using SQL-style joins. Raises `MergeError` when merge keys produce duplicate columns. See also: df.join, df.concat.", "code_signature": "pd.merge(left, right, on, how='inner')", "tags": "broadcasting, iloc, vectorization, reshape, pivot, dataframe", "url": "https://docs.pandas.org/en/stable/pd/merge.html", "author": "Official Docs", "date_published": "01-01-2022", "votes_or_stars": 4539, "relevance_label": "low", "difficulty_level": "intermediate"}}, "kb:429f6dab-2f29-4d09-88a3-ca3281fff799": {"content": "BUG: asyncio task leaks when exception is not handled\nProblem: Unhandled exceptions in fire-and-forget asyncio tasks are silently ignored, causing task leaks.\n\nFix/Discussion: Always await tasks or add exception handlers. Use `task.add_done_callback()` to log exceptions. In Python 3.11+, use TaskGroup for structured concurrency — exceptions propagate automatically. Set `asyncio.get_event_loop().set_exception_handler()` for global unhandled task exception logging.\nCode signature: status:closed", "file_path": "https://github.com/cpython/cpython/issues/5591", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "429f6dab-2f29-4d09-88a3-ca3281fff799", "source_type": "github_issue", "domain": "Asyncio/Threading", "library": "cpython", "title": "BUG: asyncio task leaks when exception is not handled", "content": "Problem: Unhandled exceptions in fire-and-forget asyncio tasks are silently ignored, causing task leaks.\n\nFix/Discussion: Always await tasks or add exception handlers. Use `task.add_done_callback()` to log exceptions. In Python 3.11+, use TaskGroup for structured concurrency — exceptions propagate automatically. Set `asyncio.get_event_loop().set_exception_handler()` for global unhandled task exception logging.", "code_signature": "status:closed", "tags": "deadlock, semaphore, executor", "url": "https://github.com/cpython/cpython/issues/5591", "author": "contributor_1630", "date_published": "13-09-2023", "votes_or_stars": 351, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:78b499b2-5eb8-48c2-a199-ef1f9b9905a7": {"content": "SQLAlchemy bulk insert: what's the fastest way?\n`session.bulk_insert_mappings(Model, list_of_dicts)` skips ORM overhead. Even faster: `session.execute(insert(Model), data)`. Avoid `session.add()` in loops. For PostgreSQL, use `insert().on_conflict_do_nothing()` for upserts. Batch in chunks of 1000-5000 rows for optimal performance.\nCode signature: Use session.bulk_insert_mappings() or execute(insert(), list_of_dicts) for maximum speed.", "file_path": "https://stackoverflow.com/questions/5977931", "function_name": "Use session.bulk_insert_mappings() or execute(insert(), list_of_dicts) for maximum speed.", "type": "stack_overflow", "metadata": {"record_id": "78b499b2-5eb8-48c2-a199-ef1f9b9905a7", "source_type": "stack_overflow", "domain": "SQLAlchemy/Databases", "library": "query", "title": "SQLAlchemy bulk insert: what's the fastest way?", "content": "`session.bulk_insert_mappings(Model, list_of_dicts)` skips ORM overhead. Even faster: `session.execute(insert(Model), data)`. Avoid `session.add()` in loops. For PostgreSQL, use `insert().on_conflict_do_nothing()` for upserts. Batch in chunks of 1000-5000 rows for optimal performance.", "code_signature": "Use session.bulk_insert_mappings() or execute(insert(), list_of_dicts) for maximum speed.", "tags": "query, orm, transaction, connection-pool, postgresql", "url": "https://stackoverflow.com/questions/5977931", "author": "user_238275", "date_published": "20-11-2022", "votes_or_stars": 666, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:dd7444e0-acf9-481f-9ce1-f803bc1656a8": {"content": "How to use Path.glob in pathlib\n`pathlib.Path.glob` yields all paths matching the given glob pattern. Example usage:\n```python\npy_files = list(Path('src').glob('**/*.py'))\n```\nThis is useful when finding files recursively, batch file processing. Performance tip: iterate the generator lazily rather than converting to list for large directories.\nCode signature: Path.glob", "file_path": "https://docs.pathlib.org/en/stable/Path/glob.html", "function_name": "Path.glob", "type": "documentation", "metadata": {"record_id": "dd7444e0-acf9-481f-9ce1-f803bc1656a8", "source_type": "documentation", "domain": "OS/Subprocess/File I/O", "library": "pathlib", "title": "How to use Path.glob in pathlib", "content": "`pathlib.Path.glob` yields all paths matching the given glob pattern. Example usage:\n```python\npy_files = list(Path('src').glob('**/*.py'))\n```\nThis is useful when finding files recursively, batch file processing. Performance tip: iterate the generator lazily rather than converting to list for large directories.", "code_signature": "Path.glob", "tags": "stdout, env-variable, subprocess, os, file-io, pathlib", "url": "https://docs.pathlib.org/en/stable/Path/glob.html", "author": "Official Docs", "date_published": "29-12-2020", "votes_or_stars": 3207, "relevance_label": "low", "difficulty_level": "beginner"}}, "kb:c9a336a1-dc73-4205-b5d7-075720957a7a": {"content": "SQLAlchemy bulk insert: what's the fastest way?\n`session.bulk_insert_mappings(Model, list_of_dicts)` skips ORM overhead. Even faster: `session.execute(insert(Model), data)`. Avoid `session.add()` in loops. For PostgreSQL, use `insert().on_conflict_do_nothing()` for upserts. Batch in chunks of 1000-5000 rows for optimal performance.\nCode signature: Use session.bulk_insert_mappings() or execute(insert(), list_of_dicts) for maximum speed.", "file_path": "https://stackoverflow.com/questions/5977931", "function_name": "Use session.bulk_insert_mappings() or execute(insert(), list_of_dicts) for maximum speed.", "type": "stack_overflow", "metadata": {"record_id": "c9a336a1-dc73-4205-b5d7-075720957a7a", "source_type": "stack_overflow", "domain": "SQLAlchemy/Databases", "library": "query", "title": "SQLAlchemy bulk insert: what's the fastest way?", "content": "`session.bulk_insert_mappings(Model, list_of_dicts)` skips ORM overhead. Even faster: `session.execute(insert(Model), data)`. Avoid `session.add()` in loops. For PostgreSQL, use `insert().on_conflict_do_nothing()` for upserts. Batch in chunks of 1000-5000 rows for optimal performance.", "code_signature": "Use session.bulk_insert_mappings() or execute(insert(), list_of_dicts) for maximum speed.", "tags": "sqlalchemy, relationship, orm", "url": "https://stackoverflow.com/questions/5977931", "author": "user_238275", "date_published": "20-10-2023", "votes_or_stars": 635, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:5f099ac7-133e-4cfa-af34-1696e4cce8e8": {"content": "BUG: requests hangs indefinitely on slow server responses\nProblem: requests.get() hangs forever when server accepts connection but never sends response.\n\nFix/Discussion: Always set both connect and read timeouts: `requests.get(url, timeout=(3.05, 27))`. Tuple: (connect_timeout, read_timeout). Or single value for both. Mount a Session with default timeouts using a custom HTTPAdapter subclass. Never use `timeout=None` in production code.\nCode signature: status:closed", "file_path": "https://github.com/requests/requests/issues/4019", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "5f099ac7-133e-4cfa-af34-1696e4cce8e8", "source_type": "github_issue", "domain": "Requests/HTTP/APIs", "library": "requests", "title": "BUG: requests hangs indefinitely on slow server responses", "content": "Problem: requests.get() hangs forever when server accepts connection but never sends response.\n\nFix/Discussion: Always set both connect and read timeouts: `requests.get(url, timeout=(3.05, 27))`. Tuple: (connect_timeout, read_timeout). Or single value for both. Mount a Session with default timeouts using a custom HTTPAdapter subclass. Never use `timeout=None` in production code.", "code_signature": "status:closed", "tags": "authentication, retry, json, oauth, timeout, websocket", "url": "https://github.com/requests/requests/issues/4019", "author": "contributor_4599", "date_published": "28-10-2021", "votes_or_stars": 246, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:f95eaa55-241b-4f85-8d51-ac753555165f": {"content": "requests — HTTP Session Management\nThe `requests.Session` function in requests creates a persistent HTTP session with connection pooling. It accepts `` as a parameter and returns Session object. Common use cases include making multiple requests to the same host efficiently. Note that always close or use Session as context manager to release connections.\nCode signature: requests.Session() -> Session object", "file_path": "https://docs.requests.org/en/stable/requests/Session.html", "function_name": "requests.Session() -> Session object", "type": "documentation", "metadata": {"record_id": "f95eaa55-241b-4f85-8d51-ac753555165f", "source_type": "documentation", "domain": "Requests/HTTP/APIs", "library": "requests", "title": "requests — HTTP Session Management", "content": "The `requests.Session` function in requests creates a persistent HTTP session with connection pooling. It accepts `` as a parameter and returns Session object. Common use cases include making multiple requests to the same host efficiently. Note that always close or use Session as context manager to release connections.", "code_signature": "requests.Session() -> Session object", "tags": "websocket, aiohttp, requests, timeout, httpx, oauth", "url": "https://docs.requests.org/en/stable/requests/Session.html", "author": "Official Docs", "date_published": "11-07-2024", "votes_or_stars": 48, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:0cdbd8a9-f303-4a5d-8abc-9db55ae6815b": {"content": "How to run asyncio code from synchronous Python?\n`asyncio.run(main())` creates an event loop, runs the coroutine, closes the loop. Never call asyncio.run() from inside a running event loop (use await instead). In Jupyter notebooks (which have a running loop), use `await coroutine()` directly or `nest_asyncio.apply()`. For mixing sync/async, use `loop.run_until_complete()`.\nCode signature: Use asyncio.run() in Python 3.7+ to execute a coroutine from sync code.", "file_path": "https://stackoverflow.com/questions/2140194", "function_name": "Use asyncio.run() in Python 3.7+ to execute a coroutine from sync code.", "type": "stack_overflow", "metadata": {"record_id": "0cdbd8a9-f303-4a5d-8abc-9db55ae6815b", "source_type": "stack_overflow", "domain": "Asyncio/Threading", "library": "gather", "title": "How to run asyncio code from synchronous Python?", "content": "`asyncio.run(main())` creates an event loop, runs the coroutine, closes the loop. Never call asyncio.run() from inside a running event loop (use await instead). In Jupyter notebooks (which have a running loop), use `await coroutine()` directly or `nest_asyncio.apply()`. For mixing sync/async, use `loop.run_until_complete()`.", "code_signature": "Use asyncio.run() in Python 3.7+ to execute a coroutine from sync code.", "tags": "race-condition, semaphore, event-loop, gather, coroutine", "url": "https://stackoverflow.com/questions/2140194", "author": "user_718011", "date_published": "22-10-2023", "votes_or_stars": 284, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:f42c2e82-9826-4826-9df5-068153dc0f2d": {"content": "asyncio — Concurrent Coroutines\nThe `asyncio.gather` function in asyncio runs multiple coroutines concurrently and collects results. It accepts `*coros, return_exceptions=False` as a parameter and returns list of results. Common use cases include parallel I/O operations such as multiple API calls. Note that set return_exceptions=True to prevent one failure from cancelling all tasks.\nCode signature: asyncio.gather(*coros, return_exceptions=False) -> list of results", "file_path": "https://docs.asyncio.org/en/stable/asyncio/gather.html", "function_name": "asyncio.gather(*coros, return_exceptions=False) -> list of results", "type": "documentation", "metadata": {"record_id": "f42c2e82-9826-4826-9df5-068153dc0f2d", "source_type": "documentation", "domain": "Asyncio/Threading", "library": "asyncio", "title": "asyncio — Concurrent Coroutines", "content": "The `asyncio.gather` function in asyncio runs multiple coroutines concurrently and collects results. It accepts `*coros, return_exceptions=False` as a parameter and returns list of results. Common use cases include parallel I/O operations such as multiple API calls. Note that set return_exceptions=True to prevent one failure from cancelling all tasks.", "code_signature": "asyncio.gather(*coros, return_exceptions=False) -> list of results", "tags": "gather, await, task", "url": "https://docs.asyncio.org/en/stable/asyncio/gather.html", "author": "Official Docs", "date_published": "17-01-2023", "votes_or_stars": 538, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:af19dc50-e4f6-413f-9d0a-b7e087090edb": {"content": "How to watch a directory for file changes in Python?\n```python\nfrom watchdog.observers import Observer\nfrom watchdog.events import FileSystemEventHandler\nhandler = FileSystemEventHandler()\nobserver = Observer()\nobserver.schedule(handler, path='.', recursive=True)\nobserver.start()\n```\nFor simple polling: `os.stat(file).st_mtime`. Linux-native: `inotify`. macOS-native: `FSEvents`.\nCode signature: Use the watchdog library for cross-platform directory monitoring.", "file_path": "https://stackoverflow.com/questions/5213115", "function_name": "Use the watchdog library for cross-platform directory monitoring.", "type": "stack_overflow", "metadata": {"record_id": "af19dc50-e4f6-413f-9d0a-b7e087090edb", "source_type": "stack_overflow", "domain": "OS/Subprocess/File I/O", "library": "env-variable", "title": "How to watch a directory for file changes in Python?", "content": "```python\nfrom watchdog.observers import Observer\nfrom watchdog.events import FileSystemEventHandler\nhandler = FileSystemEventHandler()\nobserver = Observer()\nobserver.schedule(handler, path='.', recursive=True)\nobserver.start()\n```\nFor simple polling: `os.stat(file).st_mtime`. Linux-native: `inotify`. macOS-native: `FSEvents`.", "code_signature": "Use the watchdog library for cross-platform directory monitoring.", "tags": "pipe, subprocess, os, process", "url": "https://stackoverflow.com/questions/5213115", "author": "user_959314", "date_published": "01-12-2020", "votes_or_stars": 2059, "relevance_label": "low", "difficulty_level": "beginner"}}, "kb:5ca857fb-2faf-4269-897e-70128964d7b0": {"content": "threading Thread Synchronization — official reference\nOfficial documentation for threading Thread Synchronization. The method signature is `threading.Lock()`. creates a mutual-exclusion lock for thread-safe access. Raises `RuntimeError` when lock is released without being acquired. See also: threading.RLock, threading.Semaphore, threading.Event.\nCode signature: threading.Lock()", "file_path": "https://docs.threading.org/en/stable/threading/Lock.html", "function_name": "threading.Lock()", "type": "documentation", "metadata": {"record_id": "5ca857fb-2faf-4269-897e-70128964d7b0", "source_type": "documentation", "domain": "Asyncio/Threading", "library": "threading", "title": "threading Thread Synchronization — official reference", "content": "Official documentation for threading Thread Synchronization. The method signature is `threading.Lock()`. creates a mutual-exclusion lock for thread-safe access. Raises `RuntimeError` when lock is released without being acquired. See also: threading.RLock, threading.Semaphore, threading.Event.", "code_signature": "threading.Lock()", "tags": "async, gather, race-condition, semaphore, threading, deadlock", "url": "https://docs.threading.org/en/stable/threading/Lock.html", "author": "Official Docs", "date_published": "16-11-2018", "votes_or_stars": 4855, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:6ce431ac-0342-423b-9779-ff75c421c141": {"content": "How to handle database migrations with Alembic?\nWorkflow: (1) Change SQLAlchemy models. (2) `alembic revision --autogenerate -m 'description'`. (3) Review generated migration script. (4) `alembic upgrade head` to apply. Always review autogenerated migrations — they miss some changes (indexes, constraints). Use `alembic downgrade -1` to rollback. Store migrations in version control.\nCode signature: Use alembic revision --autogenerate to create migrations from model changes.", "file_path": "https://stackoverflow.com/questions/9436429", "function_name": "Use alembic revision --autogenerate to create migrations from model changes.", "type": "stack_overflow", "metadata": {"record_id": "6ce431ac-0342-423b-9779-ff75c421c141", "source_type": "stack_overflow", "domain": "SQLAlchemy/Databases", "library": "connection-pool", "title": "How to handle database migrations with Alembic?", "content": "Workflow: (1) Change SQLAlchemy models. (2) `alembic revision --autogenerate -m 'description'`. (3) Review generated migration script. (4) `alembic upgrade head` to apply. Always review autogenerated migrations — they miss some changes (indexes, constraints). Use `alembic downgrade -1` to rollback. Store migrations in version control.", "code_signature": "Use alembic revision --autogenerate to create migrations from model changes.", "tags": "orm, relationship, alembic, postgresql, foreign-key", "url": "https://stackoverflow.com/questions/9436429", "author": "user_974047", "date_published": "14-05-2018", "votes_or_stars": 2419, "relevance_label": "low", "difficulty_level": "beginner"}}, "kb:3abfc9e6-2e2b-43ed-ad33-a5fcb2801668": {"content": "SQLAlchemy — ORM Session Query\nThe `session.query` function in SQLAlchemy constructs a SELECT query against mapped entities. It accepts `*entities` as a parameter and returns Query object. Common use cases include retrieving ORM-mapped objects from the database. Note that prefer session.execute(select(...)) in SQLAlchemy 2.0+.\nCode signature: session.query(*entities) -> Query object", "file_path": "https://docs.sqlalchemy.org/en/stable/session/query.html", "function_name": "session.query(*entities) -> Query object", "type": "documentation", "metadata": {"record_id": "3abfc9e6-2e2b-43ed-ad33-a5fcb2801668", "source_type": "documentation", "domain": "SQLAlchemy/Databases", "library": "SQLAlchemy", "title": "SQLAlchemy — ORM Session Query", "content": "The `session.query` function in SQLAlchemy constructs a SELECT query against mapped entities. It accepts `*entities` as a parameter and returns Query object. Common use cases include retrieving ORM-mapped objects from the database. Note that prefer session.execute(select(...)) in SQLAlchemy 2.0+.", "code_signature": "session.query(*entities) -> Query object", "tags": "sqlite, relationship, postgresql, session", "url": "https://docs.sqlalchemy.org/en/stable/session/query.html", "author": "Official Docs", "date_published": "10-02-2021", "votes_or_stars": 3044, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:d766ad1a-7e1d-4c1f-8c07-9c585dbdb0a3": {"content": "asyncio — Concurrent Coroutines\nThe `asyncio.gather` function in asyncio runs multiple coroutines concurrently and collects results. It accepts `*coros, return_exceptions=False` as a parameter and returns list of results. Common use cases include parallel I/O operations such as multiple API calls. Note that set return_exceptions=True to prevent one failure from cancelling all tasks.\nCode signature: asyncio.gather(*coros, return_exceptions=False) -> list of results", "file_path": "https://docs.asyncio.org/en/stable/asyncio/gather.html", "function_name": "asyncio.gather(*coros, return_exceptions=False) -> list of results", "type": "documentation", "metadata": {"record_id": "d766ad1a-7e1d-4c1f-8c07-9c585dbdb0a3", "source_type": "documentation", "domain": "Asyncio/Threading", "library": "asyncio", "title": "asyncio — Concurrent Coroutines", "content": "The `asyncio.gather` function in asyncio runs multiple coroutines concurrently and collects results. It accepts `*coros, return_exceptions=False` as a parameter and returns list of results. Common use cases include parallel I/O operations such as multiple API calls. Note that set return_exceptions=True to prevent one failure from cancelling all tasks.", "code_signature": "asyncio.gather(*coros, return_exceptions=False) -> list of results", "tags": "deadlock, gather, race-condition, task, async, event-loop", "url": "https://docs.asyncio.org/en/stable/asyncio/gather.html", "author": "Official Docs", "date_published": "26-07-2018", "votes_or_stars": 521, "relevance_label": "low", "difficulty_level": "advanced"}}, "kb:a6da5a47-c3f9-4a9c-b475-eaf164cd2610": {"content": "How to stream large HTTP responses with requests?\n```python\nwith requests.get(url, stream=True) as r:\n    r.raise_for_status()\n    with open('file', 'wb') as f:\n        for chunk in r.iter_content(chunk_size=8192):\n            f.write(chunk)\n```\nAlways use as context manager with stream=True to ensure connection is released.\nCode signature: Use stream=True and iterate over response.iter_content() or iter_lines().", "file_path": "https://stackoverflow.com/questions/3592974", "function_name": "Use stream=True and iterate over response.iter_content() or iter_lines().", "type": "stack_overflow", "metadata": {"record_id": "a6da5a47-c3f9-4a9c-b475-eaf164cd2610", "source_type": "stack_overflow", "domain": "Requests/HTTP/APIs", "library": "json", "title": "How to stream large HTTP responses with requests?", "content": "```python\nwith requests.get(url, stream=True) as r:\n    r.raise_for_status()\n    with open('file', 'wb') as f:\n        for chunk in r.iter_content(chunk_size=8192):\n            f.write(chunk)\n```\nAlways use as context manager with stream=True to ensure connection is released.", "code_signature": "Use stream=True and iterate over response.iter_content() or iter_lines().", "tags": "http, oauth, httpx, authentication, retry", "url": "https://stackoverflow.com/questions/3592974", "author": "user_980733", "date_published": "27-08-2020", "votes_or_stars": 1674, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:00840e79-2c5e-4a15-87a9-1de0942d7d61": {"content": "How to use httpx.AsyncClient in httpx\n`httpx.httpx.AsyncClient` provides an async HTTP client for non-blocking requests. Example usage:\n```python\nasync with httpx.AsyncClient() as client:\n    r = await client.get(url)\n    data = r.json()\n```\nThis is useful when making concurrent HTTP requests in async applications. Performance tip: set timeout=httpx.Timeout(10.0) to prevent hanging requests.\nCode signature: httpx.AsyncClient", "file_path": "https://docs.httpx.org/en/stable/httpx/AsyncClient.html", "function_name": "httpx.AsyncClient", "type": "documentation", "metadata": {"record_id": "00840e79-2c5e-4a15-87a9-1de0942d7d61", "source_type": "documentation", "domain": "Requests/HTTP/APIs", "library": "httpx", "title": "How to use httpx.AsyncClient in httpx", "content": "`httpx.httpx.AsyncClient` provides an async HTTP client for non-blocking requests. Example usage:\n```python\nasync with httpx.AsyncClient() as client:\n    r = await client.get(url)\n    data = r.json()\n```\nThis is useful when making concurrent HTTP requests in async applications. Performance tip: set timeout=httpx.Timeout(10.0) to prevent hanging requests.", "code_signature": "httpx.AsyncClient", "tags": "json, headers, oauth, authentication, httpx", "url": "https://docs.httpx.org/en/stable/httpx/AsyncClient.html", "author": "Official Docs", "date_published": "07-01-2024", "votes_or_stars": 4185, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:2f4bb21d-01b4-4805-9985-cd61ee6f59ab": {"content": "BUG: subprocess.Popen hangs when stdout and stderr buffers fill\nProblem: subprocess.Popen hangs indefinitely when child process produces large output.\n\nFix/Discussion: This is a classic deadlock: parent waits for child, child waits for parent to read buffer. Fix: use `subprocess.run(..., capture_output=True)` which handles this correctly. Or: `stdout, stderr = proc.communicate()`. Never use `proc.stdout.read()` and `proc.wait()` separately.\nCode signature: status:closed", "file_path": "https://github.com/cpython/cpython/issues/8586", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "2f4bb21d-01b4-4805-9985-cd61ee6f59ab", "source_type": "github_issue", "domain": "OS/Subprocess/File I/O", "library": "cpython", "title": "BUG: subprocess.Popen hangs when stdout and stderr buffers fill", "content": "Problem: subprocess.Popen hangs indefinitely when child process produces large output.\n\nFix/Discussion: This is a classic deadlock: parent waits for child, child waits for parent to read buffer. Fix: use `subprocess.run(..., capture_output=True)` which handles this correctly. Or: `stdout, stderr = proc.communicate()`. Never use `proc.stdout.read()` and `proc.wait()` separately.", "code_signature": "status:closed", "tags": "process, stdin, pipe, file-io, glob, stdout", "url": "https://github.com/cpython/cpython/issues/8586", "author": "contributor_7711", "date_published": "29-05-2022", "votes_or_stars": 428, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:b3cb0b93-fc93-4d21-83d5-8e0ec0497fa8": {"content": "PERF: requests connection not reused between calls — new TCP connection each time\nProblem: Each requests.get() call opens a new TCP connection, causing high latency.\n\nFix/Discussion: requests.get/post etc. create a new Session per call. Fix: use a persistent Session: `session = requests.Session()` and reuse it. Sessions pool connections via urllib3. For thread-safety: create one Session per thread. For async: use httpx.AsyncClient or aiohttp.ClientSession for async connection pooling.\nCode signature: status:closed", "file_path": "https://github.com/requests/requests/issues/3605", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "b3cb0b93-fc93-4d21-83d5-8e0ec0497fa8", "source_type": "github_issue", "domain": "Requests/HTTP/APIs", "library": "requests", "title": "PERF: requests connection not reused between calls — new TCP connection each time", "content": "Problem: Each requests.get() call opens a new TCP connection, causing high latency.\n\nFix/Discussion: requests.get/post etc. create a new Session per call. Fix: use a persistent Session: `session = requests.Session()` and reuse it. Sessions pool connections via urllib3. For thread-safety: create one Session per thread. For async: use httpx.AsyncClient or aiohttp.ClientSession for async connection pooling.", "code_signature": "status:closed", "tags": "headers, retry, oauth, http, rate-limiting, requests", "url": "https://github.com/requests/requests/issues/3605", "author": "contributor_1152", "date_published": "29-05-2023", "votes_or_stars": 340, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:ff7bbe5a-16c9-4f74-8c60-cc5eb97116ed": {"content": "PERF: concurrent.futures.ProcessPoolExecutor slow startup on Windows\nProblem: ProcessPoolExecutor takes 3-5 seconds to start on Windows due to process spawning overhead.\n\nFix/Discussion: Windows uses 'spawn' start method (vs 'fork' on Linux). Mitigations: (1) Reuse executor across calls — don't create/destroy in loops. (2) Use 'forkserver' or 'fork' on Linux/macOS. (3) For short tasks, ThreadPoolExecutor may be faster. (4) Use `initializer` param to pre-load modules.\nCode signature: status:closed", "file_path": "https://github.com/cpython/cpython/issues/6647", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "ff7bbe5a-16c9-4f74-8c60-cc5eb97116ed", "source_type": "github_issue", "domain": "Asyncio/Threading", "library": "cpython", "title": "PERF: concurrent.futures.ProcessPoolExecutor slow startup on Windows", "content": "Problem: ProcessPoolExecutor takes 3-5 seconds to start on Windows due to process spawning overhead.\n\nFix/Discussion: Windows uses 'spawn' start method (vs 'fork' on Linux). Mitigations: (1) Reuse executor across calls — don't create/destroy in loops. (2) Use 'forkserver' or 'fork' on Linux/macOS. (3) For short tasks, ThreadPoolExecutor may be faster. (4) Use `initializer` param to pre-load modules.", "code_signature": "status:closed", "tags": "asyncio, async, deadlock, semaphore", "url": "https://github.com/cpython/cpython/issues/6647", "author": "contributor_4097", "date_published": "01-11-2024", "votes_or_stars": 242, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:8c4e6d20-8e9d-41f9-94c0-2f7c147be8a8": {"content": "BUG: threading.Timer not cancellable after it fires\nProblem: Calling cancel() on a Timer that has already executed does not raise an error but has no effect.\n\nFix/Discussion: threading.Timer.cancel() only works before the timer fires. Store Timer reference and check `timer.finished.is_set()` to know if it's already run. For repeating timers, use a loop with `threading.Event` for clean cancellation: `stop_event = threading.Event(); stop_event.wait(interval)`.\nCode signature: status:closed", "file_path": "https://github.com/cpython/cpython/issues/4111", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "8c4e6d20-8e9d-41f9-94c0-2f7c147be8a8", "source_type": "github_issue", "domain": "Asyncio/Threading", "library": "cpython", "title": "BUG: threading.Timer not cancellable after it fires", "content": "Problem: Calling cancel() on a Timer that has already executed does not raise an error but has no effect.\n\nFix/Discussion: threading.Timer.cancel() only works before the timer fires. Store Timer reference and check `timer.finished.is_set()` to know if it's already run. For repeating timers, use a loop with `threading.Event` for clean cancellation: `stop_event = threading.Event(); stop_event.wait(interval)`.", "code_signature": "status:closed", "tags": "event-loop, await, async, race-condition", "url": "https://github.com/cpython/cpython/issues/4111", "author": "contributor_7884", "date_published": "07-07-2023", "votes_or_stars": 2, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:bf82f751-5ed3-4c34-a345-761456266f8f": {"content": "How to use create_engine in SQLAlchemy\n`SQLAlchemy.create_engine` creates a database engine representing a connection pool. Example usage:\n```python\nengine = create_engine('postgresql://user:pass@host/db', echo=True)\n```\nThis is useful when establishing the central database connection for an application. Performance tip: set pool_size and max_overflow based on expected concurrency.\nCode signature: create_engine", "file_path": "https://docs.sqlalchemy.org/en/stable/create_engine.html", "function_name": "create_engine", "type": "documentation", "metadata": {"record_id": "bf82f751-5ed3-4c34-a345-761456266f8f", "source_type": "documentation", "domain": "SQLAlchemy/Databases", "library": "SQLAlchemy", "title": "How to use create_engine in SQLAlchemy", "content": "`SQLAlchemy.create_engine` creates a database engine representing a connection pool. Example usage:\n```python\nengine = create_engine('postgresql://user:pass@host/db', echo=True)\n```\nThis is useful when establishing the central database connection for an application. Performance tip: set pool_size and max_overflow based on expected concurrency.", "code_signature": "create_engine", "tags": "orm, foreign-key, connection-pool, sqlalchemy, session, query", "url": "https://docs.sqlalchemy.org/en/stable/create_engine.html", "author": "Official Docs", "date_published": "20-08-2023", "votes_or_stars": 2062, "relevance_label": "low", "difficulty_level": "intermediate"}}, "kb:4cd03b7f-1099-4dc5-9bf6-c981f72ec5ab": {"content": "FEAT: pathlib: add support for reading/writing with encoding shorthand\nProblem: pathlib.Path.read_text() does not default to UTF-8, causing issues across platforms.\n\nFix/Discussion: Always pass encoding explicitly: `Path('file').read_text(encoding='utf-8')`. Python 3.10+ added `encoding='locale'` default change proposal. Best practice: always specify encoding in all file I/O operations for portability.\nCode signature: status:closed", "file_path": "https://github.com/cpython/cpython/issues/2707", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "4cd03b7f-1099-4dc5-9bf6-c981f72ec5ab", "source_type": "github_issue", "domain": "OS/Subprocess/File I/O", "library": "cpython", "title": "FEAT: pathlib: add support for reading/writing with encoding shorthand", "content": "Problem: pathlib.Path.read_text() does not default to UTF-8, causing issues across platforms.\n\nFix/Discussion: Always pass encoding explicitly: `Path('file').read_text(encoding='utf-8')`. Python 3.10+ added `encoding='locale'` default change proposal. Best practice: always specify encoding in all file I/O operations for portability.", "code_signature": "status:closed", "tags": "os, subprocess, process, tempfile, stdout, stderr", "url": "https://github.com/cpython/cpython/issues/2707", "author": "contributor_7877", "date_published": "03-06-2021", "votes_or_stars": 490, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:b94838d7-54f3-4da5-a6e4-1598e4831d49": {"content": "BUG: os.path.join ignores prefix when component starts with /\nProblem: os.path.join('base', '/absolute') returns '/absolute', ignoring 'base'.\n\nFix/Discussion: This is documented behavior: os.path.join discards previous components when it encounters an absolute path. Fix: use `pathlib.Path(base) / component.lstrip('/')`. Validate user-provided paths with `Path(path).resolve().is_relative_to(base_dir)` to prevent path traversal.\nCode signature: status:closed", "file_path": "https://github.com/cpython/cpython/issues/7712", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "b94838d7-54f3-4da5-a6e4-1598e4831d49", "source_type": "github_issue", "domain": "OS/Subprocess/File I/O", "library": "cpython", "title": "BUG: os.path.join ignores prefix when component starts with /", "content": "Problem: os.path.join('base', '/absolute') returns '/absolute', ignoring 'base'.\n\nFix/Discussion: This is documented behavior: os.path.join discards previous components when it encounters an absolute path. Fix: use `pathlib.Path(base) / component.lstrip('/')`. Validate user-provided paths with `Path(path).resolve().is_relative_to(base_dir)` to prevent path traversal.", "code_signature": "status:closed", "tags": "pipe, stderr, os, stdout, subprocess", "url": "https://github.com/cpython/cpython/issues/7712", "author": "contributor_8802", "date_published": "19-12-2022", "votes_or_stars": 404, "relevance_label": "low", "difficulty_level": "advanced"}}, "kb:d07c998f-ab06-40d2-ae17-14a1942b2419": {"content": "BUG: httpx: SSL certificate error on Windows despite valid cert\nProblem: httpx raises SSLCertVerificationError on Windows when system CA store has the cert.\n\nFix/Discussion: httpx uses its own CA bundle (certifi) by default, not the system store. Fix: `httpx.Client(verify=certifi.where())` — already default. For custom certs: `httpx.Client(verify='/path/to/ca-bundle.pem')`. Install `pip install truststore` and use `ssl.create_default_context(ssl.Purpose.SERVER_AUTH)` to use system store.\nCode signature: status:closed", "file_path": "https://github.com/httpx/httpx/issues/8949", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "d07c998f-ab06-40d2-ae17-14a1942b2419", "source_type": "github_issue", "domain": "Requests/HTTP/APIs", "library": "httpx", "title": "BUG: httpx: SSL certificate error on Windows despite valid cert", "content": "Problem: httpx raises SSLCertVerificationError on Windows when system CA store has the cert.\n\nFix/Discussion: httpx uses its own CA bundle (certifi) by default, not the system store. Fix: `httpx.Client(verify=certifi.where())` — already default. For custom certs: `httpx.Client(verify='/path/to/ca-bundle.pem')`. Install `pip install truststore` and use `ssl.create_default_context(ssl.Purpose.SERVER_AUTH)` to use system store.", "code_signature": "status:closed", "tags": "retry, timeout, authentication", "url": "https://github.com/httpx/httpx/issues/8949", "author": "contributor_1420", "date_published": "26-04-2019", "votes_or_stars": 443, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:a0ad8c98-43a2-455a-88a7-137b53fe1169": {"content": "FEAT: pathlib: add support for reading/writing with encoding shorthand\nProblem: pathlib.Path.read_text() does not default to UTF-8, causing issues across platforms.\n\nFix/Discussion: Always pass encoding explicitly: `Path('file').read_text(encoding='utf-8')`. Python 3.10+ added `encoding='locale'` default change proposal. Best practice: always specify encoding in all file I/O operations for portability.\nCode signature: status:closed", "file_path": "https://github.com/cpython/cpython/issues/2707", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "a0ad8c98-43a2-455a-88a7-137b53fe1169", "source_type": "github_issue", "domain": "OS/Subprocess/File I/O", "library": "cpython", "title": "FEAT: pathlib: add support for reading/writing with encoding shorthand", "content": "Problem: pathlib.Path.read_text() does not default to UTF-8, causing issues across platforms.\n\nFix/Discussion: Always pass encoding explicitly: `Path('file').read_text(encoding='utf-8')`. Python 3.10+ added `encoding='locale'` default change proposal. Best practice: always specify encoding in all file I/O operations for portability.", "code_signature": "status:closed", "tags": "stdin, stderr, glob", "url": "https://github.com/cpython/cpython/issues/2707", "author": "contributor_7877", "date_published": "09-05-2019", "votes_or_stars": 489, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:8eefd80f-a799-42e9-83d5-5ddb9304390a": {"content": "NumPy — Vectorization\nThe `np.vectorize` function in NumPy generalizes a scalar function to operate on arrays. It accepts `pyfunc` as a parameter and returns vectorized function. Common use cases include applying Python functions element-wise without explicit loops. Note that np.vectorize is a convenience wrapper; it does not improve performance like true ufuncs.\nCode signature: np.vectorize(pyfunc) -> vectorized function", "file_path": "https://docs.numpy.org/en/stable/np/vectorize.html", "function_name": "np.vectorize(pyfunc) -> vectorized function", "type": "documentation", "metadata": {"record_id": "8eefd80f-a799-42e9-83d5-5ddb9304390a", "source_type": "documentation", "domain": "NumPy/Pandas", "library": "NumPy", "title": "NumPy — Vectorization", "content": "The `np.vectorize` function in NumPy generalizes a scalar function to operate on arrays. It accepts `pyfunc` as a parameter and returns vectorized function. Common use cases include applying Python functions element-wise without explicit loops. Note that np.vectorize is a convenience wrapper; it does not improve performance like true ufuncs.", "code_signature": "np.vectorize(pyfunc) -> vectorized function", "tags": "vectorization, iloc, numpy, pandas, merge, dataframe", "url": "https://docs.numpy.org/en/stable/np/vectorize.html", "author": "Official Docs", "date_published": "26-02-2021", "votes_or_stars": 1324, "relevance_label": "low", "difficulty_level": "advanced"}}, "kb:d3b3810b-209f-4f16-86e3-907c521ed59d": {"content": "How to apply a function to each row in Pandas without using iterrows?\n`iterrows()` is slow because it creates a Series per row. Prefer `df.apply(func, axis=1)` for row-wise operations, or better yet, vectorize using NumPy: `np.where(condition, a, b)`. For complex logic, `df.assign()` with vectorized expressions is cleanest. Benchmark: vectorized > apply > itertuples > iterrows.\nCode signature: Use df.apply(func, axis=1) or vectorized NumPy operations for best performance.", "file_path": "https://stackoverflow.com/questions/5446912", "function_name": "Use df.apply(func, axis=1) or vectorized NumPy operations for best performance.", "type": "stack_overflow", "metadata": {"record_id": "d3b3810b-209f-4f16-86e3-907c521ed59d", "source_type": "stack_overflow", "domain": "NumPy/Pandas", "library": "dataframe", "title": "How to apply a function to each row in Pandas without using iterrows?", "content": "`iterrows()` is slow because it creates a Series per row. Prefer `df.apply(func, axis=1)` for row-wise operations, or better yet, vectorize using NumPy: `np.where(condition, a, b)`. For complex logic, `df.assign()` with vectorized expressions is cleanest. Benchmark: vectorized > apply > itertuples > iterrows.", "code_signature": "Use df.apply(func, axis=1) or vectorized NumPy operations for best performance.", "tags": "dtype, iloc, groupby, merge, dataframe, pandas", "url": "https://stackoverflow.com/questions/5446912", "author": "user_563306", "date_published": "28-10-2019", "votes_or_stars": 2267, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:a3a5bfd1-662a-4d40-94ca-cf55cc2f6c46": {"content": "asyncio — Concurrent Coroutines\nThe `asyncio.gather` function in asyncio runs multiple coroutines concurrently and collects results. It accepts `*coros, return_exceptions=False` as a parameter and returns list of results. Common use cases include parallel I/O operations such as multiple API calls. Note that set return_exceptions=True to prevent one failure from cancelling all tasks.\nCode signature: asyncio.gather(*coros, return_exceptions=False) -> list of results", "file_path": "https://docs.asyncio.org/en/stable/asyncio/gather.html", "function_name": "asyncio.gather(*coros, return_exceptions=False) -> list of results", "type": "documentation", "metadata": {"record_id": "a3a5bfd1-662a-4d40-94ca-cf55cc2f6c46", "source_type": "documentation", "domain": "Asyncio/Threading", "library": "asyncio", "title": "asyncio — Concurrent Coroutines", "content": "The `asyncio.gather` function in asyncio runs multiple coroutines concurrently and collects results. It accepts `*coros, return_exceptions=False` as a parameter and returns list of results. Common use cases include parallel I/O operations such as multiple API calls. Note that set return_exceptions=True to prevent one failure from cancelling all tasks.", "code_signature": "asyncio.gather(*coros, return_exceptions=False) -> list of results", "tags": "deadlock, threading, semaphore, executor, asyncio", "url": "https://docs.asyncio.org/en/stable/asyncio/gather.html", "author": "Official Docs", "date_published": "01-12-2024", "votes_or_stars": 494, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:4724bb3e-e57a-4035-b191-76b3f9d6c8cc": {"content": "How to upload a file with requests?\n```python\nwith open('file.pdf', 'rb') as f:\n    r = requests.post(url, files={'file': ('file.pdf', f, 'application/pdf')})\n```\nFor multiple files: pass a list of tuples. For multipart form data with fields: combine `files` and `data` parameters. Don't set Content-Type manually — requests sets it with boundary.\nCode signature: Use the files parameter with a file-like object or tuple.", "file_path": "https://stackoverflow.com/questions/8896868", "function_name": "Use the files parameter with a file-like object or tuple.", "type": "stack_overflow", "metadata": {"record_id": "4724bb3e-e57a-4035-b191-76b3f9d6c8cc", "source_type": "stack_overflow", "domain": "Requests/HTTP/APIs", "library": "requests", "title": "How to upload a file with requests?", "content": "```python\nwith open('file.pdf', 'rb') as f:\n    r = requests.post(url, files={'file': ('file.pdf', f, 'application/pdf')})\n```\nFor multiple files: pass a list of tuples. For multipart form data with fields: combine `files` and `data` parameters. Don't set Content-Type manually — requests sets it with boundary.", "code_signature": "Use the files parameter with a file-like object or tuple.", "tags": "aiohttp, headers, oauth, retry, rest-api", "url": "https://stackoverflow.com/questions/8896868", "author": "user_243238", "date_published": "25-08-2020", "votes_or_stars": 1549, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:5faf0623-cc3a-44a4-a65f-d4e5605b8449": {"content": "How to use df.groupby in Pandas\n`Pandas.df.groupby` groups DataFrame rows by column values. Example usage:\n```python\ndf.groupby('category')['value'].mean()\n```\nThis is useful when aggregation, transformation, filtering by group. Performance tip: avoid apply() on large DataFrames; prefer agg().\nCode signature: df.groupby", "file_path": "https://docs.pandas.org/en/stable/df/groupby.html", "function_name": "df.groupby", "type": "documentation", "metadata": {"record_id": "5faf0623-cc3a-44a4-a65f-d4e5605b8449", "source_type": "documentation", "domain": "NumPy/Pandas", "library": "Pandas", "title": "How to use df.groupby in Pandas", "content": "`Pandas.df.groupby` groups DataFrame rows by column values. Example usage:\n```python\ndf.groupby('category')['value'].mean()\n```\nThis is useful when aggregation, transformation, filtering by group. Performance tip: avoid apply() on large DataFrames; prefer agg().", "code_signature": "df.groupby", "tags": "broadcasting, iloc, groupby", "url": "https://docs.pandas.org/en/stable/df/groupby.html", "author": "Official Docs", "date_published": "29-08-2019", "votes_or_stars": 824, "relevance_label": "low", "difficulty_level": "advanced"}}, "kb:2a3b0f77-e2fe-4659-83f5-c8e2f15dd536": {"content": "BUG: os.path.join ignores prefix when component starts with /\nProblem: os.path.join('base', '/absolute') returns '/absolute', ignoring 'base'.\n\nFix/Discussion: This is documented behavior: os.path.join discards previous components when it encounters an absolute path. Fix: use `pathlib.Path(base) / component.lstrip('/')`. Validate user-provided paths with `Path(path).resolve().is_relative_to(base_dir)` to prevent path traversal.\nCode signature: status:closed", "file_path": "https://github.com/cpython/cpython/issues/7712", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "2a3b0f77-e2fe-4659-83f5-c8e2f15dd536", "source_type": "github_issue", "domain": "OS/Subprocess/File I/O", "library": "cpython", "title": "BUG: os.path.join ignores prefix when component starts with /", "content": "Problem: os.path.join('base', '/absolute') returns '/absolute', ignoring 'base'.\n\nFix/Discussion: This is documented behavior: os.path.join discards previous components when it encounters an absolute path. Fix: use `pathlib.Path(base) / component.lstrip('/')`. Validate user-provided paths with `Path(path).resolve().is_relative_to(base_dir)` to prevent path traversal.", "code_signature": "status:closed", "tags": "subprocess, process, stdout, stderr, pathlib", "url": "https://github.com/cpython/cpython/issues/7712", "author": "contributor_8802", "date_published": "18-08-2019", "votes_or_stars": 426, "relevance_label": "low", "difficulty_level": "advanced"}}, "kb:73521710-e0a4-45d6-8853-b06c6defe93b": {"content": "NumPy — Vectorization\nThe `np.vectorize` function in NumPy generalizes a scalar function to operate on arrays. It accepts `pyfunc` as a parameter and returns vectorized function. Common use cases include applying Python functions element-wise without explicit loops. Note that np.vectorize is a convenience wrapper; it does not improve performance like true ufuncs.\nCode signature: np.vectorize(pyfunc) -> vectorized function", "file_path": "https://docs.numpy.org/en/stable/np/vectorize.html", "function_name": "np.vectorize(pyfunc) -> vectorized function", "type": "documentation", "metadata": {"record_id": "73521710-e0a4-45d6-8853-b06c6defe93b", "source_type": "documentation", "domain": "NumPy/Pandas", "library": "NumPy", "title": "NumPy — Vectorization", "content": "The `np.vectorize` function in NumPy generalizes a scalar function to operate on arrays. It accepts `pyfunc` as a parameter and returns vectorized function. Common use cases include applying Python functions element-wise without explicit loops. Note that np.vectorize is a convenience wrapper; it does not improve performance like true ufuncs.", "code_signature": "np.vectorize(pyfunc) -> vectorized function", "tags": "iloc, groupby, merge", "url": "https://docs.numpy.org/en/stable/np/vectorize.html", "author": "Official Docs", "date_published": "24-11-2023", "votes_or_stars": 1344, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:ac6dfe68-4c43-4cca-9048-2cac9a0e8db0": {"content": "subprocess — Process Execution\nThe `subprocess.run` function in subprocess runs a subprocess and waits for it to complete. It accepts `args, capture_output=False, timeout=None` as a parameter and returns CompletedProcess. Common use cases include executing shell commands, scripts, or external programs from Python. Note that use shell=False (default) to avoid shell injection vulnerabilities.\nCode signature: subprocess.run(args, capture_output=False, timeout=None) -> CompletedProcess", "file_path": "https://docs.subprocess.org/en/stable/subprocess/run.html", "function_name": "subprocess.run(args, capture_output=False, timeout=None) -> CompletedProcess", "type": "documentation", "metadata": {"record_id": "ac6dfe68-4c43-4cca-9048-2cac9a0e8db0", "source_type": "documentation", "domain": "OS/Subprocess/File I/O", "library": "subprocess", "title": "subprocess — Process Execution", "content": "The `subprocess.run` function in subprocess runs a subprocess and waits for it to complete. It accepts `args, capture_output=False, timeout=None` as a parameter and returns CompletedProcess. Common use cases include executing shell commands, scripts, or external programs from Python. Note that use shell=False (default) to avoid shell injection vulnerabilities.", "code_signature": "subprocess.run(args, capture_output=False, timeout=None) -> CompletedProcess", "tags": "file-io, subprocess, shutil, glob", "url": "https://docs.subprocess.org/en/stable/subprocess/run.html", "author": "Official Docs", "date_published": "22-06-2023", "votes_or_stars": 430, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:38bad728-08fa-472c-a8bd-d1eca28e24f4": {"content": "How to run asyncio code from synchronous Python?\n`asyncio.run(main())` creates an event loop, runs the coroutine, closes the loop. Never call asyncio.run() from inside a running event loop (use await instead). In Jupyter notebooks (which have a running loop), use `await coroutine()` directly or `nest_asyncio.apply()`. For mixing sync/async, use `loop.run_until_complete()`.\nCode signature: Use asyncio.run() in Python 3.7+ to execute a coroutine from sync code.", "file_path": "https://stackoverflow.com/questions/2140194", "function_name": "Use asyncio.run() in Python 3.7+ to execute a coroutine from sync code.", "type": "stack_overflow", "metadata": {"record_id": "38bad728-08fa-472c-a8bd-d1eca28e24f4", "source_type": "stack_overflow", "domain": "Asyncio/Threading", "library": "gather", "title": "How to run asyncio code from synchronous Python?", "content": "`asyncio.run(main())` creates an event loop, runs the coroutine, closes the loop. Never call asyncio.run() from inside a running event loop (use await instead). In Jupyter notebooks (which have a running loop), use `await coroutine()` directly or `nest_asyncio.apply()`. For mixing sync/async, use `loop.run_until_complete()`.", "code_signature": "Use asyncio.run() in Python 3.7+ to execute a coroutine from sync code.", "tags": "await, coroutine, gather, executor, async", "url": "https://stackoverflow.com/questions/2140194", "author": "user_718011", "date_published": "20-04-2018", "votes_or_stars": 271, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:3579a87d-50be-49d6-a2c7-f0d037333198": {"content": "How to use df.groupby in Pandas\n`Pandas.df.groupby` groups DataFrame rows by column values. Example usage:\n```python\ndf.groupby('category')['value'].mean()\n```\nThis is useful when aggregation, transformation, filtering by group. Performance tip: avoid apply() on large DataFrames; prefer agg().\nCode signature: df.groupby", "file_path": "https://docs.pandas.org/en/stable/df/groupby.html", "function_name": "df.groupby", "type": "documentation", "metadata": {"record_id": "3579a87d-50be-49d6-a2c7-f0d037333198", "source_type": "documentation", "domain": "NumPy/Pandas", "library": "Pandas", "title": "How to use df.groupby in Pandas", "content": "`Pandas.df.groupby` groups DataFrame rows by column values. Example usage:\n```python\ndf.groupby('category')['value'].mean()\n```\nThis is useful when aggregation, transformation, filtering by group. Performance tip: avoid apply() on large DataFrames; prefer agg().", "code_signature": "df.groupby", "tags": "groupby, numpy, loc", "url": "https://docs.pandas.org/en/stable/df/groupby.html", "author": "Official Docs", "date_published": "04-07-2022", "votes_or_stars": 826, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:c5bb15e6-335d-4670-bd64-e89a4af69516": {"content": "threading Thread Synchronization — official reference\nOfficial documentation for threading Thread Synchronization. The method signature is `threading.Lock()`. creates a mutual-exclusion lock for thread-safe access. Raises `RuntimeError` when lock is released without being acquired. See also: threading.RLock, threading.Semaphore, threading.Event.\nCode signature: threading.Lock()", "file_path": "https://docs.threading.org/en/stable/threading/Lock.html", "function_name": "threading.Lock()", "type": "documentation", "metadata": {"record_id": "c5bb15e6-335d-4670-bd64-e89a4af69516", "source_type": "documentation", "domain": "Asyncio/Threading", "library": "threading", "title": "threading Thread Synchronization — official reference", "content": "Official documentation for threading Thread Synchronization. The method signature is `threading.Lock()`. creates a mutual-exclusion lock for thread-safe access. Raises `RuntimeError` when lock is released without being acquired. See also: threading.RLock, threading.Semaphore, threading.Event.", "code_signature": "threading.Lock()", "tags": "await, asyncio, gather, executor, event-loop, coroutine", "url": "https://docs.threading.org/en/stable/threading/Lock.html", "author": "Official Docs", "date_published": "20-12-2021", "votes_or_stars": 4821, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:514ac93a-badf-4504-a8b2-8b0cd4c91bce": {"content": "BUG: threading.Timer not cancellable after it fires\nProblem: Calling cancel() on a Timer that has already executed does not raise an error but has no effect.\n\nFix/Discussion: threading.Timer.cancel() only works before the timer fires. Store Timer reference and check `timer.finished.is_set()` to know if it's already run. For repeating timers, use a loop with `threading.Event` for clean cancellation: `stop_event = threading.Event(); stop_event.wait(interval)`.\nCode signature: status:closed", "file_path": "https://github.com/cpython/cpython/issues/4111", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "514ac93a-badf-4504-a8b2-8b0cd4c91bce", "source_type": "github_issue", "domain": "Asyncio/Threading", "library": "cpython", "title": "BUG: threading.Timer not cancellable after it fires", "content": "Problem: Calling cancel() on a Timer that has already executed does not raise an error but has no effect.\n\nFix/Discussion: threading.Timer.cancel() only works before the timer fires. Store Timer reference and check `timer.finished.is_set()` to know if it's already run. For repeating timers, use a loop with `threading.Event` for clean cancellation: `stop_event = threading.Event(); stop_event.wait(interval)`.", "code_signature": "status:closed", "tags": "deadlock, asyncio, coroutine, async, semaphore, task", "url": "https://github.com/cpython/cpython/issues/4111", "author": "contributor_7884", "date_published": "19-10-2019", "votes_or_stars": 56, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:fe90125c-ef28-4bee-a275-19b11e119df5": {"content": "How to upload a file with requests?\n```python\nwith open('file.pdf', 'rb') as f:\n    r = requests.post(url, files={'file': ('file.pdf', f, 'application/pdf')})\n```\nFor multiple files: pass a list of tuples. For multipart form data with fields: combine `files` and `data` parameters. Don't set Content-Type manually — requests sets it with boundary.\nCode signature: Use the files parameter with a file-like object or tuple.", "file_path": "https://stackoverflow.com/questions/8896868", "function_name": "Use the files parameter with a file-like object or tuple.", "type": "stack_overflow", "metadata": {"record_id": "fe90125c-ef28-4bee-a275-19b11e119df5", "source_type": "stack_overflow", "domain": "Requests/HTTP/APIs", "library": "requests", "title": "How to upload a file with requests?", "content": "```python\nwith open('file.pdf', 'rb') as f:\n    r = requests.post(url, files={'file': ('file.pdf', f, 'application/pdf')})\n```\nFor multiple files: pass a list of tuples. For multipart form data with fields: combine `files` and `data` parameters. Don't set Content-Type manually — requests sets it with boundary.", "code_signature": "Use the files parameter with a file-like object or tuple.", "tags": "oauth, retry, httpx, rest-api", "url": "https://stackoverflow.com/questions/8896868", "author": "user_243238", "date_published": "06-04-2019", "votes_or_stars": 1592, "relevance_label": "low", "difficulty_level": "intermediate"}}, "kb:2a15f2e8-7821-456c-93ba-04befefb3afd": {"content": "BUG: requests hangs indefinitely on slow server responses\nProblem: requests.get() hangs forever when server accepts connection but never sends response.\n\nFix/Discussion: Always set both connect and read timeouts: `requests.get(url, timeout=(3.05, 27))`. Tuple: (connect_timeout, read_timeout). Or single value for both. Mount a Session with default timeouts using a custom HTTPAdapter subclass. Never use `timeout=None` in production code.\nCode signature: status:closed", "file_path": "https://github.com/requests/requests/issues/4019", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "2a15f2e8-7821-456c-93ba-04befefb3afd", "source_type": "github_issue", "domain": "Requests/HTTP/APIs", "library": "requests", "title": "BUG: requests hangs indefinitely on slow server responses", "content": "Problem: requests.get() hangs forever when server accepts connection but never sends response.\n\nFix/Discussion: Always set both connect and read timeouts: `requests.get(url, timeout=(3.05, 27))`. Tuple: (connect_timeout, read_timeout). Or single value for both. Mount a Session with default timeouts using a custom HTTPAdapter subclass. Never use `timeout=None` in production code.", "code_signature": "status:closed", "tags": "rate-limiting, headers, timeout, requests, websocket, retry", "url": "https://github.com/requests/requests/issues/4019", "author": "contributor_4599", "date_published": "04-03-2023", "votes_or_stars": 226, "relevance_label": "low", "difficulty_level": "beginner"}}, "kb:86f10057-5c85-4142-8c44-1c6680427a03": {"content": "BUG: pd.read_csv() silently truncates large integers\nProblem: Integer columns with values > 2^53 are silently corrupted when read from CSV.\n\nFix/Discussion: Floating point cannot represent integers > 2^53 exactly. Fix: `pd.read_csv(f, dtype={'col': str})` then convert: `df['col'] = df['col'].astype('Int64')`. Use `dtype_backend='numpy_nullable'` in Pandas 2.0+ for better integer handling.\nCode signature: status:closed", "file_path": "https://github.com/pandas/pandas/issues/4449", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "86f10057-5c85-4142-8c44-1c6680427a03", "source_type": "github_issue", "domain": "NumPy/Pandas", "library": "pandas", "title": "BUG: pd.read_csv() silently truncates large integers", "content": "Problem: Integer columns with values > 2^53 are silently corrupted when read from CSV.\n\nFix/Discussion: Floating point cannot represent integers > 2^53 exactly. Fix: `pd.read_csv(f, dtype={'col': str})` then convert: `df['col'] = df['col'].astype('Int64')`. Use `dtype_backend='numpy_nullable'` in Pandas 2.0+ for better integer handling.", "code_signature": "status:closed", "tags": "pivot, merge, pandas, array", "url": "https://github.com/pandas/pandas/issues/4449", "author": "contributor_726", "date_published": "24-05-2022", "votes_or_stars": 485, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:69febb33-6e65-4b9b-b26d-66f2d503e242": {"content": "NumPy — Linear Algebra\nThe `np.einsum` function in NumPy evaluates Einstein summation convention on operands. It accepts `subscripts, *operands` as a parameter and returns ndarray. Common use cases include matrix multiplication, dot products, tensor contractions. Note that prefer np.einsum over loops for large arrays.\nCode signature: np.einsum(subscripts, *operands) -> ndarray", "file_path": "https://docs.numpy.org/en/stable/np/einsum.html", "function_name": "np.einsum(subscripts, *operands) -> ndarray", "type": "documentation", "metadata": {"record_id": "69febb33-6e65-4b9b-b26d-66f2d503e242", "source_type": "documentation", "domain": "NumPy/Pandas", "library": "NumPy", "title": "NumPy — Linear Algebra", "content": "The `np.einsum` function in NumPy evaluates Einstein summation convention on operands. It accepts `subscripts, *operands` as a parameter and returns ndarray. Common use cases include matrix multiplication, dot products, tensor contractions. Note that prefer np.einsum over loops for large arrays.", "code_signature": "np.einsum(subscripts, *operands) -> ndarray", "tags": "dataframe, vectorization, numpy", "url": "https://docs.numpy.org/en/stable/np/einsum.html", "author": "Official Docs", "date_published": "07-01-2022", "votes_or_stars": 1898, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:d6bf59a0-4186-4116-a315-3902577daf4f": {"content": "How to implement a thread pool in Python?\n```python\nfrom concurrent.futures import ThreadPoolExecutor\nwith ThreadPoolExecutor(max_workers=8) as ex:\n    futures = [ex.submit(task, arg) for arg in args]\n    results = [f.result() for f in futures]\n```\nFor CPU-bound: use ProcessPoolExecutor instead. For async integration: `await loop.run_in_executor(executor, func, *args)`.\nCode signature: Use concurrent.futures.ThreadPoolExecutor for I/O-bound tasks.", "file_path": "https://stackoverflow.com/questions/3646552", "function_name": "Use concurrent.futures.ThreadPoolExecutor for I/O-bound tasks.", "type": "stack_overflow", "metadata": {"record_id": "d6bf59a0-4186-4116-a315-3902577daf4f", "source_type": "stack_overflow", "domain": "Asyncio/Threading", "library": "coroutine", "title": "How to implement a thread pool in Python?", "content": "```python\nfrom concurrent.futures import ThreadPoolExecutor\nwith ThreadPoolExecutor(max_workers=8) as ex:\n    futures = [ex.submit(task, arg) for arg in args]\n    results = [f.result() for f in futures]\n```\nFor CPU-bound: use ProcessPoolExecutor instead. For async integration: `await loop.run_in_executor(executor, func, *args)`.", "code_signature": "Use concurrent.futures.ThreadPoolExecutor for I/O-bound tasks.", "tags": "threading, coroutine, await, event-loop, race-condition", "url": "https://stackoverflow.com/questions/3646552", "author": "user_469469", "date_published": "25-07-2022", "votes_or_stars": 1497, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:bffe4867-8501-4f51-9d65-8f14bde89181": {"content": "threading Thread Synchronization — official reference\nOfficial documentation for threading Thread Synchronization. The method signature is `threading.Lock()`. creates a mutual-exclusion lock for thread-safe access. Raises `RuntimeError` when lock is released without being acquired. See also: threading.RLock, threading.Semaphore, threading.Event.\nCode signature: threading.Lock()", "file_path": "https://docs.threading.org/en/stable/threading/Lock.html", "function_name": "threading.Lock()", "type": "documentation", "metadata": {"record_id": "bffe4867-8501-4f51-9d65-8f14bde89181", "source_type": "documentation", "domain": "Asyncio/Threading", "library": "threading", "title": "threading Thread Synchronization — official reference", "content": "Official documentation for threading Thread Synchronization. The method signature is `threading.Lock()`. creates a mutual-exclusion lock for thread-safe access. Raises `RuntimeError` when lock is released without being acquired. See also: threading.RLock, threading.Semaphore, threading.Event.", "code_signature": "threading.Lock()", "tags": "lock, async, threading, race-condition", "url": "https://docs.threading.org/en/stable/threading/Lock.html", "author": "Official Docs", "date_published": "24-12-2021", "votes_or_stars": 4832, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:de641413-f885-400e-af4c-b95547dc666b": {"content": "Python threading: how to share data between threads safely?\nFor simple counters: use `threading.Lock()` as context manager. For complex shared state: prefer `queue.Queue` (thread-safe by design). Avoid global variables without locks. `threading.local()` gives each thread its own data copy. For read-heavy workloads, `threading.RLock` allows re-entrant acquisition.\nCode signature: Use threading.Lock for mutual exclusion or queue.Queue for producer-consumer patterns.", "file_path": "https://stackoverflow.com/questions/3195773", "function_name": "Use threading.Lock for mutual exclusion or queue.Queue for producer-consumer patterns.", "type": "stack_overflow", "metadata": {"record_id": "de641413-f885-400e-af4c-b95547dc666b", "source_type": "stack_overflow", "domain": "Asyncio/Threading", "library": "lock", "title": "Python threading: how to share data between threads safely?", "content": "For simple counters: use `threading.Lock()` as context manager. For complex shared state: prefer `queue.Queue` (thread-safe by design). Avoid global variables without locks. `threading.local()` gives each thread its own data copy. For read-heavy workloads, `threading.RLock` allows re-entrant acquisition.", "code_signature": "Use threading.Lock for mutual exclusion or queue.Queue for producer-consumer patterns.", "tags": "executor, race-condition, task, coroutine, lock", "url": "https://stackoverflow.com/questions/3195773", "author": "user_714318", "date_published": "16-11-2024", "votes_or_stars": 1118, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:90a3a447-83ee-4b95-a4f2-2a21b110bb28": {"content": "requests.exceptions.SSLError: certificate verify failed\nNever use `verify=False` in production — it disables SSL verification entirely. Solutions: (1) Update certifi: `pip install --upgrade certifi`. (2) Provide CA bundle: `requests.get(url, verify='/path/to/ca-bundle.crt')`. (3) For self-signed certs in dev: `verify=False` only, and never commit this. (4) Check system time — expired system clock causes SSL failures.\nCode signature: Your SSL certificate chain is incomplete or self-signed. Fix the cert, don't disable verification.", "file_path": "https://stackoverflow.com/questions/7907400", "function_name": "Your SSL certificate chain is incomplete or self-signed. Fix the cert, don't disable verification.", "type": "stack_overflow", "metadata": {"record_id": "90a3a447-83ee-4b95-a4f2-2a21b110bb28", "source_type": "stack_overflow", "domain": "Requests/HTTP/APIs", "library": "websocket", "title": "requests.exceptions.SSLError: certificate verify failed", "content": "Never use `verify=False` in production — it disables SSL verification entirely. Solutions: (1) Update certifi: `pip install --upgrade certifi`. (2) Provide CA bundle: `requests.get(url, verify='/path/to/ca-bundle.crt')`. (3) For self-signed certs in dev: `verify=False` only, and never commit this. (4) Check system time — expired system clock causes SSL failures.", "code_signature": "Your SSL certificate chain is incomplete or self-signed. Fix the cert, don't disable verification.", "tags": "httpx, requests, headers", "url": "https://stackoverflow.com/questions/7907400", "author": "user_851204", "date_published": "07-01-2020", "votes_or_stars": 1360, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:f23e18c7-5647-4e70-8bdd-6b812828bf20": {"content": "How to use Path.glob in pathlib\n`pathlib.Path.glob` yields all paths matching the given glob pattern. Example usage:\n```python\npy_files = list(Path('src').glob('**/*.py'))\n```\nThis is useful when finding files recursively, batch file processing. Performance tip: iterate the generator lazily rather than converting to list for large directories.\nCode signature: Path.glob", "file_path": "https://docs.pathlib.org/en/stable/Path/glob.html", "function_name": "Path.glob", "type": "documentation", "metadata": {"record_id": "f23e18c7-5647-4e70-8bdd-6b812828bf20", "source_type": "documentation", "domain": "OS/Subprocess/File I/O", "library": "pathlib", "title": "How to use Path.glob in pathlib", "content": "`pathlib.Path.glob` yields all paths matching the given glob pattern. Example usage:\n```python\npy_files = list(Path('src').glob('**/*.py'))\n```\nThis is useful when finding files recursively, batch file processing. Performance tip: iterate the generator lazily rather than converting to list for large directories.", "code_signature": "Path.glob", "tags": "stderr, stdin, glob, pipe", "url": "https://docs.pathlib.org/en/stable/Path/glob.html", "author": "Official Docs", "date_published": "10-05-2020", "votes_or_stars": 3202, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:b520eb5c-8997-4223-930f-5fc1569064ba": {"content": "requests — HTTP Session Management\nThe `requests.Session` function in requests creates a persistent HTTP session with connection pooling. It accepts `` as a parameter and returns Session object. Common use cases include making multiple requests to the same host efficiently. Note that always close or use Session as context manager to release connections.\nCode signature: requests.Session() -> Session object", "file_path": "https://docs.requests.org/en/stable/requests/Session.html", "function_name": "requests.Session() -> Session object", "type": "documentation", "metadata": {"record_id": "b520eb5c-8997-4223-930f-5fc1569064ba", "source_type": "documentation", "domain": "Requests/HTTP/APIs", "library": "requests", "title": "requests — HTTP Session Management", "content": "The `requests.Session` function in requests creates a persistent HTTP session with connection pooling. It accepts `` as a parameter and returns Session object. Common use cases include making multiple requests to the same host efficiently. Note that always close or use Session as context manager to release connections.", "code_signature": "requests.Session() -> Session object", "tags": "oauth, websocket, retry", "url": "https://docs.requests.org/en/stable/requests/Session.html", "author": "Official Docs", "date_published": "20-04-2023", "votes_or_stars": 86, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:28876d7f-93cd-4f89-b308-4857e35f5e5d": {"content": "BUG: SQLAlchemy connection pool exhaustion under load\nProblem: Application hangs under concurrent load — all connections are checked out and pool is exhausted.\n\nFix/Discussion: Increase `pool_size` and `max_overflow`. Ensure sessions are always closed: use `contextmanager` or `with Session() as session:`. Set `pool_timeout` to fail fast. Enable `pool_pre_ping=True`. Monitor with `engine.pool.checkedout()`. In async: use `AsyncSession` with `async_scoped_session`.\nCode signature: status:closed", "file_path": "https://github.com/sqlalchemy/sqlalchemy/issues/106", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "28876d7f-93cd-4f89-b308-4857e35f5e5d", "source_type": "github_issue", "domain": "SQLAlchemy/Databases", "library": "sqlalchemy", "title": "BUG: SQLAlchemy connection pool exhaustion under load", "content": "Problem: Application hangs under concurrent load — all connections are checked out and pool is exhausted.\n\nFix/Discussion: Increase `pool_size` and `max_overflow`. Ensure sessions are always closed: use `contextmanager` or `with Session() as session:`. Set `pool_timeout` to fail fast. Enable `pool_pre_ping=True`. Monitor with `engine.pool.checkedout()`. In async: use `AsyncSession` with `async_scoped_session`.", "code_signature": "status:closed", "tags": "migration, orm, alembic, foreign-key, sqlalchemy", "url": "https://github.com/sqlalchemy/sqlalchemy/issues/106", "author": "contributor_5078", "date_published": "03-03-2024", "votes_or_stars": 257, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:5f7ab7ad-addc-4f9a-b2a7-ccbcd1b596b0": {"content": "Python: how to safely write to a file atomically?\n```python\nimport tempfile, os\nwith tempfile.NamedTemporaryFile('w', delete=False, dir='.') as tmp:\n    tmp.write(data)\n    tmp.flush()\n    os.fsync(tmp.fileno())\nos.replace(tmp.name, target_path)\n```\n`os.replace()` is atomic on POSIX. This prevents partial writes from corrupting files.\nCode signature: Write to a temp file then rename — rename is atomic on POSIX systems.", "file_path": "https://stackoverflow.com/questions/5394880", "function_name": "Write to a temp file then rename — rename is atomic on POSIX systems.", "type": "stack_overflow", "metadata": {"record_id": "5f7ab7ad-addc-4f9a-b2a7-ccbcd1b596b0", "source_type": "stack_overflow", "domain": "OS/Subprocess/File I/O", "library": "stdin", "title": "Python: how to safely write to a file atomically?", "content": "```python\nimport tempfile, os\nwith tempfile.NamedTemporaryFile('w', delete=False, dir='.') as tmp:\n    tmp.write(data)\n    tmp.flush()\n    os.fsync(tmp.fileno())\nos.replace(tmp.name, target_path)\n```\n`os.replace()` is atomic on POSIX. This prevents partial writes from corrupting files.", "code_signature": "Write to a temp file then rename — rename is atomic on POSIX systems.", "tags": "stdin, shutil, env-variable, stdout, os, stderr", "url": "https://stackoverflow.com/questions/5394880", "author": "user_179430", "date_published": "13-09-2020", "votes_or_stars": 546, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:7c736392-4cbb-43c1-8afe-9b973e814dd0": {"content": "How do I handle CORS in FastAPI?\n```python\nfrom fastapi.middleware.cors import CORSMiddleware\napp.add_middleware(CORSMiddleware, allow_origins=['*'], allow_methods=['*'], allow_headers=['*'])\n```\nIn production, replace `'*'` with your actual frontend domain. For credentials (cookies), set `allow_credentials=True` and specify exact origins.\nCode signature: Add CORSMiddleware from starlette to your FastAPI app.", "file_path": "https://stackoverflow.com/questions/1527021", "function_name": "Add CORSMiddleware from starlette to your FastAPI app.", "type": "stack_overflow", "metadata": {"record_id": "7c736392-4cbb-43c1-8afe-9b973e814dd0", "source_type": "stack_overflow", "domain": "FastAPI/Flask/Django", "library": "flask", "title": "How do I handle CORS in FastAPI?", "content": "```python\nfrom fastapi.middleware.cors import CORSMiddleware\napp.add_middleware(CORSMiddleware, allow_origins=['*'], allow_methods=['*'], allow_headers=['*'])\n```\nIn production, replace `'*'` with your actual frontend domain. For credentials (cookies), set `allow_credentials=True` and specify exact origins.", "code_signature": "Add CORSMiddleware from starlette to your FastAPI app.", "tags": "orm, pydantic, flask, django, request", "url": "https://stackoverflow.com/questions/1527021", "author": "user_911393", "date_published": "12-12-2021", "votes_or_stars": 971, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:cc765fc8-1b97-41fd-b50f-c48452547cdf": {"content": "SQLAlchemy — ORM Session Query\nThe `session.query` function in SQLAlchemy constructs a SELECT query against mapped entities. It accepts `*entities` as a parameter and returns Query object. Common use cases include retrieving ORM-mapped objects from the database. Note that prefer session.execute(select(...)) in SQLAlchemy 2.0+.\nCode signature: session.query(*entities) -> Query object", "file_path": "https://docs.sqlalchemy.org/en/stable/session/query.html", "function_name": "session.query(*entities) -> Query object", "type": "documentation", "metadata": {"record_id": "cc765fc8-1b97-41fd-b50f-c48452547cdf", "source_type": "documentation", "domain": "SQLAlchemy/Databases", "library": "SQLAlchemy", "title": "SQLAlchemy — ORM Session Query", "content": "The `session.query` function in SQLAlchemy constructs a SELECT query against mapped entities. It accepts `*entities` as a parameter and returns Query object. Common use cases include retrieving ORM-mapped objects from the database. Note that prefer session.execute(select(...)) in SQLAlchemy 2.0+.", "code_signature": "session.query(*entities) -> Query object", "tags": "connection-pool, foreign-key, orm, sqlite, relationship", "url": "https://docs.sqlalchemy.org/en/stable/session/query.html", "author": "Official Docs", "date_published": "04-07-2024", "votes_or_stars": 3038, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:16be93a7-0eed-4b3a-bfce-a04348d77ce7": {"content": "How to use asyncio.create_task in asyncio\n`asyncio.asyncio.create_task` schedules a coroutine as a Task in the event loop. Example usage:\n```python\ntask = asyncio.create_task(send_email(user))\nawait task\n```\nThis is useful when fire-and-forget background tasks, concurrent task management. Performance tip: use TaskGroup (Python 3.11+) for structured concurrency.\nCode signature: asyncio.create_task", "file_path": "https://docs.asyncio.org/en/stable/asyncio/create_task.html", "function_name": "asyncio.create_task", "type": "documentation", "metadata": {"record_id": "16be93a7-0eed-4b3a-bfce-a04348d77ce7", "source_type": "documentation", "domain": "Asyncio/Threading", "library": "asyncio", "title": "How to use asyncio.create_task in asyncio", "content": "`asyncio.asyncio.create_task` schedules a coroutine as a Task in the event loop. Example usage:\n```python\ntask = asyncio.create_task(send_email(user))\nawait task\n```\nThis is useful when fire-and-forget background tasks, concurrent task management. Performance tip: use TaskGroup (Python 3.11+) for structured concurrency.", "code_signature": "asyncio.create_task", "tags": "race-condition, coroutine, executor, semaphore", "url": "https://docs.asyncio.org/en/stable/asyncio/create_task.html", "author": "Official Docs", "date_published": "30-08-2021", "votes_or_stars": 4166, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:0bdc768d-b7db-487e-89c3-580007317fdc": {"content": "BUG: httpx: SSL certificate error on Windows despite valid cert\nProblem: httpx raises SSLCertVerificationError on Windows when system CA store has the cert.\n\nFix/Discussion: httpx uses its own CA bundle (certifi) by default, not the system store. Fix: `httpx.Client(verify=certifi.where())` — already default. For custom certs: `httpx.Client(verify='/path/to/ca-bundle.pem')`. Install `pip install truststore` and use `ssl.create_default_context(ssl.Purpose.SERVER_AUTH)` to use system store.\nCode signature: status:closed", "file_path": "https://github.com/httpx/httpx/issues/8949", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "0bdc768d-b7db-487e-89c3-580007317fdc", "source_type": "github_issue", "domain": "Requests/HTTP/APIs", "library": "httpx", "title": "BUG: httpx: SSL certificate error on Windows despite valid cert", "content": "Problem: httpx raises SSLCertVerificationError on Windows when system CA store has the cert.\n\nFix/Discussion: httpx uses its own CA bundle (certifi) by default, not the system store. Fix: `httpx.Client(verify=certifi.where())` — already default. For custom certs: `httpx.Client(verify='/path/to/ca-bundle.pem')`. Install `pip install truststore` and use `ssl.create_default_context(ssl.Purpose.SERVER_AUTH)` to use system store.", "code_signature": "status:closed", "tags": "oauth, rate-limiting, aiohttp, authentication, websocket, json", "url": "https://github.com/httpx/httpx/issues/8949", "author": "contributor_1420", "date_published": "24-01-2019", "votes_or_stars": 470, "relevance_label": "low", "difficulty_level": "intermediate"}}, "kb:f67134c0-fab0-4666-ab8e-673034f1a287": {"content": "BUG: threading.Timer not cancellable after it fires\nProblem: Calling cancel() on a Timer that has already executed does not raise an error but has no effect.\n\nFix/Discussion: threading.Timer.cancel() only works before the timer fires. Store Timer reference and check `timer.finished.is_set()` to know if it's already run. For repeating timers, use a loop with `threading.Event` for clean cancellation: `stop_event = threading.Event(); stop_event.wait(interval)`.\nCode signature: status:closed", "file_path": "https://github.com/cpython/cpython/issues/4111", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "f67134c0-fab0-4666-ab8e-673034f1a287", "source_type": "github_issue", "domain": "Asyncio/Threading", "library": "cpython", "title": "BUG: threading.Timer not cancellable after it fires", "content": "Problem: Calling cancel() on a Timer that has already executed does not raise an error but has no effect.\n\nFix/Discussion: threading.Timer.cancel() only works before the timer fires. Store Timer reference and check `timer.finished.is_set()` to know if it's already run. For repeating timers, use a loop with `threading.Event` for clean cancellation: `stop_event = threading.Event(); stop_event.wait(interval)`.", "code_signature": "status:closed", "tags": "await, semaphore, gather", "url": "https://github.com/cpython/cpython/issues/4111", "author": "contributor_7884", "date_published": "28-08-2023", "votes_or_stars": 41, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:3f0aa295-37dd-4d75-a3e5-8ce0242bcaf3": {"content": "How to apply a function to each row in Pandas without using iterrows?\n`iterrows()` is slow because it creates a Series per row. Prefer `df.apply(func, axis=1)` for row-wise operations, or better yet, vectorize using NumPy: `np.where(condition, a, b)`. For complex logic, `df.assign()` with vectorized expressions is cleanest. Benchmark: vectorized > apply > itertuples > iterrows.\nCode signature: Use df.apply(func, axis=1) or vectorized NumPy operations for best performance.", "file_path": "https://stackoverflow.com/questions/5446912", "function_name": "Use df.apply(func, axis=1) or vectorized NumPy operations for best performance.", "type": "stack_overflow", "metadata": {"record_id": "3f0aa295-37dd-4d75-a3e5-8ce0242bcaf3", "source_type": "stack_overflow", "domain": "NumPy/Pandas", "library": "dataframe", "title": "How to apply a function to each row in Pandas without using iterrows?", "content": "`iterrows()` is slow because it creates a Series per row. Prefer `df.apply(func, axis=1)` for row-wise operations, or better yet, vectorize using NumPy: `np.where(condition, a, b)`. For complex logic, `df.assign()` with vectorized expressions is cleanest. Benchmark: vectorized > apply > itertuples > iterrows.", "code_signature": "Use df.apply(func, axis=1) or vectorized NumPy operations for best performance.", "tags": "loc, array, numpy, iloc", "url": "https://stackoverflow.com/questions/5446912", "author": "user_563306", "date_published": "21-12-2019", "votes_or_stars": 2262, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:4154ee74-721b-42d3-9401-396f08581dba": {"content": "NumPy broadcasting error: operands could not be broadcast with shapes\nNumPy broadcasts shapes right-to-left. Arrays (3,4) and (4,) are compatible. Arrays (3,4) and (3,) are not — reshape to (3,1). Use `arr.reshape(-1,1)` to add a dimension. Always print `a.shape, b.shape` before operations to debug. `np.expand_dims(arr, axis=1)` is cleaner than reshape for adding dimensions.\nCode signature: Shapes must be compatible: dimensions must match or one of them must be 1.", "file_path": "https://stackoverflow.com/questions/4860684", "function_name": "Shapes must be compatible: dimensions must match or one of them must be 1.", "type": "stack_overflow", "metadata": {"record_id": "4154ee74-721b-42d3-9401-396f08581dba", "source_type": "stack_overflow", "domain": "NumPy/Pandas", "library": "numpy", "title": "NumPy broadcasting error: operands could not be broadcast with shapes", "content": "NumPy broadcasts shapes right-to-left. Arrays (3,4) and (4,) are compatible. Arrays (3,4) and (3,) are not — reshape to (3,1). Use `arr.reshape(-1,1)` to add a dimension. Always print `a.shape, b.shape` before operations to debug. `np.expand_dims(arr, axis=1)` is cleaner than reshape for adding dimensions.", "code_signature": "Shapes must be compatible: dimensions must match or one of them must be 1.", "tags": "pivot, vectorization, groupby, nan", "url": "https://stackoverflow.com/questions/4860684", "author": "user_627024", "date_published": "30-10-2022", "votes_or_stars": 1371, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:feb9f381-922f-4f1d-8bb3-e5f4e0643cd9": {"content": "How to use httpx.AsyncClient in httpx\n`httpx.httpx.AsyncClient` provides an async HTTP client for non-blocking requests. Example usage:\n```python\nasync with httpx.AsyncClient() as client:\n    r = await client.get(url)\n    data = r.json()\n```\nThis is useful when making concurrent HTTP requests in async applications. Performance tip: set timeout=httpx.Timeout(10.0) to prevent hanging requests.\nCode signature: httpx.AsyncClient", "file_path": "https://docs.httpx.org/en/stable/httpx/AsyncClient.html", "function_name": "httpx.AsyncClient", "type": "documentation", "metadata": {"record_id": "feb9f381-922f-4f1d-8bb3-e5f4e0643cd9", "source_type": "documentation", "domain": "Requests/HTTP/APIs", "library": "httpx", "title": "How to use httpx.AsyncClient in httpx", "content": "`httpx.httpx.AsyncClient` provides an async HTTP client for non-blocking requests. Example usage:\n```python\nasync with httpx.AsyncClient() as client:\n    r = await client.get(url)\n    data = r.json()\n```\nThis is useful when making concurrent HTTP requests in async applications. Performance tip: set timeout=httpx.Timeout(10.0) to prevent hanging requests.", "code_signature": "httpx.AsyncClient", "tags": "json, retry, httpx, rate-limiting, aiohttp", "url": "https://docs.httpx.org/en/stable/httpx/AsyncClient.html", "author": "Official Docs", "date_published": "15-03-2020", "votes_or_stars": 4218, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:5c44ca8d-37af-4806-b046-746c9c8e5f19": {"content": "Python threading: how to share data between threads safely?\nFor simple counters: use `threading.Lock()` as context manager. For complex shared state: prefer `queue.Queue` (thread-safe by design). Avoid global variables without locks. `threading.local()` gives each thread its own data copy. For read-heavy workloads, `threading.RLock` allows re-entrant acquisition.\nCode signature: Use threading.Lock for mutual exclusion or queue.Queue for producer-consumer patterns.", "file_path": "https://stackoverflow.com/questions/3195773", "function_name": "Use threading.Lock for mutual exclusion or queue.Queue for producer-consumer patterns.", "type": "stack_overflow", "metadata": {"record_id": "5c44ca8d-37af-4806-b046-746c9c8e5f19", "source_type": "stack_overflow", "domain": "Asyncio/Threading", "library": "lock", "title": "Python threading: how to share data between threads safely?", "content": "For simple counters: use `threading.Lock()` as context manager. For complex shared state: prefer `queue.Queue` (thread-safe by design). Avoid global variables without locks. `threading.local()` gives each thread its own data copy. For read-heavy workloads, `threading.RLock` allows re-entrant acquisition.", "code_signature": "Use threading.Lock for mutual exclusion or queue.Queue for producer-consumer patterns.", "tags": "threading, asyncio, executor, event-loop", "url": "https://stackoverflow.com/questions/3195773", "author": "user_714318", "date_published": "16-05-2019", "votes_or_stars": 1083, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:612a12cb-a2cc-4433-b327-9456c8bdc88c": {"content": "BUG: SettingWithCopyWarning when modifying sliced DataFrame\nProblem: Modifying a slice of a DataFrame raises SettingWithCopyWarning and may not update original.\n\nFix/Discussion: Always use .loc or .copy() when slicing. Use `df.loc[mask, 'col'] = val` for assignment. Create explicit copies: `subset = df[mask].copy()`. In Pandas 2.0+, Copy-on-Write makes this behavior consistent — enable with `pd.options.mode.copy_on_write = True`.\nCode signature: status:closed", "file_path": "https://github.com/pandas/pandas/issues/3264", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "612a12cb-a2cc-4433-b327-9456c8bdc88c", "source_type": "github_issue", "domain": "NumPy/Pandas", "library": "pandas", "title": "BUG: SettingWithCopyWarning when modifying sliced DataFrame", "content": "Problem: Modifying a slice of a DataFrame raises SettingWithCopyWarning and may not update original.\n\nFix/Discussion: Always use .loc or .copy() when slicing. Use `df.loc[mask, 'col'] = val` for assignment. Create explicit copies: `subset = df[mask].copy()`. In Pandas 2.0+, Copy-on-Write makes this behavior consistent — enable with `pd.options.mode.copy_on_write = True`.", "code_signature": "status:closed", "tags": "pivot, vectorization, loc, merge, nan, numpy", "url": "https://github.com/pandas/pandas/issues/3264", "author": "contributor_6628", "date_published": "12-03-2022", "votes_or_stars": 20, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:9c37a358-94fd-4f8c-a94d-0179eed2c19a": {"content": "BUG: Alembic autogenerate misses index changes\nProblem: Alembic --autogenerate does not detect changes to indexes or constraints on existing tables.\n\nFix/Discussion: Alembic autogenerate compares metadata to database state but has limitations. Workarounds: (1) Manually add `op.create_index()` to migration. (2) Use `include_schemas=True` in env.py. (3) Run `alembic check` to see detected changes. (4) Always review autogenerated migrations before applying.\nCode signature: status:open", "file_path": "https://github.com/alembic/alembic/issues/7344", "function_name": "status:open", "type": "github_issue", "metadata": {"record_id": "9c37a358-94fd-4f8c-a94d-0179eed2c19a", "source_type": "github_issue", "domain": "SQLAlchemy/Databases", "library": "alembic", "title": "BUG: Alembic autogenerate misses index changes", "content": "Problem: Alembic --autogenerate does not detect changes to indexes or constraints on existing tables.\n\nFix/Discussion: Alembic autogenerate compares metadata to database state but has limitations. Workarounds: (1) Manually add `op.create_index()` to migration. (2) Use `include_schemas=True` in env.py. (3) Run `alembic check` to see detected changes. (4) Always review autogenerated migrations before applying.", "code_signature": "status:open", "tags": "query, orm, sqlalchemy, alembic, connection-pool, transaction", "url": "https://github.com/alembic/alembic/issues/7344", "author": "contributor_3601", "date_published": "16-06-2019", "votes_or_stars": 212, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:f2965fdb-a74d-4a6f-9f9e-9134c1410297": {"content": "How to use df.groupby in Pandas\n`Pandas.df.groupby` groups DataFrame rows by column values. Example usage:\n```python\ndf.groupby('category')['value'].mean()\n```\nThis is useful when aggregation, transformation, filtering by group. Performance tip: avoid apply() on large DataFrames; prefer agg().\nCode signature: df.groupby", "file_path": "https://docs.pandas.org/en/stable/df/groupby.html", "function_name": "df.groupby", "type": "documentation", "metadata": {"record_id": "f2965fdb-a74d-4a6f-9f9e-9134c1410297", "source_type": "documentation", "domain": "NumPy/Pandas", "library": "Pandas", "title": "How to use df.groupby in Pandas", "content": "`Pandas.df.groupby` groups DataFrame rows by column values. Example usage:\n```python\ndf.groupby('category')['value'].mean()\n```\nThis is useful when aggregation, transformation, filtering by group. Performance tip: avoid apply() on large DataFrames; prefer agg().", "code_signature": "df.groupby", "tags": "nan, numpy, loc, iloc", "url": "https://docs.pandas.org/en/stable/df/groupby.html", "author": "Official Docs", "date_published": "24-07-2020", "votes_or_stars": 801, "relevance_label": "low", "difficulty_level": "advanced"}}, "kb:fa4e5a57-8601-4a38-89a8-303bbd154ae2": {"content": "How do I reshape a wide DataFrame to long format?\n`pd.melt(df, id_vars=['id'], value_vars=['col1','col2'])` unpivots columns to rows. `df.stack()` moves column index to row index. For the reverse, use `df.pivot()` or `df.unstack()`. Always reset_index() after stack/unstack to get a clean DataFrame.\nCode signature: Use pd.melt() or df.stack() depending on your structure.", "file_path": "https://stackoverflow.com/questions/6229731", "function_name": "Use pd.melt() or df.stack() depending on your structure.", "type": "stack_overflow", "metadata": {"record_id": "fa4e5a57-8601-4a38-89a8-303bbd154ae2", "source_type": "stack_overflow", "domain": "NumPy/Pandas", "library": "nan", "title": "How do I reshape a wide DataFrame to long format?", "content": "`pd.melt(df, id_vars=['id'], value_vars=['col1','col2'])` unpivots columns to rows. `df.stack()` moves column index to row index. For the reverse, use `df.pivot()` or `df.unstack()`. Always reset_index() after stack/unstack to get a clean DataFrame.", "code_signature": "Use pd.melt() or df.stack() depending on your structure.", "tags": "reshape, pivot, dtype, pandas, vectorization", "url": "https://stackoverflow.com/questions/6229731", "author": "user_428373", "date_published": "01-04-2020", "votes_or_stars": 815, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:2778393c-25d7-481c-a411-cab50b19a34d": {"content": "How to implement database connection pooling with SQLAlchemy?\n`create_engine(url, pool_size=10, max_overflow=20, pool_timeout=30, pool_recycle=1800)`. Use `pool_pre_ping=True` to detect stale connections. For async: use `AsyncEngine` with `create_async_engine()`. Monitor with `engine.pool.status()`. NullPool disables pooling for scripts/serverless environments.\nCode signature: create_engine() has built-in pooling. Configure pool_size and max_overflow.", "file_path": "https://stackoverflow.com/questions/7358113", "function_name": "create_engine() has built-in pooling. Configure pool_size and max_overflow.", "type": "stack_overflow", "metadata": {"record_id": "2778393c-25d7-481c-a411-cab50b19a34d", "source_type": "stack_overflow", "domain": "SQLAlchemy/Databases", "library": "session", "title": "How to implement database connection pooling with SQLAlchemy?", "content": "`create_engine(url, pool_size=10, max_overflow=20, pool_timeout=30, pool_recycle=1800)`. Use `pool_pre_ping=True` to detect stale connections. For async: use `AsyncEngine` with `create_async_engine()`. Monitor with `engine.pool.status()`. NullPool disables pooling for scripts/serverless environments.", "code_signature": "create_engine() has built-in pooling. Configure pool_size and max_overflow.", "tags": "connection-pool, migration, query, relationship, alembic, orm", "url": "https://stackoverflow.com/questions/7358113", "author": "user_12260", "date_published": "16-01-2024", "votes_or_stars": 250, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:b3de3667-b59d-4d83-b441-3b089e4333af": {"content": "How to use df.groupby in Pandas\n`Pandas.df.groupby` groups DataFrame rows by column values. Example usage:\n```python\ndf.groupby('category')['value'].mean()\n```\nThis is useful when aggregation, transformation, filtering by group. Performance tip: avoid apply() on large DataFrames; prefer agg().\nCode signature: df.groupby", "file_path": "https://docs.pandas.org/en/stable/df/groupby.html", "function_name": "df.groupby", "type": "documentation", "metadata": {"record_id": "b3de3667-b59d-4d83-b441-3b089e4333af", "source_type": "documentation", "domain": "NumPy/Pandas", "library": "Pandas", "title": "How to use df.groupby in Pandas", "content": "`Pandas.df.groupby` groups DataFrame rows by column values. Example usage:\n```python\ndf.groupby('category')['value'].mean()\n```\nThis is useful when aggregation, transformation, filtering by group. Performance tip: avoid apply() on large DataFrames; prefer agg().", "code_signature": "df.groupby", "tags": "loc, merge, numpy", "url": "https://docs.pandas.org/en/stable/df/groupby.html", "author": "Official Docs", "date_published": "02-03-2020", "votes_or_stars": 817, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:fe85764a-6cba-40db-9b28-7ee6819e359b": {"content": "How do I efficiently fill NaN values in a Pandas DataFrame?\nFor large DataFrames, avoid looping. Use `df.fillna({'col': val})` to fill specific columns. `df.ffill()` propagates last valid observation forward. For time-series data, `df.interpolate()` provides smoother results. Always check `df.isna().sum()` before and after to verify.\nCode signature: Use df.fillna(value) or df.ffill()/df.bfill() for forward/backward fill.", "file_path": "https://stackoverflow.com/questions/6438436", "function_name": "Use df.fillna(value) or df.ffill()/df.bfill() for forward/backward fill.", "type": "stack_overflow", "metadata": {"record_id": "fe85764a-6cba-40db-9b28-7ee6819e359b", "source_type": "stack_overflow", "domain": "NumPy/Pandas", "library": "loc", "title": "How do I efficiently fill NaN values in a Pandas DataFrame?", "content": "For large DataFrames, avoid looping. Use `df.fillna({'col': val})` to fill specific columns. `df.ffill()` propagates last valid observation forward. For time-series data, `df.interpolate()` provides smoother results. Always check `df.isna().sum()` before and after to verify.", "code_signature": "Use df.fillna(value) or df.ffill()/df.bfill() for forward/backward fill.", "tags": "broadcasting, array, vectorization", "url": "https://stackoverflow.com/questions/6438436", "author": "user_522340", "date_published": "11-08-2024", "votes_or_stars": 30, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:2fa87447-3398-4ceb-988f-053206331fd1": {"content": "How to run background tasks in FastAPI?\nFastAPI's `BackgroundTasks`: add `background_tasks: BackgroundTasks` as a parameter, then `background_tasks.add_task(func, *args)`. Runs after response is sent. For heavy workloads (email, ML inference), use Celery with Redis/RabbitMQ broker. For async background work, `asyncio.create_task()` works within async endpoints.\nCode signature: Use BackgroundTasks for lightweight tasks or Celery for heavy/distributed work.", "file_path": "https://stackoverflow.com/questions/8077999", "function_name": "Use BackgroundTasks for lightweight tasks or Celery for heavy/distributed work.", "type": "stack_overflow", "metadata": {"record_id": "2fa87447-3398-4ceb-988f-053206331fd1", "source_type": "stack_overflow", "domain": "FastAPI/Flask/Django", "library": "django", "title": "How to run background tasks in FastAPI?", "content": "FastAPI's `BackgroundTasks`: add `background_tasks: BackgroundTasks` as a parameter, then `background_tasks.add_task(func, *args)`. Runs after response is sent. For heavy workloads (email, ML inference), use Celery with Redis/RabbitMQ broker. For async background work, `asyncio.create_task()` works within async endpoints.", "code_signature": "Use BackgroundTasks for lightweight tasks or Celery for heavy/distributed work.", "tags": "response, rest, request", "url": "https://stackoverflow.com/questions/8077999", "author": "user_202401", "date_published": "27-03-2020", "votes_or_stars": 1858, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:a82dd512-c60e-4092-a178-996551fe7f4d": {"content": "How to add retry logic to requests?\n```python\nfrom requests.adapters import HTTPAdapter\nfrom urllib3.util.retry import Retry\nretry = Retry(total=3, backoff_factor=1, status_forcelist=[500,502,503,504])\nadapter = HTTPAdapter(max_retries=retry)\nsession.mount('https://', adapter)\n```\nCode signature: Mount an HTTPAdapter with urllib3 Retry on the Session.", "file_path": "https://stackoverflow.com/questions/1669324", "function_name": "Mount an HTTPAdapter with urllib3 Retry on the Session.", "type": "stack_overflow", "metadata": {"record_id": "a82dd512-c60e-4092-a178-996551fe7f4d", "source_type": "stack_overflow", "domain": "Requests/HTTP/APIs", "library": "websocket", "title": "How to add retry logic to requests?", "content": "```python\nfrom requests.adapters import HTTPAdapter\nfrom urllib3.util.retry import Retry\nretry = Retry(total=3, backoff_factor=1, status_forcelist=[500,502,503,504])\nadapter = HTTPAdapter(max_retries=retry)\nsession.mount('https://', adapter)\n```", "code_signature": "Mount an HTTPAdapter with urllib3 Retry on the Session.", "tags": "oauth, timeout, httpx, rate-limiting, authentication, headers", "url": "https://stackoverflow.com/questions/1669324", "author": "user_952590", "date_published": "17-12-2022", "votes_or_stars": 1531, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:5d959279-21c2-475f-acfe-a4f210e80d3f": {"content": "asyncio — Concurrent Coroutines\nThe `asyncio.gather` function in asyncio runs multiple coroutines concurrently and collects results. It accepts `*coros, return_exceptions=False` as a parameter and returns list of results. Common use cases include parallel I/O operations such as multiple API calls. Note that set return_exceptions=True to prevent one failure from cancelling all tasks.\nCode signature: asyncio.gather(*coros, return_exceptions=False) -> list of results", "file_path": "https://docs.asyncio.org/en/stable/asyncio/gather.html", "function_name": "asyncio.gather(*coros, return_exceptions=False) -> list of results", "type": "documentation", "metadata": {"record_id": "5d959279-21c2-475f-acfe-a4f210e80d3f", "source_type": "documentation", "domain": "Asyncio/Threading", "library": "asyncio", "title": "asyncio — Concurrent Coroutines", "content": "The `asyncio.gather` function in asyncio runs multiple coroutines concurrently and collects results. It accepts `*coros, return_exceptions=False` as a parameter and returns list of results. Common use cases include parallel I/O operations such as multiple API calls. Note that set return_exceptions=True to prevent one failure from cancelling all tasks.", "code_signature": "asyncio.gather(*coros, return_exceptions=False) -> list of results", "tags": "event-loop, race-condition, asyncio, semaphore", "url": "https://docs.asyncio.org/en/stable/asyncio/gather.html", "author": "Official Docs", "date_published": "07-02-2019", "votes_or_stars": 530, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:405cae83-1a46-4a3b-81d7-6f973da9767a": {"content": "Django ORM Queries — official reference\nOfficial documentation for Django ORM Queries. The method signature is `QuerySet.filter(**kwargs)`. filters QuerySet rows matching given field lookups. Raises `FieldError` when an unknown field lookup is provided. See also: QuerySet.exclude, QuerySet.annotate, Q objects.\nCode signature: QuerySet.filter(**kwargs)", "file_path": "https://docs.django.org/en/stable/QuerySet/filter.html", "function_name": "QuerySet.filter(**kwargs)", "type": "documentation", "metadata": {"record_id": "405cae83-1a46-4a3b-81d7-6f973da9767a", "source_type": "documentation", "domain": "FastAPI/Flask/Django", "library": "Django", "title": "Django ORM Queries — official reference", "content": "Official documentation for Django ORM Queries. The method signature is `QuerySet.filter(**kwargs)`. filters QuerySet rows matching given field lookups. Raises `FieldError` when an unknown field lookup is provided. See also: QuerySet.exclude, QuerySet.annotate, Q objects.", "code_signature": "QuerySet.filter(**kwargs)", "tags": "orm, dependency-injection, pydantic, django, response", "url": "https://docs.django.org/en/stable/QuerySet/filter.html", "author": "Official Docs", "date_published": "06-11-2023", "votes_or_stars": 2430, "relevance_label": "low", "difficulty_level": "intermediate"}}, "kb:dade8440-f7e1-43fb-9782-7d1602b28a52": {"content": "BUG: requests hangs indefinitely on slow server responses\nProblem: requests.get() hangs forever when server accepts connection but never sends response.\n\nFix/Discussion: Always set both connect and read timeouts: `requests.get(url, timeout=(3.05, 27))`. Tuple: (connect_timeout, read_timeout). Or single value for both. Mount a Session with default timeouts using a custom HTTPAdapter subclass. Never use `timeout=None` in production code.\nCode signature: status:closed", "file_path": "https://github.com/requests/requests/issues/4019", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "dade8440-f7e1-43fb-9782-7d1602b28a52", "source_type": "github_issue", "domain": "Requests/HTTP/APIs", "library": "requests", "title": "BUG: requests hangs indefinitely on slow server responses", "content": "Problem: requests.get() hangs forever when server accepts connection but never sends response.\n\nFix/Discussion: Always set both connect and read timeouts: `requests.get(url, timeout=(3.05, 27))`. Tuple: (connect_timeout, read_timeout). Or single value for both. Mount a Session with default timeouts using a custom HTTPAdapter subclass. Never use `timeout=None` in production code.", "code_signature": "status:closed", "tags": "http, requests, oauth, rest-api", "url": "https://github.com/requests/requests/issues/4019", "author": "contributor_4599", "date_published": "30-01-2023", "votes_or_stars": 243, "relevance_label": "low", "difficulty_level": "advanced"}}, "kb:04e651f5-157a-4140-a0ae-e0a2fd273774": {"content": "BUG: SQLAlchemy connection pool exhaustion under load\nProblem: Application hangs under concurrent load — all connections are checked out and pool is exhausted.\n\nFix/Discussion: Increase `pool_size` and `max_overflow`. Ensure sessions are always closed: use `contextmanager` or `with Session() as session:`. Set `pool_timeout` to fail fast. Enable `pool_pre_ping=True`. Monitor with `engine.pool.checkedout()`. In async: use `AsyncSession` with `async_scoped_session`.\nCode signature: status:closed", "file_path": "https://github.com/sqlalchemy/sqlalchemy/issues/106", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "04e651f5-157a-4140-a0ae-e0a2fd273774", "source_type": "github_issue", "domain": "SQLAlchemy/Databases", "library": "sqlalchemy", "title": "BUG: SQLAlchemy connection pool exhaustion under load", "content": "Problem: Application hangs under concurrent load — all connections are checked out and pool is exhausted.\n\nFix/Discussion: Increase `pool_size` and `max_overflow`. Ensure sessions are always closed: use `contextmanager` or `with Session() as session:`. Set `pool_timeout` to fail fast. Enable `pool_pre_ping=True`. Monitor with `engine.pool.checkedout()`. In async: use `AsyncSession` with `async_scoped_session`.", "code_signature": "status:closed", "tags": "orm, query, alembic, migration", "url": "https://github.com/sqlalchemy/sqlalchemy/issues/106", "author": "contributor_5078", "date_published": "12-03-2019", "votes_or_stars": 262, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:1cc6d168-4f6e-4a5b-a59e-6c5980c944d5": {"content": "PERF: requests connection not reused between calls — new TCP connection each time\nProblem: Each requests.get() call opens a new TCP connection, causing high latency.\n\nFix/Discussion: requests.get/post etc. create a new Session per call. Fix: use a persistent Session: `session = requests.Session()` and reuse it. Sessions pool connections via urllib3. For thread-safety: create one Session per thread. For async: use httpx.AsyncClient or aiohttp.ClientSession for async connection pooling.\nCode signature: status:closed", "file_path": "https://github.com/requests/requests/issues/3605", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "1cc6d168-4f6e-4a5b-a59e-6c5980c944d5", "source_type": "github_issue", "domain": "Requests/HTTP/APIs", "library": "requests", "title": "PERF: requests connection not reused between calls — new TCP connection each time", "content": "Problem: Each requests.get() call opens a new TCP connection, causing high latency.\n\nFix/Discussion: requests.get/post etc. create a new Session per call. Fix: use a persistent Session: `session = requests.Session()` and reuse it. Sessions pool connections via urllib3. For thread-safety: create one Session per thread. For async: use httpx.AsyncClient or aiohttp.ClientSession for async connection pooling.", "code_signature": "status:closed", "tags": "aiohttp, rest-api, json", "url": "https://github.com/requests/requests/issues/3605", "author": "contributor_1152", "date_published": "08-04-2019", "votes_or_stars": 351, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:4d24778b-d930-48e8-9481-a267b3789928": {"content": "threading Thread Synchronization — official reference\nOfficial documentation for threading Thread Synchronization. The method signature is `threading.Lock()`. creates a mutual-exclusion lock for thread-safe access. Raises `RuntimeError` when lock is released without being acquired. See also: threading.RLock, threading.Semaphore, threading.Event.\nCode signature: threading.Lock()", "file_path": "https://docs.threading.org/en/stable/threading/Lock.html", "function_name": "threading.Lock()", "type": "documentation", "metadata": {"record_id": "4d24778b-d930-48e8-9481-a267b3789928", "source_type": "documentation", "domain": "Asyncio/Threading", "library": "threading", "title": "threading Thread Synchronization — official reference", "content": "Official documentation for threading Thread Synchronization. The method signature is `threading.Lock()`. creates a mutual-exclusion lock for thread-safe access. Raises `RuntimeError` when lock is released without being acquired. See also: threading.RLock, threading.Semaphore, threading.Event.", "code_signature": "threading.Lock()", "tags": "await, threading, event-loop, task", "url": "https://docs.threading.org/en/stable/threading/Lock.html", "author": "Official Docs", "date_published": "07-04-2024", "votes_or_stars": 4853, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:f9f4fbaa-d28e-44a6-bf42-73418a529192": {"content": "asyncio RuntimeError: This event loop is already running\nThis error is common in Jupyter and FastAPI. Solutions: (1) In async context: just use `await coroutine()`. (2) In Jupyter: `import nest_asyncio; nest_asyncio.apply()`. (3) Run in thread: `asyncio.get_event_loop().run_in_executor(None, sync_func)`. (4) Use `asyncio.ensure_future()` to schedule from within async code.\nCode signature: You're calling asyncio.run() inside an already running event loop. Use await instead.", "file_path": "https://stackoverflow.com/questions/2229110", "function_name": "You're calling asyncio.run() inside an already running event loop. Use await instead.", "type": "stack_overflow", "metadata": {"record_id": "f9f4fbaa-d28e-44a6-bf42-73418a529192", "source_type": "stack_overflow", "domain": "Asyncio/Threading", "library": "async", "title": "asyncio RuntimeError: This event loop is already running", "content": "This error is common in Jupyter and FastAPI. Solutions: (1) In async context: just use `await coroutine()`. (2) In Jupyter: `import nest_asyncio; nest_asyncio.apply()`. (3) Run in thread: `asyncio.get_event_loop().run_in_executor(None, sync_func)`. (4) Use `asyncio.ensure_future()` to schedule from within async code.", "code_signature": "You're calling asyncio.run() inside an already running event loop. Use await instead.", "tags": "deadlock, race-condition, semaphore, async, lock, coroutine", "url": "https://stackoverflow.com/questions/2229110", "author": "user_573750", "date_published": "16-12-2019", "votes_or_stars": 2335, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:95b1457a-7718-494b-b241-8902a833b5ac": {"content": "How do I reshape a wide DataFrame to long format?\n`pd.melt(df, id_vars=['id'], value_vars=['col1','col2'])` unpivots columns to rows. `df.stack()` moves column index to row index. For the reverse, use `df.pivot()` or `df.unstack()`. Always reset_index() after stack/unstack to get a clean DataFrame.\nCode signature: Use pd.melt() or df.stack() depending on your structure.", "file_path": "https://stackoverflow.com/questions/6229731", "function_name": "Use pd.melt() or df.stack() depending on your structure.", "type": "stack_overflow", "metadata": {"record_id": "95b1457a-7718-494b-b241-8902a833b5ac", "source_type": "stack_overflow", "domain": "NumPy/Pandas", "library": "nan", "title": "How do I reshape a wide DataFrame to long format?", "content": "`pd.melt(df, id_vars=['id'], value_vars=['col1','col2'])` unpivots columns to rows. `df.stack()` moves column index to row index. For the reverse, use `df.pivot()` or `df.unstack()`. Always reset_index() after stack/unstack to get a clean DataFrame.", "code_signature": "Use pd.melt() or df.stack() depending on your structure.", "tags": "iloc, reshape, loc, pivot, numpy, dataframe", "url": "https://stackoverflow.com/questions/6229731", "author": "user_428373", "date_published": "12-09-2022", "votes_or_stars": 812, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:0894eaca-f6ba-4328-8b39-85730f7d6811": {"content": "BUG: asyncio task leaks when exception is not handled\nProblem: Unhandled exceptions in fire-and-forget asyncio tasks are silently ignored, causing task leaks.\n\nFix/Discussion: Always await tasks or add exception handlers. Use `task.add_done_callback()` to log exceptions. In Python 3.11+, use TaskGroup for structured concurrency — exceptions propagate automatically. Set `asyncio.get_event_loop().set_exception_handler()` for global unhandled task exception logging.\nCode signature: status:closed", "file_path": "https://github.com/cpython/cpython/issues/5591", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "0894eaca-f6ba-4328-8b39-85730f7d6811", "source_type": "github_issue", "domain": "Asyncio/Threading", "library": "cpython", "title": "BUG: asyncio task leaks when exception is not handled", "content": "Problem: Unhandled exceptions in fire-and-forget asyncio tasks are silently ignored, causing task leaks.\n\nFix/Discussion: Always await tasks or add exception handlers. Use `task.add_done_callback()` to log exceptions. In Python 3.11+, use TaskGroup for structured concurrency — exceptions propagate automatically. Set `asyncio.get_event_loop().set_exception_handler()` for global unhandled task exception logging.", "code_signature": "status:closed", "tags": "event-loop, asyncio, lock, race-condition", "url": "https://github.com/cpython/cpython/issues/5591", "author": "contributor_1630", "date_published": "07-05-2022", "votes_or_stars": 345, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:68db51c3-242d-430c-9cb4-2a4e7d30349d": {"content": "FEAT: pathlib: add support for reading/writing with encoding shorthand\nProblem: pathlib.Path.read_text() does not default to UTF-8, causing issues across platforms.\n\nFix/Discussion: Always pass encoding explicitly: `Path('file').read_text(encoding='utf-8')`. Python 3.10+ added `encoding='locale'` default change proposal. Best practice: always specify encoding in all file I/O operations for portability.\nCode signature: status:closed", "file_path": "https://github.com/cpython/cpython/issues/2707", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "68db51c3-242d-430c-9cb4-2a4e7d30349d", "source_type": "github_issue", "domain": "OS/Subprocess/File I/O", "library": "cpython", "title": "FEAT: pathlib: add support for reading/writing with encoding shorthand", "content": "Problem: pathlib.Path.read_text() does not default to UTF-8, causing issues across platforms.\n\nFix/Discussion: Always pass encoding explicitly: `Path('file').read_text(encoding='utf-8')`. Python 3.10+ added `encoding='locale'` default change proposal. Best practice: always specify encoding in all file I/O operations for portability.", "code_signature": "status:closed", "tags": "pipe, stderr, stdin, env-variable, pathlib", "url": "https://github.com/cpython/cpython/issues/2707", "author": "contributor_7877", "date_published": "11-09-2023", "votes_or_stars": 451, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:4cb043cf-274e-4581-86f2-686bc1a600ea": {"content": "BUG: asyncio task leaks when exception is not handled\nProblem: Unhandled exceptions in fire-and-forget asyncio tasks are silently ignored, causing task leaks.\n\nFix/Discussion: Always await tasks or add exception handlers. Use `task.add_done_callback()` to log exceptions. In Python 3.11+, use TaskGroup for structured concurrency — exceptions propagate automatically. Set `asyncio.get_event_loop().set_exception_handler()` for global unhandled task exception logging.\nCode signature: status:closed", "file_path": "https://github.com/cpython/cpython/issues/5591", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "4cb043cf-274e-4581-86f2-686bc1a600ea", "source_type": "github_issue", "domain": "Asyncio/Threading", "library": "cpython", "title": "BUG: asyncio task leaks when exception is not handled", "content": "Problem: Unhandled exceptions in fire-and-forget asyncio tasks are silently ignored, causing task leaks.\n\nFix/Discussion: Always await tasks or add exception handlers. Use `task.add_done_callback()` to log exceptions. In Python 3.11+, use TaskGroup for structured concurrency — exceptions propagate automatically. Set `asyncio.get_event_loop().set_exception_handler()` for global unhandled task exception logging.", "code_signature": "status:closed", "tags": "asyncio, semaphore, lock, race-condition, task", "url": "https://github.com/cpython/cpython/issues/5591", "author": "contributor_1630", "date_published": "24-07-2023", "votes_or_stars": 361, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:5d72c359-3fb6-4282-b470-e017c7206ca2": {"content": "How to use Path.glob in pathlib\n`pathlib.Path.glob` yields all paths matching the given glob pattern. Example usage:\n```python\npy_files = list(Path('src').glob('**/*.py'))\n```\nThis is useful when finding files recursively, batch file processing. Performance tip: iterate the generator lazily rather than converting to list for large directories.\nCode signature: Path.glob", "file_path": "https://docs.pathlib.org/en/stable/Path/glob.html", "function_name": "Path.glob", "type": "documentation", "metadata": {"record_id": "5d72c359-3fb6-4282-b470-e017c7206ca2", "source_type": "documentation", "domain": "OS/Subprocess/File I/O", "library": "pathlib", "title": "How to use Path.glob in pathlib", "content": "`pathlib.Path.glob` yields all paths matching the given glob pattern. Example usage:\n```python\npy_files = list(Path('src').glob('**/*.py'))\n```\nThis is useful when finding files recursively, batch file processing. Performance tip: iterate the generator lazily rather than converting to list for large directories.", "code_signature": "Path.glob", "tags": "glob, stdout, os", "url": "https://docs.pathlib.org/en/stable/Path/glob.html", "author": "Official Docs", "date_published": "30-11-2019", "votes_or_stars": 3220, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:b13bb9e1-b3a3-4f14-ac0f-7d0e6fbd5e01": {"content": "BUG: os.path.join ignores prefix when component starts with /\nProblem: os.path.join('base', '/absolute') returns '/absolute', ignoring 'base'.\n\nFix/Discussion: This is documented behavior: os.path.join discards previous components when it encounters an absolute path. Fix: use `pathlib.Path(base) / component.lstrip('/')`. Validate user-provided paths with `Path(path).resolve().is_relative_to(base_dir)` to prevent path traversal.\nCode signature: status:closed", "file_path": "https://github.com/cpython/cpython/issues/7712", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "b13bb9e1-b3a3-4f14-ac0f-7d0e6fbd5e01", "source_type": "github_issue", "domain": "OS/Subprocess/File I/O", "library": "cpython", "title": "BUG: os.path.join ignores prefix when component starts with /", "content": "Problem: os.path.join('base', '/absolute') returns '/absolute', ignoring 'base'.\n\nFix/Discussion: This is documented behavior: os.path.join discards previous components when it encounters an absolute path. Fix: use `pathlib.Path(base) / component.lstrip('/')`. Validate user-provided paths with `Path(path).resolve().is_relative_to(base_dir)` to prevent path traversal.", "code_signature": "status:closed", "tags": "os, shutil, env-variable", "url": "https://github.com/cpython/cpython/issues/7712", "author": "contributor_8802", "date_published": "22-05-2021", "votes_or_stars": 426, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:a1a2bd0b-3622-475f-b287-3f9f37b2e4b3": {"content": "PERF: requests connection not reused between calls — new TCP connection each time\nProblem: Each requests.get() call opens a new TCP connection, causing high latency.\n\nFix/Discussion: requests.get/post etc. create a new Session per call. Fix: use a persistent Session: `session = requests.Session()` and reuse it. Sessions pool connections via urllib3. For thread-safety: create one Session per thread. For async: use httpx.AsyncClient or aiohttp.ClientSession for async connection pooling.\nCode signature: status:closed", "file_path": "https://github.com/requests/requests/issues/3605", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "a1a2bd0b-3622-475f-b287-3f9f37b2e4b3", "source_type": "github_issue", "domain": "Requests/HTTP/APIs", "library": "requests", "title": "PERF: requests connection not reused between calls — new TCP connection each time", "content": "Problem: Each requests.get() call opens a new TCP connection, causing high latency.\n\nFix/Discussion: requests.get/post etc. create a new Session per call. Fix: use a persistent Session: `session = requests.Session()` and reuse it. Sessions pool connections via urllib3. For thread-safety: create one Session per thread. For async: use httpx.AsyncClient or aiohttp.ClientSession for async connection pooling.", "code_signature": "status:closed", "tags": "http, timeout, httpx, authentication, rate-limiting, rest-api", "url": "https://github.com/requests/requests/issues/3605", "author": "contributor_1152", "date_published": "11-04-2022", "votes_or_stars": 349, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:919076bf-1143-4345-9a1f-a8059f2e30b5": {"content": "How to handle database migrations with Alembic?\nWorkflow: (1) Change SQLAlchemy models. (2) `alembic revision --autogenerate -m 'description'`. (3) Review generated migration script. (4) `alembic upgrade head` to apply. Always review autogenerated migrations — they miss some changes (indexes, constraints). Use `alembic downgrade -1` to rollback. Store migrations in version control.\nCode signature: Use alembic revision --autogenerate to create migrations from model changes.", "file_path": "https://stackoverflow.com/questions/9436429", "function_name": "Use alembic revision --autogenerate to create migrations from model changes.", "type": "stack_overflow", "metadata": {"record_id": "919076bf-1143-4345-9a1f-a8059f2e30b5", "source_type": "stack_overflow", "domain": "SQLAlchemy/Databases", "library": "connection-pool", "title": "How to handle database migrations with Alembic?", "content": "Workflow: (1) Change SQLAlchemy models. (2) `alembic revision --autogenerate -m 'description'`. (3) Review generated migration script. (4) `alembic upgrade head` to apply. Always review autogenerated migrations — they miss some changes (indexes, constraints). Use `alembic downgrade -1` to rollback. Store migrations in version control.", "code_signature": "Use alembic revision --autogenerate to create migrations from model changes.", "tags": "postgresql, connection-pool, sqlite, relationship", "url": "https://stackoverflow.com/questions/9436429", "author": "user_974047", "date_published": "25-09-2022", "votes_or_stars": 2373, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:feeecde4-c6a3-41d1-9964-145e0306883d": {"content": "How to upload a file with requests?\n```python\nwith open('file.pdf', 'rb') as f:\n    r = requests.post(url, files={'file': ('file.pdf', f, 'application/pdf')})\n```\nFor multiple files: pass a list of tuples. For multipart form data with fields: combine `files` and `data` parameters. Don't set Content-Type manually — requests sets it with boundary.\nCode signature: Use the files parameter with a file-like object or tuple.", "file_path": "https://stackoverflow.com/questions/8896868", "function_name": "Use the files parameter with a file-like object or tuple.", "type": "stack_overflow", "metadata": {"record_id": "feeecde4-c6a3-41d1-9964-145e0306883d", "source_type": "stack_overflow", "domain": "Requests/HTTP/APIs", "library": "requests", "title": "How to upload a file with requests?", "content": "```python\nwith open('file.pdf', 'rb') as f:\n    r = requests.post(url, files={'file': ('file.pdf', f, 'application/pdf')})\n```\nFor multiple files: pass a list of tuples. For multipart form data with fields: combine `files` and `data` parameters. Don't set Content-Type manually — requests sets it with boundary.", "code_signature": "Use the files parameter with a file-like object or tuple.", "tags": "requests, oauth, timeout, json, aiohttp", "url": "https://stackoverflow.com/questions/8896868", "author": "user_243238", "date_published": "13-09-2024", "votes_or_stars": 1593, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:31869085-0150-4acf-99df-f50224d294b1": {"content": "How to use @app.route in Flask\n`Flask.@app.route` maps a URL rule to a view function. Example usage:\n```python\n@app.route('/users/<int:uid>')\ndef get_user(uid):\n    return jsonify(user)\n```\nThis is useful when defining HTTP endpoints in a Flask application. Performance tip: use Blueprints to organize large applications.\nCode signature: @app.route", "file_path": "https://docs.flask.org/en/stable/@app/route.html", "function_name": "@app.route", "type": "documentation", "metadata": {"record_id": "31869085-0150-4acf-99df-f50224d294b1", "source_type": "documentation", "domain": "FastAPI/Flask/Django", "library": "Flask", "title": "How to use @app.route in Flask", "content": "`Flask.@app.route` maps a URL rule to a view function. Example usage:\n```python\n@app.route('/users/<int:uid>')\ndef get_user(uid):\n    return jsonify(user)\n```\nThis is useful when defining HTTP endpoints in a Flask application. Performance tip: use Blueprints to organize large applications.", "code_signature": "@app.route", "tags": "rest, template, fastapi, flask, response, dependency-injection", "url": "https://docs.flask.org/en/stable/@app/route.html", "author": "Official Docs", "date_published": "04-08-2023", "votes_or_stars": 3010, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:cd631eaa-01f8-4062-a4af-10416a74ece7": {"content": "Python: how to safely write to a file atomically?\n```python\nimport tempfile, os\nwith tempfile.NamedTemporaryFile('w', delete=False, dir='.') as tmp:\n    tmp.write(data)\n    tmp.flush()\n    os.fsync(tmp.fileno())\nos.replace(tmp.name, target_path)\n```\n`os.replace()` is atomic on POSIX. This prevents partial writes from corrupting files.\nCode signature: Write to a temp file then rename — rename is atomic on POSIX systems.", "file_path": "https://stackoverflow.com/questions/5394880", "function_name": "Write to a temp file then rename — rename is atomic on POSIX systems.", "type": "stack_overflow", "metadata": {"record_id": "cd631eaa-01f8-4062-a4af-10416a74ece7", "source_type": "stack_overflow", "domain": "OS/Subprocess/File I/O", "library": "stdin", "title": "Python: how to safely write to a file atomically?", "content": "```python\nimport tempfile, os\nwith tempfile.NamedTemporaryFile('w', delete=False, dir='.') as tmp:\n    tmp.write(data)\n    tmp.flush()\n    os.fsync(tmp.fileno())\nos.replace(tmp.name, target_path)\n```\n`os.replace()` is atomic on POSIX. This prevents partial writes from corrupting files.", "code_signature": "Write to a temp file then rename — rename is atomic on POSIX systems.", "tags": "stderr, pipe, process, stdout, file-io", "url": "https://stackoverflow.com/questions/5394880", "author": "user_179430", "date_published": "20-12-2022", "votes_or_stars": 568, "relevance_label": "low", "difficulty_level": "beginner"}}, "kb:c09e40a7-e210-4e00-b709-02a71a81474a": {"content": "SQLAlchemy bulk insert: what's the fastest way?\n`session.bulk_insert_mappings(Model, list_of_dicts)` skips ORM overhead. Even faster: `session.execute(insert(Model), data)`. Avoid `session.add()` in loops. For PostgreSQL, use `insert().on_conflict_do_nothing()` for upserts. Batch in chunks of 1000-5000 rows for optimal performance.\nCode signature: Use session.bulk_insert_mappings() or execute(insert(), list_of_dicts) for maximum speed.", "file_path": "https://stackoverflow.com/questions/5977931", "function_name": "Use session.bulk_insert_mappings() or execute(insert(), list_of_dicts) for maximum speed.", "type": "stack_overflow", "metadata": {"record_id": "c09e40a7-e210-4e00-b709-02a71a81474a", "source_type": "stack_overflow", "domain": "SQLAlchemy/Databases", "library": "query", "title": "SQLAlchemy bulk insert: what's the fastest way?", "content": "`session.bulk_insert_mappings(Model, list_of_dicts)` skips ORM overhead. Even faster: `session.execute(insert(Model), data)`. Avoid `session.add()` in loops. For PostgreSQL, use `insert().on_conflict_do_nothing()` for upserts. Batch in chunks of 1000-5000 rows for optimal performance.", "code_signature": "Use session.bulk_insert_mappings() or execute(insert(), list_of_dicts) for maximum speed.", "tags": "postgresql, sqlalchemy, foreign-key, alembic, sqlite, orm", "url": "https://stackoverflow.com/questions/5977931", "author": "user_238275", "date_published": "23-06-2018", "votes_or_stars": 670, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:f4c9d248-7e04-4026-8bc5-a73ecf8839cd": {"content": "How to implement database connection pooling with SQLAlchemy?\n`create_engine(url, pool_size=10, max_overflow=20, pool_timeout=30, pool_recycle=1800)`. Use `pool_pre_ping=True` to detect stale connections. For async: use `AsyncEngine` with `create_async_engine()`. Monitor with `engine.pool.status()`. NullPool disables pooling for scripts/serverless environments.\nCode signature: create_engine() has built-in pooling. Configure pool_size and max_overflow.", "file_path": "https://stackoverflow.com/questions/7358113", "function_name": "create_engine() has built-in pooling. Configure pool_size and max_overflow.", "type": "stack_overflow", "metadata": {"record_id": "f4c9d248-7e04-4026-8bc5-a73ecf8839cd", "source_type": "stack_overflow", "domain": "SQLAlchemy/Databases", "library": "session", "title": "How to implement database connection pooling with SQLAlchemy?", "content": "`create_engine(url, pool_size=10, max_overflow=20, pool_timeout=30, pool_recycle=1800)`. Use `pool_pre_ping=True` to detect stale connections. For async: use `AsyncEngine` with `create_async_engine()`. Monitor with `engine.pool.status()`. NullPool disables pooling for scripts/serverless environments.", "code_signature": "create_engine() has built-in pooling. Configure pool_size and max_overflow.", "tags": "sqlite, relationship, migration, transaction", "url": "https://stackoverflow.com/questions/7358113", "author": "user_12260", "date_published": "14-05-2023", "votes_or_stars": 275, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:404aff84-5914-42cf-b231-adc9304dc57e": {"content": "BUG: asyncio task leaks when exception is not handled\nProblem: Unhandled exceptions in fire-and-forget asyncio tasks are silently ignored, causing task leaks.\n\nFix/Discussion: Always await tasks or add exception handlers. Use `task.add_done_callback()` to log exceptions. In Python 3.11+, use TaskGroup for structured concurrency — exceptions propagate automatically. Set `asyncio.get_event_loop().set_exception_handler()` for global unhandled task exception logging.\nCode signature: status:closed", "file_path": "https://github.com/cpython/cpython/issues/5591", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "404aff84-5914-42cf-b231-adc9304dc57e", "source_type": "github_issue", "domain": "Asyncio/Threading", "library": "cpython", "title": "BUG: asyncio task leaks when exception is not handled", "content": "Problem: Unhandled exceptions in fire-and-forget asyncio tasks are silently ignored, causing task leaks.\n\nFix/Discussion: Always await tasks or add exception handlers. Use `task.add_done_callback()` to log exceptions. In Python 3.11+, use TaskGroup for structured concurrency — exceptions propagate automatically. Set `asyncio.get_event_loop().set_exception_handler()` for global unhandled task exception logging.", "code_signature": "status:closed", "tags": "lock, semaphore, async", "url": "https://github.com/cpython/cpython/issues/5591", "author": "contributor_1630", "date_published": "20-12-2023", "votes_or_stars": 359, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:933eb5a5-c9e0-4cde-9ac8-bcf0333e90e9": {"content": "How to implement a thread pool in Python?\n```python\nfrom concurrent.futures import ThreadPoolExecutor\nwith ThreadPoolExecutor(max_workers=8) as ex:\n    futures = [ex.submit(task, arg) for arg in args]\n    results = [f.result() for f in futures]\n```\nFor CPU-bound: use ProcessPoolExecutor instead. For async integration: `await loop.run_in_executor(executor, func, *args)`.\nCode signature: Use concurrent.futures.ThreadPoolExecutor for I/O-bound tasks.", "file_path": "https://stackoverflow.com/questions/3646552", "function_name": "Use concurrent.futures.ThreadPoolExecutor for I/O-bound tasks.", "type": "stack_overflow", "metadata": {"record_id": "933eb5a5-c9e0-4cde-9ac8-bcf0333e90e9", "source_type": "stack_overflow", "domain": "Asyncio/Threading", "library": "coroutine", "title": "How to implement a thread pool in Python?", "content": "```python\nfrom concurrent.futures import ThreadPoolExecutor\nwith ThreadPoolExecutor(max_workers=8) as ex:\n    futures = [ex.submit(task, arg) for arg in args]\n    results = [f.result() for f in futures]\n```\nFor CPU-bound: use ProcessPoolExecutor instead. For async integration: `await loop.run_in_executor(executor, func, *args)`.", "code_signature": "Use concurrent.futures.ThreadPoolExecutor for I/O-bound tasks.", "tags": "async, coroutine, lock, threading", "url": "https://stackoverflow.com/questions/3646552", "author": "user_469469", "date_published": "16-08-2019", "votes_or_stars": 1517, "relevance_label": "low", "difficulty_level": "beginner"}}, "kb:787e583a-49ef-4e75-a274-356cd6fee41d": {"content": "BUG: SQLAlchemy connection pool exhaustion under load\nProblem: Application hangs under concurrent load — all connections are checked out and pool is exhausted.\n\nFix/Discussion: Increase `pool_size` and `max_overflow`. Ensure sessions are always closed: use `contextmanager` or `with Session() as session:`. Set `pool_timeout` to fail fast. Enable `pool_pre_ping=True`. Monitor with `engine.pool.checkedout()`. In async: use `AsyncSession` with `async_scoped_session`.\nCode signature: status:closed", "file_path": "https://github.com/sqlalchemy/sqlalchemy/issues/106", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "787e583a-49ef-4e75-a274-356cd6fee41d", "source_type": "github_issue", "domain": "SQLAlchemy/Databases", "library": "sqlalchemy", "title": "BUG: SQLAlchemy connection pool exhaustion under load", "content": "Problem: Application hangs under concurrent load — all connections are checked out and pool is exhausted.\n\nFix/Discussion: Increase `pool_size` and `max_overflow`. Ensure sessions are always closed: use `contextmanager` or `with Session() as session:`. Set `pool_timeout` to fail fast. Enable `pool_pre_ping=True`. Monitor with `engine.pool.checkedout()`. In async: use `AsyncSession` with `async_scoped_session`.", "code_signature": "status:closed", "tags": "sqlite, session, postgresql, orm, connection-pool, sqlalchemy", "url": "https://github.com/sqlalchemy/sqlalchemy/issues/106", "author": "contributor_5078", "date_published": "19-12-2019", "votes_or_stars": 308, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:2c3d03ff-5c97-4e3a-8dc7-21560b6b413b": {"content": "BUG: SettingWithCopyWarning when modifying sliced DataFrame\nProblem: Modifying a slice of a DataFrame raises SettingWithCopyWarning and may not update original.\n\nFix/Discussion: Always use .loc or .copy() when slicing. Use `df.loc[mask, 'col'] = val` for assignment. Create explicit copies: `subset = df[mask].copy()`. In Pandas 2.0+, Copy-on-Write makes this behavior consistent — enable with `pd.options.mode.copy_on_write = True`.\nCode signature: status:closed", "file_path": "https://github.com/pandas/pandas/issues/3264", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "2c3d03ff-5c97-4e3a-8dc7-21560b6b413b", "source_type": "github_issue", "domain": "NumPy/Pandas", "library": "pandas", "title": "BUG: SettingWithCopyWarning when modifying sliced DataFrame", "content": "Problem: Modifying a slice of a DataFrame raises SettingWithCopyWarning and may not update original.\n\nFix/Discussion: Always use .loc or .copy() when slicing. Use `df.loc[mask, 'col'] = val` for assignment. Create explicit copies: `subset = df[mask].copy()`. In Pandas 2.0+, Copy-on-Write makes this behavior consistent — enable with `pd.options.mode.copy_on_write = True`.", "code_signature": "status:closed", "tags": "dataframe, array, reshape", "url": "https://github.com/pandas/pandas/issues/3264", "author": "contributor_6628", "date_published": "07-09-2023", "votes_or_stars": 12, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:658bb87a-695d-44bd-ae24-0113b6f57661": {"content": "How to read a large file line by line in Python without loading it all into memory?\n```python\nwith open('large.txt', 'r') as f:\n    for line in f:\n        process(line.strip())\n```\nThis reads one line at a time. For binary files: use `f.read(chunk_size)` in a loop. For CSV: use `pd.read_csv(file, chunksize=10000)`. Never use `f.readlines()` on large files.\nCode signature: Iterate over the file object directly — it yields lines lazily.", "file_path": "https://stackoverflow.com/questions/2737893", "function_name": "Iterate over the file object directly — it yields lines lazily.", "type": "stack_overflow", "metadata": {"record_id": "658bb87a-695d-44bd-ae24-0113b6f57661", "source_type": "stack_overflow", "domain": "OS/Subprocess/File I/O", "library": "shutil", "title": "How to read a large file line by line in Python without loading it all into memory?", "content": "```python\nwith open('large.txt', 'r') as f:\n    for line in f:\n        process(line.strip())\n```\nThis reads one line at a time. For binary files: use `f.read(chunk_size)` in a loop. For CSV: use `pd.read_csv(file, chunksize=10000)`. Never use `f.readlines()` on large files.", "code_signature": "Iterate over the file object directly — it yields lines lazily.", "tags": "pipe, stdin, process, os", "url": "https://stackoverflow.com/questions/2737893", "author": "user_994539", "date_published": "05-07-2019", "votes_or_stars": 2281, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:51c0c6d3-c8e0-4315-abeb-f1b91c4ee8a2": {"content": "How to use df.groupby in Pandas\n`Pandas.df.groupby` groups DataFrame rows by column values. Example usage:\n```python\ndf.groupby('category')['value'].mean()\n```\nThis is useful when aggregation, transformation, filtering by group. Performance tip: avoid apply() on large DataFrames; prefer agg().\nCode signature: df.groupby", "file_path": "https://docs.pandas.org/en/stable/df/groupby.html", "function_name": "df.groupby", "type": "documentation", "metadata": {"record_id": "51c0c6d3-c8e0-4315-abeb-f1b91c4ee8a2", "source_type": "documentation", "domain": "NumPy/Pandas", "library": "Pandas", "title": "How to use df.groupby in Pandas", "content": "`Pandas.df.groupby` groups DataFrame rows by column values. Example usage:\n```python\ndf.groupby('category')['value'].mean()\n```\nThis is useful when aggregation, transformation, filtering by group. Performance tip: avoid apply() on large DataFrames; prefer agg().", "code_signature": "df.groupby", "tags": "numpy, groupby, broadcasting, pandas, merge, loc", "url": "https://docs.pandas.org/en/stable/df/groupby.html", "author": "Official Docs", "date_published": "16-01-2023", "votes_or_stars": 824, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:fed54d13-d20f-40f6-9268-50d23ab3a222": {"content": "How to limit concurrency with asyncio?\n```python\nsem = asyncio.Semaphore(10)\nasync def limited(url):\n    async with sem:\n        return await fetch(url)\nresults = await asyncio.gather(*[limited(u) for u in urls])\n```\nThis prevents overwhelming APIs or databases with too many simultaneous connections.\nCode signature: Use asyncio.Semaphore to cap the number of concurrent coroutines.", "file_path": "https://stackoverflow.com/questions/2375453", "function_name": "Use asyncio.Semaphore to cap the number of concurrent coroutines.", "type": "stack_overflow", "metadata": {"record_id": "fed54d13-d20f-40f6-9268-50d23ab3a222", "source_type": "stack_overflow", "domain": "Asyncio/Threading", "library": "asyncio", "title": "How to limit concurrency with asyncio?", "content": "```python\nsem = asyncio.Semaphore(10)\nasync def limited(url):\n    async with sem:\n        return await fetch(url)\nresults = await asyncio.gather(*[limited(u) for u in urls])\n```\nThis prevents overwhelming APIs or databases with too many simultaneous connections.", "code_signature": "Use asyncio.Semaphore to cap the number of concurrent coroutines.", "tags": "race-condition, task, gather, async, executor", "url": "https://stackoverflow.com/questions/2375453", "author": "user_449589", "date_published": "10-09-2022", "votes_or_stars": 2449, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:58d4d137-ebe8-4616-b276-00e4bdf30258": {"content": "Why is my Pandas merge creating duplicate rows?\nDuplicate rows after merge usually mean the join key is not unique. Diagnose with `df.duplicated(subset=['key']).sum()`. Use `how='left'` carefully when right table has multiple matches. Set `validate='one_to_many'` or `'many_to_one'` to enforce cardinality. Consider deduplicating with `df.drop_duplicates(subset=['key'])` before merging.\nCode signature: Your merge key has duplicates in one or both DataFrames. Use validate='one_to_one' to catch this early.", "file_path": "https://stackoverflow.com/questions/2321324", "function_name": "Your merge key has duplicates in one or both DataFrames. Use validate='one_to_one' to catch this early.", "type": "stack_overflow", "metadata": {"record_id": "58d4d137-ebe8-4616-b276-00e4bdf30258", "source_type": "stack_overflow", "domain": "NumPy/Pandas", "library": "array", "title": "Why is my Pandas merge creating duplicate rows?", "content": "Duplicate rows after merge usually mean the join key is not unique. Diagnose with `df.duplicated(subset=['key']).sum()`. Use `how='left'` carefully when right table has multiple matches. Set `validate='one_to_many'` or `'many_to_one'` to enforce cardinality. Consider deduplicating with `df.drop_duplicates(subset=['key'])` before merging.", "code_signature": "Your merge key has duplicates in one or both DataFrames. Use validate='one_to_one' to catch this early.", "tags": "dataframe, dtype, vectorization, pandas", "url": "https://stackoverflow.com/questions/2321324", "author": "user_99814", "date_published": "24-09-2024", "votes_or_stars": 247, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:285270ed-4755-4cad-9d26-7591266f5c5a": {"content": "PERF: requests connection not reused between calls — new TCP connection each time\nProblem: Each requests.get() call opens a new TCP connection, causing high latency.\n\nFix/Discussion: requests.get/post etc. create a new Session per call. Fix: use a persistent Session: `session = requests.Session()` and reuse it. Sessions pool connections via urllib3. For thread-safety: create one Session per thread. For async: use httpx.AsyncClient or aiohttp.ClientSession for async connection pooling.\nCode signature: status:closed", "file_path": "https://github.com/requests/requests/issues/3605", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "285270ed-4755-4cad-9d26-7591266f5c5a", "source_type": "github_issue", "domain": "Requests/HTTP/APIs", "library": "requests", "title": "PERF: requests connection not reused between calls — new TCP connection each time", "content": "Problem: Each requests.get() call opens a new TCP connection, causing high latency.\n\nFix/Discussion: requests.get/post etc. create a new Session per call. Fix: use a persistent Session: `session = requests.Session()` and reuse it. Sessions pool connections via urllib3. For thread-safety: create one Session per thread. For async: use httpx.AsyncClient or aiohttp.ClientSession for async connection pooling.", "code_signature": "status:closed", "tags": "retry, oauth, rate-limiting, websocket, rest-api, authentication", "url": "https://github.com/requests/requests/issues/3605", "author": "contributor_1152", "date_published": "18-01-2024", "votes_or_stars": 347, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:158b45c2-54b9-4acf-b6e3-0fbbcd17e8fe": {"content": "How to use Path.glob in pathlib\n`pathlib.Path.glob` yields all paths matching the given glob pattern. Example usage:\n```python\npy_files = list(Path('src').glob('**/*.py'))\n```\nThis is useful when finding files recursively, batch file processing. Performance tip: iterate the generator lazily rather than converting to list for large directories.\nCode signature: Path.glob", "file_path": "https://docs.pathlib.org/en/stable/Path/glob.html", "function_name": "Path.glob", "type": "documentation", "metadata": {"record_id": "158b45c2-54b9-4acf-b6e3-0fbbcd17e8fe", "source_type": "documentation", "domain": "OS/Subprocess/File I/O", "library": "pathlib", "title": "How to use Path.glob in pathlib", "content": "`pathlib.Path.glob` yields all paths matching the given glob pattern. Example usage:\n```python\npy_files = list(Path('src').glob('**/*.py'))\n```\nThis is useful when finding files recursively, batch file processing. Performance tip: iterate the generator lazily rather than converting to list for large directories.", "code_signature": "Path.glob", "tags": "process, os, stderr, file-io, subprocess", "url": "https://docs.pathlib.org/en/stable/Path/glob.html", "author": "Official Docs", "date_published": "24-12-2021", "votes_or_stars": 3223, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:a606e766-68b6-489a-bc88-903e36ce0628": {"content": "requests — HTTP Session Management\nThe `requests.Session` function in requests creates a persistent HTTP session with connection pooling. It accepts `` as a parameter and returns Session object. Common use cases include making multiple requests to the same host efficiently. Note that always close or use Session as context manager to release connections.\nCode signature: requests.Session() -> Session object", "file_path": "https://docs.requests.org/en/stable/requests/Session.html", "function_name": "requests.Session() -> Session object", "type": "documentation", "metadata": {"record_id": "a606e766-68b6-489a-bc88-903e36ce0628", "source_type": "documentation", "domain": "Requests/HTTP/APIs", "library": "requests", "title": "requests — HTTP Session Management", "content": "The `requests.Session` function in requests creates a persistent HTTP session with connection pooling. It accepts `` as a parameter and returns Session object. Common use cases include making multiple requests to the same host efficiently. Note that always close or use Session as context manager to release connections.", "code_signature": "requests.Session() -> Session object", "tags": "retry, oauth, json", "url": "https://docs.requests.org/en/stable/requests/Session.html", "author": "Official Docs", "date_published": "04-04-2019", "votes_or_stars": 82, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:57c49432-d2a8-495c-93ab-1e02b80129aa": {"content": "How to read a large file line by line in Python without loading it all into memory?\n```python\nwith open('large.txt', 'r') as f:\n    for line in f:\n        process(line.strip())\n```\nThis reads one line at a time. For binary files: use `f.read(chunk_size)` in a loop. For CSV: use `pd.read_csv(file, chunksize=10000)`. Never use `f.readlines()` on large files.\nCode signature: Iterate over the file object directly — it yields lines lazily.", "file_path": "https://stackoverflow.com/questions/2737893", "function_name": "Iterate over the file object directly — it yields lines lazily.", "type": "stack_overflow", "metadata": {"record_id": "57c49432-d2a8-495c-93ab-1e02b80129aa", "source_type": "stack_overflow", "domain": "OS/Subprocess/File I/O", "library": "shutil", "title": "How to read a large file line by line in Python without loading it all into memory?", "content": "```python\nwith open('large.txt', 'r') as f:\n    for line in f:\n        process(line.strip())\n```\nThis reads one line at a time. For binary files: use `f.read(chunk_size)` in a loop. For CSV: use `pd.read_csv(file, chunksize=10000)`. Never use `f.readlines()` on large files.", "code_signature": "Iterate over the file object directly — it yields lines lazily.", "tags": "shutil, env-variable, stdin, stdout, pipe", "url": "https://stackoverflow.com/questions/2737893", "author": "user_994539", "date_published": "03-10-2020", "votes_or_stars": 2282, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:146f29e3-2884-4ad7-ab1f-c766dccfb757": {"content": "How to implement database connection pooling with SQLAlchemy?\n`create_engine(url, pool_size=10, max_overflow=20, pool_timeout=30, pool_recycle=1800)`. Use `pool_pre_ping=True` to detect stale connections. For async: use `AsyncEngine` with `create_async_engine()`. Monitor with `engine.pool.status()`. NullPool disables pooling for scripts/serverless environments.\nCode signature: create_engine() has built-in pooling. Configure pool_size and max_overflow.", "file_path": "https://stackoverflow.com/questions/7358113", "function_name": "create_engine() has built-in pooling. Configure pool_size and max_overflow.", "type": "stack_overflow", "metadata": {"record_id": "146f29e3-2884-4ad7-ab1f-c766dccfb757", "source_type": "stack_overflow", "domain": "SQLAlchemy/Databases", "library": "session", "title": "How to implement database connection pooling with SQLAlchemy?", "content": "`create_engine(url, pool_size=10, max_overflow=20, pool_timeout=30, pool_recycle=1800)`. Use `pool_pre_ping=True` to detect stale connections. For async: use `AsyncEngine` with `create_async_engine()`. Monitor with `engine.pool.status()`. NullPool disables pooling for scripts/serverless environments.", "code_signature": "create_engine() has built-in pooling. Configure pool_size and max_overflow.", "tags": "sqlalchemy, foreign-key, sqlite", "url": "https://stackoverflow.com/questions/7358113", "author": "user_12260", "date_published": "04-12-2023", "votes_or_stars": 265, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:a4426707-14ce-4bb7-9278-ccf7da35a8ab": {"content": "PERF: requests connection not reused between calls — new TCP connection each time\nProblem: Each requests.get() call opens a new TCP connection, causing high latency.\n\nFix/Discussion: requests.get/post etc. create a new Session per call. Fix: use a persistent Session: `session = requests.Session()` and reuse it. Sessions pool connections via urllib3. For thread-safety: create one Session per thread. For async: use httpx.AsyncClient or aiohttp.ClientSession for async connection pooling.\nCode signature: status:closed", "file_path": "https://github.com/requests/requests/issues/3605", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "a4426707-14ce-4bb7-9278-ccf7da35a8ab", "source_type": "github_issue", "domain": "Requests/HTTP/APIs", "library": "requests", "title": "PERF: requests connection not reused between calls — new TCP connection each time", "content": "Problem: Each requests.get() call opens a new TCP connection, causing high latency.\n\nFix/Discussion: requests.get/post etc. create a new Session per call. Fix: use a persistent Session: `session = requests.Session()` and reuse it. Sessions pool connections via urllib3. For thread-safety: create one Session per thread. For async: use httpx.AsyncClient or aiohttp.ClientSession for async connection pooling.", "code_signature": "status:closed", "tags": "rate-limiting, rest-api, http", "url": "https://github.com/requests/requests/issues/3605", "author": "contributor_1152", "date_published": "19-03-2021", "votes_or_stars": 379, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:8a1295b3-7a92-4d54-be01-897718316dcf": {"content": "BUG: Flask session not persisting between requests in tests\nProblem: Session data set in one request is not available in the next request during testing.\n\nFix/Discussion: Use Flask test client's session_transaction() context manager: `with client.session_transaction() as sess:\n    sess['user_id'] = 1`. Ensure SECRET_KEY is set. For testing: `app.config['TESTING'] = True; app.config['SECRET_KEY'] = 'test'`.\nCode signature: status:closed", "file_path": "https://github.com/flask/flask/issues/2141", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "8a1295b3-7a92-4d54-be01-897718316dcf", "source_type": "github_issue", "domain": "FastAPI/Flask/Django", "library": "flask", "title": "BUG: Flask session not persisting between requests in tests", "content": "Problem: Session data set in one request is not available in the next request during testing.\n\nFix/Discussion: Use Flask test client's session_transaction() context manager: `with client.session_transaction() as sess:\n    sess['user_id'] = 1`. Ensure SECRET_KEY is set. For testing: `app.config['TESTING'] = True; app.config['SECRET_KEY'] = 'test'`.", "code_signature": "status:closed", "tags": "blueprint, rest, pydantic", "url": "https://github.com/flask/flask/issues/2141", "author": "contributor_5020", "date_published": "27-04-2020", "votes_or_stars": 168, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:995c4d32-3600-49e9-8abe-0bdb3f315ea0": {"content": "NumPy — Linear Algebra\nThe `np.einsum` function in NumPy evaluates Einstein summation convention on operands. It accepts `subscripts, *operands` as a parameter and returns ndarray. Common use cases include matrix multiplication, dot products, tensor contractions. Note that prefer np.einsum over loops for large arrays.\nCode signature: np.einsum(subscripts, *operands) -> ndarray", "file_path": "https://docs.numpy.org/en/stable/np/einsum.html", "function_name": "np.einsum(subscripts, *operands) -> ndarray", "type": "documentation", "metadata": {"record_id": "995c4d32-3600-49e9-8abe-0bdb3f315ea0", "source_type": "documentation", "domain": "NumPy/Pandas", "library": "NumPy", "title": "NumPy — Linear Algebra", "content": "The `np.einsum` function in NumPy evaluates Einstein summation convention on operands. It accepts `subscripts, *operands` as a parameter and returns ndarray. Common use cases include matrix multiplication, dot products, tensor contractions. Note that prefer np.einsum over loops for large arrays.", "code_signature": "np.einsum(subscripts, *operands) -> ndarray", "tags": "vectorization, reshape, pandas, numpy, dtype", "url": "https://docs.numpy.org/en/stable/np/einsum.html", "author": "Official Docs", "date_published": "22-12-2020", "votes_or_stars": 1861, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:d76bfbaa-119f-40c1-bc0d-2624b3798042": {"content": "How to run asyncio code from synchronous Python?\n`asyncio.run(main())` creates an event loop, runs the coroutine, closes the loop. Never call asyncio.run() from inside a running event loop (use await instead). In Jupyter notebooks (which have a running loop), use `await coroutine()` directly or `nest_asyncio.apply()`. For mixing sync/async, use `loop.run_until_complete()`.\nCode signature: Use asyncio.run() in Python 3.7+ to execute a coroutine from sync code.", "file_path": "https://stackoverflow.com/questions/2140194", "function_name": "Use asyncio.run() in Python 3.7+ to execute a coroutine from sync code.", "type": "stack_overflow", "metadata": {"record_id": "d76bfbaa-119f-40c1-bc0d-2624b3798042", "source_type": "stack_overflow", "domain": "Asyncio/Threading", "library": "gather", "title": "How to run asyncio code from synchronous Python?", "content": "`asyncio.run(main())` creates an event loop, runs the coroutine, closes the loop. Never call asyncio.run() from inside a running event loop (use await instead). In Jupyter notebooks (which have a running loop), use `await coroutine()` directly or `nest_asyncio.apply()`. For mixing sync/async, use `loop.run_until_complete()`.", "code_signature": "Use asyncio.run() in Python 3.7+ to execute a coroutine from sync code.", "tags": "coroutine, lock, await", "url": "https://stackoverflow.com/questions/2140194", "author": "user_718011", "date_published": "15-08-2022", "votes_or_stars": 316, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:19c3dd6b-d242-4d8d-86a9-cd4b3460434a": {"content": "BUG: SQLAlchemy connection pool exhaustion under load\nProblem: Application hangs under concurrent load — all connections are checked out and pool is exhausted.\n\nFix/Discussion: Increase `pool_size` and `max_overflow`. Ensure sessions are always closed: use `contextmanager` or `with Session() as session:`. Set `pool_timeout` to fail fast. Enable `pool_pre_ping=True`. Monitor with `engine.pool.checkedout()`. In async: use `AsyncSession` with `async_scoped_session`.\nCode signature: status:closed", "file_path": "https://github.com/sqlalchemy/sqlalchemy/issues/106", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "19c3dd6b-d242-4d8d-86a9-cd4b3460434a", "source_type": "github_issue", "domain": "SQLAlchemy/Databases", "library": "sqlalchemy", "title": "BUG: SQLAlchemy connection pool exhaustion under load", "content": "Problem: Application hangs under concurrent load — all connections are checked out and pool is exhausted.\n\nFix/Discussion: Increase `pool_size` and `max_overflow`. Ensure sessions are always closed: use `contextmanager` or `with Session() as session:`. Set `pool_timeout` to fail fast. Enable `pool_pre_ping=True`. Monitor with `engine.pool.checkedout()`. In async: use `AsyncSession` with `async_scoped_session`.", "code_signature": "status:closed", "tags": "postgresql, transaction, sqlalchemy, session, connection-pool, relationship", "url": "https://github.com/sqlalchemy/sqlalchemy/issues/106", "author": "contributor_5078", "date_published": "08-08-2019", "votes_or_stars": 272, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:ee5aeeb0-3043-4a14-8d72-4f86a8a48a27": {"content": "PERF: requests connection not reused between calls — new TCP connection each time\nProblem: Each requests.get() call opens a new TCP connection, causing high latency.\n\nFix/Discussion: requests.get/post etc. create a new Session per call. Fix: use a persistent Session: `session = requests.Session()` and reuse it. Sessions pool connections via urllib3. For thread-safety: create one Session per thread. For async: use httpx.AsyncClient or aiohttp.ClientSession for async connection pooling.\nCode signature: status:closed", "file_path": "https://github.com/requests/requests/issues/3605", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "ee5aeeb0-3043-4a14-8d72-4f86a8a48a27", "source_type": "github_issue", "domain": "Requests/HTTP/APIs", "library": "requests", "title": "PERF: requests connection not reused between calls — new TCP connection each time", "content": "Problem: Each requests.get() call opens a new TCP connection, causing high latency.\n\nFix/Discussion: requests.get/post etc. create a new Session per call. Fix: use a persistent Session: `session = requests.Session()` and reuse it. Sessions pool connections via urllib3. For thread-safety: create one Session per thread. For async: use httpx.AsyncClient or aiohttp.ClientSession for async connection pooling.", "code_signature": "status:closed", "tags": "authentication, http, timeout", "url": "https://github.com/requests/requests/issues/3605", "author": "contributor_1152", "date_published": "22-06-2022", "votes_or_stars": 344, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:faf74b97-0215-4cf6-9c72-b905a0471e74": {"content": "NumPy broadcasting error: operands could not be broadcast with shapes\nNumPy broadcasts shapes right-to-left. Arrays (3,4) and (4,) are compatible. Arrays (3,4) and (3,) are not — reshape to (3,1). Use `arr.reshape(-1,1)` to add a dimension. Always print `a.shape, b.shape` before operations to debug. `np.expand_dims(arr, axis=1)` is cleaner than reshape for adding dimensions.\nCode signature: Shapes must be compatible: dimensions must match or one of them must be 1.", "file_path": "https://stackoverflow.com/questions/4860684", "function_name": "Shapes must be compatible: dimensions must match or one of them must be 1.", "type": "stack_overflow", "metadata": {"record_id": "faf74b97-0215-4cf6-9c72-b905a0471e74", "source_type": "stack_overflow", "domain": "NumPy/Pandas", "library": "numpy", "title": "NumPy broadcasting error: operands could not be broadcast with shapes", "content": "NumPy broadcasts shapes right-to-left. Arrays (3,4) and (4,) are compatible. Arrays (3,4) and (3,) are not — reshape to (3,1). Use `arr.reshape(-1,1)` to add a dimension. Always print `a.shape, b.shape` before operations to debug. `np.expand_dims(arr, axis=1)` is cleaner than reshape for adding dimensions.", "code_signature": "Shapes must be compatible: dimensions must match or one of them must be 1.", "tags": "reshape, pivot, groupby, vectorization, merge", "url": "https://stackoverflow.com/questions/4860684", "author": "user_627024", "date_published": "12-10-2022", "votes_or_stars": 1392, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:a3b8cf7f-ba84-48ae-bbb2-2ae97b661159": {"content": "How to use create_engine in SQLAlchemy\n`SQLAlchemy.create_engine` creates a database engine representing a connection pool. Example usage:\n```python\nengine = create_engine('postgresql://user:pass@host/db', echo=True)\n```\nThis is useful when establishing the central database connection for an application. Performance tip: set pool_size and max_overflow based on expected concurrency.\nCode signature: create_engine", "file_path": "https://docs.sqlalchemy.org/en/stable/create_engine.html", "function_name": "create_engine", "type": "documentation", "metadata": {"record_id": "a3b8cf7f-ba84-48ae-bbb2-2ae97b661159", "source_type": "documentation", "domain": "SQLAlchemy/Databases", "library": "SQLAlchemy", "title": "How to use create_engine in SQLAlchemy", "content": "`SQLAlchemy.create_engine` creates a database engine representing a connection pool. Example usage:\n```python\nengine = create_engine('postgresql://user:pass@host/db', echo=True)\n```\nThis is useful when establishing the central database connection for an application. Performance tip: set pool_size and max_overflow based on expected concurrency.", "code_signature": "create_engine", "tags": "transaction, alembic, relationship, orm, query", "url": "https://docs.sqlalchemy.org/en/stable/create_engine.html", "author": "Official Docs", "date_published": "15-05-2021", "votes_or_stars": 2072, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:0797f323-f7fd-4eca-bc17-29e57639c828": {"content": "How to apply a function to each row in Pandas without using iterrows?\n`iterrows()` is slow because it creates a Series per row. Prefer `df.apply(func, axis=1)` for row-wise operations, or better yet, vectorize using NumPy: `np.where(condition, a, b)`. For complex logic, `df.assign()` with vectorized expressions is cleanest. Benchmark: vectorized > apply > itertuples > iterrows.\nCode signature: Use df.apply(func, axis=1) or vectorized NumPy operations for best performance.", "file_path": "https://stackoverflow.com/questions/5446912", "function_name": "Use df.apply(func, axis=1) or vectorized NumPy operations for best performance.", "type": "stack_overflow", "metadata": {"record_id": "0797f323-f7fd-4eca-bc17-29e57639c828", "source_type": "stack_overflow", "domain": "NumPy/Pandas", "library": "dataframe", "title": "How to apply a function to each row in Pandas without using iterrows?", "content": "`iterrows()` is slow because it creates a Series per row. Prefer `df.apply(func, axis=1)` for row-wise operations, or better yet, vectorize using NumPy: `np.where(condition, a, b)`. For complex logic, `df.assign()` with vectorized expressions is cleanest. Benchmark: vectorized > apply > itertuples > iterrows.", "code_signature": "Use df.apply(func, axis=1) or vectorized NumPy operations for best performance.", "tags": "array, vectorization, dtype, iloc, numpy, pivot", "url": "https://stackoverflow.com/questions/5446912", "author": "user_563306", "date_published": "02-06-2023", "votes_or_stars": 2249, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:4173e4a7-04a1-4b59-bac1-5682e2b41397": {"content": "How to add retry logic to requests?\n```python\nfrom requests.adapters import HTTPAdapter\nfrom urllib3.util.retry import Retry\nretry = Retry(total=3, backoff_factor=1, status_forcelist=[500,502,503,504])\nadapter = HTTPAdapter(max_retries=retry)\nsession.mount('https://', adapter)\n```\nCode signature: Mount an HTTPAdapter with urllib3 Retry on the Session.", "file_path": "https://stackoverflow.com/questions/1669324", "function_name": "Mount an HTTPAdapter with urllib3 Retry on the Session.", "type": "stack_overflow", "metadata": {"record_id": "4173e4a7-04a1-4b59-bac1-5682e2b41397", "source_type": "stack_overflow", "domain": "Requests/HTTP/APIs", "library": "websocket", "title": "How to add retry logic to requests?", "content": "```python\nfrom requests.adapters import HTTPAdapter\nfrom urllib3.util.retry import Retry\nretry = Retry(total=3, backoff_factor=1, status_forcelist=[500,502,503,504])\nadapter = HTTPAdapter(max_retries=retry)\nsession.mount('https://', adapter)\n```", "code_signature": "Mount an HTTPAdapter with urllib3 Retry on the Session.", "tags": "retry, rest-api, headers", "url": "https://stackoverflow.com/questions/1669324", "author": "user_952590", "date_published": "31-03-2022", "votes_or_stars": 1496, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:c2dbf47c-5874-4c95-b199-85943cb32840": {"content": "asyncio — Concurrent Coroutines\nThe `asyncio.gather` function in asyncio runs multiple coroutines concurrently and collects results. It accepts `*coros, return_exceptions=False` as a parameter and returns list of results. Common use cases include parallel I/O operations such as multiple API calls. Note that set return_exceptions=True to prevent one failure from cancelling all tasks.\nCode signature: asyncio.gather(*coros, return_exceptions=False) -> list of results", "file_path": "https://docs.asyncio.org/en/stable/asyncio/gather.html", "function_name": "asyncio.gather(*coros, return_exceptions=False) -> list of results", "type": "documentation", "metadata": {"record_id": "c2dbf47c-5874-4c95-b199-85943cb32840", "source_type": "documentation", "domain": "Asyncio/Threading", "library": "asyncio", "title": "asyncio — Concurrent Coroutines", "content": "The `asyncio.gather` function in asyncio runs multiple coroutines concurrently and collects results. It accepts `*coros, return_exceptions=False` as a parameter and returns list of results. Common use cases include parallel I/O operations such as multiple API calls. Note that set return_exceptions=True to prevent one failure from cancelling all tasks.", "code_signature": "asyncio.gather(*coros, return_exceptions=False) -> list of results", "tags": "gather, asyncio, task, race-condition, semaphore, threading", "url": "https://docs.asyncio.org/en/stable/asyncio/gather.html", "author": "Official Docs", "date_published": "13-03-2019", "votes_or_stars": 485, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:429ad8e5-46cd-4fc7-bfda-fc112ac372a1": {"content": "How to add retry logic to requests?\n```python\nfrom requests.adapters import HTTPAdapter\nfrom urllib3.util.retry import Retry\nretry = Retry(total=3, backoff_factor=1, status_forcelist=[500,502,503,504])\nadapter = HTTPAdapter(max_retries=retry)\nsession.mount('https://', adapter)\n```\nCode signature: Mount an HTTPAdapter with urllib3 Retry on the Session.", "file_path": "https://stackoverflow.com/questions/1669324", "function_name": "Mount an HTTPAdapter with urllib3 Retry on the Session.", "type": "stack_overflow", "metadata": {"record_id": "429ad8e5-46cd-4fc7-bfda-fc112ac372a1", "source_type": "stack_overflow", "domain": "Requests/HTTP/APIs", "library": "websocket", "title": "How to add retry logic to requests?", "content": "```python\nfrom requests.adapters import HTTPAdapter\nfrom urllib3.util.retry import Retry\nretry = Retry(total=3, backoff_factor=1, status_forcelist=[500,502,503,504])\nadapter = HTTPAdapter(max_retries=retry)\nsession.mount('https://', adapter)\n```", "code_signature": "Mount an HTTPAdapter with urllib3 Retry on the Session.", "tags": "aiohttp, headers, websocket", "url": "https://stackoverflow.com/questions/1669324", "author": "user_952590", "date_published": "09-06-2024", "votes_or_stars": 1487, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:b9d67217-ef6c-4b30-8ceb-135f64834014": {"content": "SQLAlchemy — ORM Session Query\nThe `session.query` function in SQLAlchemy constructs a SELECT query against mapped entities. It accepts `*entities` as a parameter and returns Query object. Common use cases include retrieving ORM-mapped objects from the database. Note that prefer session.execute(select(...)) in SQLAlchemy 2.0+.\nCode signature: session.query(*entities) -> Query object", "file_path": "https://docs.sqlalchemy.org/en/stable/session/query.html", "function_name": "session.query(*entities) -> Query object", "type": "documentation", "metadata": {"record_id": "b9d67217-ef6c-4b30-8ceb-135f64834014", "source_type": "documentation", "domain": "SQLAlchemy/Databases", "library": "SQLAlchemy", "title": "SQLAlchemy — ORM Session Query", "content": "The `session.query` function in SQLAlchemy constructs a SELECT query against mapped entities. It accepts `*entities` as a parameter and returns Query object. Common use cases include retrieving ORM-mapped objects from the database. Note that prefer session.execute(select(...)) in SQLAlchemy 2.0+.", "code_signature": "session.query(*entities) -> Query object", "tags": "relationship, connection-pool, postgresql", "url": "https://docs.sqlalchemy.org/en/stable/session/query.html", "author": "Official Docs", "date_published": "10-06-2022", "votes_or_stars": 3058, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:dd6814d3-91d3-43b8-aa1c-9926ec9b918b": {"content": "How to use Path.glob in pathlib\n`pathlib.Path.glob` yields all paths matching the given glob pattern. Example usage:\n```python\npy_files = list(Path('src').glob('**/*.py'))\n```\nThis is useful when finding files recursively, batch file processing. Performance tip: iterate the generator lazily rather than converting to list for large directories.\nCode signature: Path.glob", "file_path": "https://docs.pathlib.org/en/stable/Path/glob.html", "function_name": "Path.glob", "type": "documentation", "metadata": {"record_id": "dd6814d3-91d3-43b8-aa1c-9926ec9b918b", "source_type": "documentation", "domain": "OS/Subprocess/File I/O", "library": "pathlib", "title": "How to use Path.glob in pathlib", "content": "`pathlib.Path.glob` yields all paths matching the given glob pattern. Example usage:\n```python\npy_files = list(Path('src').glob('**/*.py'))\n```\nThis is useful when finding files recursively, batch file processing. Performance tip: iterate the generator lazily rather than converting to list for large directories.", "code_signature": "Path.glob", "tags": "file-io, os, pathlib", "url": "https://docs.pathlib.org/en/stable/Path/glob.html", "author": "Official Docs", "date_published": "16-08-2024", "votes_or_stars": 3182, "relevance_label": "low", "difficulty_level": "advanced"}}, "kb:be54710f-1a45-42fa-9edb-f17ac5a3cf44": {"content": "PERF: requests connection not reused between calls — new TCP connection each time\nProblem: Each requests.get() call opens a new TCP connection, causing high latency.\n\nFix/Discussion: requests.get/post etc. create a new Session per call. Fix: use a persistent Session: `session = requests.Session()` and reuse it. Sessions pool connections via urllib3. For thread-safety: create one Session per thread. For async: use httpx.AsyncClient or aiohttp.ClientSession for async connection pooling.\nCode signature: status:closed", "file_path": "https://github.com/requests/requests/issues/3605", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "be54710f-1a45-42fa-9edb-f17ac5a3cf44", "source_type": "github_issue", "domain": "Requests/HTTP/APIs", "library": "requests", "title": "PERF: requests connection not reused between calls — new TCP connection each time", "content": "Problem: Each requests.get() call opens a new TCP connection, causing high latency.\n\nFix/Discussion: requests.get/post etc. create a new Session per call. Fix: use a persistent Session: `session = requests.Session()` and reuse it. Sessions pool connections via urllib3. For thread-safety: create one Session per thread. For async: use httpx.AsyncClient or aiohttp.ClientSession for async connection pooling.", "code_signature": "status:closed", "tags": "oauth, authentication, aiohttp, retry, rate-limiting, http", "url": "https://github.com/requests/requests/issues/3605", "author": "contributor_1152", "date_published": "08-10-2019", "votes_or_stars": 382, "relevance_label": "medium", "difficulty_level": "intermediate"}}, "kb:baf7083c-27e4-4c2f-9584-bd36f2b312e4": {"content": "Django N+1 query problem: how to fix?\nN+1 occurs when accessing related objects in a loop. Diagnose with `django-debug-toolbar` or `connection.queries`. Fix ForeignKey: `User.objects.select_related('profile').all()`. Fix M2M: `Post.objects.prefetch_related('tags').all()`. Use `only()` to fetch specific fields and reduce data transfer.\nCode signature: Use select_related() for ForeignKey and prefetch_related() for ManyToMany relationships.", "file_path": "https://stackoverflow.com/questions/8106470", "function_name": "Use select_related() for ForeignKey and prefetch_related() for ManyToMany relationships.", "type": "stack_overflow", "metadata": {"record_id": "baf7083c-27e4-4c2f-9584-bd36f2b312e4", "source_type": "stack_overflow", "domain": "FastAPI/Flask/Django", "library": "request", "title": "Django N+1 query problem: how to fix?", "content": "N+1 occurs when accessing related objects in a loop. Diagnose with `django-debug-toolbar` or `connection.queries`. Fix ForeignKey: `User.objects.select_related('profile').all()`. Fix M2M: `Post.objects.prefetch_related('tags').all()`. Use `only()` to fetch specific fields and reduce data transfer.", "code_signature": "Use select_related() for ForeignKey and prefetch_related() for ManyToMany relationships.", "tags": "pydantic, routing, rest, middleware, flask, orm", "url": "https://stackoverflow.com/questions/8106470", "author": "user_441071", "date_published": "13-04-2022", "votes_or_stars": 1786, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:fdd3d1c7-1b13-40d6-abf5-bfdcd5f4edab": {"content": "SQLAlchemy — ORM Session Query\nThe `session.query` function in SQLAlchemy constructs a SELECT query against mapped entities. It accepts `*entities` as a parameter and returns Query object. Common use cases include retrieving ORM-mapped objects from the database. Note that prefer session.execute(select(...)) in SQLAlchemy 2.0+.\nCode signature: session.query(*entities) -> Query object", "file_path": "https://docs.sqlalchemy.org/en/stable/session/query.html", "function_name": "session.query(*entities) -> Query object", "type": "documentation", "metadata": {"record_id": "fdd3d1c7-1b13-40d6-abf5-bfdcd5f4edab", "source_type": "documentation", "domain": "SQLAlchemy/Databases", "library": "SQLAlchemy", "title": "SQLAlchemy — ORM Session Query", "content": "The `session.query` function in SQLAlchemy constructs a SELECT query against mapped entities. It accepts `*entities` as a parameter and returns Query object. Common use cases include retrieving ORM-mapped objects from the database. Note that prefer session.execute(select(...)) in SQLAlchemy 2.0+.", "code_signature": "session.query(*entities) -> Query object", "tags": "foreign-key, alembic, sqlite, session", "url": "https://docs.sqlalchemy.org/en/stable/session/query.html", "author": "Official Docs", "date_published": "04-09-2020", "votes_or_stars": 3027, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:ed12241e-3861-43c3-b8f3-39e7f3edeb91": {"content": "PERF: requests connection not reused between calls — new TCP connection each time\nProblem: Each requests.get() call opens a new TCP connection, causing high latency.\n\nFix/Discussion: requests.get/post etc. create a new Session per call. Fix: use a persistent Session: `session = requests.Session()` and reuse it. Sessions pool connections via urllib3. For thread-safety: create one Session per thread. For async: use httpx.AsyncClient or aiohttp.ClientSession for async connection pooling.\nCode signature: status:closed", "file_path": "https://github.com/requests/requests/issues/3605", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "ed12241e-3861-43c3-b8f3-39e7f3edeb91", "source_type": "github_issue", "domain": "Requests/HTTP/APIs", "library": "requests", "title": "PERF: requests connection not reused between calls — new TCP connection each time", "content": "Problem: Each requests.get() call opens a new TCP connection, causing high latency.\n\nFix/Discussion: requests.get/post etc. create a new Session per call. Fix: use a persistent Session: `session = requests.Session()` and reuse it. Sessions pool connections via urllib3. For thread-safety: create one Session per thread. For async: use httpx.AsyncClient or aiohttp.ClientSession for async connection pooling.", "code_signature": "status:closed", "tags": "timeout, websocket, json, requests, retry", "url": "https://github.com/requests/requests/issues/3605", "author": "contributor_1152", "date_published": "01-07-2020", "votes_or_stars": 369, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:e3de5ae4-16fe-400c-a360-5f11f0aee558": {"content": "How to limit concurrency with asyncio?\n```python\nsem = asyncio.Semaphore(10)\nasync def limited(url):\n    async with sem:\n        return await fetch(url)\nresults = await asyncio.gather(*[limited(u) for u in urls])\n```\nThis prevents overwhelming APIs or databases with too many simultaneous connections.\nCode signature: Use asyncio.Semaphore to cap the number of concurrent coroutines.", "file_path": "https://stackoverflow.com/questions/2375453", "function_name": "Use asyncio.Semaphore to cap the number of concurrent coroutines.", "type": "stack_overflow", "metadata": {"record_id": "e3de5ae4-16fe-400c-a360-5f11f0aee558", "source_type": "stack_overflow", "domain": "Asyncio/Threading", "library": "asyncio", "title": "How to limit concurrency with asyncio?", "content": "```python\nsem = asyncio.Semaphore(10)\nasync def limited(url):\n    async with sem:\n        return await fetch(url)\nresults = await asyncio.gather(*[limited(u) for u in urls])\n```\nThis prevents overwhelming APIs or databases with too many simultaneous connections.", "code_signature": "Use asyncio.Semaphore to cap the number of concurrent coroutines.", "tags": "await, coroutine, lock, executor, threading, semaphore", "url": "https://stackoverflow.com/questions/2375453", "author": "user_449589", "date_published": "18-06-2020", "votes_or_stars": 2452, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:99b78854-d151-4d3f-a1e1-d1e8e229817f": {"content": "How to limit concurrency with asyncio?\n```python\nsem = asyncio.Semaphore(10)\nasync def limited(url):\n    async with sem:\n        return await fetch(url)\nresults = await asyncio.gather(*[limited(u) for u in urls])\n```\nThis prevents overwhelming APIs or databases with too many simultaneous connections.\nCode signature: Use asyncio.Semaphore to cap the number of concurrent coroutines.", "file_path": "https://stackoverflow.com/questions/2375453", "function_name": "Use asyncio.Semaphore to cap the number of concurrent coroutines.", "type": "stack_overflow", "metadata": {"record_id": "99b78854-d151-4d3f-a1e1-d1e8e229817f", "source_type": "stack_overflow", "domain": "Asyncio/Threading", "library": "asyncio", "title": "How to limit concurrency with asyncio?", "content": "```python\nsem = asyncio.Semaphore(10)\nasync def limited(url):\n    async with sem:\n        return await fetch(url)\nresults = await asyncio.gather(*[limited(u) for u in urls])\n```\nThis prevents overwhelming APIs or databases with too many simultaneous connections.", "code_signature": "Use asyncio.Semaphore to cap the number of concurrent coroutines.", "tags": "threading, semaphore, event-loop, executor", "url": "https://stackoverflow.com/questions/2375453", "author": "user_449589", "date_published": "22-01-2022", "votes_or_stars": 2416, "relevance_label": "high", "difficulty_level": "intermediate"}}, "kb:1ec71ab7-6dda-4d62-acb4-22f596db2ef4": {"content": "BUG: SQLAlchemy connection pool exhaustion under load\nProblem: Application hangs under concurrent load — all connections are checked out and pool is exhausted.\n\nFix/Discussion: Increase `pool_size` and `max_overflow`. Ensure sessions are always closed: use `contextmanager` or `with Session() as session:`. Set `pool_timeout` to fail fast. Enable `pool_pre_ping=True`. Monitor with `engine.pool.checkedout()`. In async: use `AsyncSession` with `async_scoped_session`.\nCode signature: status:closed", "file_path": "https://github.com/sqlalchemy/sqlalchemy/issues/106", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "1ec71ab7-6dda-4d62-acb4-22f596db2ef4", "source_type": "github_issue", "domain": "SQLAlchemy/Databases", "library": "sqlalchemy", "title": "BUG: SQLAlchemy connection pool exhaustion under load", "content": "Problem: Application hangs under concurrent load — all connections are checked out and pool is exhausted.\n\nFix/Discussion: Increase `pool_size` and `max_overflow`. Ensure sessions are always closed: use `contextmanager` or `with Session() as session:`. Set `pool_timeout` to fail fast. Enable `pool_pre_ping=True`. Monitor with `engine.pool.checkedout()`. In async: use `AsyncSession` with `async_scoped_session`.", "code_signature": "status:closed", "tags": "relationship, orm, session", "url": "https://github.com/sqlalchemy/sqlalchemy/issues/106", "author": "contributor_5078", "date_published": "17-02-2021", "votes_or_stars": 308, "relevance_label": "high", "difficulty_level": "beginner"}}, "kb:712adcf7-9e3e-4b7b-86c4-3b19905a29af": {"content": "How do I handle CORS in FastAPI?\n```python\nfrom fastapi.middleware.cors import CORSMiddleware\napp.add_middleware(CORSMiddleware, allow_origins=['*'], allow_methods=['*'], allow_headers=['*'])\n```\nIn production, replace `'*'` with your actual frontend domain. For credentials (cookies), set `allow_credentials=True` and specify exact origins.\nCode signature: Add CORSMiddleware from starlette to your FastAPI app.", "file_path": "https://stackoverflow.com/questions/1527021", "function_name": "Add CORSMiddleware from starlette to your FastAPI app.", "type": "stack_overflow", "metadata": {"record_id": "712adcf7-9e3e-4b7b-86c4-3b19905a29af", "source_type": "stack_overflow", "domain": "FastAPI/Flask/Django", "library": "flask", "title": "How do I handle CORS in FastAPI?", "content": "```python\nfrom fastapi.middleware.cors import CORSMiddleware\napp.add_middleware(CORSMiddleware, allow_origins=['*'], allow_methods=['*'], allow_headers=['*'])\n```\nIn production, replace `'*'` with your actual frontend domain. For credentials (cookies), set `allow_credentials=True` and specify exact origins.", "code_signature": "Add CORSMiddleware from starlette to your FastAPI app.", "tags": "blueprint, routing, request, dependency-injection", "url": "https://stackoverflow.com/questions/1527021", "author": "user_911393", "date_published": "23-09-2019", "votes_or_stars": 961, "relevance_label": "medium", "difficulty_level": "advanced"}}, "kb:70cde696-bc47-4301-9fd1-bf65739ff0a9": {"content": "How to watch a directory for file changes in Python?\n```python\nfrom watchdog.observers import Observer\nfrom watchdog.events import FileSystemEventHandler\nhandler = FileSystemEventHandler()\nobserver = Observer()\nobserver.schedule(handler, path='.', recursive=True)\nobserver.start()\n```\nFor simple polling: `os.stat(file).st_mtime`. Linux-native: `inotify`. macOS-native: `FSEvents`.\nCode signature: Use the watchdog library for cross-platform directory monitoring.", "file_path": "https://stackoverflow.com/questions/5213115", "function_name": "Use the watchdog library for cross-platform directory monitoring.", "type": "stack_overflow", "metadata": {"record_id": "70cde696-bc47-4301-9fd1-bf65739ff0a9", "source_type": "stack_overflow", "domain": "OS/Subprocess/File I/O", "library": "env-variable", "title": "How to watch a directory for file changes in Python?", "content": "```python\nfrom watchdog.observers import Observer\nfrom watchdog.events import FileSystemEventHandler\nhandler = FileSystemEventHandler()\nobserver = Observer()\nobserver.schedule(handler, path='.', recursive=True)\nobserver.start()\n```\nFor simple polling: `os.stat(file).st_mtime`. Linux-native: `inotify`. macOS-native: `FSEvents`.", "code_signature": "Use the watchdog library for cross-platform directory monitoring.", "tags": "tempfile, env-variable, stdout, glob, os", "url": "https://stackoverflow.com/questions/5213115", "author": "user_959314", "date_published": "12-09-2020", "votes_or_stars": 2086, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:88c0659d-abe5-47fe-946f-4747eb34576f": {"content": "How to use df.groupby in Pandas\n`Pandas.df.groupby` groups DataFrame rows by column values. Example usage:\n```python\ndf.groupby('category')['value'].mean()\n```\nThis is useful when aggregation, transformation, filtering by group. Performance tip: avoid apply() on large DataFrames; prefer agg().\nCode signature: df.groupby", "file_path": "https://docs.pandas.org/en/stable/df/groupby.html", "function_name": "df.groupby", "type": "documentation", "metadata": {"record_id": "88c0659d-abe5-47fe-946f-4747eb34576f", "source_type": "documentation", "domain": "NumPy/Pandas", "library": "Pandas", "title": "How to use df.groupby in Pandas", "content": "`Pandas.df.groupby` groups DataFrame rows by column values. Example usage:\n```python\ndf.groupby('category')['value'].mean()\n```\nThis is useful when aggregation, transformation, filtering by group. Performance tip: avoid apply() on large DataFrames; prefer agg().", "code_signature": "df.groupby", "tags": "vectorization, array, reshape, broadcasting, iloc, pandas", "url": "https://docs.pandas.org/en/stable/df/groupby.html", "author": "Official Docs", "date_published": "04-02-2020", "votes_or_stars": 799, "relevance_label": "low", "difficulty_level": "beginner"}}, "kb:3eb7c4cb-c16b-4642-b2cc-a1989eaf4d3a": {"content": "Flask: how to return JSON responses properly?\n`return jsonify({'key': 'value'})` sets Content-Type to application/json. Flask 1.0+ automatically converts returned dicts to JSON responses. For custom status codes: `return jsonify(data), 201`. For errors: use `abort(404)` or return `({'error': 'Not found'}, 404)`.\nCode signature: Use jsonify() or return a dict directly (Flask 1.0+).", "file_path": "https://stackoverflow.com/questions/8930103", "function_name": "Use jsonify() or return a dict directly (Flask 1.0+).", "type": "stack_overflow", "metadata": {"record_id": "3eb7c4cb-c16b-4642-b2cc-a1989eaf4d3a", "source_type": "stack_overflow", "domain": "FastAPI/Flask/Django", "library": "django", "title": "Flask: how to return JSON responses properly?", "content": "`return jsonify({'key': 'value'})` sets Content-Type to application/json. Flask 1.0+ automatically converts returned dicts to JSON responses. For custom status codes: `return jsonify(data), 201`. For errors: use `abort(404)` or return `({'error': 'Not found'}, 404)`.", "code_signature": "Use jsonify() or return a dict directly (Flask 1.0+).", "tags": "django, response, rest, template", "url": "https://stackoverflow.com/questions/8930103", "author": "user_264801", "date_published": "02-10-2020", "votes_or_stars": 2197, "relevance_label": "high", "difficulty_level": "advanced"}}, "kb:2ab08505-1c66-4ef0-9274-159a3f9c8ebc": {"content": "How to use httpx.AsyncClient in httpx\n`httpx.httpx.AsyncClient` provides an async HTTP client for non-blocking requests. Example usage:\n```python\nasync with httpx.AsyncClient() as client:\n    r = await client.get(url)\n    data = r.json()\n```\nThis is useful when making concurrent HTTP requests in async applications. Performance tip: set timeout=httpx.Timeout(10.0) to prevent hanging requests.\nCode signature: httpx.AsyncClient", "file_path": "https://docs.httpx.org/en/stable/httpx/AsyncClient.html", "function_name": "httpx.AsyncClient", "type": "documentation", "metadata": {"record_id": "2ab08505-1c66-4ef0-9274-159a3f9c8ebc", "source_type": "documentation", "domain": "Requests/HTTP/APIs", "library": "httpx", "title": "How to use httpx.AsyncClient in httpx", "content": "`httpx.httpx.AsyncClient` provides an async HTTP client for non-blocking requests. Example usage:\n```python\nasync with httpx.AsyncClient() as client:\n    r = await client.get(url)\n    data = r.json()\n```\nThis is useful when making concurrent HTTP requests in async applications. Performance tip: set timeout=httpx.Timeout(10.0) to prevent hanging requests.", "code_signature": "httpx.AsyncClient", "tags": "retry, aiohttp, websocket, httpx, rest-api", "url": "https://docs.httpx.org/en/stable/httpx/AsyncClient.html", "author": "Official Docs", "date_published": "19-09-2024", "votes_or_stars": 4208, "relevance_label": "medium", "difficulty_level": "beginner"}}, "kb:d8a38a6e-d974-4a9e-86f3-722e322a5efd": {"content": "PERF: Django template rendering bottleneck in production\nProblem: Template rendering takes 500ms+ for complex pages with many database queries.\n\nFix/Discussion: Primary cause is N+1 queries in template. Solutions: (1) Use select_related/prefetch_related. (2) Cache rendered templates with `cache` template tag. (3) Use `django-cachalot` for query caching. (4) Move heavy computation to views, pass pre-computed context. (5) Consider server-side caching with Redis.\nCode signature: status:closed", "file_path": "https://github.com/django/django/issues/3243", "function_name": "status:closed", "type": "github_issue", "metadata": {"record_id": "d8a38a6e-d974-4a9e-86f3-722e322a5efd", "source_type": "github_issue", "domain": "FastAPI/Flask/Django", "library": "django", "title": "PERF: Django template rendering bottleneck in production", "content": "Problem: Template rendering takes 500ms+ for complex pages with many database queries.\n\nFix/Discussion: Primary cause is N+1 queries in template. Solutions: (1) Use select_related/prefetch_related. (2) Cache rendered templates with `cache` template tag. (3) Use `django-cachalot` for query caching. (4) Move heavy computation to views, pass pre-computed context. (5) Consider server-side caching with Redis.", "code_signature": "status:closed", "tags": "template, blueprint, django", "url": "https://github.com/django/django/issues/3243", "author": "contributor_6988", "date_published": "26-10-2019", "votes_or_stars": 79, "relevance_label": "high", "difficulty_level": "beginner"}}}}